{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How ChatGPT Works Part 1: Supervised Fine-Tuning\n",
    "\n",
    "Supervised Fine-Tuning (SFT) is a popular technique used to train a language model by fine-tuning a pre-trained large language model on a specific task or domain. In SFT, the pre-trained model is fine-tuned by providing it with labeled data, which enables the model to learn the nuances of the task or domain it is being trained for. SFT has been widely used in Natural Language Processing (NLP) for various applications such as sentiment analysis, machine translation, and text classification.\n",
    "\n",
    "> We will fine-tune the state-of-the-art language model, GPT-2, on a language modelling task using a chatbot text dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### Run the following cell to download the necessary files for this lesson { display-mode: \"form\" } \n",
    "#@markdown Don't worry about what's in this collapsed cell\n",
    "\n",
    "!pip install -q transformers\n",
    "print('Downloading raw_data.json...')\n",
    "!wget https://s3-eu-west-1.amazonaws.com/aicore-portal-public-prod-307050600709/lesson_files/ee4b4212-a54e-46fb-8d8e-07d0a26a36d5/raw_data.json -q -O raw_data.json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Prompts\n",
    "\n",
    "To fine-tune our language model to behave like a chatbot, we'll need a labelled dataset of human-chatbot dialogue.\n",
    "\n",
    "Submit some prompts [here](https://docs.google.com/forms/d/e/1FAIpQLSfSBFzS4yrdUwy3DjEj2kskTc1JXk-T47TbmK8TaEgSt4fkcA/viewform?usp=sf_link).\n",
    "\n",
    "## Collecting Ideal Prompt Responses\n",
    "\n",
    "These days, there are more sophisticated ways to manage data collection for AI applications than just using a basic form.\n",
    "\n",
    "> OpenAI use [Scale AI](https://scale.com) to manage their labelling workforce and data.\n",
    "\n",
    "Submit your email [here](https://docs.google.com/forms/d/e/1FAIpQLScsMfW1Fh0bwget7cZKCzm6TQ-1c0AsvQFHtBain2l1mjnIcQ/viewform?usp=sf_link) so I can allow you to join my demo labelling workforce and you can access the Scale labelling platform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "class SFTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Supervised Fine-Tuning Dataset\n",
    "\n",
    "    Returns:\n",
    "        prompt: str\n",
    "        response: str\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        with open(\"raw_data.json\") as f:\n",
    "            self.data = json.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Defines the length of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Defines how to get a sample from the dataset by indexing it.\n",
    "\n",
    "        Returns:\n",
    "            prompt: str\n",
    "            response: str\n",
    "        \"\"\"\n",
    "        return self.data[idx][\"prompt\"], self.data[idx][\"response\"]\n",
    "    \n",
    "\n",
    "dataset = SFTDataset()\n",
    "print(dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, GPT-3.5 is not currently available to download. \n",
    "The model parameters are closed source and owned by OpenAI.\n",
    "So instead, we'll work with GPT-2 - a smaller predecessor of the model trained on the same task of language modelling. \n",
    "\n",
    "[GPT-2](https://huggingface.co/gpt2), and the original [GPT](https://huggingface.co/openai-gpt) model are available through HuggingFace.\n",
    "\n",
    "Here are the key differences:\n",
    "- Size:\n",
    "    - GPT has 117M parameters\n",
    "    - GPT-2 has 1.5B parameters\n",
    "    - GPT-3 has 175B parameters (800GB storage required)\n",
    "    \n",
    "- Training data size: \n",
    "    <!-- - GPT: -->\n",
    "    - GPT-2: 40GB (8M webpages)\n",
    "    - GPT-3: 45TB\n",
    "<!-- - Training data variety:\n",
    "The GPT models were trained on increasingly larger and more diverse datasets, with GPT-3 trained on a massive corpus of web pages, books, and other text sources. -->\n",
    "- Task performance: GPT-3 has demonstrated better performance on a wide range of natural language tasks, including question answering, language translation, and natural language generation. It has also shown an ability to perform some common sense reasoning and to generate coherent and informative responses even to complex prompts.\n",
    "\n",
    "- Speed and efficiency: Because of its size, GPT-3 is slower and more resource-intensive to run than GPT-2 or the original GPT. However, it can generate high-quality outputs with fewer prompts or examples.\n",
    "\n",
    "- Release date: The original GPT was released in 2018, GPT-2 in 2019, and GPT-3 in 2020. Each new model represents a significant advance in natural language processing capabilities.\n",
    "\n",
    "Let's load in GPT-2 and make sure we're comfortable with how it can be used to generate new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", config=configuration) # Load the tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # Load the model\n",
    "\n",
    "# generate a sequence of tokens using the model's forward method\n",
    "prompt = \"Hello, I am a language model.\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remind yourself of the architecture of the insides of a GPT model by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.modules)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement a function to initiate a back and forth chat with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    \"\"\"Chat with the model.\"\"\"\n",
    "    prompt = \"\"\n",
    "    while True:\n",
    "        # GET USER INPUT\n",
    "        next_input = \"You: \" + input(\"You: \") + \"\\nBot: \"\n",
    "        print(next_input)\n",
    "        prompt += next_input\n",
    "\n",
    "        # GENERATE A SEQUENCE OF TOKENS USING THE MODEL'S FORWARD METHOD\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        output = model.generate(input_ids, max_length=100, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)\n",
    "\n",
    "        # PRINT THE RESPONSE AND UPDATE THE PROMPT\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(response)\n",
    "        prompt += response\n",
    "\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# print(model.modules)\n",
    "# scsdc\n",
    "\n",
    "def train(epochs=10):\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = SFTDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.95)) # as used in the InstructGPT paper\n",
    "\n",
    "    # Set up logging\n",
    "    writer = SummaryWriter() # for logging our loss to TensorBoard\n",
    "    batch_idx = 0 # for setting the x-axis of our TensorBoard plots (loss vs. batch index)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        for batch in tqdm(dataloader):\n",
    "            # Get the data\n",
    "            prompt, response = batch\n",
    "            prompt = prompt[0]\n",
    "            response = response[0]\n",
    "\n",
    "            # Encode the data\n",
    "            entire_text = prompt + response\n",
    "            context_dict = tokenizer(\n",
    "                '<|startoftext|>' + entire_text + '<|endoftext|>',\n",
    "                                    #    truncation=True, \n",
    "                                    #    max_length=max_length, \n",
    "                                    #    padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "            input_ids = torch.tensor(context_dict.input_ids)\n",
    "            labels = torch.tensor(context_dict.input_ids)\n",
    "            attention_mask = torch.tensor(context_dict.attention_mask)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "\n",
    "            # logits = outputs.logits\n",
    "\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Log the loss\n",
    "            # print(f\"Loss: {loss.item()}\", batch_idx)\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "1. Look at the attention parameters inside the model\n",
    "1. Write a function to generate from the sequence by passing in the sequence so far repeatedly, each time appending the new generated token to the end of the input sequence\n",
    "1. Split the dataset into a train and test split\n",
    "1. Adapt your chat function to print the response of both a untuned model and your trained and saved model\n",
    "1. Get a pretrained toxicity detection model and firstly apply it to the sequence of tokens input by the user, then apply it to the tokens generated by the model\n",
    "1. Your first use case of seeing why `loss.backward()` doesn't overwrite the `.grad` attribute of model parameters, but instead adds to them: Because so many values need to be stored and computed per forward pass, it can be tough to fit many batches into your machine at once for some models. Implement _proxy batching_ by allowing gradients to accumulate for `batch_size` sequential forward passes \n",
    "1. Write a function to count the number of parameters in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
