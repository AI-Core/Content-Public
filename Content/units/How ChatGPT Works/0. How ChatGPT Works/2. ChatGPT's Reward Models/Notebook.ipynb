{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How ChatGPT Works Part 2: The Reward Model\n",
    "\n",
    "> Given a prompt and a response, the reward model is a model trained to predict a scalar value representing how good a response is.\n",
    "\n",
    "![](./images/Reward%20Model.png)\n",
    "\n",
    "\n",
    "> The point of the reward model is to enable us to do reinforcement learning in step 3, by providing a reward signal for new responses generated.\n",
    "\n",
    "## Data Collection and Labelling\n",
    "\n",
    "The model is trained on a dataset of multiple responses to the same prompt.\n",
    "\n",
    "To construct a dataset, human labellers were asked to rank different responses to the same prompt.\n",
    "\n",
    "Typically, humans can store 5-9 items in their working memory.\n",
    "So to make labelling faster and more accurate, it can be useful to limit the number of items that need to be compared.\n",
    "\n",
    "> In the procedure used to train InstructGPT, the labellers were asked to rank between $K=4$ and $K=9$ responses to a single prompt at once.\n",
    "\n",
    "This produces ${K \\choose 2}$ (\"K choose 2\" = how many ways can you choose 2 items from a set of K) different pairs of examples \n",
    "\n",
    "- E.g ${4 \\choose 2}$ = 6 is equivalent to saying that there are 6 different ways you can choose 2 different items out of a set of 4. \n",
    "    - `{A, B, C, D} -> {A, B}, {A, C}, {A, D}, {B, C}, {B, D}, {C, D}`\n",
    "\n",
    "But to train our reward model, we need a label for the reward, not just rankings.\n",
    "However, the absolute value of the score is actually not important.\n",
    "\n",
    "> The absolute value of the reward predicted by the model is not important. What's important, is the difference between the reward predicted for different responses to a prompt. Preferred responses should have higher reward.\n",
    "\n",
    "## The Loss Function\n",
    "\n",
    "Remember, the loss is a measure of how bad the model is, so it is the thing we want to minimise.\n",
    "\n",
    "We don't have a regression target to train the regression reward model with, so we need something else. \n",
    "If we take the difference of the two rewards, then a positive value indicates that the reward model predicts the first response is better, and if the difference is negative, then it indicates that the model predicts the second response is better.\n",
    "If we pass this difference throught a sigmoid function, then we can interpret the output (a value between 0-1) as a confidence that the model predicts the first input is preferable.\n",
    "We can then use this confidence score in the cross entropy loss function.\n",
    "\n",
    "> The cross entropy loss function is used to train the reward model based on these rankings. \n",
    "\n",
    "![](./images/loss%20function.png)\n",
    "\n",
    "Where $r(x, y)$ is the scalar output of the reward model for prompt $x$ and completion $y$ with parameters\n",
    "$θ$, $yw$ is the preferred completion out of the pair of $yw$ and $yl$ (the ordering matters), and $D$ is the dataset of human\n",
    "comparisons.\n",
    "\n",
    "Let's implement a function that takes in the two scalar rewards and returns a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### Run the following cell to download the necessary files for this lesson { display-mode: \"form\" } \n",
    "#@markdown Don't worry about what's in this collapsed cell\n",
    "\n",
    "!pip install -q transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.3133)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def loss_function(preferred_response_reward, alternate_response_reward):\n",
    "    return -torch.mean(torch.log(torch.sigmoid(alternate_response_reward - preferred_response_reward)))\n",
    "\n",
    "example_preferred_response_reward = torch.tensor([1.0])\n",
    "example_alternate_response_reward = torch.tensor([0.0])\n",
    "loss_function(example_preferred_response_reward, example_alternate_response_reward) # test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the dataset which should return us a context (prompt), a preferred response and an alternate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of response pairs: 54\n",
      "Dataset length: 54\n",
      "('What are some easy and healthy snack ideas?', 'Greek yogurt with berries and honey', 'Apple slices with almond butter')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def create_response_pairs():\n",
    "\n",
    "    data = pd.read_csv('reward_dataset.csv', sep=\"|\")\n",
    "\n",
    "    data = data.to_dict(orient=\"records\")\n",
    "    response_pairs = []\n",
    "\n",
    "    for row in data:\n",
    "        prompt = row[\"Prompt\"]\n",
    "        response_pairs.append((prompt, row[\"Most preferable response\"], row[\"Somewhat preferable response\"]))\n",
    "        response_pairs.append((prompt, row[\"Most preferable response\"], row[\"Least preferable response\"]))\n",
    "        response_pairs.append((prompt, row[\"Somewhat preferable response\"], row[\"Least preferable response\"]))\n",
    "        \n",
    "    return response_pairs\n",
    "\n",
    "\n",
    "class RewardDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the dataset.\"\"\"\n",
    "        self.response_pairs = create_response_pairs()\n",
    "        print(\"Number of response pairs:\", len(self.response_pairs))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset.\"\"\"\n",
    "        return len(self.response_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns the example in the dataset at the given index.\"\"\"\n",
    "\n",
    "        # Get the response pair at the given index\n",
    "        response_pair = self.response_pairs[idx]\n",
    "        prompt, preferred_response, alternate_response = response_pair\n",
    "\n",
    "        # Return the preferred response, alternate response\n",
    "        return prompt, preferred_response, alternate_response\n",
    "\n",
    "\n",
    "dataset = RewardDataset()\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "example_idx = random.randint(0, len(dataset))\n",
    "print(dataset[example_idx])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT2 model doesn't output regression predictions off the shelf. \n",
    "Instead, it outputs the deep contextual representations of each input vector, as an output set of vectors.\n",
    "We can ignore all of the vector outputs except the last one, and apply a regression head to it to combine that final representation into a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferred response reward: -2.683429718017578\n",
      "Alternate response reward: -2.70759654045105\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "class ChatGPT2RewardModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = GPT2Model.from_pretrained('gpt2')\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.regression_head = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, context, response):\n",
    "        \"\"\"\n",
    "        Returns a scalar value representing the reward for this response, given the context.\n",
    "        Args:\n",
    "            context (str): The context. aka. the prompt.\n",
    "            response (str): The response. aka. the response to the prompt.\n",
    "        Returns:\n",
    "            float: The reward for generating this response given the context.    \n",
    "        \"\"\"\n",
    "\n",
    "        entire_text = context + response\n",
    "        context_dict = self.tokenizer(\n",
    "            '<|startoftext|>' + entire_text + '<|endoftext|>',\n",
    "            #    truncation=True,\n",
    "            #    max_length=max_length,\n",
    "            #    padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        input_ids = torch.tensor(context_dict.input_ids)\n",
    "        attention_mask = torch.tensor(context_dict.attention_mask)\n",
    "\n",
    "        # Forward pass\n",
    "        gpt2_outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        all_output_vectors = gpt2_outputs.last_hidden_state\n",
    "        last_output_vector = all_output_vectors[-1]\n",
    "\n",
    "        # add batch_size dimension\n",
    "        last_output_vector = last_output_vector.unsqueeze(0)\n",
    "        reward = self.regression_head(last_output_vector)\n",
    "\n",
    "        return reward\n",
    "\n",
    "model = ChatGPT2RewardModel()\n",
    "\n",
    "example = dataset[example_idx]\n",
    "prompt, preferred_response, alternate_response = example\n",
    "\n",
    "preferred_response_reward = model(prompt, preferred_response)\n",
    "alternate_response_reward = model(prompt, alternate_response)\n",
    "\n",
    "print(\"Preferred response reward:\", preferred_response_reward.item())\n",
    "print(\"Alternate response reward:\", alternate_response_reward.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of response pairs: 54\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████| 54/54 [01:59<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/54 [00:03<02:59,  3.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3285d7f1dd57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-3285d7f1dd57>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    158\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    214\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(epochs=10):\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = RewardDataset()\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=1e-5, betas=(0.9, 0.95)) # as used in the InstructGPT paper\n",
    "\n",
    "    # Set up logging\n",
    "    writer = SummaryWriter()  # for logging our loss to TensorBoard\n",
    "    batch_idx = 0 # for setting the x-axis of our TensorBoard plots (loss vs. batch index)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        for batch in tqdm(dataset):\n",
    "            \n",
    "            # Get the data\n",
    "            prompt, preferred_reponse, alternate_response = batch\n",
    "            preferred_response_reward = model(prompt, preferred_reponse)\n",
    "            alternate_response_reward = model(prompt, alternate_response)\n",
    "\n",
    "            loss = loss_function(preferred_response_reward, alternate_response_reward)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Log the loss\n",
    "            # print(f\"Loss: {loss.item()}\", batch_idx)\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the changing loss in Tensorboard and make sure to qualitatively check your results too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Some Details\n",
    "\n",
    "Using each ${K \\choose 2}$ comparisons as separate datapoints was found to lead to overfitting. \n",
    "This is because each completion can appear in $K-1$ different batches and influence the gradient that many times.\n",
    "Instead, each of those comparison pairs produced by one ranking task by a labeller was put into a single batch, so that the \n",
    "\n",
    "This means that the batch size varies between batches, which is an unusual case.\n",
    "\n",
    "We'll need a custom dataloader to implement this.\n",
    "\n",
    "<!-- As the model trains, it will try to learn to produce scores that output scores that would rank all items in the batch in the same order that a human labeller would. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO extra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "- Adapt the code so that the inputs are batched together\n",
    "- Log the text outputs to tensorboard along with the score so you can get a qualitative idea of how things are going and sanity check your implementation\n",
    "- Put the model on a GPU provided by Google Colab\n",
    "- Experiment with the change in performance between batching all comparisons from a single task together and shuffling them into separate batches.\n",
    "- Implement your own reward transformer using PyTorch's `TransformerDecoder` class without using a pre-trained backbone and compare the performance with GPT2. How does a RNN compare? Explore the relative time taken for each using tensorboard.\n",
    "- Experiment with different optimisers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
