{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of ChatGPT\n",
    "\n",
    "## Introduction\n",
    "\n",
    "> ChatGPT is an AI system that can engage in back and forth conversational interactions in a chatbot-style interface. It is capable of writing code, correcting or adjusting its responses based on feedback from the user.\n",
    "\n",
    "ChatGPT is a complicated system that builds on top of a large language model, like GPT-3. The sum of this amounts to a breakthrough that has truly democratised AI by making it available in an intuitive interface that anyone can use.\n",
    "\n",
    "ChatGPT is certainly capable of making mistakes, including:\n",
    "- Providing infactual information\n",
    "- Producing biased responses\n",
    "- Sometimes (although more and more rarely) producing inappropriate or harmful responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How ChatGPT Works\n",
    "\n",
    "So how does it work under the hood?\n",
    "\n",
    "ChatGPT is implemented in 3 steps, as shown below:\n",
    "\n",
    "![](./images/How%20chatGPT%20is%20trained.png)\n",
    "\n",
    "\n",
    "Your first win, when it comes to understanding how ChatGPT works, is to understand these 3 steps:\n",
    "\n",
    "1. Supervised Fine-Tuning (SFT): Fine tune a pre-trained language model (GPT-3.5) to act like a chatbot\n",
    "2. The Reward Model (RM): Train a new _reward model_ to identify which responses generated by the chatbot are better than others\n",
    "3. Reinforcement Learning with Human Feedback (RLHF): Use the reward model to score generated responses, and update the language model to prefer responses with a higher score\n",
    "\n",
    "> The point of SFT and RLHF are to make the language model better and more aligned with human intention. The RM is a necessary component to do RLHF.\n",
    "\n",
    "## InstructGPT\n",
    "\n",
    "Before the development and release of ChatGPT, this method was used to produce a model called InstructGPT, a language model based on GPT-3 which interprets prompts as instructions rather than as some text that needs continuing on from. \n",
    "This makes the models more easy to interact with because you can just give them commands, instead of having to do prompt engineering.\n",
    "\n",
    "Nowadays, all models deployed on the OpenAI API use the InstructGPT variant.\n",
    "\n",
    "### InstructGPT Results:\n",
    "\n",
    "As reported in the [paper on InstructGPT](https://arxiv.org/pdf/2203.02155.pdf)\n",
    "- Labelers significantly prefer InstructGPT outputs over outputs from GPT-3\n",
    "- InstructGPT models show improvements in truthfulness over GPT-3\n",
    "- InstructGPT shows small improvements in toxicity over GPT-3, but not bias\n",
    "- InstructGPT still makes simple mistakes\n",
    "- And more\n",
    "\n",
    "Aside from that, it's clear how ChatGPT has become extremely useful in many use cases by following the same training approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "> For all steps of training, data is required\n",
    "\n",
    "As described in their [paper](https://arxiv.org/pdf/2203.02155.pdf), to collect data to fine tune the very initial InstructGPT models, OpenAI had human labellers create prompts. The three requested prompt types were:\n",
    "- Plain: Ask the labelers to come up with an arbitrary task, while ensuring the\n",
    "tasks had sufficient diversity.\n",
    "- Few-shot: Ask the labelers to come up with an instruction, and multiple query/response\n",
    "pairs for that instruction\n",
    "- User-based: Ask labelers to come up with prompts corresponding to use-cases stated in waitlist applications to the OpenAI\n",
    "API.\n",
    "\n",
    "These manually created prompts led to three datasets, used for the three stages of training:\n",
    "- The supervised fine-tuning (SFT) dataset\n",
    "    - Features: Prompts\n",
    "    - Labels: Ideal responses\n",
    "- The reward model (RM) dataset\n",
    "    - Features: Prompts & responses\n",
    "    - Labels: Rankings of each response \n",
    "- The PPO dataset\n",
    "    - Features: Prompts & responses\n",
    "    - No labels\n",
    "\n",
    "To create the datasets for the original InstructGPT models, OpenAI hired a team of 40 labellers from [Upwork](https://www.upwork.com/) and used [Scale AI](https://scale.com/rlhf) to manage the datasets.\n",
    "\n",
    "This means that a team of humans literally write out acceptable responses to a range of prompts. These responses are saved, and make up the raw data for a dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning (SFT)\n",
    "\n",
    "The first step, is to _train a supervised policy_.\n",
    "\n",
    "> A policy is something that defines how you act in a certain context. In the case of ChatGPT, the context is the instruction written by the user (or the conversation so far), and the policy defines what response ChatGPT should produce.\n",
    "\n",
    "The policy is determined by the parameters of the language model. Initially, this policy is defined by the parameters of the model used as a starting point (the backbone).\n",
    "\n",
    "> The starting point for ChatGPT was to use [GPT-3.5](https://platform.openai.com/docs/model-index-for-researchers/models-referred-to-as-gpt-3-5) as a backbone\n",
    "\n",
    "The backbone GPT-3.5 model is a large language model (LLM) already highly competent when it comes to language generation. It is able to generate novel text in a variety of formats including:\n",
    "- Syntactically perfect English\n",
    "- Working code\n",
    "- Answers to trivia and uncommon knowledge\n",
    "- Consistent short stories and text\n",
    "- Stylised writing\n",
    "- And more\n",
    "\n",
    "However, the original language model has still learnt some undesirable behaviours due to the data it was trained on (a large portion of the internet):\n",
    "- Biased and inappropriate answers\n",
    "- Lack of consistent, professional, or helpful tone\n",
    "\n",
    "> The point of the supervised fine-tuning (step 2) and RLHF (step 3) is to straighten out the undesired behaviours of a general purpose language model.\n",
    "\n",
    "> SFT is used as a way to get a good model parameter initialisation to further fune-tune using RLHF later\n",
    "\n",
    "To do this, in the SFT stage, the pre-trained language model is trained further, updating its parameters, to do language modelling (predicting the next word) on the SFT dataset explained earlier, which contains examples of prompts and corresponding responses.\n",
    "\n",
    "> SFT requires a labelled dataset (with model responses rather than just prompts)\n",
    "\n",
    "By seeing many of these examples, the language model should learn:\n",
    "- The style of a chatbot\n",
    "- To avoid some biases and inappropriate responses\n",
    "- To be helpful to the user\n",
    "\n",
    "> Like RLHF (step 3), the point of supervised fine-tuning is to make the language model better at acting like a helpful chatbot and more aligned with human intention.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train a Reward Model\n",
    "\n",
    "> The second step is to _train a reward model_, which will be used in step 3 to score the quality of responses generated by the language model.\n",
    "\n",
    "> The reward model is trained to output a scalar value, the reward, that tells the model how good its response was.\n",
    "\n",
    "The dataset used to train the reward model is a list of several outputs for the same prompt ranked in order of preference by a human labeller. \n",
    "The aim of the model is to predict a higher score for the higher ranked response.\n",
    "\n",
    "\n",
    "## Step 3: Reinforcement Learning with Human Feedback\n",
    "\n",
    "### Recap: What is Reinforcement Learning?\n",
    "\n",
    "> Reinforcement learning is where an agent (in our case, the AI system) interacts with an an environment (in our case, interacting with the chat interface by responding to prompts), and tries to maximise a reward which is receives for doing well (or a punishment for not doing well).\n",
    "\n",
    "### No Labels, No Problem\n",
    "\n",
    "The fine-tuned model can use the reward model (RM) generated in step 2 to evaluate new generated text, without requiring labelled ideal responses.\n",
    "\n",
    "> RLHF does not require the prompts in its dataset to be labelled with ideal responses, but it does require the reward model (RM)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
