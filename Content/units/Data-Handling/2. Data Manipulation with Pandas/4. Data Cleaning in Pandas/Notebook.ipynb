{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning in `Pandas`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a74e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ### Run the following cell to download the necessary files for this lesson { display-mode: \"form\" } \n",
    "#@markdown Don't worry about what's in this collapsed cell\n",
    "\n",
    "print('Downloading flights_sample.csv...')\n",
    "!wget https://s3-eu-west-1.amazonaws.com/aicore-portal-public-prod-307050600709/lesson_files/3fad431d-917f-4989-abc8-6d22d2961e37/flights_sample.csv -q -O flights_sample.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal of data cleaning is to convert your raw data into a format that is ready to be used for analysis. The nature and extent of the cleaning you need to do is therefore a function of both the dataset and your analysis goals. However it typically includes the following:\n",
    "\n",
    "- Converting columns into the correct **data types**, for example ensuring numbers are represented by a numeric format like `int` or `float` that can support computation\n",
    "<br><br>\n",
    "- Handling data issues that could impact your analysis, for example the presence of `NaN` or `NULL` values or *duplicated* data\n",
    "<br><br>\n",
    "- Creating new columns from existing columns to support your analysis, such as categorising a list of products as `High`, `Mid` or `Low` price, based on a price column\n",
    "\n",
    "\n",
    "This lesson will address how to perform these tasks using the `Pandas` library in Python, and provide examples for how to address certain common data issues. As we will see, there is no specific order in which we should approach the tasks, as they are interdependent. The order in which you perform them will depend on your analysis goals and the specifics of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "Once you have imported your data, a common first step is to check the data type of each column, and adjust any that aren't optimal for your analysis. When you import your raw data, `Pandas` will attempt to automatically assign a data type to each column, but it doesn't always make the best choice.\n",
    "\n",
    "Let's import some example data to take a look at how to do this. Run the code block below to import the `flights.txt` file and print the head of the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRANSACTIONID</th>\n",
       "      <th>FLIGHTDATE</th>\n",
       "      <th>AIRLINECODE</th>\n",
       "      <th>TAILNUM</th>\n",
       "      <th>FLIGHTNUM</th>\n",
       "      <th>ORIGINAIRPORTCODE</th>\n",
       "      <th>DESTAIRPORTCODE</th>\n",
       "      <th>CRSDEPTIME</th>\n",
       "      <th>DEPTIME</th>\n",
       "      <th>DEPDELAY</th>\n",
       "      <th>ARRTIME</th>\n",
       "      <th>ARRDELAY</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117865800</td>\n",
       "      <td>20110511</td>\n",
       "      <td>YV</td>\n",
       "      <td>N77302</td>\n",
       "      <td>1026</td>\n",
       "      <td>HNL</td>\n",
       "      <td>KOA</td>\n",
       "      <td>1305</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>1336.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>163 miles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146323500</td>\n",
       "      <td>20160520</td>\n",
       "      <td>B6</td>\n",
       "      <td>N203JB</td>\n",
       "      <td>1068</td>\n",
       "      <td>CHS</td>\n",
       "      <td>BOS</td>\n",
       "      <td>700</td>\n",
       "      <td>649.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>False</td>\n",
       "      <td>F</td>\n",
       "      <td>818 miles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71623600</td>\n",
       "      <td>20040922</td>\n",
       "      <td>XE</td>\n",
       "      <td>N14939</td>\n",
       "      <td>2024</td>\n",
       "      <td>CLE</td>\n",
       "      <td>ROC</td>\n",
       "      <td>945</td>\n",
       "      <td>937.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>245 miles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147762700</td>\n",
       "      <td>20160527</td>\n",
       "      <td>EV</td>\n",
       "      <td>N695CA</td>\n",
       "      <td>5325</td>\n",
       "      <td>ATL</td>\n",
       "      <td>OMA</td>\n",
       "      <td>850</td>\n",
       "      <td>847.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>955.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>F</td>\n",
       "      <td>821 miles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29225500</td>\n",
       "      <td>19970511</td>\n",
       "      <td>US</td>\n",
       "      <td>N284AU</td>\n",
       "      <td>1001</td>\n",
       "      <td>CMH</td>\n",
       "      <td>PHL</td>\n",
       "      <td>755</td>\n",
       "      <td>754.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>405 miles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TRANSACTIONID  FLIGHTDATE AIRLINECODE TAILNUM  FLIGHTNUM ORIGINAIRPORTCODE  \\\n",
       "0      117865800    20110511          YV  N77302       1026               HNL   \n",
       "1      146323500    20160520          B6  N203JB       1068               CHS   \n",
       "2       71623600    20040922          XE  N14939       2024               CLE   \n",
       "3      147762700    20160527          EV  N695CA       5325               ATL   \n",
       "4       29225500    19970511          US  N284AU       1001               CMH   \n",
       "\n",
       "  DESTAIRPORTCODE  CRSDEPTIME  DEPTIME  DEPDELAY  ARRTIME  ARRDELAY CANCELLED  \\\n",
       "0             KOA        1305   1250.0     -15.0   1336.0      -9.0         F   \n",
       "1             BOS         700    649.0     -11.0    847.0     -27.0     False   \n",
       "2             ROC         945    937.0      -8.0   1045.0      -2.0         F   \n",
       "3             OMA         850    847.0      -3.0    955.0     -16.0     False   \n",
       "4             PHL         755    754.0      -1.0    912.0       1.0         0   \n",
       "\n",
       "  DIVERTED   DISTANCE  \n",
       "0    False  163 miles  \n",
       "1        F  818 miles  \n",
       "2    False  245 miles  \n",
       "3        F  821 miles  \n",
       "4        0  405 miles  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None) # You can use this setting display all the columns in your dataframe\n",
    "flights_df = pd.read_csv(\"flights_sample.csv\") \n",
    "flights_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the first few rows of data that there is a variety of different types of information in the table. Some columns contain text and others numerals, but there is also variation in terms of the type of information that is being encoded.\n",
    "\n",
    "For example the `FLIGHTNUM` and `DISTANCE` columns both contain numeric information, but while the `DISTANCE` column contains information that we might want to perform arithmetic on, the numeric information in `FLIGHTNUM` is presumably just a set of unique identifiers for different flight routes. \n",
    "\n",
    "Complications like these indicate why it is always advisable to do at least some manual cleaning of the data. Even though packages exist to automate many of the cleaning tasks, the cleaning process is also an important step in your EDA, allowing you to get familiar with your dataset and anticipate any issues that might arise later in your analysis.\n",
    "\n",
    "### Checking the Data Type\n",
    "\n",
    "The data types of your columns can be accessed via the `.dtypes` attribute, or by calling the `.info()` method. \n",
    "\n",
    "- The `.dtypes` attribute only returns the data type of each column\n",
    "- The `.info()` method returns both the data type and some additional information: the number of rows and the memory usage of the dataframe, as well as the number of non-null values in each column. We will deal with handling `NULL` values later in the lesson.\n",
    "\n",
    "Run the two code blocks below to see the difference in output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRANSACTIONID          int64\n",
       "FLIGHTDATE             int64\n",
       "AIRLINECODE           object\n",
       "TAILNUM               object\n",
       "FLIGHTNUM              int64\n",
       "ORIGINAIRPORTCODE     object\n",
       "DESTAIRPORTCODE       object\n",
       "CRSDEPTIME             int64\n",
       "DEPTIME              float64\n",
       "DEPDELAY             float64\n",
       "ARRTIME              float64\n",
       "ARRDELAY             float64\n",
       "CANCELLED             object\n",
       "DIVERTED              object\n",
       "DISTANCE              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119180 entries, 0 to 119179\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   TRANSACTIONID      119180 non-null  int64  \n",
      " 1   FLIGHTDATE         119180 non-null  int64  \n",
      " 2   AIRLINECODE        119180 non-null  object \n",
      " 3   TAILNUM            103353 non-null  object \n",
      " 4   FLIGHTNUM          119180 non-null  int64  \n",
      " 5   ORIGINAIRPORTCODE  119180 non-null  object \n",
      " 6   DESTAIRPORTCODE    119180 non-null  object \n",
      " 7   CRSDEPTIME         119180 non-null  int64  \n",
      " 8   DEPTIME            116328 non-null  float64\n",
      " 9   DEPDELAY           116328 non-null  float64\n",
      " 10  ARRTIME            116129 non-null  float64\n",
      " 11  ARRDELAY           116040 non-null  float64\n",
      " 12  CANCELLED          119180 non-null  object \n",
      " 13  DIVERTED           119180 non-null  object \n",
      " 14  DISTANCE           119180 non-null  object \n",
      "dtypes: float64(4), int64(4), object(7)\n",
      "memory usage: 13.6+ MB\n"
     ]
    }
   ],
   "source": [
    "flights_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types in `Pandas`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Below is a table that shows various `Pandas` data types, their corresponding Python data types, and a brief description of each.\n",
    "\n",
    "| Pandas Dtype | Python Dtype        | Description                                           |\n",
    "|--------------|---------------------|-------------------------------------------------------|\n",
    "| object       | str                 | Used for text or mixed types of data                  |\n",
    "| int64        | int                 | Integer numbers                                       |\n",
    "| float64      | float               | Floating-point numbers                                |\n",
    "| bool         | bool                | Boolean values (True/False)                           |\n",
    "| datetime64   | datetime.datetime   | Date and time values                                  |\n",
    "| timedelta64  | datetime.timedelta  | Differences between two datetimes                     |\n",
    "| category     | (special type)      | Finite list of text values                            |\n",
    "| period       | pd.Period           | Periods of time, useful for time-series data          |\n",
    "| sparse       | (special type)      | Sparse array to contain mostly NaN values             |\n",
    "| string       | str                 | Text                                                  |\n",
    "Note that:\n",
    "\n",
    "- The `int64` and `float64` data types indicate 64-bit storage for integer and floating-point numbers, respectively. `Pandas` also supports other sizes (like `int32` and `float32`) to save memory when the larger sizes are not necessary.\n",
    "- The `category` dtype is not a native Python data type but is provided by `Pandas` to optimize memory usage and performance for data with a small number of distinct values\n",
    "- The `sparse` dtype is used for data that is mostly missing to save memory by only storing the non-missing values\n",
    "- The `period` dtype is specific to `Pandas` and is used for handling period data, which is not directly analogous to a native Python type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Data Type\n",
    "\n",
    "You can change the data type of a column using the `astype()` method. Let's look at a simple example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Name    7 non-null      object\n",
      " 1   Age     7 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 244.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Create a simple dataframe of names and ages\n",
    "data = {'Name': ['Alice', 'Bashar', 'Carlos', 'Diana', \"Ephraim\", \"Frank\", \"Gina\"],\n",
    "        'Age': [21, 22, 'n/a', 24, 25, 'missing', 27]}\n",
    "age_df = pd.DataFrame(data)\n",
    "age_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output of `info()`, both columns have defaulted to the `Object` datatype. We can easily change the `Name` column to the `string` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Name    7 non-null      string\n",
      " 1   Age     7 non-null      object\n",
      "dtypes: object(1), string(1)\n",
      "memory usage: 244.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "age_df.Name = age_df.Name.astype('string')\n",
    "age_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when we try to cast the `Age` column as `int64`, the method throws a `ValueError`, because it encountered some values (`missing` and `n/a`) that it could not cast as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'n/a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/timhowe/Documents/CODE/000_content/Content-Projects/Content/units/Data-Handling/2. Data Manipulation with Pandas/4. Data Cleaning in Pandas/Notebook.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/timhowe/Documents/CODE/000_content/Content-Projects/Content/units/Data-Handling/2.%20Data%20Manipulation%20with%20Pandas/4.%20Data%20Cleaning%20in%20Pandas/Notebook.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m age_df\u001b[39m.\u001b[39mAge \u001b[39m=\u001b[39m age_df\u001b[39m.\u001b[39;49mAge\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mint64\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/openai/lib/python3.11/site-packages/pandas/core/generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6233\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m   6234\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   6235\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m   6236\u001b[0m     ]\n\u001b[1;32m   6238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6239\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6240\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   6241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6243\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/openai/lib/python3.11/site-packages/pandas/core/internals/managers.py:448\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mastype\u001b[39m(\u001b[39mself\u001b[39m: T, dtype, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m--> 448\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/openai/lib/python3.11/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/openai/lib/python3.11/site-packages/pandas/core/internals/blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m--> 526\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m    528\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    529\u001b[0m newb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/openai/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m values\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    298\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    300\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m     \u001b[39m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/openai/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    227\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    229\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     values \u001b[39m=\u001b[39m astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    232\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/openai/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:170\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mor\u001b[39;00m is_object_dtype(arr\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[1;32m    169\u001b[0m     \u001b[39m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'n/a'"
     ]
    }
   ],
   "source": [
    "age_df.Age = age_df.Age.astype('int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `astype()` method throws an error when it encounters a non-convertible value, as this prevents accidental data loss. We can override this behaviour by setting the `errors` flag to `ignore` rather than `raise`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Name    7 non-null      string\n",
      " 1   Age     7 non-null      object\n",
      "dtypes: object(1), string(1)\n",
      "memory usage: 244.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "age_df.Age = age_df.Age.astype('int64', errors='ignore')\n",
    "age_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this actually fails to alter the datatype, as it leaves the unconvertible values unchanged, as can be seen in the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bashar</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carlos</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diana</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ephraim</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gina</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name      Age\n",
       "0    Alice       21\n",
       "1   Bashar       22\n",
       "2   Carlos      n/a\n",
       "3    Diana       24\n",
       "4  Ephraim       25\n",
       "5    Frank  missing\n",
       "6     Gina       27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, we are happy to lose the information in the non-numeric values, for the sake of being able to treat the column as integers. To do this, we can use a separate function, `pd.to_numeric()`, which we can use with the `errors` parameter set to `coerce` to force the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Name    7 non-null      string \n",
      " 1   Age     5 non-null      float64\n",
      "dtypes: float64(1), string(1)\n",
      "memory usage: 244.0 bytes\n"
     ]
    }
   ],
   "source": [
    "age_df.Age = pd.to_numeric(age_df.Age, errors='coerce')\n",
    "age_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bashar</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carlos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diana</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ephraim</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gina</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name   Age\n",
       "0    Alice  21.0\n",
       "1   Bashar  22.0\n",
       "2   Carlos   NaN\n",
       "3    Diana  24.0\n",
       "4  Ephraim  25.0\n",
       "5    Frank   NaN\n",
       "6     Gina  27.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now successfully converted our column to numeric values, with non-numeric values being replaced with `NaN`, but the data type is `float64` not `int64`. Attempting to cast it to `int64` will still fail, as the data type doesn't support `NaN` values. We will need return to this issue in a later section of our data cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Name    7 non-null      string \n",
      " 1   Age     5 non-null      float64\n",
      "dtypes: float64(1), string(1)\n",
      "memory usage: 244.0 bytes\n"
     ]
    }
   ],
   "source": [
    "age_df.Age = age_df.Age.astype('int64', errors='ignore')\n",
    "age_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `datetime64` Data Type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In `Pandas`, the `datetime64` data type provides a memory-efficient structure for working with date and time data, allowing for operations like time-based indexing, slicing, and resampling to be performed. This data type is necessary for effective time-series data analysis, as it allows complex temporal computations and aggregations to be performed with relative ease.\n",
    "\n",
    "Date-time columns can be challenging to assign correctly, because there is a very large range of ways that date and time columns can be formatted, and there is no guarantee that each column will only use one of these formats, so it is important to determine which formats are used in your data before attempting to convert a column to `datetime64`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting a Column to Datetime\n",
    "\n",
    "The `pd.to_datetime` function can be used to cast a column to `datetime64`. In order to ensure that the conversion is accurate, it is necessary to consider the format that the date/time values are in.\n",
    "\n",
    "Run the three code blocks below to perform a simple example conversion, and confirm that the `datetime64`-encoded dates are correct. In this case, the data are initially formatted as strings, and the date format is unambiguous, and so the conversion works without any additional work:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 1 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date_strings  4 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 164.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'date_strings': ['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01']\n",
    "}\n",
    "date_df = pd.DataFrame(data)\n",
    "\n",
    "date_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 1 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date_strings  4 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](1)\n",
      "memory usage: 164.0 bytes\n"
     ]
    }
   ],
   "source": [
    "date_df.date_strings = pd.to_datetime(date_df.date_strings)\n",
    "date_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_strings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_strings\n",
       "0   2023-01-01\n",
       "1   2023-02-01\n",
       "2   2023-03-01\n",
       "3   2023-04-01"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Common Issues: *Epoch* Time\n",
    "\n",
    "Date and time are typically represented in computers using a system known as *epoch time* or *Unix time*, which counts the number of seconds that have elapsed since a predefined point in time, known as the *epoch*. The **epoch** is set at `00:00:00 UTC on Thursday, January 1, 1970` and the count of seconds (or milliseconds in some applications) from this point is used to represent subsequent points in time, allowing for a standardized, system-independent representation of time that can be easily computed and converted into various human-readable formats.\n",
    "\n",
    "This convention is the source of common issue encountered when converting columns to datetime. For an example, return to the `flights_df`, and look at the `FLIGHTDATE` column:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20110511\n",
       "1    20160520\n",
       "2    20040922\n",
       "3    20160527\n",
       "4    19970511\n",
       "Name: FLIGHTDATE, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df['FLIGHTDATE'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`Pandas` has detected the `dtype` of the  `FLIGHTDATE` column as `int64`, whereas it would be more helpful to have it as `datetime64`, so that it can be used in time-series analysis.\n",
    "\n",
    "Run the two code block below, to see what happens when we try using the function on the `FLIGHTDATE` column:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   1970-01-01 00:00:00.020110511\n",
       "1   1970-01-01 00:00:00.020160520\n",
       "2   1970-01-01 00:00:00.020040922\n",
       "3   1970-01-01 00:00:00.020160527\n",
       "4   1970-01-01 00:00:00.019970511\n",
       "Name: FLIGHTDATE, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(flights_df['FLIGHTDATE']).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the first few values of the column are in January, 1970. This should arouse suspicion, given what we know about **Epoch time**. We can see from the head of the original column that the first 5 values should be between 1997 and 2016, not in 1970. The function has interpreted the integer values in `FLIGHTDATE` as a number of seconds after the **Epoch**, when actually it should be a date in the format`YYYmmdd`.\n",
    "\n",
    "To work around this, we can specify a format as an argument to the `to_datetime` function, as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2011-05-11\n",
       "1        2016-05-20\n",
       "2        2004-09-22\n",
       "3        2016-05-27\n",
       "4        1997-05-11\n",
       "            ...    \n",
       "119175   2016-05-18\n",
       "119176   2014-09-25\n",
       "119177   2016-01-13\n",
       "119178   1997-01-04\n",
       "119179   2012-09-07\n",
       "Name: FLIGHTDATE, Length: 119180, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Assign date_format\n",
    "date_format = \"%Y%m%d\"\n",
    "pd.to_datetime(flights_df[\"FLIGHTDATE\"], format=date_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Issues: Mixed Date Formats\n",
    "\n",
    "Handling multiple date formats in a single column can be a bit tricky, but `pd.to_datetime` is quite flexible and can infer different formats automatically in most cases. Below is a simple example of a column called `mixed_dates`, which has dates in multiple formats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   mixed_dates\n",
      "0   01/02/2023\n",
      "1   2023-03-01\n",
      "2  04-Apr-2023\n",
      "3     20230505\n",
      "\n",
      "Data types:\n",
      "mixed_dates    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataframe with multiple date formats\n",
    "data = {\n",
    "    'mixed_dates': ['01/02/2023', '2023-03-01', '04-Apr-2023', '20230505']\n",
    "}\n",
    "mixed_date_df = pd.DataFrame(data)\n",
    "\n",
    "# Displaying the original dataframe\n",
    "print(\"Original Dataframe:\")\n",
    "print(mixed_date_df)\n",
    "print(\"\\nData types:\")\n",
    "print(mixed_date_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modified DataFrame:\n",
      "   mixed_dates      dates\n",
      "0   01/02/2023 2023-01-02\n",
      "1   2023-03-01        NaT\n",
      "2  04-Apr-2023        NaT\n",
      "3     20230505        NaT\n",
      "\n",
      "Data types:\n",
      "mixed_dates            object\n",
      "dates          datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Converting the 'mixed_dates' column to datetime\n",
    "# Note: infer_datetime_format=True can help to infer different formats, but might not handle all cases\n",
    "mixed_date_df['dates'] = pd.to_datetime(mixed_date_df['mixed_dates'], infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "# Displaying the modified dataframe\n",
    "print(\"\\nModified dataframe:\")\n",
    "print(mixed_date_df)\n",
    "print(\"\\nData types:\")\n",
    "print(mixed_date_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately in this case, automatic conversion has not been very effective, and we can see multiple values have been returned as `NaT` (Not a Time).\n",
    "\n",
    "A more effective approach is to use the `parse` function from the `dateutil` library, in conjunction with the `.apply` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modified DataFrame:\n",
      "\n",
      "   mixed_dates      dates\n",
      "0   01/02/2023 2023-01-02\n",
      "1   2023-03-01 2023-03-01\n",
      "2  04-Apr-2023 2023-04-04\n",
      "3     20230505 2023-05-05\n",
      "\n",
      "Data types:\n",
      "\n",
      "mixed_dates            object\n",
      "dates          datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from dateutil.parser import parse\n",
    "mixed_date_df['dates'] = mixed_date_df['mixed_dates'].apply(parse)\n",
    "mixed_date_df['dates'] = pd.to_datetime(mixed_date_df['dates'], infer_datetime_format=True, errors='coerce')\n",
    "print(\"\\nModified dataframe:\\n\")\n",
    "print(mixed_date_df)\n",
    "print(\"\\nData types:\\n\")\n",
    "print(mixed_date_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `timedelta64` Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `timedelta64` data type is used to represent differences in `datetime64` objects. While a `datetime64` object represents a specific point in time, with a defined year, month, day, hour, minute, and so on, a `timedelta64` object represents a duration that is not anchored to a specific start or end point. It tells you how much time is between two points, without specifying what those points are.\n",
    "\n",
    "The distinction between `timedelta64` and `datetime64` data types in `Pandas` (and similarly, `timedelta` and `datetime` in Python's `datetime` module) is crucial due to the inherent differences in representing and utilizing points in time versus durations of time, which are fundamentally different concepts.\n",
    "\n",
    "**Arithmetic Operations:**\n",
    "   - When you perform arithmetic with two `datetime64` objects, the result is a `timedelta64` object because subtracting one point in time from another gives you a duration\n",
    "   - Conversely, when you add or subtract a `timedelta64` from a `datetime64` object, you get another `datetime64` object because you're shifting a point in time by a certain duration\n",
    "\n",
    "By having separate data types, `Pandas` (and Python more broadly) allows for clear, intuitive operations on time data, ensuring that the operations are semantically meaningful and that the results are what users expect when performing arithmetic or comparisons with time-related data. This distinction also helps prevent misinterpretation of the data and ensures that operations are performed with the appropriate level of precision and efficiency for each type of data.\n",
    "\n",
    "For example, the code block below creates a new `timedelta64` column by subtracting a specific timestamp from the `dates` column of the `mixed_dates_df` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modified DataFrame:\n",
      "   mixed_dates      dates date_difference\n",
      "0   01/02/2023 2023-01-02          1 days\n",
      "1   2023-03-01 2023-03-01         59 days\n",
      "2  04-Apr-2023 2023-04-04         93 days\n",
      "3     20230505 2023-05-05        124 days\n"
     ]
    }
   ],
   "source": [
    "# Subtracting a single date from the 'dates' column\n",
    "single_date = pd.Timestamp('2023-01-01')  # Creating a Timestamp object\n",
    "mixed_date_df['date_difference'] = mixed_date_df['dates'] - single_date  # Subtracting the single date\n",
    "\n",
    "# Displaying the modified dataframe\n",
    "print(\"\\nModified dataframe:\")\n",
    "print(mixed_date_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Subset of Data\n",
    "\n",
    ">The process of selecting a subset of your data based on the evaluation of a logical expression is known as *logical indexing* or *logic masking*. There may be times when you wish to separate out sections of your data, based on the value taken by a specific column. For example if you are working with a dataframe of customers for an international business, you might want to separate out just those customers where the `country` column contains the value `Switzerland`. Alternatively you might wish to send out an email to only those of your customers who are within a certain age range. These are both situations where **logical indexing** is useful.\n",
    "\n",
    "You have already met the `loc` method in a previous lesson, in the context of selecting a row based on an index. You can also use it to select a row based on a logical expression. \n",
    "\n",
    "Let's take the example of selecting the entries in a `customers` dataframe where the country matches `Switzerland`. We use the `loc` function to select rows in which a logical expression is evaluated to true. The syntax to assign matches to a new dataframe is as follows:\n",
    "\n",
    "`new_df = df.loc[<your logical expression>]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "       Name  Age      Country\n",
      "0  Alansana   23          USA\n",
      "1    Briana   34  Switzerland\n",
      "2  Chanmony   45           UK\n",
      "3  Dietrich   36  Switzerland\n",
      "4       Eva   50       Canada\n",
      "\n",
      "Swiss Customers:\n",
      "       Name  Age      Country\n",
      "1    Briana   34  Switzerland\n",
      "3  Dietrich   36  Switzerland\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Creating a sample dataframe\n",
    "data = {\n",
    "    'Name': ['Alansana', 'Briana', 'Chanmony', 'Dietrich', 'Eva'],\n",
    "    'Age': [23, 34, 45, 36, 50],\n",
    "    'Country': ['USA', 'Switzerland', 'UK', 'Switzerland', 'Canada']\n",
    "}\n",
    "customers = pd.DataFrame(data)\n",
    "\n",
    "# Displaying the original dataframe\n",
    "print(\"Original dataframe:\")\n",
    "print(customers)\n",
    "\n",
    "# Using `loc` to select rows based on a logical expression\n",
    "swiss_customers = customers.loc[customers['Country'] == 'Switzerland']\n",
    "\n",
    "# Displaying the dataframe with only Swiss customers\n",
    "print(\"\\nSwiss Customers:\")\n",
    "print(swiss_customers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to select multiple values from your column for a subset of the data, you can use the `.isin()` method to achieve this. The `isin()` method returns `True` if a row's value is a member of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subset DataFrame (Countries: USA, UK):\n",
      "       Name  Age Country  marketing_email\n",
      "0  Alansana   23     USA            False\n",
      "2  Chanmony   45      UK            False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Selecting rows where 'Country' is either 'USA' or 'UK'\n",
    "selected_countries = ['USA', 'UK']\n",
    "mask = customers['Country'].isin(selected_countries)\n",
    "\n",
    "# Using the mask to get the subset of the DataFrame\n",
    "subset_df = customers[mask]\n",
    "\n",
    "# Displaying the subset of the DataFrame\n",
    "print(\"\\nSubset DataFrame (Countries: USA, UK):\")\n",
    "print(subset_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical indexing can also be used in conjunction with other methods, such as dropping values. In many cases this requires the creation of a *logical mask*, which is a Boolean data series of the same length as the number of rows in the dataframe. This can then be passed to other functions, or used to selectively apply a function to the logical subset of the indices.\n",
    "\n",
    "Let's look at an example, where we want to create a new column called `marketing_email` which is set to `True` if the customer age is less than 40 and greater than or equal to 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3     True\n",
      "4    False\n",
      "Name: Age, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Creating a logical mask for ages between 30 and 40\n",
    "age_mask = (customers['Age'] >= 30) & (customers['Age'] < 40)\n",
    "\n",
    "print(age_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logical mask is constructed by chaining together a set of conditions (e.g. `customers['Age'] >=30`) with logical operators (e.g. `&` and `or`), and enclosing the result in brackets. Each condition must evaluate to a Boolean series of the same length as the number of rows in the dataframe.\n",
    "\n",
    "Once the logical mask has been generated, it can be used to create a new column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with 'marketing_email' Column:\n",
      "       Name  Age      Country  marketing_email\n",
      "0  Alansana   23          USA            False\n",
      "1    Briana   34  Switzerland             True\n",
      "2  Chanmony   45           UK            False\n",
      "3  Dietrich   36  Switzerland             True\n",
      "4       Eva   50       Canada            False\n"
     ]
    }
   ],
   "source": [
    "# Using the logical mask to create a new column 'marketing_email'\n",
    "# Set to True where the condition is met, and False otherwise\n",
    "customers['marketing_email'] = False  # Initialize column with False\n",
    "customers.loc[age_mask, 'marketing_email'] = True  # Apply mask to set True where condition is met\n",
    "\n",
    "# Displaying the dataframe with the new column\n",
    "print(\"\\ndataframe with 'marketing_email' Column:\")\n",
    "print(customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing and Incorrect Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values and `NaN`s\n",
    "\n",
    ">It is typically necessary to find a way to handle situations where some columns contain rows without data. Missing values can cause errors or incorrect responses, and will need to be dealt with by removing the rows that contain missing data, or by imputing an appropriate value. \n",
    "\n",
    "An obvious example of where missing values can interfere with analysis is the presence of `NaN` values in a numeric column. While many `Pandas` aggregation functions can handle `NaN`s by skipping them, even a single `NaN` in a numeric column can prevent some aggregation functions from returning a usable result, and will also break many machine learning algorithms. \n",
    "\n",
    "Returning to the earlier example of the column of ages in `age_df`, we saw earlier that we could not assign the data type of `int64`, because we had `NaN` values in the column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bashar</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carlos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diana</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ephraim</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gina</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name   Age\n",
       "0    Alice  21.0\n",
       "1   Bashar  22.0\n",
       "2   Carlos   NaN\n",
       "3    Diana  24.0\n",
       "4  Ephraim  25.0\n",
       "5    Frank   NaN\n",
       "6     Gina  27.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two `NaN` values in the column, and we have the choice to either drop of impute the values to resolve this. The decision of which action to perform depends on your data and analysis goals. For example in this case, we cannot be certain of the ages of `Frank` and `Carlos`, so if our analysis depends on the `Age` column, it might be better to drop the missing values. This can be achieved with the `.dropna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bashar</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diana</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ephraim</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gina</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name   Age\n",
       "0    Alice  21.0\n",
       "1   Bashar  22.0\n",
       "3    Diana  24.0\n",
       "4  Ephraim  25.0\n",
       "6     Gina  27.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_df.dropna()  # Drop rows with any column having NA/null data. If we wanted to apply this result permanently, we would need to assign it to a new dataframe or use the `inplace=True`` argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we might know in advance that all of the students are between the ages of 20 and 30, and so it might be acceptable to replace the missing values with the mean or median average. This can be achieved with the `fillna()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_impute = age_df.Age.mean().round()  # Calculate the mean of the Age column and round it to the nearest integer\n",
    "\n",
    "age_df.fillna(value_to_impute, inplace = True)  # Fill NaNs with the value we calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling the `NaN` values will now permit the column to be cast to an `int64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Name    7 non-null      string\n",
      " 1   Age     7 non-null      int64 \n",
      "dtypes: int64(1), string(1)\n",
      "memory usage: 244.0 bytes\n"
     ]
    }
   ],
   "source": [
    "age_df.Age = age_df.Age.astype('int')  # Round the Age column and convert it to an integer\n",
    "age_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting and Fixing Incorrect Values\n",
    "\n",
    "Another scenario that occurs often is the case where some values in a column are incorrect in some way. This is heavily data dependent, and will require an understanding of what the column is supposed to contain. \n",
    "\n",
    "Let's consider a couple of examples. Our `flights_df` contain two columns that appear to be intended as Boolean values: `CANCELLED` and `DIVERTED`. We can see what values are in the column, and the number of instances of each value, by using the `value_counts()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    63587\n",
       "0        34964\n",
       "F        17752\n",
       "True      1619\n",
       "1          851\n",
       "T          407\n",
       "Name: CANCELLED, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df['CANCELLED'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the column is intended as a Boolean, but the values have been expressed in a variety of ways. There are a couple of techniques to fix this situation. The first is to use the `.replace()` method to replace one value with another. For example we can replace all the `0` values with `False` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    63587\n",
       "False    34964\n",
       "F        17752\n",
       "True      1619\n",
       "1          851\n",
       "T          407\n",
       "Name: CANCELLED, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df['CANCELLED'].replace({'0': False}, inplace=True)\n",
    "flights_df['CANCELLED'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `False` appears twice in the `value_counts` result. This is because `Pandas` is distinguishing between the string  `\"False\"` and the Boolean value `False`. If we want to convert the column to a Boolean type, we will need to ensure that all values in it are of Boolean type.\n",
    "\n",
    "The `.replace()` method can also accept a dictionary, where the dictionary keys are the values to match, and the dictionary values are the replacement values, e.g. `df.replace({'0': False, '1' : True})` to replace all instances of `0` or `1` with `False` and `True` respectively. This can also be achieved using the `map` function, which can apply a mapping (or even a function) to all the elements in a column or series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    116303\n",
       "True       2877\n",
       "Name: CANCELLED, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_dictionary = {'0': False, '1': True, 'F': False, 'T': True, 'True': True, 'False': False}\n",
    "flights_df['CANCELLED'].replace(mapping_dictionary, inplace=True)\n",
    "flights_df['CANCELLED'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our column contains just `True` and `False` values, we can convert it to ` bool` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datatype of flights_df[\"CANCELLED\"]:\n",
      "bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    116303\n",
       "True       2877\n",
       "Name: CANCELLED, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df['CANCELLED'] = flights_df['CANCELLED'].astype('bool')\n",
    "\n",
    "print('datatype of flights_df[\"CANCELLED\"]:')\n",
    "print(flights_df['CANCELLED'].dtype)\n",
    "\n",
    "flights_df['CANCELLED'].value_counts() # note that the result of value_counts will always show the data type as an integer, as it is a count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forcing Values to Adhere to a Pattern\n",
    "\n",
    "In some cases, it might be clear from the data in a column that a particular pattern should be expected for all values, in which case it may make sense to remove or replace any values that do not adhere to this pattern. An example might be a column containing UK phone numbers. There are multiple ways to represent a UK phone number, for example `+44 7555 555 555` or `07555 555555`. To handle the situation where multiple possible formats exist, the solution is to apply a *regular expression* to handle as many cases as possible. \n",
    "\n",
    ">A **regular expression**, often abbreviated as *regex*, is a sequence of characters that defines a search pattern that can be used for matching, allowing for complex search, replace, and validation operations.\n",
    "\n",
    "**Regex** is an extensive topic, and the details of constructing **regex** patterns are beyond the scope of this lesson, but as with much in the world of data, the work has often already been done for you, and can be found on various internet websites such as [Stack Overflow](https://stackoverflow.com/), or in various searchable **regex** repositories such as [regexlib](https://regexlib.com/). Searching **regexlib** for `UK Phone Number` provides this option:\n",
    "\n",
    " `^((\\(?0\\d{4}\\)?\\s?\\d{3}\\s?\\d{3})|(\\(?0\\d{3}\\)?\\s?\\d{3}\\s?\\d{4})|(\\(?0\\d{2}\\)?\\s?\\d{4}\\s?\\d{4}))(\\s?\\#(\\d{4}|\\d{3}))?$`\n",
    "\n",
    "Which covers the majority of UK phone number variants, including area codes with brackets (e.g. `(020)`), and extensions following a `#` symbol. \n",
    "\n",
    "Let's try it out on an example column of phone numbers. In the code block below, we will create an example dataframe of phone numbers, including some invalid numbers, and then write code to apply the **regex** to each row in the column, and replace any values that do not comply with `NaN`. We will use the `str.match()` method to apply the **regex** expression, and then use logical indexing to replace the non-matching values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>0123456789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>01234 567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>+441234567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diana</td>\n",
       "      <td>0123-456-789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eva</td>\n",
       "      <td>(0123) 456789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>1234567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grace</td>\n",
       "      <td>0123456789a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hank</td>\n",
       "      <td>01234-567-890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ivy</td>\n",
       "      <td>+44 1234 567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jack</td>\n",
       "      <td>01234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name            Phone\n",
       "0    Alice       0123456789\n",
       "1      Bob     01234 567890\n",
       "2  Charlie    +441234567890\n",
       "3    Diana     0123-456-789\n",
       "4      Eva    (0123) 456789\n",
       "5    Frank       1234567890\n",
       "6    Grace      0123456789a\n",
       "7     Hank    01234-567-890\n",
       "8      Ivy  +44 1234 567890\n",
       "9     Jack            01234"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a sample dataframe\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eva', 'Frank', 'Grace', 'Hank', 'Ivy', 'Jack'],\n",
    "    'Phone': ['0123456789', '01234 567890', '+441234567890', '0123-456-789', \n",
    "              '(0123) 456789', '1234567890', '0123456789a', '01234-567-890', \n",
    "              '+44 1234 567890', '01234']\n",
    "}\n",
    "\n",
    "phone_df = pd.DataFrame(data)\n",
    "\n",
    "phone_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>0123456789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>01234 567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>+441234567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diana</td>\n",
       "      <td>0123-456-789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eva</td>\n",
       "      <td>(0123) 456789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grace</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hank</td>\n",
       "      <td>01234-567-890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ivy</td>\n",
       "      <td>+44 1234 567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jack</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name            Phone\n",
       "0    Alice       0123456789\n",
       "1      Bob     01234 567890\n",
       "2  Charlie    +441234567890\n",
       "3    Diana     0123-456-789\n",
       "4      Eva    (0123) 456789\n",
       "5    Frank              NaN\n",
       "6    Grace              NaN\n",
       "7     Hank    01234-567-890\n",
       "8      Ivy  +44 1234 567890\n",
       "9     Jack              NaN"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # We will need the `nan` constant from the numpy library to apply to missing values\n",
    "\n",
    "regex_expression = '^(?:(?:\\(?(?:0(?:0|11)\\)?[\\s-]?\\(?|\\+)44\\)?[\\s-]?(?:\\(?0\\)?[\\s-]?)?)|(?:\\(?0))(?:(?:\\d{5}\\)?[\\s-]?\\d{4,5})|(?:\\d{4}\\)?[\\s-]?(?:\\d{5}|\\d{3}[\\s-]?\\d{3}))|(?:\\d{3}\\)?[\\s-]?\\d{3}[\\s-]?\\d{3,4})|(?:\\d{2}\\)?[\\s-]?\\d{4}[\\s-]?\\d{4}))(?:[\\s-]?(?:x|ext\\.?|\\#)\\d{3,4})?$' #Our regular expression to match\n",
    "phone_df.loc[~phone_df['Phone'].str.match(regex_expression), 'Phone'] = np.nan # For every row  where the Phone column does not match our regular expression, replace the value with NaN\n",
    "phone_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Numeric Columns with `.replace()`\n",
    "\n",
    "The `.replace()` method can also be used to clean up numeric data, for example if you have a column of prices that contain the `£` symbol, thereby preventing the column from being cast to a numeric data type. \n",
    "\n",
    "In the example of the phone numbers dataframe, we still have a variety of non-numeric characters in the data which should be replaced in order to regularise the numbers. To rectify this the following actions are needed:\n",
    "\n",
    "- Replace any instances of `+44` with `0`, as this is how to write the number for calling within the UK\n",
    "- Replace the `(` and `-` characters with nothing (i.e. remove them)\n",
    "- Remove all spaces\n",
    "\n",
    "The code block below shows how to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>0123456789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>01234567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>01234567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diana</td>\n",
       "      <td>0123456789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eva</td>\n",
       "      <td>0123456789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grace</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hank</td>\n",
       "      <td>01234567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ivy</td>\n",
       "      <td>01234567890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jack</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name        Phone\n",
       "0    Alice   0123456789\n",
       "1      Bob  01234567890\n",
       "2  Charlie  01234567890\n",
       "3    Diana   0123456789\n",
       "4      Eva   0123456789\n",
       "5    Frank          NaN\n",
       "6    Grace          NaN\n",
       "7     Hank  01234567890\n",
       "8      Ivy  01234567890\n",
       "9     Jack          NaN"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can do each step one by one, for example with the following syntax for the `+44`: 0 replacement:\n",
    "\n",
    "phone_df['Phone'] = phone_df['Phone'].str.replace('+44', '0', regex=False)\n",
    "phone_df\n",
    "\n",
    "# Or by setting `regex=True`, you can do it all in one step:\n",
    "\n",
    "phone_df['Phone'] = phone_df['Phone'].replace({r'\\+44': '0', r'\\(': '', r'\\)': '', r'-': '', r' ': ''}, regex=True)\n",
    "phone_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Values\n",
    "\n",
    "It is sometimes necessary to find the number of unique values in a column. For example we might be working with a column of product IDs, where it would negatively affect our analysis to have multiple products with the same ID. To check whether an issue like this exists, we can use the methods `unique` and `nunique`.\n",
    "\n",
    "- The `unique` method returns all the unique (i.e. distinct) values in the data series. For example from a series of `[ 1, 1, 2, 3, 4]` it would return `[1, 2, 3, 4]`.\n",
    "- The `nunique` method returns the **count** of unique values in the series. For example from a series of `[ 1, 1, 2, 3, 4]` it would return `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  product_ids\n",
      "0        P001\n",
      "1        P002\n",
      "2        P003\n",
      "3        P001\n",
      "4        P004\n",
      "5        P005\n",
      "6        P003\n",
      "7        P006\n",
      "8        P002\n",
      "\n",
      "Unique product IDs:\n",
      "['P001' 'P002' 'P003' 'P004' 'P005' 'P006']\n",
      "\n",
      "Number of unique product IDs:\n",
      "6\n",
      "\n",
      "Total number of rows in the DataFrame:\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Creating a sample dataframe with a column of product IDs\n",
    "data = {'product_ids': ['P001', 'P002', 'P003', 'P001', 'P004', 'P005', 'P003', 'P006', 'P002']}\n",
    "products_df = pd.DataFrame(data)\n",
    "\n",
    "# Using `unique` to get unique product IDs\n",
    "unique_ids = products_df['product_ids'].unique()\n",
    "\n",
    "# Using `nunique` to get the number of unique product IDs\n",
    "num_unique_ids = products_df['product_ids'].nunique()\n",
    "\n",
    "# Displaying the original dataframe\n",
    "print(\"Original dataframe:\")\n",
    "print(products_df)\n",
    "\n",
    "# Displaying the unique product IDs\n",
    "print(\"\\nUnique product IDs:\")\n",
    "print(unique_ids)\n",
    "\n",
    "# Displaying the number of unique product IDs\n",
    "print(\"\\nNumber of unique product IDs:\")\n",
    "print(num_unique_ids)\n",
    "\n",
    "# Display the total number of rows in the dataframe\n",
    "print(\"\\nTotal number of rows in the dataframe:\")\n",
    "print(len(products_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "\n",
    "> *Duplicates* in data refer to two or more rows that are identical across all columns, or, depending on the context, identical in a subset of columns, which can lead to redundancy and inaccuracies in data analysis and interpretation. The presence of duplicates is a common data cleaning issue, as duplicated data can distort descriptive statistics and data visualizations, leading to inaccurate insights and misinformed decisions. For instance, duplicate entries can artificially inflate the count of a category, skewing measures of central tendency like the mean and median, and affecting the distribution of data in visual representations.\n",
    "\n",
    "Duplicates can either be *exact*, where a row is identical to another row across all columns, or *fuzzy*, which is where the two rows differ in some columns, but appear to describe the same entity. \n",
    "\n",
    "Exact duplicates are trivial to handle in `Pandas`. You can find all the duplicated rows in a dataframe using the `.duplicated()` method, or drop them using the `drop_duplicates()` method. Run the two code blocks below to generate some example duplicate data, and then use the `drop_duplicates()` method to drop the duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>28</td>\n",
       "      <td>123-456</td>\n",
       "      <td>alice@email.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>34</td>\n",
       "      <td>456-789</td>\n",
       "      <td>bob@email.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>45</td>\n",
       "      <td>789-012</td>\n",
       "      <td>charlie@email.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice</td>\n",
       "      <td>28</td>\n",
       "      <td>123-456</td>\n",
       "      <td>alice@email.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eva</td>\n",
       "      <td>23</td>\n",
       "      <td>345-678</td>\n",
       "      <td>eva@email.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>45</td>\n",
       "      <td>789-012</td>\n",
       "      <td>charlie@email.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name  Age    Phone              Email\n",
       "0    Alice   28  123-456    alice@email.com\n",
       "1      Bob   34  456-789      bob@email.com\n",
       "2  Charlie   45  789-012  charlie@email.com\n",
       "3    Alice   28  123-456    alice@email.com\n",
       "4      Eva   23  345-678      eva@email.com\n",
       "5  Charlie   45  789-012  charlie@email.com"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a sample dataframe\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Eva', 'Charlie'],\n",
    "    'Age': [28, 34, 45, 28, 23, 45],\n",
    "    'Phone': ['123-456', '456-789', '789-012', '123-456', '345-678', '789-012'],\n",
    "    'Email': ['alice@email.com', 'bob@email.com', 'charlie@email.com', \n",
    "              'alice@email.com', 'eva@email.com', 'charlie@email.com']\n",
    "}\n",
    "duplicate_df = pd.DataFrame(data)\n",
    "duplicate_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame After Removing Duplicates:\n",
      "      Name  Age    Phone              Email\n",
      "0    Alice   28  123-456    alice@email.com\n",
      "1      Bob   34  456-789      bob@email.com\n",
      "2  Charlie   45  789-012  charlie@email.com\n",
      "4      Eva   23  345-678      eva@email.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identifying and dropping exact duplicates\n",
    "df_no_duplicates = duplicate_df.drop_duplicates()\n",
    "\n",
    "# Displaying the dataframe after removing duplicates\n",
    "print(\"\\ndataframe After Removing Duplicates:\")\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible existence of **fuzzy** duplicates can present a greater challenge. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First_Name</th>\n",
       "      <th>Last_Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Smith</td>\n",
       "      <td>28</td>\n",
       "      <td>123-456</td>\n",
       "      <td>alice@email.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Smith</td>\n",
       "      <td>34</td>\n",
       "      <td>456-789</td>\n",
       "      <td>alice@smith.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Smith</td>\n",
       "      <td>45</td>\n",
       "      <td>123-456</td>\n",
       "      <td>alice@theinternet.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Smith</td>\n",
       "      <td>45</td>\n",
       "      <td>123-456</td>\n",
       "      <td>Alice@theinternet.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  First_Name Last_Name  Age    Phone                  Email\n",
       "0      Alice     Smith   28  123-456        alice@email.com\n",
       "1      Alice     Smith   34  456-789        alice@smith.com\n",
       "2      Alice     Smith   45  123-456  alice@theinternet.com\n",
       "3      Alice     Smith   45  123-456  Alice@theinternet.com"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'First_Name': ['Alice', 'Alice', 'Alice',  'Alice'],\n",
    "    'Last_Name': ['Smith', 'Smith', 'Smith', 'Smith'],\n",
    "    'Age': [28, 34, 45, 45],\n",
    "    'Phone': ['123-456', '456-789', '123-456', '123-456'],\n",
    "    'Email': ['alice@email.com', 'alice@smith.com', \n",
    "              'alice@theinternet.com',  'Alice@theinternet.com']\n",
    "}\n",
    "\n",
    "fuzzy_duplicates_df = pd.DataFrame(data)\n",
    "fuzzy_duplicates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, each column is a partial match for all the other columns. Handling this kind of partial (**fuzzy**) matching requires consideration of what each column represents. For example the name `Alice Smith` is relatively common in English-speaking countries, so we should not assume that all of the entries are the same person. Meanwhile ages can be mistyped, and it's even possible that there are multiple people called Alice Smith at the same address, sharing the same phone number. On the other hand row IDs `2` and `3` are probably the same person, given that the only difference is the capitalisation in the `Email` column.\n",
    "\n",
    "How you choose to handle **fuzzy** duplicates depends on your analysis goals. In some cases it might make sense to take a conservative approach to avoid losing data unnecessarily, whereas in other cases it might be necessary to apply more stringent measures to avoid duplicates. You will also need to decide whether to simply drop the **fuzzy** duplicates you identify, or average the values of certain columns (e.g. a product price column).\n",
    "\n",
    "Generally, the more columns you have for reference, the easier it is to determine whether a partial match is a duplicate. There are also software tools to help you, for example this python [library](https://pypi.org/project/fuzzywuzzy/) that uses the [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) to calculate the differences between strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "\n",
    "> Categorical columns are those in which the values are drawn from a predefined set of categories. Examples include the possible colour schemes of a product like a laptop, the country of residence of a customer, or the manufacturer of a car.  We will learn more about the different types of categories elsewhere in this course. \n",
    "\n",
    "In the context of data cleaning, the thing that the categorical column is describing can inform you as to whether the column requires cleaning. Ideally you want each type of thing that the column is describing to have a single entry in the set of unique values in the column. For example, when dealing with countries, there should be somewhere in the region of 193 to 237 possible entries, depending on the definition of the word \"country\" that is being used. It is possible for the same country to go by multiple names in your column, in which case it would be helpful to regularise the names such that each country is represented by only a single value in the column. \n",
    "\n",
    "Look at the dataframe below. The column `postal region` contains the country names `UK`, `England`, `Wales`, `Cymru` and `Scotland`, among others. For the purposes of the cost of sending mail, these are all one region, the `United Kingdom`. It would therefore be preferable to set them all to the same value. As with missing values, we can fix this representation with the `.replace()` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Customer Name Postal Region\n",
      "0         Amina            UK\n",
      "1         Bahru       England\n",
      "2       Charlie         Wales\n",
      "3          Dion         Cymru\n",
      "4           Ebo      Scotland\n",
      "5         Frank           USA\n",
      "6         Giana        Canada\n"
     ]
    }
   ],
   "source": [
    "# Creating a sample dataframe\n",
    "data = {\n",
    "    'Customer Name': ['Amina', 'Bahru', 'Charlie', 'Dion', 'Ebo', 'Frank', 'Giana'],\n",
    "    'Postal Region': ['UK', 'England', 'Wales', 'Cymru', 'Scotland', 'USA', 'Canada']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Displaying the original dataframe\n",
    "print(\"Original dataframe:\")\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame After Cleaning 'Postal Country' Column:\n",
      "  Customer Name  Postal Country\n",
      "0         Amina  United Kingdom\n",
      "1         Bahru  United Kingdom\n",
      "2       Charlie  United Kingdom\n",
      "3          Dion  United Kingdom\n",
      "4           Ebo  United Kingdom\n",
      "5         Frank             USA\n",
      "6         Giana          Canada\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping dictionary to unify the country names\n",
    "country_mapping = {\n",
    "    'UK': 'United Kingdom',\n",
    "    'England': 'United Kingdom',\n",
    "    'Wales': 'United Kingdom',\n",
    "    'Cymru': 'United Kingdom',\n",
    "    'Scotland': 'United Kingdom'\n",
    "}\n",
    "\n",
    "# Replacing the country names in the 'Postal Country' column\n",
    "df['Postal Region'] = df['Postal Region'].replace(country_mapping)\n",
    "\n",
    "# Displaying the DataFrame after cleaning the 'Postal Country' column\n",
    "print(\"\\nDataFrame After Cleaning 'Postal Country' Column:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Categorical Columns from Continuous Data\n",
    "\n",
    "Continuous variables are those which can take any value on a spectrum. For example the price of an item can potentially take any value greater than zero, usually with a granularity of 1 penny. You might meet instances in your dataset where you might want to generate new categories based on continuous data.  This can be achieved through a process called *binning*, which means dividing the spectrum of possible values into regions, known as **bins**. \n",
    "\n",
    "As an example, consider a dataframe of flight routes, together with their distances in miles. An airline might want to divide them into `short haul`, `medium haul` and `long haul` based on threshold values. To achieve this, we can use the `cut()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     Route  Distance\n",
      "0  NYC-LON      3461\n",
      "1  LON-PAR       214\n",
      "2  NYC-TOK      6749\n",
      "3  LON-SYD     10562\n",
      "4  PAR-BER       546\n",
      "\n",
      "DataFrame with 'Flight Type' Column:\n",
      "     Route  Distance  Flight Type\n",
      "0  NYC-LON      3461  medium haul\n",
      "1  LON-PAR       214   short haul\n",
      "2  NYC-TOK      6749    long haul\n",
      "3  LON-SYD     10562    long haul\n",
      "4  PAR-BER       546   short haul\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a sample dataframe\n",
    "data = {\n",
    "    'Route': ['NYC-LON', 'LON-PAR', 'NYC-TOK', 'LON-SYD', 'PAR-BER'],\n",
    "    'Distance': [3461, 214, 6749, 10562, 546]\n",
    "}\n",
    "flights = pd.DataFrame(data)\n",
    "\n",
    "# Displaying the original dataframe\n",
    "print(\"Original dataframe:\")\n",
    "print(flights)\n",
    "\n",
    "# Defining the bin edges and labels\n",
    "bin_edges = [0, 1500, 4000, 12000]  # in miles\n",
    "bin_labels = ['short haul', 'medium haul', 'long haul']\n",
    "\n",
    "# Creating a new categorical column 'Flight Type' by binning the 'Distance' column\n",
    "flights['Flight Type'] = pd.cut(flights['Distance'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "# Displaying the dataframe with the new 'Flight Type' column\n",
    "print(\"\\ndataframe with 'Flight Type' Column:\")\n",
    "print(flights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example example:\n",
    "- We create a dataframe `flights` with columns `Route` and `Distance`, containing various flight routes and their distances in miles\n",
    "- We define the bin edges `bin_edges` and labels `bin_labels` to specify the ranges and labels for our new categorical data. Note that there is one more element in the `bin edges` list than the number of bins we need. The bin edges define the lower and upper bounds of each bin. So a short haul flight will be from `0` to `1500` miles in this case.\n",
    "- We use `pd.cut()` to create a new column `Flight Type` by binning the `Distance` column based on the defined bins and labels. The argument `right = False` is used to specify that the bins are *left-closed*, meaning that the left bin edge is included in the bin, but the right bin edge is not.\n",
    "- Finally, we display the original and modified dataframes to observe the changes\n",
    "\n",
    "This approach allows you to categorize continuous data into discrete bins, simplifying analysis and enabling you to gain insights into the distribution and frequency of the data across different categories. This is particularly useful when you want to analyze or visualize your data at a higher or more generalized level than the raw, continuous data allows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can check the data type of your dataframe columns using the `dtypes` attribute or the `.info()` method\n",
    "- You can change the data type of the column using the `astype()` method, and convert from string to numeric information using the `to_numeric()` method\n",
    "- The `pd.Datetime` function is used to convert a column to the `datetime64` data type\n",
    "- Common issues with converting to `datetime64` include `Pandas` assuming an integer representation is encoding **Epoch** time, and handling columns with multiple date formats\n",
    "- The `timedelta64` data type is used to represent time differences between datetime objects\n",
    "- The `.dropna()` and `.fillna()` methods can be used to handle `NaN` and `NULL` values\n",
    "- The `.replace()` method can be used to handle incorrect values\n",
    "- You can force a column's values to stick to a pattern using **regular expressions**\n",
    "- The `.unique()` and `.nunique()` methods can be used to identify and count the unique values in a column\n",
    "- **Exact** duplicates can be removed using the `.drop_duplicates()` method, whereas **fuzzy** duplicates might need more advanced methods to identify and handle\n",
    "- **Categorical** data should be **regularised**, so that each relevant entity is described by a single category label\n",
    "- You can create **categorical** data from **continuous** data by **binning**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
