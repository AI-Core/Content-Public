{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Classification is the problem of predicting one of a discrete number of outputs, rather than predicting something continuous.\n",
    "\n",
    "> Classification and regression are the two types of predictions that can be made by machine learning models\n",
    "\n",
    "For example, predicting whether an image contains a dog or not is a classification task. In that problem the images with a dog in would have a label equal to `1` and images without a dog in would have a label `0`. These numbers represent the index of the classification (class) which the example belogs to.\n",
    "\n",
    "The simplest case for classification is _binary classification_\n",
    "\n",
    "> Binary classification is where examples can be a member of only one or two classes\n",
    "\n",
    "In binary classification, examples belonging to the class index `0` are said to belong to the _negative class_.\n",
    "Examples belonging to the class index `1` are said to belong to the _positive class_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression, the output of our model (our target to predict) could be any real number: negative or positive, unbounded in magnitude. \n",
    "In the case of classification, however, the targets are discrete labels that represent the index of a particular class.\n",
    "\n",
    "> In classification, the model output can be interpreted as a degree of confidence that the example belongs to a particular class. That is, a probability between 0 and 1.\n",
    "\n",
    "Let's run some code to generate a fake binary classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_binary_data(m=50): \n",
    "    X = np.random.randn(m) #sample from a normal distribution\n",
    "    X = np.array(sorted(X))\n",
    "    Y = X > 0.2    #return the binary vector with true if X is above some threshold and false if below\n",
    "    return X, Y #returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', marker='x')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "X, Y = make_binary_data()\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> These refer to vectors (or scalar in the case of a binary) of unnormalised probabilities in the range $(-\\infty, \\infty)$.\n",
    "\n",
    "You may have noticed that the output is the same as __that obtained using regression analysis__.\n",
    "\n",
    "Logits are similar to probabilities, with the following specific relations:\n",
    "- __50% chance (probability=$0.5$) is equal to logit=$0$.__\n",
    "- Below-zero values indicate a low probability, while above-zero values indicate a high probability.\n",
    "\n",
    "Logits are used for classification for the following reasons:\n",
    "- To prevent unnecessary operations that transform them into probabilities if we are after the label (we will see the transformation shortly).\n",
    "- For binary classification, every output from the classifier that is greater than `1` is considered `True`, while any input below is `False`.\n",
    "\n",
    "__Note:__ Mathematically, the term, logit, refers to the log of the odds of a probability, and it __transforms probability (p below) back into the $(-\\infty, \\infty)$ range__.\n",
    "\n",
    "It has the following general formula:\n",
    "\n",
    "$$\n",
    "    L = \\ln \\frac{p}{1 - p}\n",
    "$$\n",
    "\n",
    "However, we will employ the term, logit, later to simply refer to the values outputted by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Converting Logits into Probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logits can be converted into probabilities by applying a **sigmoid** function to the output:\n",
    "\n",
    "<p align=center><img src=images/sigmoid.jpg width=1000></p>\n",
    "\n",
    "> The sigmoid function squashes the logits from the $(-\\infty, \\infty)$ to $(0, 1)$ range, which can be easily interpreted.\n",
    "\n",
    "Additionally, we can write a function to compute the derivative of the sigmoid function.\n",
    "This is required to differentiate the loss with respect to the model parameters, as they only affect the loss through the sigmoid.\n",
    "\n",
    "__Note:__ The sigmoid is the inverse function of the logit.\n",
    "\n",
    "<p align=center><img src=images/sigmoid_deriv.jpg width=1000></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function for Binary Classification: Binary Cross Entropy (BCE)\n",
    "\n",
    "The basic formula for BCE is\n",
    "\n",
    "$$\n",
    "    BCE = - (y\\ln\\hat{y} + (1-y)\\ln(1-\\hat{y}))\n",
    "$$\n",
    "\n",
    "The formula can be broken down into parts exclusively:\n",
    "- If the label is `1`, $(1-y)\\ln(1-\\hat{y})$ is `0`.\n",
    "- If the label is `0`, $y\\ln\\hat{y}$ is `0`.\n",
    "\n",
    "<p align=center><img src=images/bce.jpg width=1000></p>\n",
    "\n",
    "### Calculating One Part Per Label\n",
    "\n",
    "It is possible to calculate one part for the `1` label and the second for the `0` label with, for example, the `if label == 0` switch. However, occasionally, in ML and deep learning, it is preferable to use a technique called `label smoothing`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Smoothing\n",
    "\n",
    "> Label smoothing changes the `{0, 1}` labels to 'soft targets', e.g. `{0.1, 0.9}`.\n",
    "\n",
    "### Reasons for application\n",
    "\n",
    "- In practice, it is impossible to reach the `1` value for sigmoid. Therefore, even if the predicted probability is `0.99999`, some loss will remain.\n",
    "- This is also the case for `0` as it is quite difficult for the classifier to reach this exact value.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "- There are many implementation approaches. Conventionally, we subtract a constant from the largest labels (`1`) and add it to the `0` labels (for the binary case).\n",
    "- $\\alpha$ is the __smoothing hyperparameter__, usually around `0.1`.\n",
    "\n",
    "### Use cases and benefits\n",
    "\n",
    "- The classifier is __less confident__ about its predictions: Due to the inherent noise in the data, label smoothing is often desirable, particularly for more complex and powerful models, e.g. neural networks.\n",
    "- Predictions are smooth and gradual: Instead of having rough jumps from the `0.999` probability to the `0.0001` probability in another case, our algorithms attempt to distribute the probability more evenly.\n",
    "- Classifier exhibits significantly __low accuracy__ for hard examples: Since the value has to reach `0.9` instead of `1`, `0.8` will also be acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCE implementation\n",
    "\n",
    "We begin with a naive implementation; simply follow the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(prediction, label):\n",
    "    return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47407698 0.47407698 0.69314718]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "prediction = sigmoid(np.array([-0.5, 0.5, 0])) \n",
    "labels = np.array([0, 1, 1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as is conventional, the __numerical instability emerges gradually__. As a rule of thumb, always inspect `exp` and its inverse function, `ln`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([1])\n",
    "labels = np.array([0])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([0])\n",
    "labels = np.array([1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n",
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([0])\n",
    "labels = np.array([0])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n",
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([1])\n",
    "labels = np.array([1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- If the model __is highly confident and wrong__ (first two cases), we are left with  `np.log(0)`, which goes towards $-\\infty$ (when we take `-`, we observe $\\infty$).\n",
    "- If the model __is highly confident and right__ (last two cases), we are left with $ 0 * \\ln{0} $ and, consequently, $ 0 * \\infty $, which is an undefined `NaN` (not a number) type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable BCE\n",
    "\n",
    "A possible method to stabilise the BCE loss is to combine the activation we saw earlier (`sigmoid`) and the `binary cross entropy`.\n",
    "\n",
    "### Formulation\n",
    "\n",
    "A numerically stable version can be derived. Note that we have skipped a few steps to save time. You can view the formula that we will use [here](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    BCE & = - (y\\ln\\hat{y} + (1-y)\\ln(1-\\hat{y})) \\\\\n",
    "        & = - (y\\ln(\\frac{1}{1+e^{-x}}) + (1-y)\\ln(1-\\frac{1}{1+e^{-x}}) \\\\\n",
    "        & ... \\\\\n",
    "        & = x - xy + \\ln(1 + e^{-x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Unfortunately, this leaves us with $e^{-x}$ once more. After taking the required mathematical steps, we arrive at this formula:\n",
    "\n",
    "$$\n",
    "    \\max(0, x) - xy + \\ln(1 + e^{-|x|}),\n",
    "$$\n",
    "\n",
    "where `x` refers to logit and `y` to labels.\n",
    "\n",
    "Evidently, regardless of the `x` value, a negative value will be obtained. Thus, `e` can only underflow, which is fine for our case (and $\\ln(1 + 0) = 0$). We urge you to explore the other formulas [here](https://discuss.pytorch.org/t/numerical-stability-of-bcewithlogitsloss/8246).\n",
    "\n",
    "### Derivative\n",
    "\n",
    "Furthermore, we have to calculate the derivative of `BCEWithLogits` to backpropagate through our model.\n",
    "\n",
    "Recall that you can always use [Wolfram Alpha](https://www.wolframalpha.com/) for such laborious tasks or, at least, to have an idea or a direction.\n",
    "\n",
    "Given that, the derivative is presented as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial BCE}{\\partial x} &= \\sigma(x) - y \\\\\n",
    "    \\frac{\\partial BCE}{\\partial y} &= -x\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This BCE will work directly on the `logits`.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Binary Classifier Model Class\n",
    "\n",
    "For this task, we will use the chain rule to compute the gradient with the previously defined loss function.\n",
    "\n",
    "\n",
    "<p align=center><img src=images/binary_classification.jpg width=1000></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has several different classification algorithms. Here, we will use the `LogisticRegression` class from `sklearn.linear_model`, which is one of the most popular and simple ones. We will explore more algorithms eventually; however, note that they all share a similar implementation procedure.\n",
    "\n",
    "To test it, we will use the breast-cancer dataset from `sklearn.datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded and split our data, the next step involves building and training our model. As with linear regression, we train the instance of the model on the training set and thereafter predict the labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9532163742690059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivanyingx/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the utilised solver did not converge. This is attributed to the simplicity of the dataset. Therefore, as stated in the sklearn documentation, we can choose a different solver.\n",
    "\n",
    "- For small datasets, `liblinear` is a good choice, whereas `sag` and `saga` are relatively fast for large ones.\n",
    "- For multiclass problems, only `newton-cg`, `sag`, `saga` and `lbfgs` can handle the multinomial loss.\n",
    "- The `liblinear` solver is limited to one-versus-rest schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the `liblinear` solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Not only did the algorithm converge, but also the accuracy improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "At this point, you should have a good understanding of\n",
    "\n",
    "- the labels for classification problems.\n",
    "- `logits` and `sigmoid`.\n",
    "- naive implementation of `sigmoid` and its more stable versions.\n",
    "- BCE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
