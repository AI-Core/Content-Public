{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting and Adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">The boosting ensemble method combines a sequence of weak classifiers that are fit on successively modified versions of a dataset. This method increasingly prioritises the examples misclassified by the previous model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bagging vs Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Not all ensemble methods are designed to regularise the overall model. In boosting, ensembling is employed to increase the capacity of individual models.\n",
    "\n",
    "Similar to bagging, boosting creates an ensemble of weak learners (models that do more than make random guesses) to form a single strong learner.\n",
    "Bagging simply combines the predictions of different models that were fit to the same dataset independently (trained in parallel).\n",
    "Conversely, boosting combines the predictions of different models that were fit depending on the performance of the previous model (trained in sequence).\n",
    "\n",
    "<p align=center><img width=900 src=images/bagging_vs_boosting.jpg></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## AdaBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Boosting algorithms vary in how they adjust the weights of the examples that are sampled in each successive bootstrapped dataset and in how they weight the contribution of each hypothesis to the final prediction.\n",
    "AdaBoost is the most popular boosting algorithm that is employed for classification problems. The name AdaBoost is short for **adaptive boosting**, and it is a classification algorithm.\n",
    "\n",
    "In AdaBoost, the example labels are coded as $Y=-1$ and $Y=+1$ for examples in the negative and positive classes, respectively.\n",
    "\n",
    "Furthermore, each model is a significantly weak classification tree with a depth of 1. Such limited trees are called 'stumps'.\n",
    "AdaBoost converts many 'weak learners' into a single 'strong learner' by combining these stumps. Furthermore, it combines the predictions of all of the classifiers to make a final prediction by evaluating **the sign** term:\n",
    "\n",
    "<p align=center><img width=900 src=images/adaboost_hypothesis.jpg></p>\n",
    "\n",
    "This is simply the sign of a weighted combination of predictions.\n",
    "\n",
    "If the sign is positive, the example will be classified as a member of the positive class; otherwise, it will be classified as a member of the negative class. Intuitively, this indicates that the predictions of the models in the boosting sequence push or pull the hypothesis over the point where the decision boundary lies, i.e at zero, with the prediction from each model being scaled by that model's weight, $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The origin of the weights for each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The models are applied sequentially, and due to their limited capacity, they are each likely to make a mistake.\n",
    "\n",
    "The error of a model is calculated as follows:\n",
    "\n",
    "<p align=center><img width=900 src=images/err_l.jpg></p>\n",
    "\n",
    "Subsequently, the weight of the prediction of each model is computed based on the error rate given by\n",
    "\n",
    "<p align=center><img width=900 src=images/boosting_model_weight.PNG></p>\n",
    "\n",
    "A high negative model weight is indicative of a poor-performing model.\n",
    "A high positive model weight is indicative of a high-performing model.\n",
    "A zero model weight is indicative of a moderately performing model, i.e. the model is equivalent to one that makes random guesses.\n",
    "\n",
    "The weights of each example increase if they were incorrectly classified by the previous model and decrease if they were classified correctly.\n",
    "\n",
    "For the first model in the sequence, the importance of classifying each example correctly is equal. That is, we weight the error contribution for each example in the dataset by the same amount, $w_i= \\frac{1}{m}$.\n",
    "For the next weighted sample from the dataset, to sample the bootstrapped dataset for the next model in the sequence to be trained on, we set the weight of each example to the following:\n",
    "\n",
    "<p align=center><img width=900 src=images/boosting_example_weight.jpg></p>\n",
    "\n",
    "Next, we consider what this means for a variety of cases.\n",
    "- Positive model weight and correct classification: weight of the example pushed down.\n",
    "- Negative model weight and correct classification: weight of the example pushed up.\n",
    "- Positive model weight and incorrect classification: weight of the example pushed up.\n",
    "- Negative model weight and incorrect classification: weight of the example pushed down.\n",
    "\n",
    "**Note**: Even though a model in a certain position in the boosting sequence may not fit every example in the dataset (because they may not all be chosen to be a member of the training sample), the weights for **every** example are updated based on whether the model performs a correct classification.\n",
    "If this were not the case, the weights of examples that were predicted correctly by previous models (which are unlikely to be sampled as the training data for any subsequent model) would not be updated later.\n",
    "This prevents us from increasingly focusing on misclassified examples and losing sight of the big picture, i.e. achieving a high performance for all examples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The role of this weight calculation\n",
    "Most examples can be correctly classified by very simple, weak classifier stumps. It is the edge cases that require extra attention.\n",
    "Therefore, sequentially, the importance of examples that could not be correctly classified by the previous model is increased and vice versa.\n",
    "Thus, models later in the sequence focus on examples that are difficult to classify. As the depth increases, the importance of easy-to-classify examples diminishes, tending towards zero.\n",
    "This effectively removes them from the dataset, leaving fewer examples for the later models to classify. The few examples can be separated with a relatively simple decision boundary.\n",
    "\n",
    "The weighting of each model prediction serves to increase the influence of the models that correctly classify examples from the bootstrapped dataset on which they are trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adaboost Algorithm Outline\n",
    "\n",
    "- Initialise each example weight as $\\frac{1}{m}$.\n",
    "- For however many models in your boosting sequence,\n",
    "    - create a bootstrapped dataset by taking a sample from the original dataset, weighted by the example weights.\n",
    "    - fit the model on this bootstrapped dataset.\n",
    "    - compute the proportion of incorrect predictions weighted by the corresponding example weights.\n",
    "    - use this to compute the model weight.\n",
    "    - increase the example weight of poorly predicted examples, and decrease the example weight of well-predicted examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download the necessary package to run the next cells\n",
    "!wget \"https://aicore-files.s3.amazonaws.com/Data-Science/data_utils/get_colors.py\" \"https://aicore-files.s3.amazonaws.com/Data-Science/data_utils/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "from utils import get_classification_data, calc_accuracy, visualise_predictions, show_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def encode_labels(labels):\n",
    "    labels[labels == 0] = -1\n",
    "    labels[labels == 1] = +1\n",
    "    return labels\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_layers=20):\n",
    "        self.n_layers = n_layers\n",
    "        self.models = [] # init empty list of models\n",
    "\n",
    "    def sample(self, X, Y, weights):\n",
    "        idxs = np.random.choice(range(len(X)), size=len(X), replace=True, p=weights)\n",
    "        X = X[idxs]\n",
    "        Y = Y[idxs]\n",
    "        return X, Y\n",
    "\n",
    "    def calc_model_error(self, predictions, labels, example_weights):\n",
    "        \"\"\"Compute the classifier error rate\"\"\"\n",
    "        diff = predictions != labels\n",
    "        diff = diff.astype(float)\n",
    "        diff *= example_weights\n",
    "        diff /= np.sum(example_weights)\n",
    "        return np.sum(diff)\n",
    "\n",
    "    def calc_model_weight(self, error, delta=0.01):\n",
    "        z = (1 - error) / (error + delta) + delta\n",
    "        return 0.5 * np.log(z)\n",
    "\n",
    "    def update_weights(self, predictions, labels, model_weight):\n",
    "        weights = np.exp(- model_weight * predictions * labels)\n",
    "        weights /= np.sum(weights)\n",
    "        return weights\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        example_weights = np.full(len(X), 1/len(X)) # assign initial importance of classifying each example as uniform and equal\n",
    "        for layer_idx in range(self.n_layers):\n",
    "            model = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "            bootstrapped_X, bootstrapped_Y = self.sample(X, Y, example_weights)\n",
    "            model.fit(bootstrapped_X, bootstrapped_Y)\n",
    "            predictions = model.predict(X) # make predictions for all examples\n",
    "            model_error = self.calc_model_error(predictions, Y, example_weights)\n",
    "            model_weight = self.calc_model_weight(model_error)\n",
    "            model.weight = model_weight\n",
    "            self.models.append(model)\n",
    "            example_weights = self.update_weights(predictions, Y, model_weight)\n",
    "            # print(f'trained model {layer_idx}')\n",
    "            # print()\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            prediction += model.weight * model.predict(X)\n",
    "        prediction = np.sign(prediction) # comment out this line to visualise the predictions in a more interpretable way\n",
    "        return prediction\n",
    "\n",
    "    def __repr__(self):\n",
    "        return json.dumps([m.weight for m in self.models])\n",
    "        return json.dumps([\n",
    "            {\n",
    "                'weight': model.weight\n",
    "            }\n",
    "            for model in self.models\n",
    "        ], indent=4)\n",
    "\n",
    "X, Y = get_classification_data(sd=1)\n",
    "Y = encode_labels(Y)\n",
    "adaBoost = AdaBoost()\n",
    "adaBoost.fit(X, Y)\n",
    "predictions = adaBoost.predict(X)\n",
    "print(f'accuracy: {calc_accuracy(predictions, Y)}')\n",
    "visualise_predictions(adaBoost.predict, X, Y)\n",
    "show_data(X, Y)\n",
    "print(adaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.add_subplot(211)\n",
    "X, Y = get_classification_data(variant='circles')\n",
    "\n",
    "for i in range(20):\n",
    "    adaBoost = AdaBoost(n_layers=i)\n",
    "    adaBoost.fit(X, Y)\n",
    "    predictions = adaBoost.predict(X)\n",
    "    print(f'model {i}')\n",
    "    print(f'accuracy: {calc_accuracy(predictions, Y)}')\n",
    "    print(f'weights: {[ round(m.weight, 2) for m in adaBoost.models]}')\n",
    "    visualise_predictions(adaBoost.predict, X, Y)\n",
    "    # show_data(X, Y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "adaBoost = sklearn.ensemble.AdaBoostClassifier()\n",
    "adaBoost.fit(X, Y)\n",
    "predictions = adaBoost.predict(X)\n",
    "calc_accuracy(predictions, Y)\n",
    "visualise_predictions(adaBoost.predict, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have a good understanding of\n",
    "\n",
    "- how to implement a boosted model: Adaboost.\n",
    "- boosting and how to apply it to decision trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('main': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   },
   "name": "Python 3.8.6 64-bit ('main': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
