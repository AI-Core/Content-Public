{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements about XGBoost are true? Select all that apply.\n",
    "\n",
    "- XGBoost is a type of ensemble method ***\n",
    "- XGBoost is only suitable for classification problems, not for regression problems\n",
    "- XGBoost is a type of unsupervised learning method\n",
    "- XGBoost selects the best split points during tree construction using a greedy algorithm ***\n",
    "- XGBoost can measure the importance of each feature in the original dataset ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements about the parameters of XGBoost is false?\n",
    "\n",
    "- The `learning rate` parameter controls the weights given to incorrectly classified examples ***\n",
    "- The `max_depth` parameter controls the maximum depth of each decision tree in the ensemble\n",
    "- The `subsample` parameter controls the percentage of training examples used to build each tree\n",
    "- The `n_estimators` parameter controls the maximum number of decision trees in the ensemble\n",
    "- The `regularization` parameter controls the complexity of each decision tree in the ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does pruning work in XGBoost? Select all that apply.\n",
    "\n",
    "- Pruning reduces the number of input data taken by the model when making predictions\n",
    "- Pruning is a technique used to reduce the complexity of a decision tree by removing nodes that do not contribute to the final prediction ***\n",
    "- The tree is pruned based on the similarity scores between the nodes or leaves of the tree ***\n",
    "- Pruning reduces the number of hyperparameters that need to be tuned in XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the lambda parameter in XGBoost control the complexity of the model?\n",
    "\n",
    "- By adding a penalty term to the objective function being optimized\n",
    "- By increasing the number of trees in the model\n",
    "- By decreasing the learning rate of the model\n",
    "- By reducing the depth of each tree in the model\n",
    "- By increasing the size of the training data used to fit the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the differences between XGBoost and regular gradient boosting? Select all that apply.\n",
    "\n",
    "- XGBoost uses a greedy algorithm ***\n",
    "- XGBoost uses decision trees as weak learners, while regular gradient boosting can use any type of weak learner\n",
    "- XGBoost optimizes the objective function by approximating the second derivative, while regular gradient boosting only approximates the first derivative\n",
    "- XGBoost is generally faster than regular gradient boosting ***\n",
    "- There is no difference between XGBoost and regular gradient boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What class in `sklearn` is used to implement XGBoost?\n",
    "\n",
    "- `sklearn.ensemble.XGBoostClassifier`\n",
    "- `sklearn.ensemble.XGBoostRegressor`\n",
    "- `sklearn.ensemble.XGBoost`\n",
    "- `sklearn.ensemble.GradientBoostingClassifier`\n",
    "- `sklearn.ensemble.GradientBoostingRegressor`\n",
    "- `sklearn` does not have a class to implement XGBoost ***\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
