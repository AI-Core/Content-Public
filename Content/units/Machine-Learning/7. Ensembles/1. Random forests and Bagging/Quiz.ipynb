{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests and Bagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following are ensemble methods? Select all that apply.\n",
    "\n",
    "- Random forests ***\n",
    "- Gradient boosting ***\n",
    "- Decision trees\n",
    "- Support vector machines\n",
    "- Logistic regression\n",
    "- AdaBoost ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the main idea behind ensemble methods? Select all that apply.\n",
    "\n",
    "- To combine the predictions of several models to improve generalisation or performance ***\n",
    "- To create a model that is more interpretable than the individual constituent models\n",
    "- To create a model that performs better than the individual constituent models ***\n",
    "- To reduce the variance of a black-box model like a neural network\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does _bootstraping_ refer to?\n",
    "\n",
    "- A method for fitting a decision tree\n",
    "- A method for training a model with different combinations of hyperparameters\n",
    "- A method for taking random samples from a dataset with replacement ***\n",
    "- A method for taking random samples from a dataset without replacement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## What does bagging refer to?\n",
    "\n",
    "- Bagging is a method for fitting a decision tree\n",
    "- Bagging is a method for training a model with different combinations of hyperparameters\n",
    "- Bagging is a method that bootstraps the data and then fits a model to each bootstrap sample\n",
    "- Bagging is a method that bootstraps the data,fits a model to each bootstrap sample, and then aggregates the predictions ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements are true about weak learners in ensemble methods? Select all that apply.\n",
    "\n",
    "- They have low variance and they don't usually overfit ***\n",
    "- They have high bias, so they can not solve hard learning problems ***\n",
    "- They have high variance and they don't usually overfit\n",
    "- They have low bias, so they can solve hard learning problems\n",
    "- They are not very accurate on their own ***\n",
    "- Different learners can come from same algorithm with different hyper parameters ***\n",
    "- Different learners can come from different algorithms ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements are true about ensemble methods? Select all that apply.\n",
    "\n",
    "- They will yield bad results when there is significant diversity among the models\n",
    "- Ensemble learning can only be applied to supervised learning methods.\n",
    "- They provide a better performance than the individual models ***\n",
    "- Ensemble classifiers that are more “sure” can vote with more conviction ***\n",
    "- You need to tune the same hyperparameters for each base model\n",
    "- An ensemble method works better if the individual base models have high lower correlations with each other ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements are true about random forests? Select all that apply.\n",
    "\n",
    "- They can be used fot classification and regression problems ***\n",
    "- They handle real values attributes by discretising them ***\n",
    "- They are boosting methods\n",
    "- They are bagging methods ***\n",
    "- They provide a good level of interpretability\n",
    "- If use 3 estimators and a `max_depth` of 2, assuming each estimator has an accuracy of 70%, the minimum accuracy is 70%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In `sklearn`, which of the following codes will create a random forest classifier with 100 trees and a `max_depth` of 5?\n",
    "\n",
    "- This code \n",
    "```python\n",
    "from sklearn.tree import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "```\n",
    "- This code \n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "rf = DecisionTreeClassifier(n_estimators=100, max_depth=5, ensemble='random_forest')\n",
    "```\n",
    "- This code  ***\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "```\n",
    "- This code \n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, ensemble='random_forest')\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using random forests, can you obtain how important each feature is for the model?\n",
    "\n",
    "- Yes, using the `feature_importances_` attribute ***\n",
    "- Yes, shuffling the data and comparing the accuracy\n",
    "- No, because random forests are not interpretable\n",
    "- No, because the output of random forests is a probability\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
