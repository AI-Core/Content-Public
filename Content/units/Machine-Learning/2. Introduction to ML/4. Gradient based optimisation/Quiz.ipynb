{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Based Optimisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements are true about the gradient? Select all that apply.\n",
    "\n",
    "- The gradient can't be equal to zero\n",
    "- The gradient is the slope of the tangent line at a particular point ***\n",
    "- For a linear function, the gradient has a constant value ***\n",
    "- A gradient needs to cross the origin to be valid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why would you want to calculate the gradient of a function?\n",
    "\n",
    "- To find the minimum value of the output. If we calculate the gradient of the loss function, we can see where the loss is the lowest ***\n",
    "- To find the maximum value of the output. If we calculate the gradient of the loss function, we can see where the loss is the highest\n",
    "- To check if the function is linear. If the gradient is constant, the function is linear\n",
    "- To check if the function is quadratic. If the gradient is constant, the function is quadratic\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the gradient of the following function?\n",
    "\n",
    "```\n",
    "x^2 + 2x + 1\n",
    "```\n",
    "\n",
    "- 2x + 2 ***\n",
    "- 2x + 1\n",
    "- 2x\n",
    "- 2\n",
    "- We can't calculate the gradient of this function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What value of x minimises the output of the following function?\n",
    "\n",
    "```\n",
    "x^2 + 2x + 1\n",
    "```\n",
    "\n",
    "- 0\n",
    "- 1\n",
    "- -1 ***\n",
    "- 2\n",
    "- -2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When we perform gradient descent, what are we deriving the loss function with respect to? Select all that apply.\n",
    "\n",
    "- The input data\n",
    "- The output data\n",
    "- The weights of the model ***\n",
    "- The bias of the model ***\n",
    "- The learning rate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's say we are looking for the minimum value of a loss function. Which of the following statements are true? Select all that apply.\n",
    "\n",
    "- If the gradient is positive, we should move in the direction of the positive gradient\n",
    "- If the gradient is positive, we should move in the direction of the negative gradient ***\n",
    "- If the gradient is negative, we should move in the direction of the positive gradient ***\n",
    "- If the gradient is negative, we should move in the direction of the negative gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the learning rate?\n",
    "\n",
    "- The rate at which the model uses new data points to learn\n",
    "- The rate at which the model changes the features of the data\n",
    "- The rate at which the model changes the labels of the data\n",
    "- The rate at which we update the weights of the model ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you update the weights of a model during gradient descent?\n",
    "\n",
    "- We multiply the gradient by the learning rate and subtract the result from the current weight value ***\n",
    "- We multiply the gradient by the learning rate and add the result to the current weight value\n",
    "- We multiply the gradient by the learning rate and divide the result by the current weight value\n",
    "- We multiply the gradient by the learning rate and multiply the result by the current weight value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens if the learning rate is too high?\n",
    "\n",
    "- The model will converge to a minimum value too quickly\n",
    "- The model will converge to a minimum value too slowly\n",
    "- The model will never converge to a minimum value ***\n",
    "- There is no such thing as a too high learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the following image represent?\n",
    "\n",
    "image: ![](quiz_images/lr.png)\n",
    "\n",
    "- A model with a learning rate that is too high ***\n",
    "- A model with a learning rate that is too low\n",
    "- A model with a learning rate that is just right\n",
    "- A model that has converged to a minimum value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the usual steps to perform gradient descent?\n",
    "\n",
    "- Calculate the gradient of the loss function, make a prediction, calculate the loss, update the weights and bias, repeat until the loss is low enough or we reach a maximum number of iterations\n",
    "- Make a prediction, calculate the loss, calculate the gradient of the loss function, update the weights and bias, repeat until the loss is low enough or we reach a maximum number of iterations ***\n",
    "- Calculate the loss, make a prediction, calculate the gradient of the loss function, update the weights and bias, repeat until the loss is low enough or we reach a maximum number of iterations\n",
    "- Update the weights and bias, make a prediction, calculate the loss, calculate the gradient of the loss function, repeat until the loss is low enough or we reach a maximum number of iterations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the disadvantages of using the whole dataset to calculate the gradient (full batch gradient descent)? Select all that apply.\n",
    "\n",
    "- It can be very time consuming due to the large amount of data ***\n",
    "- It can be very memory intensive due to the large amount of data ***\n",
    "- It can lead to overfitting ***\n",
    "- It can lead to underfitting\n",
    "- It will never converge to a minimum value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the disadvantages of using a single data point to calculate the gradient (stochastic gradient descent)? Select all that apply.\n",
    "\n",
    "- It can be very time consuming due to the large amount of data\n",
    "- It can be very memory intensive due to the large amount of data\n",
    "- It can lead to overfitting\n",
    "- Having a single data point can lead to a lot of noise in the gradient ***\n",
    "- Having outliers in the data can lead to a lot of noise in the gradient ***\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the following image, which parameter will have the biggest impact on the gradient?\n",
    "\n",
    "image: ![](quiz_images/loss.png)\n",
    "\n",
    "- Weight 1\n",
    "- Weight 2 ***\n",
    "- Both weights have the same impact on the gradient\n",
    "- There is no impact on the gradient\n",
    "- There is not enough information to answer this question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09a03f703d9594344a2b9942d46f9fa7ca1a719272e47a84f9d4a0d86a51c104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
