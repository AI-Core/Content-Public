{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation & Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "Understand what is going on with:\n",
    "\n",
    "- Grid search and how to use it for hyperparameter finding\n",
    "- K-Fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "One __algorithmic__ approach for finding hyperparameters is __grid search__.\n",
    "\n",
    "> In Grid Search, a grid consisting of possible hyperparameters is employed, and each combination is used to learn the algorithm of choice and validate the results.\n",
    "\n",
    "Consider the example grid for RandomForest:\n",
    "\n",
    "| `n_estimators`  | `criterion`  | `min_samples_leaf` |\n",
    "|:--:|:---:|:---:|\n",
    "| 10  | \"mse\"  | 2  |\n",
    "| 50  | \"mae\"  | 1  |\n",
    "| 100  |   |   |\n",
    "\n",
    "Each time, a row is created from different values so that all combinations are checked. __Please note that the grid does not have to be a 'square'; therefore,__ any number of different hyperparameters can be checked together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here, we implement a simple Grid Search.\n",
    "\n",
    "- Create a dictionary containing the hyperparameter names and their allowed values (as `list`). Use `4` values at most:\n",
    "    - `n_estimators` with the values, `[10, 50, 100]`, (or others; not more than a `1000`).\n",
    "    - `criterion` (check possible values).\n",
    "    - `min_samples_leaf` with values `[2, 1]`.\n",
    "\n",
    "### Code understanding\n",
    "\n",
    "- What is the role of `yield`, and when should it be used? \n",
    "- What is the name of the function that ends with `yield`?\n",
    "- What is the `grid_search` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import typing\n",
    "\n",
    "\n",
    "def grid_search(hyperparameters: typing.Dict[str, typing.Iterable]):\n",
    "    keys, values = zip(*hyperparameters.items())\n",
    "    yield from (dict(zip(keys, v)) for v in itertools.product(*values))\n",
    "\n",
    "\n",
    "grid = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"criterion\": [\"mse\", \"mae\"],\n",
    "    \"min_samples_leaf\": [2, 1],\n",
    "}\n",
    "\n",
    "for i, hyperparams in enumerate(grid_search(grid)):\n",
    "    print(i, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search evaluation\n",
    "\n",
    "Now that we have a grid-creating function, we can evaluate RandomForest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "best_hyperparams, best_loss = None, np.inf\n",
    "\n",
    "for hyperparams in grid_search(grid):\n",
    "    model = RandomForestRegressor(**hyperparams)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    validation_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "\n",
    "    print(f\"H-Params: {hyperparams} Validation loss: {validation_loss}\")\n",
    "    if validation_loss < best_loss:\n",
    "        best_loss = validation_loss\n",
    "        best_hyperparams = hyperparams\n",
    "\n",
    "print(f\"Best loss: {best_loss}\")\n",
    "print(f\"Best hyperparameters: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Above, you can see how one can find the best model based on the validation loss. Note, however, that there are some drawbacks to this method:\n",
    "\n",
    "- There is only one validation set (what would occur if we were to choose a different part of the data as the validation set?).\n",
    "- Every combination must be checked (this can be costly and ineffective as a __high number__ of checked hyperparameters does not correspond to an improved score).\n",
    "\n",
    "Fortunately, there are some solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "> K-Fold splits datasets into multiple parts, which are, in turn, utilised for training and validation.\n",
    "\n",
    "The illustration below will aid your understanding:\n",
    "\n",
    "![](images/k-fold.svg)\n",
    "\n",
    "### Algorithmic flow\n",
    "\n",
    "1. Split dataset into `K` parts (predefined, usually `5` or `3`).\n",
    "2. Set `i=0`.\n",
    "3. Take `parts[i]` as the validation set and the rest as the training set.\n",
    "4. Train on the training dataset.\n",
    "5. Calculate the metrics (in our case, the loss) on the validation set and save them.\n",
    "6. Increment i and repeat until the last part is utilised as the validation set.\n",
    "7. Take the mean of the validation results.\n",
    "\n",
    "### Reasons for K-Fold\n",
    "\n",
    "K-Fold is a common practice in ML and the de facto standard for the following reason.\n",
    "\n",
    "- Evaluation on a single-validation set can be quite noisy. Depending on the data employed for validation, the results may vary (significantly on occasions). This gives us a __false impression regarding the model's performance__ (usually the model performs worse than expected).\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "K-Fold is not standard in the deep-learning community because it is __time-intensive__. For `5` splits, we have to fit `5` separate models. This is not feasible for larger models (which may be trained for weeks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here, we will implement K-Fold from scratch. However, unlike the Grid Search case, we will do it with __`yield`.__\n",
    "\n",
    "- Define a `k_fold` function taking `dataset` and `n_splits` (which is the number of splits). Set the default value of `n_splits` to `5`, and specify the `type` as `int`.\n",
    "- Use [`np.array_split`](https://numpy.org/doc/stable/reference/generated/numpy.array_split.html) to split the `dataset` into `n_splits` __chunks__.\n",
    "- In a `for` loop,\n",
    "    - obtain and employ the `i`-th chunk as the validation dataset.\n",
    "    - obtain the remaining chunks (up to `i`) and from `i + 1`.\n",
    "    - concatenate these chunks, thereby creating the training data.\n",
    "    - `yield` a tuple containing the `training` and `validation` datasets.\n",
    "    \n",
    "Finally, conduct a thorough inspection of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(dataset, n_splits: int = 5):\n",
    "    chunks = np.array_split(dataset, n_splits)\n",
    "    for i in range(n_splits):\n",
    "        training = chunks[:i] + chunks[i + 1 :]\n",
    "        validation = chunks[i]\n",
    "        yield np.concatenate(training), validation\n",
    "\n",
    "# K-Fold evaluation\n",
    "loss = 0\n",
    "n_splits = 5\n",
    "for (X_train, X_validation), (y_train, y_validation) in zip(\n",
    "    k_fold(X, n_splits), k_fold(y, n_splits)\n",
    "):\n",
    "    # Hyperparameters?\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    fold_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "    loss += fold_loss\n",
    "    print(f\"Fold loss: {fold_loss}\")\n",
    "\n",
    "# We divide by the number of splits to obtain the mean\n",
    "print(f\"K-Fold estimated loss: {loss / n_splits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- As you can observe, the loss for each fold varies __considerably__. There is a significant difference between `46` and `8`.\n",
    "- We have more reliable statistics, attributed to K-Fold. \n",
    "\n",
    "> Regarding hyperparameter search, Grid Search can be mixed with K-Fold validation. This is the standard approach, and it is offered by `sklearn`.\n",
    "\n",
    "The only requirement is two nested loops. Now, you probably have more clarity on why it is not used in deep learning:\n",
    "\n",
    "- The complexity will be `O(n*m)`, where `n` is the number of hyperparameters and `m` is the number of splits we intend to use (thus, `60` models in our simple case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search + K-Fold\n",
    "\n",
    "### Example\n",
    "\n",
    "Here, we will perform a Grid Search with K-Fold within.\n",
    "\n",
    "- Iterate over `grid_search(grid)` in a `for` loop.\n",
    "    - Set `loss` to `0`.\n",
    "    - Iterate over the training and validation datasets via `zip` as we saw above.\n",
    "        - Create the `RandomForestRegressor` model with `hyperparams` via __dictionary unpacking__ with `**` (check online for more information).\n",
    "        - Fit the model to the training data.\n",
    "        - Predict on the validation dataset.\n",
    "        - Calculate the MSE loss for this fold.\n",
    "        - Add the fold loss to `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold evaluation\n",
    "best_hyperparams, best_loss = None, np.inf\n",
    "n_splits = 5\n",
    "# Grid search goes first\n",
    "for hyperparams in grid_search(grid):\n",
    "    loss = 0\n",
    "    # Instead of validation we use K-Fold\n",
    "    for (X_train, X_validation), (y_train, y_validation) in zip(\n",
    "        k_fold(X, n_splits), k_fold(y, n_splits)\n",
    "    ):\n",
    "        model = RandomForestRegressor(**hyperparams)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_validation_pred = model.predict(X_validation)\n",
    "        fold_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "        loss += fold_loss\n",
    "    # Take the mean of all the folds as the final validation score\n",
    "    total_loss = loss / n_splits\n",
    "    print(f\"H-Params: {hyperparams} Loss: {total_loss}\")\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_hyperparams = hyperparams\n",
    "\n",
    "# See the final results\n",
    "print(f\"Best loss: {best_loss}\")\n",
    "print(f\"Best hyperparameters: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- Our results changed drastically, indicating that a different criterion and a different number of estimators are the best. \n",
    "- Although the results may vary here as well, they are considerably less spurious than those in the previous case (in magnitude).\n",
    "- Additionally, the results indicate that the performance is considerably worse (more than double).\n",
    "- The results provide a __better estimation__ of the algorithm's behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Variants\n",
    "\n",
    "There are many variants of K-Fold. As always, we urge you to explore them on your own. Here are some variants:\n",
    "\n",
    "- [Stratified K-Fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html): used when __classification__ is carried out with an __unbalanced__ dataset (e.g. `20` examples for class `0` and `500` for class `1`).\n",
    "- [Time Series cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html): should be used when working with time data (where events follow each other).\n",
    "- Leave One Out Cross Validation (LOOCV): similar to K-Fold, but the `validation` set is a single sample. \n",
    "    - Highly inefficient as we have to fit as many models as the number of samples (over `500` in our case). \n",
    "    - Provides noisy estimates and is overly optimistic about the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More On Grid Search\n",
    "\n",
    "### Merits\n",
    "\n",
    "- Easy parallelisation (each hyperparameter can be run on a different core or even computer).\n",
    "- Simple and useful when the __search space__ (set of all the hyperparameters that might be worth examining) is small.\n",
    "\n",
    "### Demerits\n",
    "\n",
    "- Every combination of hyperparameters is tried, and previous results do not inform the next trials. Therefore, bad combinations will be tried. Explore other versions, such as genetic algorithms, so that each trial provides feedback to the hyperparameter-optimising algorithm.\n",
    "- Requires discrete values. Some algorithms (e.g. [Random Search](https://en.wikipedia.org/wiki/Random_search)) can work on probability distributions, which are, in turn, sampled to obtain values.\n",
    "- Impossible to use with a large hyperparameter space that grows __exponentially__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Optimal Hyperparameters\n",
    "\n",
    "Having established the best model, the identified hyperparameters should be __re-trained on a whole dataset__.\n",
    "\n",
    "Validation is no longer required for the following reasons:\n",
    "\n",
    "- We already found a good model and estimated its performance on the left-out dataset (for a more rigorous approach, read up on [Nested Cross Validation](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/).\n",
    "- The __samples are precious__. We intend to use all (or as many as possible) samples to train the model so it gathers more knowledge about the task.\n",
    "\n",
    "__Note:__ For some models, validation may be required __regardless__, particularly for those that tend to overfit the data (neural networks are the prime example; XGBoost is another example). It is task-specific, and with time, your understanding will improve.\n",
    "\n",
    "> As a sanity check, you should always leave a small chunk of the data (say `5%`) and validate on that after finding the hyperparameters.\n",
    "\n",
    "Lastly, save the model for re-use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- __Grid Search__ in conjunction with __K-Fold__ can be simply used to find hyperparameters of our model\n",
    "- __K-Fold__ creates many `(train, validation)` sets. Using it with Grid Search helps us being __way more confident__ about the results \n",
    "- __Grid Search__ has it's downsides and it might be worth checking other options out\n",
    "- __Always employ domain knowledge__ - if you know some set of hyperparameters will fail, do not try them out. Check the most promising combinations. Check all if you have no sensible idea or even better ask an expert"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
