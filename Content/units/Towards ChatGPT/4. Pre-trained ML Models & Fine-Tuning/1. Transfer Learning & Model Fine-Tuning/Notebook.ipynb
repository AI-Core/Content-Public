{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Pre-trained Machine Learning Models\n",
    "\n",
    "There are many cases where taking a pre-trained model off the shelf can be a great way to tackle a problem. \n",
    "You've seen that there are tons of models available online for tackling all kinds of problems.\n",
    "\n",
    "However, most models available online are dataset-specific or problem-specific.\n",
    "\n",
    "Generic examples:\n",
    "- Most text models are trained on a generic text dataset, but are not specialised to any particular domain\n",
    "- Many of the top image classification models are trained on [ImageNet](https://www.image-net.org/) -- which has 1000 possible classification classes -- but your problem might only have 3 different classes that you want to distinguish between. That means that the off-the-shelf model is going to have the wrong output shape.\n",
    "\n",
    "Specific examples:\n",
    "- There may be a generic image classifier online, but it's unlikely there is a model for classifying different types of bicycles in New York City\n",
    "- There may be a generic sentiment classifier online, but it's unlikely that there is one for detecting specific variations of positive sentiments in recently evolved colloquial language (slang, abbreviations, acronyms) used by a particular segment of TikTok users in comments.\n",
    "- There may be a generic text summarisation model online, but it's unlikely that there is one for summarising a specific type of medical literature on the conditions relating to human memory\n",
    "\n",
    "The more obscure or specific your problem, the less likely there's a model trained for the exact task available off the shelf.\n",
    "\n",
    "It sounds like we might have to start from scratch. But is there a middle ground?\n",
    "\n",
    "![](./images/Pre-trained%20vs%20train%20from%20scratch%20tradeoff%20question.png)\n",
    "\n",
    "Lots of the things that these off-the-shelf models must have learnt, are still relevant to the models that we want to produce.\n",
    "\n",
    "For example:\n",
    "- A generic image classification model trained on imagenet will have had to learnt how to recognise different types of shapes and textures. \n",
    "- A generic language model will have learnt what the general structure of sentences looks like\n",
    "\n",
    "This knowledge, stored as the parameters of the models, can be useful for our specific task, and should be transferred to our new model.\n",
    "\n",
    "This is where _model fine-tuning (or transfer learning)_ comes in.\n",
    "\n",
    "> Model fine-tuning is the process of taking a model pre-trained on a related problem as a starting point, and training it further to work for your specific case.\n",
    "\n",
    "![](./images/Pre-trained%20vs%20train%20from%20scratch%20tradeoff.png)\n",
    "\n",
    "> The starting model, which will be fine tuned, is commonly called _the backbone_\n",
    "\n",
    "E.g: \"I trained an image classifier using a ResNet-50 model as the backbone\"\n",
    "\n",
    "Typically, the backbone is a large model that a big company spent a lot of time and compute training (check out some of Microsoft's models [here](https://huggingface.co/microsoft)).\n",
    "It may not be possible for you to train that model from scratch in your lifetime (e.g GPT-2 (yes, 2, not 3) would have taken around 13 years to train on a single GPU), but it is incredible that you can take it [straight off the shelf](https://huggingface.co/gpt2) (assuming you have enough computer memory).\n",
    "\n",
    "Model fine-tuning usually consists of two steps:\n",
    "1. Replacing the model head\n",
    "1. Training the parameters of the head (and perhaps the rest of the model too)\n",
    "\n",
    "## Fine-tuning step 1: Replacing the model head\n",
    "\n",
    "> We call the part of the model that makes the final predictions from the outputs of the last hidden layer the _model head_.\n",
    "\n",
    "![](./images/Model%20Head.png)\n",
    "\n",
    "> The first step in transfer learning is to replace the head of the starting model.\n",
    "\n",
    "Many people make the mistake of keeping the head, and building another layer on top of it, instead of throwing it away and replacing it.\n",
    "\n",
    "Picture this scenario:\n",
    "- You want to train a classifier to classify 3 different models of sports car, starting with a classifier trained on ImageNet.\n",
    "- If you do not throw away the head, then the head will produce the same output corresponding to the class \"sports car\" (a class which appears in the imagenet dataset) for every image in your specific castle dataset\n",
    "- This means that the input to your new additional layer will be the same for every example - a vector of low values everywhere except for the node corresponding to the \"sports car\" logit\n",
    "- All of the information about the high level features will be compressed through this bottleneck, and the features that will help you to distinguish between different planes will be lost\n",
    "\n",
    "![](./images/Problem%20with%20not%20replacing%20fine-tuning%20head.png)\n",
    "\n",
    "Because of this, the original model head should be thrown away, and replaced with a new head which we will train in the next step.\n",
    "\n",
    "> Adding a new layer **on top** of the existing model head, **instead of replacing it**, is a very common mistake.\n",
    "\n",
    "![](./images/Fine-Tuning%20Head%20Incorrect%20%26%20Correct.png)\n",
    "\n",
    "The only case where you do not need to replace the model head is if you are training the model on the same problem **and the same targets as the model was trained**. \n",
    "For example, if you are training an image classification model, there may be new styles of vehicles and clothing in a more recent set of images. \n",
    "This \"further training\" is typically useful because as trends change, the distribution of types of data that appears in your dataset may change. \n",
    "\n",
    "Let's run through fine-tuning an image classification model, using a typical [ResNet-50](https://huggingface.co/microsoft/resnet-50) backbone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the model, we need to find and remove the head.\n",
    "One trick that can be useful here (at least with PyTorch models) is to print the model's `.modules` attribute, which will show all of the transformations that make up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.modules)\n",
    "# print(dir(model)) # another way to find the head is by looking at the list of attributes of the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the list of the model's modules, you can see the final layer which is named `classifier`.\n",
    "It is a linear layer that takes in 2048 hidden activations, which represent high level features of the input, and combines them into the logits for the 1000 different ImageNet classes which the backbone is trained to classify.\n",
    "This is the model head that we should replace.\n",
    "\n",
    "\"Named modules\" like this `classifier` module can be accessed through the dot operator as attributes of the model instance.\n",
    "\n",
    "Note that, in the case of ResNet, the `torch.nn.Flatten` module is applied before the final linear transformation.\n",
    "If you're interested in understanding why, you should look into convolutional neural networks.\n",
    "We will also need to include the flatten module in our new replacement head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model.classifier = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(2048, 3))# TODO replace the model head\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we created the whole model just to highlight how the head would be replaced manually. However, in the HuggingFace documentation, you can also see that many models can be initialised using the `AutoFeatureExtractor` class. What's that?\n",
    "\n",
    "> A feature extractor typically refers to the model without the head\n",
    "\n",
    "The forward pass of an example through the feature extractor returns you the activations of one of the last layers. \n",
    "High values here represent the presence of high-level features that have been \"extracted\" from the raw input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature extractor is the thing that we can directly stack a new head on top of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\") # TODO define feature extractor backbone\n",
    "        self.head = torch.nn.Sequential( # TODO create sequential layer for the head\n",
    "            torch.nn.Flatten(), # TODO add flatten module required for ResNet\n",
    "            torch.nn.Linear(2048, 3) # TODO add linear layer for head\n",
    "        )\n",
    "\n",
    "    def forward(self, X): # TODO define forward pass\n",
    "        extracted_features = self.feature_extractor(X) # TODO pass input through feature extractor\n",
    "        logits = self.head(extracted_features) # TODO pass extracted features through classification head\n",
    "        return logits # TODO return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning step 2: Final training\n",
    "\n",
    "Because the new model head is initialised with random parameters, you need to train them.\n",
    "\n",
    "There are two fundamental approaches to the final training phase of fine-tuning:\n",
    "1. _Weight freezing_: Keep the parameters of the original model fixed\n",
    "1. _Discriminated learning rates_: Update the parameters of both the new head and the original model, but use a lower learning rate on the original model so that the original knowledge is not lost\n",
    "\n",
    "### Fine-tuning by weight freezing\n",
    "\n",
    "Weight freezing is where you totally prevent the parameters of the original model from updating.\n",
    "\n",
    "Advantages of weight freezing:\n",
    "- The backward pass and parameter updates during training is a lot faster, because only the parameters of the head have to be updated\n",
    "    - The original model can contain +99% of the parameters\n",
    "- All of the original knowledge is retained\n",
    "\n",
    "Disadvantages of weight freezing:\n",
    "- No opportunity for most of the parameters to specialise for the new task\n",
    "- A single layer head may not have the capacity to learn the function to usefully combine the input activations into the desired output\n",
    "\n",
    "To implement weight freezing efficiently:\n",
    "- Set the `requires_grad` attribute of all backbone parameters to `False`\n",
    "- Train as usual\n",
    "\n",
    "_Note: you could achieve the same updates by passing only the model head's parameters to the optimiser to optimise. However, this would still lead to the many gradients in the backbone being calculated upon calling `loss.backward()`, which could be expensive computationally, but none of those calculated gradients would be used by the optimiser, so this would be inefficient._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightFreezingClassifier(Classifier):\n",
    "    def freeze_weights(self):\n",
    "        for param in self.feature_extractor.parameters(): # TODO for each parameter in the feature extractor\n",
    "            param.requires_grad = False # TODO prevent \n",
    "\n",
    "    def unfreeze_weights(self):\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "def train_with_weight_freezing(model, dataloader, epochs=10):\n",
    "    model.freeze_weights() # TODO freeze weights\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            predictions = model(features)\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "model = WeightFreezingClassifier()\n",
    "train_with_weight_freezing(model, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fine-tuning by using discriminated learning rates\n",
    "\n",
    "Using discriminated learning rates is the approach to fine tuning where you update the parameters of both the original model and the new head, but use a lower learning rate to update the original model's parameters, such that they don't change too drastically, and as such most of the knowledge is retained.\n",
    "\n",
    "Advantages of discriminated learning rates:\n",
    "- All parameters have the opportunity to specialise for the new task\n",
    "    - Earlier layers can adapt to learn representations that the new head is able to make accurate predictions from with just a single layer\n",
    "\n",
    "Disadvantages of discriminated learning rates:\n",
    "- The backward pass and parameter updates during training is a lot slower, because every parameter requires its gradient to be computed and requires updating on every optimisation step\n",
    "    - The original model can contain +99% of the parameters\n",
    "- During training, especially early on when the head is random and the predictions are bad, the high gradients can cause the model to catastrophically forget some fundamental knowledge\n",
    "- Using an additional optimiser means you have at least one additional hyperparameter: the new optimiser's learning rate\n",
    "\n",
    "Implementing discriminated learning rates:\n",
    "- Create two separate optimisers with different learning rates\n",
    "- Pass the backbone parameters to the optimiser with the smaller learning rate\n",
    "- Pass the head parameters to the optimiser with the larger learning rate\n",
    "- Call `.step()` and `.zero_grad()` on both optimisers during the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_with_discriminated_learning_rates(model, dataloader, epochs=10):\n",
    "    backbone_optimiser = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    head_optimiser = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            predictions = model(features)\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            loss.backward() # populate gradients of all model parameters (backbone and head)\n",
    "            backbone_optimiser.step()\n",
    "            backbone_optimiser.zero_grad()\n",
    "            head_optimiser.step()\n",
    "            head_optimiser.zero_grad()\n",
    "\n",
    "model = Classifier()\n",
    "train(model, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to be careful of when fine-tuning\n",
    "\n",
    "- There is little point in using a starting model that was trained on a very different dataset. For example, a starting model trained on ImageNet will not be a useful starting point for a model trained to classify x-rays, because many of the representations learnt by the starting model do not apply.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (default, May 18 2021, 12:31:01) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
