{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning an Image Classifier in Pytorch\n",
    "\n",
    "In this practical, we will fine-tune a pretrained ResNet-18 model to work with a custom image dataset. The task in this case is to train the model to classify photos depending on which of 10 different global cities they were taken in. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import CitiesDataset\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we will define our network. We create a class that inherits from `torch.nn.Module`, and call the `super().__init__()` method to inherit the methods from the parent class. We have also loaded the `ResNet50` model using the pretrained weights.\n",
    "\n",
    "- Add code to set the `grad_required` argument to False for all the `ResNet` layers.\n",
    "- Define a set of three linear layers for the model, assigning them to the variable `linear_layers`. \n",
    "- The output size of the last layer of `ResNet` is 2048.\n",
    "- The middle layer should have input size 256 and output size of 128.\n",
    "- The output layer should have an output size equivalent to the number of classes in the dataset.\n",
    "\n",
    "- Define the forward method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearning(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = resnet50(weights=ResNet50_Weights)\n",
    "        # TODO - set grad_required = False for all layers of the ResNet50.\n",
    "        \n",
    "        # TODO define a three layer network, complete with activation functions between the layers, asssign to a variable called \"linear layers\"\n",
    "        linear_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10),\n",
    "        )\n",
    "        self.layers.fc = linear_layers\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        # define the forward method\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a transform, and split the data between train, test and validation sets.\n",
    "\n",
    "- Split the data into train, validation and test sets. The training set should be 70% of dataset length, with the validation and test sets 15% each.\n",
    "- Create a dataloader for each split. Set the batch size to 32, and make sure to set shuffle=True for the training set loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomCrop((size, size), pad_if_needed=True),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = CitiesDataset(transform=transform)\n",
    "model = TransferLearning()\n",
    "\n",
    "# TODO - Split the dataset into train, validation and test sets. The train set should be 70% of dataset length, and the validation and test sets 15% each.\n",
    "# TODO - Create dataloaders for train, validation and test sets. Set batch size to 32, and make sure shuffle=True in the train loader.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to training, it will be interesting to see how the classifier performs straight out of the box. In the box below, pass a single example from the test set to the model, with the model in evaluation mode. Get the prediction and compare it to the real label. How did the model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label:  8\n",
      "Prediction category:  Sydney, Australia\n",
      "target label: 0\n",
      "target city  Beijing, China\n"
     ]
    }
   ],
   "source": [
    "features,label=test_set[1]\n",
    "features=features.unsqueeze(0)\n",
    "\n",
    "# TODO - Set model to evaluation mode.\n",
    "# TODO - Pass the features to the model, and assign to a variable called 'outputs'.\n",
    "# TODO - Get a prediction from the output logits, and assign to a variable called 'prediciton'\n",
    "\n",
    "print(\"Prediction label: \", prediction.item())\n",
    "class_label = dataset.idx_to_city_name[prediction.item()]\n",
    "print(\"Prediction category: \", class_label)\n",
    "\n",
    "print(\"target label:\", label )\n",
    "target_label=dataset.idx_to_city_name[label]\n",
    "print( \"target city \", target_label )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train loop. Some of this has been filled in for you already, but manually fill in the sections corresponding to the `TODO` instructions. This training loop also utilises a learning rate scheduler, which steps the learning rate after a given number of epochs. You don't need to worry about this, but it's good to understand why this can be useful in terms of gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    lr=0.1,\n",
    "    epochs=15,\n",
    "    optimiser=torch.optim.SGD\n",
    "):\n",
    " \n",
    "    writer = SummaryWriter()\n",
    "    # initialise an optimiser\n",
    "    optimiser = optimiser(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimiser, milestones=[5,15], gamma=0.1,verbose=True) # learning rate scheduler drops the LR after n epochs, given by \"milestones\"\n",
    "    batch_idx = 0\n",
    "    epoch_idx= 0\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        # \n",
    "        \n",
    "        print('Epoch:', epoch_idx,'LR:', scheduler.get_lr())\n",
    "        epoch_idx +=1\n",
    "        \n",
    "        for batch in train_loader:  # for each batch in the dataloader\n",
    "            features, labels = batch\n",
    "            # TODO - Make a prediction by passing the features to the model\n",
    "            # TODO - Calculate the loss \n",
    "            # TODO - Calculate the gradient of the loss with respect to each model parameter\n",
    "            # TODO - Use the optimiser to update the model parameters using those gradients\n",
    "            print(\"Epoch:\", epoch, \"Batch:\", batch_idx,\n",
    "                  \"Loss:\", loss.item())  # log the loss\n",
    "\n",
    "            # TODO - Zero the grad\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "                   \n",
    "        scheduler.step() # step the learning rate scheduler\n",
    "        print('Evaluating on valiudation set')\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        writer.add_scalar(\"Loss/Val\", val_loss, batch_idx)\n",
    "        writer.add_scalar(\"Accuracy/Val\", val_acc, batch_idx)\n",
    "    \n",
    "    \n",
    "    print('Evaluating on test set')\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "    model.test_loss = test_loss\n",
    "    \n",
    "    return model   # return trained model\n",
    "    \n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    n_examples = 0\n",
    "    for batch in dataloader:\n",
    "        features, labels = batch\n",
    "        prediction = model(features)\n",
    "        loss = F.cross_entropy(prediction, labels)\n",
    "        losses.append(loss.detach())\n",
    "        correct += torch.sum(torch.argmax(prediction, dim=1) == labels)\n",
    "        n_examples += len(labels)\n",
    "    avg_loss = np.mean(losses)\n",
    "    accuracy = correct / n_examples\n",
    "    print(\"Loss:\", avg_loss, \"Accuracy:\", accuracy.detach().numpy())\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code block below to train the model, and leave it running until it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "trained_model=train(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                test_loader,\n",
    "                epochs=15,\n",
    "                lr=0.0001,\n",
    "                optimiser=torch.optim.AdamW\n",
    "                )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's view training performance in `tensorboard`. Run the cell below to open a `tensorboard` instance, then select `time series` from the dropdown, and press refresh to view the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's re-run our prediction code to see how the trained model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,label=test_set[1]\n",
    "features=features.unsqueeze(0)\n",
    "model.eval()\n",
    "outputs=model(features)\n",
    "\n",
    "dummy, pred = torch.max(outputs, 1)\n",
    "print(\"Prediction label: \", pred.item())\n",
    "class_label = dataset.idx_to_city_name[pred.item()]\n",
    "print(\"Prediction category: \", class_label)\n",
    "\n",
    "print(\"target label:\", label )\n",
    "target_label=dataset.idx_to_city_name[label]\n",
    "print( \"target city \", target_label )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e36d4b688d7e3685ae8ad6703c0e99019531dd9f05b6e8f8c82292a1f759bcdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
