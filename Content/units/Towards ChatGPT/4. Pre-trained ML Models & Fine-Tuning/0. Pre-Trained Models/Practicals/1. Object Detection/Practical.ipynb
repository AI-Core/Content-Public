{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Object detection networks are used to detect and localize objects within an image or video. These networks can be used in a wide range of applications, such as autonomous vehicles, surveillance systems, object tracking in videos, human-computer interaction, and advanced driver assistance systems. Object detection has become a crucial part of computer vision and has seen significant improvements in recent years, thanks to deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.transforms.functional import pil_to_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up some transforms we are going to use on our input image. We need to resize the image and convert it to a torch tensor so that we can pass it to the model. We will also need a separate version of the image, which we keep as a PIL image , so that we can use it to plot the bounding boxes of the items detected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transform resizes the image, and converts it to a tensor to use as an input to the model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(800),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# This transform just resizes the image, and keeps it as a PIL image. We will use it for plotting the bounding boxes\n",
    "resize_transform=transforms.Resize(800)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to import the image, ensuring it is converted to RGB format, and then use the transforms we composed in the previous code block to transform the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('street_scene.jpg').convert('RGB')\n",
    "image_for_output=pil_to_tensor(img)\n",
    "\n",
    "image_for_output=resize_transform(image_for_output)\n",
    "img_tensor = transform(img).unsqueeze(0)   \n",
    "\n",
    "img.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download our pretrained model. We will be using `fasterrcnn_resnet50_fpn`, a popular object detection architecture. It is a two-stage object detector that first generates region proposals and then classifies the regions using a convolutional neural network. The ResNet-50 is a deep residual network that has 50 layers and is used as the backbone network to extract features from the image. The FPN (Feature Pyramidal Network) is used to incorporate features from different scales, making the network more robust to objects of different sizes.\n",
    "\n",
    "Once the model is downloaded, we set it to evaluation mode using the `eval` command, and pass it our image tensor to predict the object bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_model = fasterrcnn_resnet50_fpn(pretrained=True, progress=False)\n",
    "object_detection_model.eval()\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    street_preds = object_detection_model(img_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will output a number of boxes around items it thinks are in the image, each with different confidence scores. We can view the confidence scores as follows. Confidence varies between 0 and 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_preds[0][\"scores\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not too interested in objects it predicted with a low confidence, so let's set a threshold of 0.8 to just select the ones it is pretty certain are there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get separate bits, over threshold score\n",
    "street_preds[0][\"boxes\"] = street_preds[0][\"boxes\"][street_preds[0][\"scores\"] > 0.8]\n",
    "street_preds[0][\"labels\"] = street_preds[0][\"labels\"][street_preds[0][\"scores\"] > 0.8]\n",
    "street_preds[0][\"scores\"] = street_preds[0][\"scores\"][street_preds[0][\"scores\"] > 0.8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual for a machine learning model, the labels are currently in integer format, so not very human readable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_preds[0][\"labels\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import the decoder dictionary, and make the labels human readable. We can also format them for display on the image when we visualise it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annFile='instances_val2017.json'\n",
    "coco=COCO(annFile)\n",
    "\n",
    "street_labels = coco.loadCats(street_preds[0][\"labels\"].numpy())\n",
    "street_annot_labels = [\"{}-{:.2f}\".format(label[\"name\"], prob) for label, prob in zip(street_labels, street_preds[0][\"scores\"].detach().numpy())]\n",
    "\n",
    "street_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the `draw_bounding_boxes` method from `torchvision.utils` to plot the bounding boxes on the image, and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_output = draw_bounding_boxes(image=image_for_output,\n",
    "                             boxes=street_preds[0][\"boxes\"],\n",
    "                             labels=street_annot_labels,\n",
    "                             colors=[\"red\" if label[\"name\"]==\"person\" else \"green\" for label in street_labels],\n",
    "                             width=2\n",
    "                            )\n",
    "\n",
    "to_pil_image(street_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 08:45:25) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44b49826f70757dd05c2075de036ba047f37ef083b3bf9e82fcf656fa7d561e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
