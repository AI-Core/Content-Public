{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Analogies\n",
    "\n",
    "Word embeddings allow us to process text data in all kinds of interesting ways. \n",
    "One experiment is to use code to solve _word analogies_.\n",
    "\n",
    "> Solving a word analogy \"A is to B as X is to Y\" means to find one of the parameters, given the other three.\n",
    "\n",
    "For example:\n",
    "- London is to UK as Moscow is to what?\n",
    "- Cat is to kitten as dog is to what?\n",
    "\n",
    "> Word analogies can be solved using word embeddings\n",
    "\n",
    "What is the point of this?\n",
    "- There seems to be little practical application\n",
    "- But it can help \n",
    "    - To understand what word vectors represent\n",
    "    - To determine if you've found a useful set of word embeddings\n",
    "\n",
    "Mathematically, that means finding the vector between $a$ and $b$, then adding that to $x$.\n",
    "\n",
    "# TODO diagram of adding analogy vector to source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's get some pre-trained word embeddings from an extremely widely used embedding model named BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/opt/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 36.7kB/s]\n",
      "Downloading: 100%|██████████| 440M/440M [00:30<00:00, 14.5MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 642kB/s] \n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 8.70kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([30000, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# %% GET BERT\n",
    "model_name = 'bert-base-uncased' \n",
    "model = BertModel.from_pretrained(model_name) # TODO get BERT model from huggingface\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name) # TODO get BERT tokeniser from huggingface\n",
    "\n",
    "# EXAMPLE TOKENISATION\n",
    "sentence = \"Now I want to know what does this vector refers to in dictionary\"\n",
    "tokens = bert_tokenizer.encode(sentence) # TODO encode the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.modules)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that list of modules, you can see that the first one is the embedding layer. \n",
    "The weights of this layer are the input representation that BERT has learnt for each word.\n",
    "These are the pre-trained embeddings that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_matrix = model.embeddings.word_embeddings.weight # TODO get weight parameters from model\n",
    "embedding_matrix = embedding_matrix.detach() # TODO detach parameters from graph\n",
    "\n",
    "n_embeddings = 30000\n",
    "embedding_matrix = embedding_matrix[:n_embeddings] # TODO get the first n_embeddings\n",
    "\n",
    "print(\"Embedding shape:\", embedding_matrix.shape) # TODO print embedding matrix shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the embeddings, we want to determine which row corresponds to which token. We can get this mapping from the pre-trained BERT tokeniser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_labels = list(bert_tokenizer.ids_to_tokens.values())[:n_embeddings] # TODO get the names of the tokens from the tokeniser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly define a helper function to visualise our embeddings using Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "Total time: 32.30235695838928\n",
      "Embedding done\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from time import time\n",
    "\n",
    "def visualise_embeddings(embeddings, labels=None, label_names=\"Label\"):\n",
    "    print(\"Embedding\")\n",
    "\n",
    "    writer = SummaryWriter() # TODO initialise tensorboard summarywriter\n",
    "    start = time()\n",
    "    writer.add_embedding( # TODO add embeddings to tensorboard\n",
    "        mat=embeddings,\n",
    "        metadata=labels,\n",
    "        metadata_header=label_names\n",
    "    )\n",
    "    print(f\"Total time:\", time() - start)\n",
    "\n",
    "    print(\"Embedding done\")\n",
    "\n",
    "visualise_embeddings(embedding_matrix, embedding_labels) # TODO call visualise_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the vector that represents the transformation between $a$ & $b$, we'll need to firstly get the embedding for each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6887e-02, -7.4078e-03, -7.0792e-02, -7.2979e-02,  2.6306e-02,\n",
      "         1.2412e-02, -1.5166e-02, -5.7818e-02, -1.7665e-02, -6.0178e-02,\n",
      "        -6.9499e-02, -8.4558e-02, -6.2827e-02, -3.8619e-02, -4.2123e-02,\n",
      "        -3.3479e-02,  7.6708e-03, -5.8426e-02,  1.4515e-02, -1.3542e-01,\n",
      "         4.4417e-02, -7.0895e-02,  3.5826e-02, -2.9868e-02, -3.8617e-02,\n",
      "        -4.9124e-02, -7.3432e-02, -4.7727e-02, -1.3144e-02, -6.3145e-02,\n",
      "        -8.0265e-02,  8.6743e-03, -2.0196e-02, -2.2212e-02, -4.2043e-02,\n",
      "        -4.5627e-02, -5.2184e-02, -1.3404e-02, -3.0210e-02, -3.4542e-02,\n",
      "        -6.8846e-03, -5.2005e-02,  9.3773e-03, -3.4767e-02,  1.5441e-02,\n",
      "        -1.1546e-02, -4.0174e-02, -2.2193e-02, -9.8711e-02, -4.5019e-02,\n",
      "        -2.8062e-02,  3.6789e-02, -1.1174e-02, -6.9229e-02, -4.1744e-03,\n",
      "         1.7117e-02, -2.2168e-02,  2.7866e-02,  2.4114e-02, -5.9043e-03,\n",
      "         2.2167e-02, -1.1400e-01, -6.4697e-02, -2.7417e-02, -9.1516e-02,\n",
      "        -2.1976e-02,  1.4345e-02,  6.0132e-02, -5.4546e-03,  1.9291e-02,\n",
      "        -4.0754e-02, -9.8876e-02, -9.4682e-02, -2.3095e-02, -6.2225e-02,\n",
      "        -2.5741e-02, -3.5946e-02,  5.3289e-02, -5.0859e-02, -3.9425e-02,\n",
      "        -6.7551e-02,  7.5478e-03,  3.0372e-02,  2.0382e-02,  1.1138e-02,\n",
      "        -1.6378e-02, -8.1785e-03, -1.6955e-02, -6.9471e-02, -8.8842e-02,\n",
      "        -3.0517e-02, -3.2614e-03, -4.5941e-02,  6.6552e-02, -5.6424e-02,\n",
      "        -5.9632e-02,  4.9773e-03, -1.2007e-02, -3.6049e-02,  4.7132e-02,\n",
      "        -1.2688e-02, -1.6028e-03,  2.7030e-02,  7.2294e-03, -4.0138e-02,\n",
      "         7.5290e-02, -7.6855e-02,  2.1382e-02, -3.9924e-02, -5.7441e-02,\n",
      "         4.1032e-02,  2.0858e-02, -4.0117e-02, -1.9602e-02, -9.0653e-03,\n",
      "         5.4736e-02,  8.6492e-03,  5.0555e-03, -7.0881e-02, -3.3765e-02,\n",
      "         2.6181e-02,  6.9354e-02, -5.7248e-02, -3.4752e-02, -7.0002e-02,\n",
      "        -6.0714e-02,  2.8399e-02, -8.0713e-02, -1.3262e-02, -4.6641e-02,\n",
      "        -1.3659e-01,  4.4518e-02,  4.2130e-02,  1.6415e-02, -3.5850e-02,\n",
      "        -2.1382e-02, -6.3208e-02, -3.2177e-02, -1.5578e-02, -1.6985e-03,\n",
      "        -4.8062e-02,  3.5123e-02, -5.8394e-02,  1.2595e-02, -3.7701e-02,\n",
      "        -2.3355e-02, -1.1527e-02,  6.4990e-04, -6.0022e-02, -5.1200e-02,\n",
      "        -2.2760e-02, -8.9297e-02, -9.9514e-02, -7.9438e-02, -1.0251e-01,\n",
      "         9.5375e-03,  2.9559e-02, -1.7555e-02, -2.0506e-02, -1.7861e-02,\n",
      "        -2.4006e-02, -3.5319e-02,  4.6563e-02, -2.4435e-02,  3.3681e-02,\n",
      "         1.7049e-02, -8.6688e-03, -6.6308e-02, -4.1241e-03, -3.2171e-02,\n",
      "         2.1380e-02, -3.2154e-02,  2.0708e-02, -5.9220e-02, -6.1810e-02,\n",
      "        -1.7265e-02, -3.9475e-02,  1.6181e-02,  9.7703e-02, -2.0828e-02,\n",
      "         4.0723e-03, -4.9305e-02, -8.5607e-03,  5.3366e-02, -6.7252e-02,\n",
      "        -1.3814e-02, -1.7524e-02, -6.0302e-02, -2.2457e-02, -1.2554e-02,\n",
      "        -8.5939e-02, -7.2272e-02, -5.6735e-02,  1.8070e-02, -4.4660e-02,\n",
      "        -7.5952e-02, -5.5986e-02, -1.3917e-02, -5.0446e-02, -3.5245e-02,\n",
      "        -1.7316e-02, -3.4758e-02, -4.5526e-03, -6.3603e-02, -3.8392e-02,\n",
      "        -5.4144e-02, -7.6014e-03, -7.3896e-02, -1.7186e-03,  1.4973e-02,\n",
      "        -3.9671e-03, -9.3749e-03, -5.4431e-02,  7.9100e-03,  3.9007e-02,\n",
      "        -6.6695e-02, -9.7638e-02, -5.8403e-02,  1.2279e-02, -4.3355e-02,\n",
      "        -3.7052e-02, -4.4801e-02, -7.4045e-02,  1.5477e-03, -6.0634e-02,\n",
      "        -5.0731e-02, -3.6273e-02, -2.3357e-02,  6.8242e-03, -3.2893e-02,\n",
      "        -5.2170e-02, -1.0363e-03, -6.5099e-02,  6.3511e-02, -9.0810e-02,\n",
      "        -3.8255e-02, -2.2438e-02,  1.7847e-02, -6.3076e-02, -1.2530e-02,\n",
      "        -7.3756e-02, -8.8043e-03,  1.7662e-03, -3.3967e-02, -4.9889e-02,\n",
      "         2.9848e-02, -7.2760e-03, -1.0351e-02,  3.8895e-02, -1.0193e-01,\n",
      "        -4.1914e-02, -2.0998e-02, -7.9974e-02,  3.3123e-03, -1.8489e-02,\n",
      "        -9.1617e-02, -3.7972e-03, -1.6432e-02,  3.0840e-03, -1.9744e-02,\n",
      "         1.0773e-02,  4.3507e-03, -8.2295e-02, -1.9194e-02, -2.9942e-02,\n",
      "        -5.9083e-02, -1.2514e-02, -7.9207e-02,  2.6897e-02, -6.7854e-03,\n",
      "         4.2422e-02,  1.4562e-02, -3.7926e-02, -3.1837e-02, -4.9075e-02,\n",
      "        -5.9526e-02, -1.0726e-01,  3.9529e-02, -5.4806e-02,  7.0696e-03,\n",
      "         4.6617e-02,  3.7514e-02,  4.5362e-03,  1.6164e-02, -2.1350e-02,\n",
      "        -6.4501e-02,  4.4596e-02,  3.9275e-02, -1.7119e-02,  7.5528e-02,\n",
      "        -5.6361e-03, -2.9093e-03, -1.6229e-02, -6.1099e-02,  1.6207e-02,\n",
      "        -2.3473e-02, -4.9565e-02, -7.3906e-02,  1.3786e-02,  1.8197e-02,\n",
      "        -1.0610e-02,  2.8981e-02,  2.7481e-02, -7.6686e-02, -7.5606e-02,\n",
      "        -4.1539e-02, -1.7961e-02,  1.9618e-02,  4.7277e-02, -4.6706e-02,\n",
      "        -4.2808e-02, -3.1092e-02, -1.1832e-02,  9.8888e-03, -6.3629e-02,\n",
      "        -1.0904e-02,  1.4904e-02,  7.3716e-03, -2.1639e-02, -6.3455e-02,\n",
      "        -2.3924e-02,  7.2425e-02, -8.6245e-03,  2.0330e-02,  1.1801e-02,\n",
      "        -1.9606e-02, -6.9980e-02,  2.1383e-05, -1.0693e-01,  1.3852e-02,\n",
      "        -1.1705e-02, -1.1073e-02, -2.0025e-02, -6.0763e-02,  3.6410e-02,\n",
      "        -5.0281e-02, -6.6830e-02,  1.0082e-02, -9.0306e-03, -3.5419e-02,\n",
      "        -5.2664e-02, -3.8354e-02, -2.6845e-02, -4.7421e-02,  4.5381e-02,\n",
      "         2.3193e-02,  7.9351e-03, -7.7151e-02, -1.5950e-02, -2.4941e-02,\n",
      "        -1.5243e-02, -4.5929e-02,  3.2535e-02, -5.9184e-02, -4.0786e-02,\n",
      "         4.4666e-03, -3.3205e-02, -4.3294e-02,  1.7569e-03,  6.1801e-03,\n",
      "        -2.8233e-02, -8.6388e-03, -5.0246e-02, -6.3511e-02, -3.4909e-04,\n",
      "         5.2316e-02,  2.0357e-02, -2.5503e-02, -6.1803e-03, -6.8696e-03,\n",
      "         1.4025e-02, -3.3741e-02, -1.8879e-02, -2.2564e-02, -2.2037e-02,\n",
      "        -2.5192e-02, -1.4785e-02, -4.4827e-02, -4.2174e-02, -6.3934e-03,\n",
      "        -2.6418e-02, -5.1273e-02, -8.4848e-02, -1.6266e-02, -4.0835e-02,\n",
      "        -1.1576e-03, -3.6600e-02,  3.6084e-02, -9.6799e-02,  3.6416e-02,\n",
      "        -3.0597e-02,  5.5892e-02, -5.9850e-02, -4.8356e-02, -3.0009e-03,\n",
      "        -2.6533e-02, -4.0349e-02,  1.1201e-03, -3.2538e-02, -3.9227e-02,\n",
      "         4.7912e-02, -9.7761e-02, -5.1713e-02, -3.1985e-04, -5.5267e-02,\n",
      "        -3.6340e-02, -2.4495e-02, -5.3122e-02, -2.1725e-02,  2.1798e-03,\n",
      "        -2.0573e-02, -4.0739e-02, -7.1502e-02, -2.9550e-02,  1.4499e-02,\n",
      "        -3.5155e-02, -1.1295e-02, -1.6457e-02,  4.5757e-04,  1.3233e-01,\n",
      "        -1.7150e-02, -5.4894e-02, -7.1558e-02, -2.7249e-02, -5.0267e-02,\n",
      "         1.0638e-02, -6.1243e-02, -4.7117e-02, -1.1922e-01, -7.6871e-02,\n",
      "         9.4372e-03,  3.9690e-02, -4.9001e-02,  1.8183e-02, -4.6705e-02,\n",
      "         4.3540e-02, -5.3728e-02, -7.9260e-02, -2.8949e-02, -6.2478e-02,\n",
      "        -1.8650e-02,  4.2937e-02, -6.0715e-02, -5.3072e-03, -2.3460e-02,\n",
      "        -8.8940e-02,  2.8895e-02, -4.5360e-02,  1.0381e-02, -3.7492e-02,\n",
      "         5.8284e-03,  1.5823e-02, -9.5281e-03, -4.8347e-02, -5.0965e-02,\n",
      "        -3.1134e-02, -5.4119e-02, -5.3510e-02,  2.1172e-02,  2.3001e-02,\n",
      "        -4.7015e-02, -1.0015e-01, -1.8939e-02, -3.8537e-02,  7.5742e-03,\n",
      "        -1.0991e-02, -5.5817e-02,  6.2144e-03,  1.9961e-02,  5.3962e-02,\n",
      "        -4.3101e-02, -5.1277e-02,  4.2422e-03, -7.5927e-02, -2.0711e-02,\n",
      "        -3.4992e-02, -1.2748e-02, -4.7205e-02, -5.0865e-02, -5.2897e-02,\n",
      "        -1.0049e-01,  1.0214e-02, -1.0637e-02, -2.8113e-02, -2.9307e-03,\n",
      "        -4.3877e-02, -6.3509e-02,  1.7263e-02, -3.4171e-02, -4.8710e-02,\n",
      "         1.8219e-02, -5.5334e-02, -3.6588e-02, -3.2882e-02, -7.4643e-02,\n",
      "        -4.6213e-02, -7.6624e-02, -3.5060e-03, -3.6759e-02, -1.9418e-02,\n",
      "         1.2310e-02,  6.1000e-02, -4.0226e-02, -1.2163e-01, -3.2602e-02,\n",
      "        -6.2954e-02,  1.6483e-02, -4.6363e-02,  4.1444e-02, -2.9535e-05,\n",
      "        -5.3510e-04, -3.5481e-02,  1.2433e-02,  4.6156e-02,  7.4272e-03,\n",
      "        -2.0954e-03, -1.8911e-02, -3.0581e-02, -2.9119e-02, -7.1344e-02,\n",
      "         6.8095e-04, -4.0248e-02,  6.5040e-02,  1.7039e-02,  8.4927e-03,\n",
      "        -2.8101e-02, -1.4613e-02, -9.2269e-03,  1.1429e-02,  3.7254e-02,\n",
      "        -2.0019e-02, -5.5260e-02, -3.7069e-02,  7.5192e-03, -2.4293e-03,\n",
      "        -1.8612e-02,  1.7994e-02,  1.6291e-02,  4.2644e-03, -8.5499e-02,\n",
      "        -5.4414e-03,  1.4205e-02, -4.8013e-03, -6.9501e-02, -8.5247e-02,\n",
      "         1.3255e-02,  3.0229e-03, -1.9117e-02, -7.8295e-02, -1.4923e-02,\n",
      "        -5.4919e-02, -3.7345e-02,  2.7646e-02, -7.8475e-03, -1.6959e-02,\n",
      "        -9.8385e-03,  5.9657e-03,  3.6808e-02,  1.5840e-03,  5.7530e-02,\n",
      "         3.6948e-02, -7.8069e-02, -2.9275e-02,  5.8160e-02, -8.2516e-02,\n",
      "        -4.4587e-02, -5.9426e-02, -3.8528e-02, -7.2440e-02, -9.1451e-05,\n",
      "        -1.6792e-02, -1.0745e-02, -6.0721e-02, -4.0513e-02, -1.9143e-02,\n",
      "        -4.5836e-02, -6.8649e-03,  5.0540e-02, -6.8287e-03, -8.3999e-02,\n",
      "        -6.3004e-02, -4.7222e-02, -5.2943e-02, -4.8639e-02,  5.1419e-02,\n",
      "        -3.6763e-03, -7.1649e-03, -4.6673e-02, -5.6377e-02, -3.0752e-02,\n",
      "        -6.5309e-02, -2.2946e-02,  8.5839e-03, -5.2153e-02, -1.9798e-02,\n",
      "         7.1710e-02, -4.1076e-02, -3.2367e-04, -7.9540e-02, -3.6079e-02,\n",
      "        -1.7784e-02,  5.4705e-03, -7.2849e-02, -1.4122e-02, -7.7942e-02,\n",
      "        -2.8420e-02, -6.7649e-04, -5.9376e-02, -2.3388e-02,  3.7141e-03,\n",
      "        -1.0042e-01, -3.1966e-02, -1.2940e-02,  1.3559e-02, -2.6769e-03,\n",
      "         2.0429e-02, -6.1267e-02, -1.6235e-02, -3.5289e-02,  1.7592e-02,\n",
      "        -7.2751e-03, -6.5838e-02, -4.1000e-02, -6.1715e-02, -4.4777e-02,\n",
      "        -4.2784e-02, -3.0245e-02, -4.1258e-02, -5.1529e-02, -1.7607e-02,\n",
      "         2.8316e-03, -7.8716e-03, -1.7722e-02,  8.4068e-03, -4.7027e-03,\n",
      "        -4.7444e-02, -1.1994e-02, -4.1883e-03, -8.5086e-02,  6.5492e-03,\n",
      "        -2.6252e-02,  1.4227e-02, -7.3809e-02,  2.9828e-03, -1.1241e-01,\n",
      "        -5.6228e-02, -7.5807e-02, -2.6855e-02, -5.1368e-02,  7.1816e-03,\n",
      "        -8.1702e-02, -6.7510e-03, -3.1342e-02, -8.1625e-02,  8.3312e-02,\n",
      "         4.0720e-02, -7.6974e-02, -2.3902e-02,  5.6933e-02, -1.9412e-03,\n",
      "        -2.2690e-02, -6.9812e-02, -1.7358e-02, -9.5381e-02, -2.5412e-02,\n",
      "        -1.8735e-02, -1.6252e-03, -3.0092e-02, -2.5989e-02, -6.3209e-02,\n",
      "         5.1277e-03, -3.2165e-02, -5.8643e-02, -1.5665e-02, -1.1244e-02,\n",
      "        -1.5433e-02, -4.7574e-02, -3.3278e-02,  4.2825e-03, -1.2271e-01,\n",
      "         4.6698e-02, -7.9930e-02,  2.3401e-03, -7.0778e-02,  2.7406e-02,\n",
      "        -1.0460e-02, -9.8428e-04,  2.4141e-02, -3.6095e-02,  2.3247e-02,\n",
      "        -2.0202e-02, -4.1548e-02, -5.6786e-02,  2.8137e-02, -4.7835e-02,\n",
      "        -5.9593e-02,  2.3209e-02, -1.9705e-02, -6.2279e-02, -5.2001e-02,\n",
      "         3.6869e-02, -9.2625e-03,  1.7897e-02, -6.1516e-02, -2.9259e-02,\n",
      "        -1.6698e-02,  1.7020e-02,  2.7161e-03,  6.7851e-03, -2.2047e-02,\n",
      "         2.2674e-03,  3.5010e-02, -4.4969e-02, -2.3636e-02,  8.9070e-02,\n",
      "        -1.0221e-01, -1.4553e-02, -3.0655e-02, -2.5877e-03, -3.8208e-02,\n",
      "        -3.9286e-02, -1.0077e-01, -3.9521e-02, -3.3520e-02,  7.4754e-03,\n",
      "        -3.7791e-02, -5.5427e-02,  3.0250e-02, -2.7843e-02, -2.3540e-02,\n",
      "         7.5511e-03,  3.3514e-03,  4.1608e-02, -3.2117e-02, -5.1973e-02,\n",
      "        -4.5160e-02, -3.4289e-02,  7.3697e-02, -2.4727e-02, -8.7639e-03,\n",
      "        -6.5018e-02, -1.2546e-01, -3.9588e-02, -4.4705e-02, -4.9957e-02,\n",
      "         4.5392e-04, -6.9659e-02, -5.0979e-02,  5.1570e-02,  3.5614e-02,\n",
      "         9.2371e-03, -5.4764e-02, -9.0633e-02, -4.2104e-02,  3.6904e-02,\n",
      "        -1.7937e-02, -4.3437e-02, -2.8863e-02, -1.2558e-01, -1.5941e-02,\n",
      "        -5.9123e-02, -1.7583e-02, -4.5987e-02,  1.8102e-02, -6.4596e-03,\n",
      "         2.0880e-02, -5.8782e-02, -8.8503e-02])\n"
     ]
    }
   ],
   "source": [
    "def get_word_embedding(word):\n",
    "\n",
    "    tokens_to_ids = {token: id for id, token in bert_tokenizer.ids_to_tokens.items()} # TODO create a mapping from the tokeniser's ids_to_tokens attribute by reversing it with a dictionary comprehension\n",
    "\n",
    "    token_id = tokens_to_ids[word] # TODO get the id from the tokeniser\n",
    "    embedding = embedding_matrix[token_id] # TODO index embedding for this id out of the embedding matrix\n",
    "    return embedding\n",
    "\n",
    "example_word_embedding = get_word_embedding(\"apple\")\n",
    "print(example_word_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the closest vector to an embedding, we'll need to compare its distace to all other token embeddings. An effective way to do that is by taking their cosine similarity. \n",
    "\n",
    "## TODO diagram of comparing word vectors with cosine similarity\n",
    "\n",
    "We could implement the cosine similarity ourselves, but we can also get a function to do that off the shelf, from the `torchmetrics` library. You can check out the documentation [here](https://torchmetrics.readthedocs.io/en/stable/pairwise/cosine_similarity.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from torchmetrics) (1.21.2)\n",
      "Requirement already satisfied: torch>=1.3.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from torchmetrics) (1.12.1)\n",
      "Requirement already satisfied: packaging in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing_extensions in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the nearest token to the solved analogy embedding is the token that you started with or its plural.\n",
    "So, we might want to get more than just the closest one. \n",
    "\n",
    "Now let's define a function to get the nearest $n$ tokens to an embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['apple',\n",
       " 'apples',\n",
       " '880',\n",
       " '1620',\n",
       " '930',\n",
       " '1100',\n",
       " '910',\n",
       " '870',\n",
       " '1621',\n",
       " '1682',\n",
       " '840',\n",
       " '680',\n",
       " '1650',\n",
       " '850',\n",
       " '820',\n",
       " '280',\n",
       " '1628',\n",
       " '1683',\n",
       " '318',\n",
       " '980']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "def get_nearest_n_tokens_from_embedding(embedding, n=20):\n",
    "    # cosine similarity from d_embedding to embedding of all words\n",
    "    similarity = torchmetrics.functional.pairwise_cosine_similarity( # TODO take the pairwise cosine distance\n",
    "        embedding.unsqueeze(0), embedding_matrix).squeeze()\n",
    "    similarity_idx = reversed(torch.argsort(similarity, dim=0)) # TODO argsort by similarity score\n",
    "    print(similarity_idx.shape)\n",
    "    similarity_idx = similarity_idx[:n] # TODO slice out the indexes of the top n\n",
    "    return [list(bert_tokenizer.ids_to_tokens.values())[idx] for idx in similarity_idx] # TODO get the top n most similar tokens from the tokeniser\n",
    "\n",
    "get_nearest_n_tokens_from_embedding(example_word_embedding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement a function to solve the analogy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_solver(a, b, c, embedding_matrix, labels, n=5):\n",
    "    \"\"\"\n",
    "    Solves A is to B what C is to D, given, A, B & C, returning D\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # GET EMBEDDINGS FOR KNOWN WORDS\n",
    "    a_embedding = get_word_embedding(a)\n",
    "    b_embedding = get_word_embedding(b)\n",
    "\n",
    "    # GET TRANSFORMATION APPLIED\n",
    "    transformation_vector = b_embedding - a_embedding # TODO calculate vector difference between a and b\n",
    "\n",
    "    c_embedding = get_word_embedding(c) # TODO get word embedding of c\n",
    "    print(c_embedding.shape)\n",
    "    d_embedding = c_embedding + transformation_vector # TODO add difference between a and b to c\n",
    "    print(d_embedding.shape)\n",
    "    nearest_tokens = get_nearest_n_tokens_from_embedding(d_embedding, n=n+1) # TODO get n+1 nearest tokens (n+1 because the most similar to c is often itself)\n",
    "    for d in nearest_tokens: # TODO for each nearest token\n",
    "        if d == c: # TODO skip if d == c\n",
    "            continue\n",
    "        print(f\"{a} is to {b} as {c} is to {d}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use that to solve a few analogies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([30000])\n",
      "man is to woman as king is to queen\n",
      "man is to woman as king is to woman\n",
      "man is to woman as king is to princess\n",
      "man is to woman as king is to kings\n",
      "man is to woman as king is to queens\n",
      "\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([30000])\n",
      "london is to uk as moscow is to uk\n",
      "london is to uk as moscow is to ussr\n",
      "london is to uk as moscow is to russians\n",
      "london is to uk as moscow is to kyiv\n",
      "london is to uk as moscow is to leningrad\n",
      "\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([30000])\n",
      "puppy is to dog as kitten is to dog\n",
      "puppy is to dog as kitten is to dogs\n",
      "puppy is to dog as kitten is to cat\n",
      "puppy is to dog as kitten is to cats\n",
      "puppy is to dog as kitten is to parrot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analogy_solver(\"man\", \"woman\", \"king\", embedding_matrix, embedding_labels)\n",
    "analogy_solver(\"london\", \"uk\", \"moscow\", embedding_matrix, embedding_labels)\n",
    "analogy_solver(\"puppy\", \"dog\", \"kitten\", embedding_matrix, embedding_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the analogies seem to work (roughly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
