{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding PyTorch\n",
    "\n",
    "Consider implementing a linear regression model from scratch, along the lines of that shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        self.weights = np.random.randn(num_inputs, num_outputs)\n",
    "        self.bias = np.random.randn(num_outputs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.weights + self.bias\n",
    "\n",
    "    def get_gradients(self, X):\n",
    "        grad_w = X\n",
    "        grad_b = 1\n",
    "        return grad_w, grad_b\n",
    "\n",
    "\n",
    "model = LinearRegression(2, 1)\n",
    "X = np.array([[1, 2], [3, 4]]) # fake data\n",
    "y_hat = model.predict(X)\n",
    "print(\"Predictions:\", y_hat)\n",
    "grads = model.get_gradients(X)\n",
    "print(\"Weight gradient:\", grads[0])\n",
    "print(\"Bias gradient:\", grads[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model implements a straight-line function that changes as the weights and bias change.\n",
    "\n",
    "![](./images/prediction.gif)\n",
    "\n",
    "To find good enough parameters, we typically use gradient descent. As a recap, gradient descent involves:\n",
    "- Making predictions and computing the loss\n",
    "- Looking at which direction each parameter needs to move to decrease the loss \n",
    "    - This is the negative gradient of the loss with respect to each parameter\n",
    "- Moving the parameters by a small amount in that direction\n",
    "    - the step size is the gradient, scaled by a proportionality constant: the learning rate, $\\alpha$\n",
    "- Repeat\n",
    "\n",
    "This involves knowing the gradient of the loss with respect to each model parameter, which is why we need `get_gradients()` in the implementation above.\n",
    "\n",
    "![](./images/Side%20by%20side%20Hypothesis%2C%20Loss%20Surface%2C%20Loss%20Curve.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the Problem with this Implementation?\n",
    "\n",
    "Linear regression is a useful model, but it is unlikely to be able to tackle most problems of interest, because it can only represent simple linear input-output relationships.\n",
    "\n",
    "Lots of real world prediction problems require a more complicated model, that can represent a more complex input-output function.\n",
    "So we will probably want to replace the predict method with some other mathematical function.\n",
    "\n",
    "![](./images/Toy%20Problem%20vs%20Interesting%20Problem.png)\n",
    "\n",
    "The problem with the above implementation, is that:\n",
    "1. Every time you change the model, you need to change how the gradients are calculated\n",
    "    - The faster we can change the model, the faster we can experiment and determine what doesn't work\n",
    "2. More complex models have more complex gradient calculations\n",
    "    - It's easier to make mistakes when implementing the gradient calculations from scratch\n",
    "    - It might take a bit of research to find out how to calculate the gradient for functions you aren't familiar with\n",
    "    - Many lines of code might be required to compute individual gradients for models that apply many transformations\n",
    "\n",
    "These problems become a big concern when dealing with large and complicated models, which are the kind of models that are used to tackle many modern AI problems.\n",
    "Pytorch is designed to directly address these, specifically for the application of building deep learning models (neural networks).\n",
    "\n",
    "## So What is PyTorch?\n",
    "\n",
    "> PyTorch automatically computes gradients of the different transformations applied by the model, so that you don't have to.\n",
    "\n",
    "PyTorch may alse be described as:\n",
    "- A deep learning framework\n",
    "- A library containing common utilities for deep learning and gradient-based optimisation\n",
    "- A library for performing efficient mathematical computations\n",
    "\n",
    "But...\n",
    "\n",
    "> PyTorch's main feature is its ability to perform automatic differentiation.\n",
    "\n",
    "I don't expect you to understand this yet, but the equivalent implementation in PyTorch would be as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PyTorchLinearRegression(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        self.linear_layer = torch.nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear_layer(X)\n",
    "\n",
    "    # NO NEED TO IMPLEMENT get_gradients() here!\n",
    "\n",
    "model = PyTorchLinearRegression(2, 1)\n",
    "X = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) # fake data\n",
    "y_hat = model(X) # forward method is called when you call the model (see below)\n",
    "print(\"Predictions:\", y_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the PyTorch implementation does not require you to define how to compute the gradients of the output with respect to the model parameters.\n",
    "\n",
    "Aside from that, there are three key differences to address between the from-scratch implementation and the PyTorch implementation:\n",
    "1. The model inherits from a class named `torch.nn.Module`\n",
    "1. The method that takes in the inputs and makes a prediction has a special name `forward`\n",
    "1. Instead of defining the parameters, we define an attribute of the class that represents the transformation\n",
    "    - This is done using a class from PyTorch\n",
    "    - The layer here contains both the bias and the weights\n",
    "    - The instance of the class is callable, and when called on the inputs it performs the linear transformation (weighted sum of inputs + bias)\n",
    "\n",
    "All of these points are really a result of the model inheriting from `torch.nn.Module`\n",
    "\n",
    "The name \"module\" here refers to building blocks of computation which need to store an internal state (like parameters)\n",
    "- The entire linear regression model is a module\n",
    "- The linear layer itself (torch.nn.Linear(in, out)) is a module\n",
    "\n",
    "This parent class does a lot of important things under the hood, which we will uncover as we need them going forward, but some key things to notice are:\n",
    "1. When we call the model (by doing `model(input_data)`), `torch.nn.Module` tells it to run its `forward` method on the input data\n",
    "1. `torch.nn.Module` module looks for any attributes of the class that also inherit from `torch.nn.Module`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter values are initialised randomly by PyTorch and can be found inside the instance of the linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchLinearRegression(3, 1)\n",
    "print(\"Weight:\", model.linear_layer.weight)\n",
    "print(\"Bias:\", model.linear_layer.bias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So where are the gradients?\n",
    "\n",
    "Firstly, I need to introduce the main data type used in PyTorch: the _torch tensor_.\n",
    "\n",
    "The model input must be a `torch.tensor`. The model output will be a `torch.tensor`. The model parameters are all `torch.tensors`.\n",
    "\n",
    "Quickly scan the docs [here](https://pytorch.org/docs/stable/tensors.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 3.0]]) # create a torch tensor\n",
    "y = torch.tensor([[2.3]]) # create a label for this example\n",
    "\n",
    "model = PyTorchLinearRegression(3, 1) # initialise the model\n",
    "prediction = model(X) # TODO call the model on the data\n",
    "loss = F.mse_loss(prediction, y)\n",
    "\n",
    "print(type(prediction)) # TODO print the type of the model output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calling `any_variable.backward()` on any torch tensor populates the `.grad` attribute of all torch tensors which contribute to it, recursively. The value of a torch tensor's `.grad` attribute is the value of the gradient of `any_variable` with respect to that torch tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the directed graph of the simplest machine learning model, linear regression, and its loss:\n",
    "\n",
    "![linear](./images/Linear%20Regression%20Directed%20Graph%20Backward%20Pass.gif)\n",
    "\n",
    "_Note: See detail 1 below for why not all nodes have backward arrows_\n",
    "\n",
    "### 2 more details of `.backward()`\n",
    "\n",
    "#### Detail 1: Which tensors get their gradients populated?\n",
    "\n",
    "Gradients are populated recursively, for not only all of the variables that contributed to whatever you called `.backward` on, but to whatever variables contributed to those, and so on...\n",
    "\n",
    "However, not all tensors get their gradients populated.\n",
    "\n",
    "Every `torch.tensor` has a `requires_grad` attribute, which is either `True` or `False`. The `requires_grad` attribute determines whether the gradient should be computed.\n",
    "    - Our end goal is to use the gradient to optimise model parameters, so the `.grad` attribute of the model weights and bias will be `True`\n",
    "    - We do not care about optimising the input data, which is fixed, so its `.grad` attribute will be `False`\n",
    "    - Any `torch.tensor` computed from a tensor with `.requires_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input data `requires_grad`:\", X.requires_grad)\n",
    "print(\"Weights `requires_grad`:\", model.linear_layer.weight.requires_grad)\n",
    "print(\"Bias `requires_grad`:\", model.linear_layer.bias.requires_grad)\n",
    "print(\"Prediction `requires_grad`:\", prediction.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/Linear%20Regression%20Directed%20Graph%20Backward%20Pass%20Requires%20Grad.png)\n",
    "\n",
    "#### Detail 2: What Value Actually Goes in `.grad`?\n",
    "\n",
    "If we know the (differentiable) function that was used to compute a tensor (e.g. add, softmax, matrix multiply), then we know the function that tells us the gradient of the function for any input value.\n",
    "We call this the gradient function.\n",
    "\n",
    "> Every function has a corresponding gradient function that tells you how steep the original function is for any given inputs. This is the function we need to use in the backward pass.\n",
    "\n",
    "![](./images/Corresponding%20Gradient%20Functions.png)\n",
    "\n",
    "Mathematicians found the gradient functions for different operations, and programmers implemented them as part of PyTorch.\n",
    "A lot of the code under the hood of PyTorch defines the gradient functions for different operations.\n",
    "\n",
    "About the gradient function:\n",
    "- The resulting tensor from an operation stores that operation's gradient function in its `grad_fn` attribute.\n",
    "- The gradient function represents the backward pass.\n",
    "- Every PyTorch tensor that is computed from a tensor with `requires_grad=True` will have a `grad_fn` attribute.\n",
    "- The gradient function should be called on the inputs to the forward pass to compute the `.grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(3.0)\n",
    "t.requires_grad = True\n",
    "prediction = t * X.T + 1\n",
    "\n",
    "print(\"Input data `grad_fn`:\", X.grad_fn)\n",
    "print(\"Weights `grad_fn`:\", model.linear_layer.weight.grad_fn)\n",
    "print(\"Bias `grad_fn`:\", model.linear_layer.bias.grad_fn)\n",
    "print(\"Prediction `grad_fn`:\", prediction.grad_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember... the gradients are populated recursively. \n",
    "\n",
    "Imagine:\n",
    "1. You compute $b = f(a)$ then $c = g(b)$\n",
    "1. You call `c.backward()`\n",
    "\n",
    "Common misconception: `b.grad` $=\\frac{\\partial b}{\\partial a}$\n",
    "\n",
    "In fact: `b.grad` $=\\frac{\\partial c}{\\partial b}\\frac{\\partial b}{\\partial a}$\n",
    "\n",
    "That is, `.backward()`...\n",
    "- ...not only computes the rate of change of the immediate output with respect to the input\n",
    "- ...but multiplies that with all of the other gradients \n",
    "\n",
    "> The value of `.grad` is populated by the multiplication of all gradients between whatever you called `.backward()` on and the tensor whos `.grad` you are populating. I.e. the chain rule of differentiation is applied.\n",
    "\n",
    "![](./images/Linear%20Regression%20Directed%20Graph%20Backward%20Pass%20Chain%20Rule.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor shapes: Specifically, PyTorch's batch dimension\n",
    "\n",
    "You may have noticed that earlier, when defining our dataset (just a single example), `X`, it was a list of lists, each containing 3 numbers, rather than just a list of 3 numbers.\n",
    "The inner list represents an example datapoint with 3 features.\n",
    "The outer list represents the whole dataset, and the dataset will always contain more than just a single example.\n",
    "\n",
    "> In PyTorch, the first dimension is the batch dimension\n",
    "\n",
    "That means, if you have a dataset of 100 examples with 4 features each, then it will have size (100, 4).\n",
    "\n",
    "This is important because it's expected by models and many other functions in PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dummy Dataloader\n",
    "\n",
    "During training using mini-batch gradient descent, we will iterate through batches of data.\n",
    "\n",
    "The cell below creates a larger dataset, and splits the data up into batches that we can iterate through.\n",
    "\n",
    "Each of those batches has size (`batch_size`, `num_features`). Going forward we will reference this size as `(B, N)`.\n",
    "\n",
    "Don't worry about the exact code inside the cell below, there's a much better way to build PyTorch dataloaders, but just understand that it splits the dataset into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_dataset(num_examples, num_features):\n",
    "    X = torch.randn((num_examples, num_features))\n",
    "    y = torch.randn((num_examples, 1)) # 1 label each\n",
    "    return X, y\n",
    "\n",
    "def create_dummy_dataloader(X, y, batch_size=4):\n",
    "    def create_batches(data):\n",
    "        return [\n",
    "            data[idx*batch_size: (idx+1) * batch_size] if (idx + 1) * batch_size < len(data)\n",
    "            else data[idx*batch_size:]\n",
    "            for idx in range(len(data) // batch_size)\n",
    "        ]\n",
    "    batched_X = create_batches(X)\n",
    "    batched_y = create_batches(y)\n",
    "    return list(zip(batched_X, batched_y))\n",
    "    \n",
    "X, y = create_dummy_dataset(10, 4)\n",
    "print(X, y)\n",
    "\n",
    "dataloader = create_dummy_dataloader(X, y)\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    print(f'Batch {idx}')\n",
    "    X, y = batch\n",
    "    print(\"Features:\")\n",
    "    print(X)\n",
    "    print(\"Labels:\")\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation in PyTorch\n",
    "\n",
    "> PyTorch includes many typical gradient based optimisation algorithms\n",
    "\n",
    "Here's how you can get the classic stochastic gradient descent optimiser in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All PyTorch optimisers take in 2 key parameters:\n",
    "- The parameters which they will be used to optimise\n",
    "- `lr`: The (initial) learning rate\n",
    "\n",
    "About the optimiser:\n",
    "- Under the hood, each PyTorch optimiser defines its corresponding parameter update rule.\n",
    "- All that the optimiser requires (other than potentially some intenal parameters), is the gradient of each parameter, stored in that parameters `.grad` attribute.\n",
    "- When the `.step()` method of an optimiser is called, it iterates through each of the parameters passed in upon initialisation, and updates them using the `.grad` attribute and the parameter update rule.\n",
    "\n",
    "Here's how one optimisation step would be performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchLinearRegression(3, 1)\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1)\n",
    "prediction = model(X)\n",
    "\n",
    "print(\"Initial parameter value:\", model.linear_layer.weight)\n",
    "print(\"Initial grad:\", model.linear_layer.weight.grad)\n",
    "\n",
    "prediction.backward() # populate gradients\n",
    "\n",
    "print(\"grad after `.backward()`\", model.linear_layer.weight.grad)\n",
    "\n",
    "optimiser.step()\n",
    "print(\"Final parameter value:\", model.linear_layer.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to know about `.grad` and the optimiser:\n",
    "- When `.backward()` is called and `.grad` is populated, the previous value is not replaced new one\n",
    "- instead, it is accumulated (added to)\n",
    "- this can be useful in rare occaisions\n",
    "- most of the time, you should make sure to call `optimiser.zero_grad()` after `optimiser.step()`\n",
    "    - that's because the old `.grad` value is now meaningless, since that was the gradient of the loss with respect to the tensor at a previous parameter value, which has since been updated by `optimiser.zero_grad()`\n",
    "    - `optimiser.zero_grad()` iterates through all of the parameters tracked by the optimiser, then sets their `.grad` attribute to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.linear_layer.weight.grad)\n",
    "optimiser.zero_grad()\n",
    "print(model.linear_layer.weight.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all of that together, here's how we would implement a very basic training loop to optimise our PyTorch model (missing many fancy things we will introduce later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def train(model, dataloader, epochs=10):\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    for epoch in range(epochs):\n",
    "        shuffle(dataloader)\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            prediction = model(features)\n",
    "            loss = torch.nn.functional.mse_loss(prediction, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "        print(\"Loss:\", loss.item())\n",
    "        print()\n",
    "            \n",
    "\n",
    "X, y = create_dummy_dataset(100, 4) # 100 examples, 4 features each\n",
    "dataloader = create_dummy_dataloader(X, y)\n",
    "model = PyTorchLinearRegression(4, 1)\n",
    "train(model, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the data is random, the loss doesn't improve visibly. This notebook focuses on understanding PyTorch rather than introducing specific datasets that require more code. Try this on a real dataset to visualise the performance improvement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining what we mean by the forward and backward pass\n",
    "\n",
    "Forward pass: \n",
    "- Going from input values to a prediction\n",
    "\n",
    "Backward pass:\n",
    "- Determining the gradient of the model\n",
    "- Essentially computing the derivative of the model with respect to its parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-projects_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b961f8166aad6ccb4cf65d0f9c742ef9c6c23ffe83ad932438cd83ed96aebaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
