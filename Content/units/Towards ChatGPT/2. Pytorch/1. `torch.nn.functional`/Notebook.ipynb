{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `torch.nn.functional`\n",
    "\n",
    "If you've combed through the PyTorch docs, you might notice that many utilities are implemented as both a class and a function. \n",
    "\n",
    "For example, the sigmoid can be implemented as either a class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.8808, 0.9526]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "sigmoid(torch.tensor([[1, 2, 3]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or functionally from the `torch.nn.functional` module, which is canonically imported as `F`, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/opt/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.8808, 0.9526]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.sigmoid(torch.tensor([[1, 2, 3]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not just be consistent and keep everything as a class?\n",
    "\n",
    "As you can see, the class implementation requires an extra line of code.\n",
    "\n",
    "### Why not just be consistent and make everything a function?\n",
    "\n",
    "Many objects require an internal state that needs to be tracked and persisted.\n",
    "For example, neural network layers require parameters to be stored between passing different examples through the same layer.\n",
    "In Python storing internal object states is typically done by creating an instance of a class, which can then store them as attributes of that instance.\n",
    "\n",
    "Utilities that can be implemented as functions on the other hand, require no internal parameters - the outputs are purely a function of their input. \n",
    "\n",
    "For example, the sigmoid function will always produce the same output, given the same input. In contrast, the forward pass through a linear neural network layer will produce a different output dependent on its current parameterisation, which will change during training.\n",
    "\n",
    "\n",
    "## Use cases\n",
    "\n",
    "The most common things you will see `torch.nn.functional` used for are:\n",
    "- Activation functions\n",
    "- Loss functions\n",
    "Which typically have no internal state.\n",
    "\n",
    "### Activation functions\n",
    "One common use of torch.nn.functional is applying activation functions to tensors.\n",
    "\n",
    "#### ReLU\n",
    "F.relu applies the rectified linear unit (ReLU) function elementwise to the input tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([-1.0, 1.0, 0.0])\n",
    "output = F.relu(input)\n",
    "print(output)  # prints tensor([0.0, 1.0, 0.0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "F.sigmoid applies the sigmoid function elementwise to the input tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input = torch.tensor([-1.0, 1.0, 0.0])\n",
    "output = F.sigmoid(input)\n",
    "print(output)  # prints tensor([0.2689, 0.7311, 0.5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh\n",
    "F.tanh applies the hyperbolic tangent function elementwise to the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input = torch.tensor([-1.0, 1.0, 0.0])\n",
    "output = F.tanh(input)\n",
    "print(output)  # prints tensor([-0.7616, 0.7616, 0.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "Another common use of torch.nn.functional is computing loss functions.\n",
    "\n",
    "#### Binary cross entropy\n",
    "F.binary_cross_entropy computes the binary cross entropy loss. This loss is often used for binary classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape: (batch_size, num_classes)\n",
    "input = torch.tensor([[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]])\n",
    "\n",
    "# target shape: (batch_size, num_classes)\n",
    "target = torch.tensor([[1.0, 0.0], [1.0, 0.0], [1.0, 0.0]])\n",
    "\n",
    "loss = F.binary_cross_entropy(input, target)\n",
    "print(loss)  # prints tensor(1.4133)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "F.cross_entropy computes the cross entropy loss. This loss is often used for multi-class classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7409)\n"
     ]
    }
   ],
   "source": [
    "# input shape: (batch_size, num_classes)\n",
    "input = torch.tensor([[0.0, 1.0, 2.0], [0.0, 1.0, 2.0], [0.0, 1.0, 2.0]])\n",
    "\n",
    "# target shape: (batch_size)\n",
    "target = torch.tensor([1, 1, 0])\n",
    "\n",
    "loss = F.cross_entropy(input, target)\n",
    "print(loss)  # prints tensor(1.7409)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- The functional implementations are stored in a module named `torch.nn.functional`.\n",
    "- It is a module containing a large number of functions related to implementing neural networks. \n",
    "- \"Functional\" means that the utilities can be implemented as functions rather than classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
