{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To use Selenium, we need to create an instance that is going to \"drive\" us through the webpage.  \n",
    "\n",
    "Here is what it could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://zoopla.co.uk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we've navigated to the [Zoopla.co.uk](https://www.zoopla.co.uk/) website. We can search for elements via `Xpath` and can also send mouse and keyboard actions through Selenium as well. Let's recall the challenge we want to solve - extracting data for 50 houses:\n",
    "- **Sale Price**: Our response variable\n",
    "- Number of bedrooms\n",
    "- Square footage\n",
    "- Description\n",
    "- Address\n",
    "\n",
    "We'll focus our efforts just in the London area the next cell will take us to the URL corresponding to properties in London:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome() \n",
    "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
    "driver.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh... Looks like cookies are blocking us... We need to find a way to get around this. Let's start by using Xpath to find the \"Accept All Cookies\" button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: The Zoopla website has a frame in the website. The 'Accept Cookies' is in this frame, so we have to tell Selenium to access the frame. Usually, if it doesn't have a frame, you can ignore the `switch_to_frame` method_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome() \n",
    "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
    "driver.get(URL)\n",
    "time.sleep(2) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
    "try:\n",
    "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
    "    accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
    "    accept_cookies_button.click()\n",
    "\n",
    "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
    "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
    "    accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
    "    accept_cookies_button.click()\n",
    "\n",
    "except:\n",
    "    pass # If there is no cookies button, we won't find it, so we can pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we rune the code, the webdriver will go to the website and click the button for us. So, analyse the methods we used:\n",
    "- `find_element()` To make the driver point to the element\n",
    "- `click()` To make the driver click on the element that was pointed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so it is time to start extracting the data we are interested on. Let's extract the price, address, number of bedrooms and the description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, observe the HTML code corresponding to a property:\n",
    "<p align=center><img src=images/Selenium_1.png width=900></p>\n",
    "<figcaption align=\"center\"><cite>Zoopla Website and Corresponding HTML Code</cite></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get the XPath of that property, it will look like this:\n",
    "\n",
    "`//*[@id=\"listing_60212639\"]`\n",
    "\n",
    "Which is fine if we want to find a single property, but not so great if we want to list all the properties in that page. We will focus on how to get all the properties shortly, for now, let's extract the URL of that property, and extract the information we need. \n",
    "\n",
    "_Note: Zoopa is constantly adding new properties, it is likely that the Xpath changed, so make sure that you are following all the steps and using the correct XPath_\n",
    "\n",
    "Let's take a look again at the HTML code, you will notice that there are some `<a>` tags in the HTML code. Usually, these tags are used to include a hyper reference (`href`). Selenium allows us to get that href, but first we need to locate the `<a>` tag containing the href.\n",
    "\n",
    "So, if you expand one of the `<div>` tags corresponding to a property, you will see something like this:\n",
    "\n",
    "<p align=center><img src=images/Selenium_2.png width=900></p>\n",
    "<figcaption align=\"center\"><cite>Property Div Tag</cite></figcaption>\n",
    "\n",
    "Can you see the `<a>` tag? That is the tag that contains the URL we need. So, let's tell Selenium to extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome() \n",
    "URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
    "driver.get(URL)\n",
    "time.sleep(2) # Wait a couple of seconds, so the website doesn't suspect you are a bot\n",
    "try:\n",
    "    driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
    "    accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
    "    accept_cookies_button.click()\n",
    "\n",
    "except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
    "    driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
    "    accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
    "    accept_cookies_button.click()\n",
    "\n",
    "except:\n",
    "    pass\n",
    "time.sleep(2)\n",
    "house_property = driver.find_element(by=By.XPATH, value='//*[@id=\"listing_61920149\"]') # Change this xpath with the xpath the current page has in their properties\n",
    "a_tag = house_property.find_element(by=By.TAG_NAME. value='a')\n",
    "link = a_tag.get_attribute('href')\n",
    "print(link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, now we can visit that link using Selenium. Alternatively, you can also click on the `property` element (`property.click()`) and it will take you to the same page. But you will have to:\n",
    "- Click the element\n",
    "- Sleep\n",
    "- Extract the information\n",
    "- Go back\n",
    "- Sleep\n",
    "- Find the next property \n",
    "- Click\n",
    "- Sleep\n",
    "\n",
    "On the other hand, if you have the links, you can visit them like this:\n",
    "\n",
    "- Extract all the links\n",
    "- Iterate through the list, and for each iteration, visit the corresponding URL\n",
    "- Sleep\n",
    "- Extract the information of the property\n",
    "- Visit the next URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it's up to you, but for many different websites, creating a list with links (which is usually called \"crawler\"), is much more efficient\n",
    "\n",
    "Enough talking (or writing), let's visit the link we extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it moved us to the webpage of that property\n",
    "\n",
    "<p align=center><img src=images/Selenium_3.png width=900></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, you can see the price, address, number of bedrooms, and the description. As always, let's take a look at the XPath corresponding to each property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center><img src=images/Selenium_4.png width=900></p>\n",
    "<figcaption align=\"center\"><cite>Property Xpath</cite></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is, if you do the same with the number of bedrooms, the address and the description, you should have something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
    "print(price)\n",
    "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
    "print(address)\n",
    "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
    "print(bedrooms)\n",
    "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
    "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
    "description = span_tag.text\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a button, we can send a click action to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_properties = {'Price': [], 'Address': [], 'Bedrooms': [], 'Description': []}\n",
    "price = driver.find_element(by=By.XPATH, value='//p[@data-testid=\"price\"]').text\n",
    "dict_properties['Price'].append(price)\n",
    "address = driver.find_element(by=By.XPATH, value='//address[@data-testid=\"address-label\"]').text\n",
    "dict_properties['Address'].append(address)\n",
    "bedrooms = driver.find_element(by=By.XPATH, value='//div[@class=\"c-PJLV c-PJLV-iiNveLf-css\"]').text\n",
    "dict_properties['Bedrooms'].append(bedrooms)\n",
    "div_tag = driver.find_element(by=By.XPATH, value='//div[@data-testid=\"truncated_text_container\"]')\n",
    "span_tag = div_tag.find_element(by=By.XPATH, value='.//span')\n",
    "description = span_tag.text\n",
    "dict_properties['Description'] = description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding links to a list: Creating a Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, it would be more efficient to create a list with all the links and then iterate through that list. Here, I am going to give a small teaser of what it looks like, but, ultimately, it will be your task to complete the whole scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, I am going to create a list with the \"Accept Cookies\" functionality, so we don't have to repeat myself so many times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "def load_and_accept_cookies() -> webdriver.Chrome:\n",
    "    '''\n",
    "    Open Zoopla and accept the cookies\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    driver: webdriver.Chrome\n",
    "        This driver is already in the Zoopla webpage\n",
    "    '''\n",
    "    driver = webdriver.Chrome() \n",
    "    URL = \"https://www.zoopla.co.uk/new-homes/property/london/?q=London&results_sort=newest_listings&search_source=new-homes&page_size=25&pn=1&view_type=list\"\n",
    "    driver.get(URL)\n",
    "    time.sleep(3) \n",
    "    try:\n",
    "        driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame\n",
    "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
    "        accept_cookies_button.click()\n",
    "        time.sleep(1)\n",
    "    except AttributeError: # If you have the latest version of Selenium, the code above won't run because the \"switch_to_frame\" is deprecated\n",
    "        driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame\n",
    "        accept_cookies_button = driver.find_element(by=By.XPATH, value='//*[@id=\"save\"]')\n",
    "        accept_cookies_button.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return driver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function from now on, it will make our code much more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = load_and_accept_cookies() # In case it works, driver should be in the Zoopla webpage with the cookies button clicked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's observe the list of properties one more time. All the properties are in a container as we can see in this image:\n",
    "\n",
    "<p align=center><img src=images/Selenium_5.png width=900></p>\n",
    "<figcaption align=\"center\"><cite>Properties Container</cite></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, each property is one `<div>` tag inside that container. For example, for the two first properties:\n",
    "\n",
    "<p align=center><img src=images/Selenium_7.png width=450><img src=images/Selenium_6.png width=450></p>\n",
    "<figcaption align=\"center\"><cite>Property and Corresponding Div Tag</cite></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can find a way to iterate through _all_ the properties in that list, and for each iteration, extract the link. We saw earlier that if you have the property, you can easily find the `<a>` tag that contains the `href` like this:\n",
    "```\n",
    "property = driver.find_element(by=By.XPATH, value='//*[@id=\"listing_61920149\"]') # Change this Xpath with the Xpath the current page has in their properties\n",
    "a_tag = property.find_element(by=By.TAG_NAME, value='a')\n",
    "link = a_tag.get_attribute('href')\n",
    "```\n",
    "Let's use something similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = load_and_accept_cookies()\n",
    "prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e5pbze00\"]') # XPath corresponding to the Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `prop_container` is pointing to the list of properties on the website. We have to get ALL the `<div>` tags inside, but only those that are direct children. So, we have to use a relative Xpath: `./div`\n",
    "- The dot represents that it is relative\n",
    "- The single slash represents direct children\n",
    "\n",
    "Also, take into account that we are looking for ALL occurrence of this XPath, so we have to use the `find_elementS_by_xpath` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
    "link_list = []\n",
    "\n",
    "for house_property in prop_list:\n",
    "    a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
    "    link = a_tag.get_attribute('href')\n",
    "    link_list.append(link)\n",
    "    \n",
    "print(f'There are {len(link_list)} properties in this page')\n",
    "print(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of the links of the properties in that page. How awesome is that?\n",
    "\n",
    "Next, we need to iterate through this list and start visiting each link to extract the data we were interested on (Price, Address, Number of Bedroom, Description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "With the new acquired knowledge, extract the data from all the properties in 5 different Zoopla pages. This means that, once you finish scraping a page, you have to click the 'Next Page' button (you can also change the URL if you know how to tweak it). So, once you extract the 25 links, you can go to the next page by clicking 'Next':\n",
    "\n",
    "<p align=center><img src=images/Selenium_8.png width=450></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a template you can use to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "def get_links(driver: webdriver.Chrome) -> list:\n",
    "    '''\n",
    "    Returns a list with all the links in the current page\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver: webdriver.Chrome\n",
    "        The driver that contains information about the current page\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    link_list: list\n",
    "        A list with all the links in the page\n",
    "    '''\n",
    "\n",
    "    prop_container = driver.find_element(by=By.XPATH, value='//div[@class=\"css-1itfubx e5pbze00\"]')\n",
    "    prop_list = prop_container.find_elements(by=By.XPATH, value='./div')\n",
    "    link_list = []\n",
    "\n",
    "    for house_property in prop_list:\n",
    "        a_tag = house_property.find_element(by=By.TAG_NAME, value='a')\n",
    "        link = a_tag.get_attribute('href')\n",
    "        link_list.append(link)\n",
    "\n",
    "    return link_list\n",
    "\n",
    "big_list = []\n",
    "driver = load_and_accept_cookies()\n",
    "\n",
    "for i in range(5): # The first 5 pages only\n",
    "    big_list.extend(get_links(driver)) # Call the function we just created and extend the big list with the returned list\n",
    "    ## TODO: Click the next button. Don't forget to use sleeps, so the website doesn't suspect\n",
    "    pass # This pass should be removed once the code is complete\n",
    "\n",
    "\n",
    "for link in big_list:\n",
    "    ## TODO: Visit all the links, and extract the data. Don't forget to use sleeps, so the website doesn't suspect\n",
    "    pass # This pass should be removed once the code is complete\n",
    "\n",
    "driver.quit() # Close the browser when you finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "- In order to start using Selenium, we'll need to first create an instance of it using the appropriate webdriver (such as Chromedriver)\n",
    "- If a website has cookies blocking access, one way around that is to use Xpath to find the \"Accept All Cookies\" button and press it before crawling the data\n",
    "- To extract the data for multiple items on a website, we'll need to find the href `<a>` tags to create a list with links to crawl, and then iterate through that list\n",
    "- To find different details of items on a website, we can use the `.find_element()` command and then instruct Selenium to take an action on each item (such as a click)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
