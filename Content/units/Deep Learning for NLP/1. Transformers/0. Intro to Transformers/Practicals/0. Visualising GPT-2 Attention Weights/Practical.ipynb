{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising GPT-2 Attentions\n",
    "\n",
    "GPT's attention layers allow the model to weigh the importance of each input token based on its relationship with other tokens in the sequence. This enables the model to capture long-range dependencies and generate more coherent and contextually-appropriate text.\n",
    "\n",
    "In this practical we will configure the GPT-2 model to output attention weights, and then view the attention weights as a heatmap.\n",
    "\n",
    "Run the cell below to load the pretrained GPT-2 Model and tokeniser, and configure the model to return attention weights along with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from examples/run_lm_finetuning.py, how to use pretrained model and tokenizer(this paper used pretrain tokenizer throughout)\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "config_class = GPT2Config \n",
    "model_class = GPT2LMHeadModel \n",
    "tokenizer_class = GPT2Tokenizer\n",
    "\n",
    "config = config_class.from_pretrained('gpt2')\n",
    "# Change default config to make model output attention probs as well\n",
    "config.output_attentions = True\n",
    "tokenizer = tokenizer_class.from_pretrained('gpt2')\n",
    "\n",
    "model = model_class.from_pretrained(\n",
    "    'gpt2',\n",
    "    config=config\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we will encode our example sentence and get GPT-2 to predict the next word. \n",
    "\n",
    "- Encode the example sentence and assign to a variable called \"indexed_tokens\".\n",
    "- Convert the indexed tokens to a pytorch tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest land animal in the world is the elephant\n"
     ]
    }
   ],
   "source": [
    "example_text='The biggest land animal in the world is the' \n",
    " # TODO - encode the example sentence and assign to a variable called \"indexed_tokens\"\n",
    "\n",
    "\n",
    "# Convert indexed tokens in a PyTorch tensor\n",
    " # TODO - convert the indexed tokens to a pytorch tensor\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0] \n",
    "\n",
    "# get the predicted last token\n",
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "print(predicted_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this, we can now examine our `outputs` variable, which we have configured to additionally output the transformer attentions layers.\n",
    "\n",
    "- The variable `outputs` is a dictionary. Print the keys of the dictionary to find the attentions outputs.\n",
    "- Assign the attentions of the final transformer layer to a variable called `final_layer_attentions`.\n",
    "- Print the shape of the final layer attentions. Why are the dimensions as they are?\n",
    "- Assign the attentions of a single head to a variable called `example_attention_head`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Print the keys of the outputs\n",
    "#TODO - Assign the final layer of attentions to a variable called final_layer_attentions.\n",
    "# TODO print the shape of the attentions. Why are the dimensions as they are?\n",
    "# TODO -  Assign a single attention head to a variable called \"example_attention_head\". It should have a 2d shape.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualise the attentions, we need to create a list of the decoded tokens to use as axis tick labels. \n",
    "\n",
    "- Create an empty list called `tokens`. \n",
    "- Decode the tokens in the input sequence one by one, and append them to the `tokens` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO - create an empty list called `tokens`. Decode the tokens in the input sentence one by one, and append them to the list.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a heatmap of the tokens. Using `Matplotlib.plt`, plot the attention wieghts for the example attention head you have chosen.\n",
    "- The weights should be displayed as a heatmap. Use the cmap `reds` as the colour map.\n",
    "- Assign the X and Y ticks such that they display the decoded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO - Using Matplotlib.plt, plot the attention weights for the example attention head you have chosen. Assign the decoded tokens to the axis ticks.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e36d4b688d7e3685ae8ad6703c0e99019531dd9f05b6e8f8c82292a1f759bcdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
