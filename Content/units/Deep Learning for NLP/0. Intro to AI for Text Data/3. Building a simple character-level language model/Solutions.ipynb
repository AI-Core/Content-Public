{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple Character-Level Language Model\n",
    "\n",
    "Aims:\n",
    "- Build your first language model to generate rap lyrics\n",
    "- Understand how to implement recurrent neural networks in PyTorch\n",
    "- Get familiar with PyTorch's embedding layer\n",
    "\n",
    "## What is Language Modelling?\n",
    "\n",
    "> Given a sequence of words, the language model assigns a probability to each possible word that might come next in the sequence. \n",
    "\n",
    "![](./images/Language%20Model.png)\n",
    "\n",
    "Language modeling is the process of predicting the next word in a sequence of words based on the context provided by the previous words. It is a core task in natural language processing (NLP) and is used in a wide range of applications, including speech recognition, machine translation, and chatbots.\n",
    "\n",
    "This can be used to predict the next word in a sequence, generate text that is similar to a given input, or to evaluate the quality of a translation or a summary by comparing the probability of the generated text to the probability of the original text.\n",
    "\n",
    "> It's easy to acquire data for training language models because the label is simply the next word.\n",
    "\n",
    "Language models are typically trained on large corpora of text, such as books, articles, and websites, in order to learn the statistical properties of the language and the dependencies between words. They can be implemented using various types of neural networks, such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, or transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Some Data\n",
    "\n",
    "In this example, we'll try to generate lyrics like those from your favourite artist.\n",
    "\n",
    "If you want to use your own data, you can either:\n",
    "- Copy lyrics into the code below to define your corpus (easy difficulty)\n",
    "- Create a GitHub repo and upload the lyrics there, then paste in the raw URLs, as below (intermediate difficulty)\n",
    "- Build a web-scraper to collect lyrics [like I did](https://github.com/life-efficient/Lyric-Generation/tree/main/data) (hardcore difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_country_music_lyrics_corpus():\n",
    "    \"\"\"Get the country music lyrics corpus.\"\"\"\n",
    "    raw_urls = [\n",
    "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Ashe-emotional-lyrics.txt\",\n",
    "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Creedence-clearwater-revival-have-you-ever-seen-the-rain-lyrics.txt\",\n",
    "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Hardy-and-lainey-wilson-wait-in-the-truck-lyrics.txt\",\n",
    "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Johnny-cash-folsom-prison-blues-lyrics.txt\",\n",
    "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Johnny-cash-ring-of-fire-lyrics.txt\",\n",
    "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Koe-wetzel-creeps-lyrics.txt\",\n",
    "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Rosa-linn-snap-lyrics.txt\",\n",
    "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Taylor-swift-all-too-well-10-minute-version-taylors-version-from-the-vault-lyrics.txt\"\n",
    "    ]\n",
    "    corpus = \"\"\n",
    "    for url in raw_urls:\n",
    "        response = requests.get(url)\n",
    "        lines = response.text.splitlines()\n",
    "        lines = [line for line in lines if line != '']\n",
    "        lyrics = \" \".join(lines)\n",
    "        corpus += lyrics\n",
    "    return corpus\n",
    "\n",
    "get_country_music_lyrics_corpus()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tokeniser\n",
    "\n",
    "![](./images/Tokeniser.png)\n",
    "\n",
    "The first thing we need to do is to create a tokeniser that can take in our raw text and split it into a sequence of tokens.\n",
    "\n",
    "In this simple example, we will create a character-level tokeniser:\n",
    "- The tokeniser should be able to encode any string into a sequence of character, then turn them into their integer index.\n",
    "- In most real applications, you'd use a word-level or subword-level tokeniser instead. \n",
    "- Here, we implement our own tokeniser for practice. In a real-world example, you can find pre-built tokenisers online, for example in [HuggingFace](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel.forward.example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser:\n",
    "    def __init__(self, txt):\n",
    "        txt = self.preprocess(txt) # Preprocess the text\n",
    "        unique_chars = set(txt) # TODO Create a set of unique characters in the input text\n",
    "        self.vocab_size = len(unique_chars) # TODO Get the vocabulary size\n",
    "        self.id_to_token = dict(enumerate(unique_chars)) # TODO Create a dictionary that maps character IDs to characters\n",
    "        self.token_to_id = {v: k for k, v in self.id_to_token.items()} # TODO Create a reverse dictionary that maps characters to character IDs\n",
    "\n",
    "    def preprocess(self, txt):\n",
    "        txt = txt.lower() # TODO Convert the lyrics to lowercase\n",
    "        # other preprocessing steps can be added here\n",
    "        return txt\n",
    "\n",
    "    def encode(self, txt):\n",
    "        txt = self.preprocess(txt) # Preprocess\n",
    "        token_ids = [self.token_to_id[char] for char in str.strip(txt)] # TODO Encode the input string by mapping its characters to character IDs\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return \"\".join([self.id_to_token[id] for id in token_ids]) # TODO Decode the input list of character IDs by mapping them to characters\n",
    "\n",
    "\n",
    "corpus = get_country_music_lyrics_corpus()\n",
    "tokeniser = Tokeniser(corpus) # TODO create a tokeniser object\n",
    "\n",
    "tokens = tokeniser.encode(\"This is truly excellent\") # TODO encode a sentence\n",
    "print(\"Tokens:\", tokens)\n",
    "tokeniser.decode(tokens) # TODO decode the tokens\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple character-level language modelling dataset\n",
    "\n",
    "A language modelling dataset consists of:\n",
    "- features: \n",
    "    - the sequential words/tokens in a body of text\n",
    "- targets: \n",
    "    - the next token for each position in time\n",
    "    - i.e. the features shifted one step forward in time\n",
    "\n",
    "Implementation details:\n",
    "- Like all PyTorch datasets, our dataset needs a `__len__` method. \n",
    "    - In this case, set the length of the dataset to be the number of chunks of text of the provided `chunk_size` that could fit in the dataset.\n",
    "- Define the `__getitem__` to get a random chunk of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LyricDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokeniser, chunk_size=30):\n",
    "        \"\"\"\n",
    "        Initialize a LyricDataset object.\n",
    "        \n",
    "        Parameters:\n",
    "        chunk_size (int): The size of each chunk of data to be returned by the iterator.\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size  # The size of each chunk of data to be returned by the iterator\n",
    "        self.tokeniser = tokeniser\n",
    "\n",
    "        txt = get_country_music_lyrics_corpus()\n",
    "\n",
    "        self.X = torch.tensor(self.tokeniser.encode(txt)) # TODO Encode the text and store it in a tensor\n",
    "        self.Y = torch.tensor(np.roll(self.X, -1, axis=0)) # TODO Shift the encoded text by one character and store it in a tensor\n",
    "        self.vocab_size = len(set(txt)) # TODO Store the size of the vocabulary (i.e. the number of unique characters in the text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) // self.chunk_size # TODO return the number of chunks in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k = np.random.randint(0, len(self.X) - self.chunk_size) # TODO Select a random starting index for the chunk\n",
    "        # Select the chunk using a slice object\n",
    "        return self.X[k: k+self.chunk_size], self.Y[k+self.chunk_size-1]\n",
    "\n",
    "\n",
    "dataset = LyricDataset(tokeniser) # TODO create a dataset object\n",
    "\n",
    "print(\"Vocabulary size:\", dataset.vocab_size)\n",
    "print(\"Length of dataset:\", len(dataset))\n",
    "print(\"First chunk of data:\")\n",
    "for idx, (x, y) in enumerate(dataset):\n",
    "    print(\"X:\", x)\n",
    "    print(\"Y:\", y)\n",
    "\n",
    "    print(\"Sequence so far:\", tokeniser.decode(list(int(xx) for xx in x)))\n",
    "    print(\"Target next character:\", tokeniser.decode([int(y)]))\n",
    "    if idx > 3:\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your labels should look the same as your features, just shifted by one position in time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a dataloader to batch and shuffle the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True) # TODO create a dataloader object\n",
    "\n",
    "print(\"First batch of data:\")\n",
    "example_batch = next(iter(dataloader))\n",
    "print(\"X:\", example_batch[0])\n",
    "print(\"Y:\", example_batch[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the RNN model\n",
    "\n",
    "One of the simplest kinds of language models you can implement is using a many-to-one recurrent neural network that processes a sequence of many tokens to produce one classification - a classification of which word comes next.\n",
    "\n",
    "![](./images/RNN%20Text%20Classifier.png)\n",
    "\n",
    "Firstly, to initialise the model, we'll define the modules that will be needed to make the forward pass:\n",
    "- An embedding layer that takes in a sequence of token ids and turns them into a sequence of embeddings.\n",
    "    - See the docs [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
    "    - When called on a sequence of length $T$, an embedding layer that produces $d$ dimensional embeddings for each token will output a matrix of size ($T$, $d$), which represents a $d$ dimensional token embedding for each of the $T$ timesteps.\n",
    "- An RNN layer\n",
    "    - Requires an embedding size $d$\n",
    "    - Requires a hidden size $h$\n",
    "    - Can be multi-layer\n",
    "- A classification head\n",
    "    - Will combine the final hidden state activations into logits for a classification\n",
    "        - We'll output the logits rather than the probabilities so that we can train the model using the `cross_entropy` loss function\n",
    "    - The classification should have the same dimensionality as the vocab size - a probability for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size , hidden_size, n_layers=1):\n",
    "        super().__init__() # TODO initialise parent class\n",
    "\n",
    "        # STORE HYPERPARAMETERS\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # DEFINE MODEL MODULES\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size) # TODO inintialise embedding layer\n",
    "        self.rnn = torch.nn.RNN(embedding_size, hidden_size, n_layers, batch_first=True)  # TODO initialise RNN layer\n",
    "        self.classification_head = torch.nn.Linear(hidden_size, vocab_size) # TODO initialise classification head\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass # we will do this in the next step\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        pass # we will do this in the next step\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every computation performed by an RNN depends on having an initial hidden state. As per the equations, is needs to be combined with the input data at each timestep.\n",
    "\n",
    "> Typically, we initialise the hidden state of an RNN as a vector of zeros.\n",
    "\n",
    "Check out the [docs](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#rnn) to make sure you implement the correct shaped tensor.\n",
    "\n",
    "So let's define a method that does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size=32, hidden_size=32, n_layers=1):\n",
    "        super().__init__() # TODO initialise parent class\n",
    "\n",
    "        # STORE HYPERPARAMETERS\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # DEFINE MODEL MODULES\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size) # TODO inintialise embedding layer\n",
    "        self.rnn = torch.nn.RNN(embedding_size, hidden_size, n_layers, batch_first=True)  # TODO initialise RNN layer\n",
    "        self.classification_head = torch.nn.Linear(hidden_size, vocab_size) # TODO initialise classification head\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size) # TODO initialise hidden state\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass # we will do this in the next step\n",
    "\n",
    "\n",
    "rnn = RNN(tokeniser.vocab_size)\n",
    "# print(\"Hidden before initialisation:\", rnn.hidden)\n",
    "rnn.init_hidden(batch_size=2)\n",
    "print(\"Hidden after initialisation:\", rnn.hidden)\n",
    "print(\"Hidden shape:\", rnn.hidden.shape) # (L, B, H)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the forward pass.\n",
    "\n",
    "Torch's recurrent layers are a little different to other layers in a few ways:\n",
    "1. The first dimension is not the batch dimension by default! Instead, it's the time dimension, followed by the batch dimension.\n",
    "1. They take in more than one argument:\n",
    "    - The input data, as usual\n",
    "    - The current hidden state\n",
    "1. They return more than one thing:\n",
    "    - The final hidden values of every layer\n",
    "    - The output from each timestep\n",
    "\n",
    "![](./images/PyTorch%20RNN%20Outputs.png)\n",
    "\n",
    "The output from each timestep is the activations of the final recurrent layer for every timestep.\n",
    "\n",
    "In our case, we won't need to use the hidden states output from the RNN layer.\n",
    "\n",
    "These behaviours might seem unusual, but as you get more familiar with using recurrent networks, you'll realise how they can be useful and make RNNs very flexible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size=32, hidden_size=32, n_layers=1):\n",
    "        super().__init__() # TODO initialise parent class\n",
    "\n",
    "        # STORE HYPERPARAMETERS\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # DEFINE MODEL MODULES\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size) # TODO inintialise embedding layer\n",
    "        self.rnn = torch.nn.RNN(embedding_size, hidden_size, n_layers, batch_first=True)  # TODO initialise RNN layer\n",
    "        self.classification_head = torch.nn.Linear(hidden_size, vocab_size) # TODO initialise classification head\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size) # TODO initialise hidden state\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.init_hidden(X.shape[0])\n",
    "        embedding = self.embedding(X)\n",
    "        outputs, final_hidden = self.rnn(embedding, self.hidden)\n",
    "        predictions = self.classification_head(outputs)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "features, labels = example_batch\n",
    "print(\"Batch size:\", features.shape[0])\n",
    "print(\"Sequence length:\", features.shape[1])\n",
    "print(\"Vocabulary size:\", tokeniser.vocab_size)\n",
    "rnn = RNN(tokeniser.vocab_size)\n",
    "prediction = rnn(features)\n",
    "print(prediction.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new text\n",
    "\n",
    "Now we need to implement a method of our model that takes what it knows and uses it to generate new text.\n",
    "\n",
    "Initially, our generated text will be awful, because we haven't trained the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size=32, hidden_size=32, n_layers=1):\n",
    "        super().__init__()  # TODO initialise parent class\n",
    "\n",
    "        # STORE HYPERPARAMETERS\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # DEFINE MODEL MODULES\n",
    "        # TODO inintialise embedding layer\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        # TODO initialise RNN layer\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.classification_head = torch.nn.Linear(\n",
    "            hidden_size, vocab_size)  # TODO initialise classification head\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # TODO initialise hidden state\n",
    "        self.hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.init_hidden(X.shape[0])\n",
    "        embedding = self.embedding(X)\n",
    "        outputs, final_hidden = self.rnn(embedding, self.hidden)\n",
    "        outputs = outputs[:, -1] # get last output\n",
    "        predictions = self.classification_head(outputs)\n",
    "        return predictions\n",
    "\n",
    "    def generate(self):\n",
    "        self.init_hidden(batch_size=1)\n",
    "        initial_token_id = random.randint(0, 49-1)\n",
    "        generated_token_ids = [initial_token_id]\n",
    "        initial_token_batch = torch.tensor(initial_token_id).unsqueeze(\n",
    "            0).unsqueeze(0)  # TODO SOS token\n",
    "        embedding = self.embedding(initial_token_batch)\n",
    "        for idx in range(100):  # generate 100 character sequence\n",
    "            outputs, self.hidden = self.rnn(embedding, self.hidden)\n",
    "            predictions = self.classification_head(outputs)\n",
    "            # outputs has shape BxLxN=1x1xN\n",
    "            predictions = predictions.squeeze()  # remove 1-dims\n",
    "            chosen_token_id = torch.argmax(predictions)\n",
    "            generated_token_ids.append(int(chosen_token_id))\n",
    "            embedding = self.embedding(\n",
    "                chosen_token_id).unsqueeze(0).unsqueeze(0)\n",
    "        return generated_token_ids\n",
    "\n",
    "\n",
    "rnn = RNN(tokeniser.vocab_size)\n",
    "myrnn = RNN(tokeniser.vocab_size, 32, 32, 1)\n",
    "\n",
    "generated_tokens = rnn.generate()\n",
    "print(\"Generated text:\", tokeniser.decode(generated_tokens))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training loop\n",
    "\n",
    "Now we have the model and the dataset, we need to pass the model through the dataset repeatedly and iteratively optimise the model parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(model, dataset, tokeniser, epochs=1):\n",
    "    writer = SummaryWriter()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)  # choose optimiser\n",
    "    n_steps = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0  # stored the loss per epoch\n",
    "        for X, y in dataloader:\n",
    "            \n",
    "            predictions = model(X)\n",
    "            # seq_targets = seq_targets.unsqueeze(0)\n",
    "            # predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            # seq_targets = seq_targets.view(-1)  # BxT targets all in a line\n",
    "            # print(tokeniser.decode([int(x) for x in X[0, -20:]]))\n",
    "\n",
    "            # print(tokeniser.decode([int(torch.argmax(y[0]))]))\n",
    "            loss = F.cross_entropy(predictions, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # OPTIMISE\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # LOGGING\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), n_steps)\n",
    "            n_steps += 1\n",
    "\n",
    "        epoch_loss /= len(dataset)  # avg loss per epoch\n",
    "\n",
    "        print('Epoch ', epoch, ' Avg loss/chunk: ', epoch_loss)\n",
    "        generated_token_ids = model.generate()\n",
    "        writer.add_text(\"Generated Text\", tokeniser.decode(\n",
    "            generated_token_ids)[:300], epoch)\n",
    "            # TODO stop on EOS token\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # HYPER-PARAMS\n",
    "    lr = 0.05\n",
    "    epochs = 5000\n",
    "    chunk_size = 30  # the length of the sequences which we will optimize over\n",
    "    batch_size = 32\n",
    "\n",
    "    # MODEL ARCHITECTURE\n",
    "    embedding_size = 64\n",
    "    hidden_size = 64\n",
    "    n_layers = 2\n",
    "\n",
    "    # LOAD DATA\n",
    "    corpus = get_country_music_lyrics_corpus()\n",
    "    tokeniser = Tokeniser(corpus)\n",
    "    dataset = LyricDataset(tokeniser, chunk_size=chunk_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    n_tokens = len(dataset.tokeniser.id_to_token)\n",
    "    # instantiate our model from the class defined earlier\n",
    "    myrnn = RNN(n_tokens, embedding_size, hidden_size, n_layers)\n",
    "    train(myrnn, dataset, tokeniser, epochs)\n",
    "    # myrnn = RNN(n_tokens, hidden_size, n_layers)\n",
    "    # train(myrnn, dataset, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
