{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Analogies\n",
    "\n",
    "Word embeddings allow us to process text data in all kinds of interesting ways. \n",
    "One experiment is to use code to solve _word analogies_.\n",
    "\n",
    "> Solving a word analogy \"A is to B as X is to Y\" means to find one of the parameters, given the other three.\n",
    "\n",
    "For example:\n",
    "- London is to UK as Moscow is to what?\n",
    "- Cat is to kitten as dog is to what?\n",
    "\n",
    "> Word analogies can be solved using word embeddings\n",
    "\n",
    "What is the point of this?\n",
    "- There seems to be little practical application\n",
    "- But it can help \n",
    "    - To understand what word vectors represent\n",
    "    - To determine if you've found a useful set of word embeddings\n",
    "\n",
    "Mathematically, that means finding the vector between $a$ and $b$, then adding that to $x$.\n",
    "\n",
    "# TODO diagram of adding analogy vector to source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's get some pre-trained word embeddings from an extremely widely used embedding model named BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# %% GET BERT\n",
    "model_name = 'bert-base-uncased' \n",
    "model = BertModel.from_pretrained(model_name) # TODO get BERT model from huggingface\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name) # TODO get BERT tokeniser from huggingface\n",
    "\n",
    "# EXAMPLE TOKENISATION\n",
    "sentence = \"Now I want to know what does this vector refers to in dictionary\"\n",
    "tokens = bert_tokenizer.encode(sentence) # TODO encode the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.modules)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that list of modules, you can see that the first one is the embedding layer. \n",
    "The weights of this layer are the input representation that BERT has learnt for each word.\n",
    "These are the pre-trained embeddings that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_matrix = model.embeddings.word_embeddings.weight # TODO get weight parameters from model\n",
    "embedding_matrix = embedding_matrix.detach() # TODO detach parameters from graph\n",
    "\n",
    "n_embeddings = 30000\n",
    "embedding_matrix = embedding_matrix[:n_embeddings] # TODO get the first n_embeddings\n",
    "\n",
    "print(\"Embedding shape:\", embedding_matrix.shape) # TODO print embedding matrix shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the embeddings, we want to determine which row corresponds to which token. We can get this mapping from the pre-trained BERT tokeniser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_labels = list(bert_tokenizer.ids_to_tokens.values())[:n_embeddings] # TODO get the names of the tokens from the tokeniser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly define a helper function to visualise our embeddings using Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from time import time\n",
    "\n",
    "def visualise_embeddings(embeddings, labels=None, label_names=\"Label\"):\n",
    "    print(\"Embedding\")\n",
    "\n",
    "    writer = SummaryWriter() # TODO initialise tensorboard summarywriter\n",
    "    start = time()\n",
    "    writer.add_embedding( # TODO add embeddings to tensorboard\n",
    "        mat=embeddings,\n",
    "        metadata=labels,\n",
    "        metadata_header=label_names\n",
    "    )\n",
    "    print(f\"Total time:\", time() - start)\n",
    "\n",
    "    print(\"Embedding done\")\n",
    "\n",
    "visualise_embeddings(embedding_matrix, embedding_labels) # TODO call visualise_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the vector that represents the transformation between $a$ & $b$, we'll need to firstly get the embedding for each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "\n",
    "    tokens_to_ids = {token: id for id, token in bert_tokenizer.ids_to_tokens.items()} # TODO create a mapping from the tokeniser's ids_to_tokens attribute by reversing it with a dictionary comprehension\n",
    "\n",
    "    token_id = tokens_to_ids[word] # TODO get the id from the tokeniser\n",
    "    embedding = embedding_matrix[token_id] # TODO index embedding for this id out of the embedding matrix\n",
    "    return embedding\n",
    "\n",
    "example_word_embedding = get_word_embedding(\"apple\")\n",
    "print(example_word_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the closest vector to an embedding, we'll need to compare its distace to all other token embeddings. An effective way to do that is by taking their cosine similarity. \n",
    "\n",
    "## TODO diagram of comparing word vectors with cosine similarity\n",
    "\n",
    "We could implement the cosine similarity ourselves, but we can also get a function to do that off the shelf, from the `torchmetrics` library. You can check out the documentation [here](https://torchmetrics.readthedocs.io/en/stable/pairwise/cosine_similarity.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the nearest token to the solved analogy embedding is the token that you started with or its plural.\n",
    "So, we might want to get more than just the closest one. \n",
    "\n",
    "Now let's define a function to get the nearest $n$ tokens to an embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "def get_nearest_n_tokens_from_embedding(embedding, n=20):\n",
    "    # cosine similarity from d_embedding to embedding of all words\n",
    "    similarity = torchmetrics.functional.pairwise_cosine_similarity( # TODO take the pairwise cosine distance\n",
    "        embedding.unsqueeze(0), embedding_matrix).squeeze()\n",
    "    similarity_idx = reversed(torch.argsort(similarity, dim=0)) # TODO argsort by similarity score\n",
    "    print(similarity_idx.shape)\n",
    "    similarity_idx = similarity_idx[:n] # TODO slice out the indexes of the top n\n",
    "    return [list(bert_tokenizer.ids_to_tokens.values())[idx] for idx in similarity_idx] # TODO get the top n most similar tokens from the tokeniser\n",
    "\n",
    "get_nearest_n_tokens_from_embedding(example_word_embedding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement a function to solve the analogy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_solver(a, b, c, embedding_matrix, labels, n=5):\n",
    "    \"\"\"\n",
    "    Solves A is to B what C is to D, given, A, B & C, returning D\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # GET EMBEDDINGS FOR KNOWN WORDS\n",
    "    a_embedding = get_word_embedding(a)\n",
    "    b_embedding = get_word_embedding(b)\n",
    "\n",
    "    # GET TRANSFORMATION APPLIED\n",
    "    transformation_vector = b_embedding - a_embedding # TODO calculate vector difference between a and b\n",
    "\n",
    "    c_embedding = get_word_embedding(c) # TODO get word embedding of c\n",
    "    print(c_embedding.shape)\n",
    "    d_embedding = c_embedding + transformation_vector # TODO add difference between a and b to c\n",
    "    print(d_embedding.shape)\n",
    "    nearest_tokens = get_nearest_n_tokens_from_embedding(d_embedding, n=n+1) # TODO get n+1 nearest tokens (n+1 because the most similar to c is often itself)\n",
    "    for d in nearest_tokens: # TODO for each nearest token\n",
    "        if d == c: # TODO skip if d == c\n",
    "            continue\n",
    "        print(f\"{a} is to {b} as {c} is to {d}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use that to solve a few analogies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_solver(\"man\", \"woman\", \"king\", embedding_matrix, embedding_labels)\n",
    "analogy_solver(\"london\", \"uk\", \"moscow\", embedding_matrix, embedding_labels)\n",
    "analogy_solver(\"puppy\", \"dog\", \"kitten\", embedding_matrix, embedding_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the analogies seem to work (roughly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
