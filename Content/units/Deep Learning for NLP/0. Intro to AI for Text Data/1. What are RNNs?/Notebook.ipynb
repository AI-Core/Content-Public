{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Recurrent Neural Networks (RNNs)?\n",
    "\n",
    "## Motivation\n",
    "\n",
    "> Much of the data found in the real world is sequential data - its order matters\n",
    "\n",
    "Examples:\n",
    "- X-Y-Z coordinates of an object over time\n",
    "- Price of an artwork over time\n",
    "- Text\n",
    "- Audio waveforms\n",
    "\n",
    "To process sequential data, it needs to be represented mathematically:\n",
    "\n",
    "![](./images/Sequential%20Features.png)\n",
    "\n",
    "You can thing of each input as a vector representation for each timestep:\n",
    "\n",
    "![](./images/Sequential%20Data%20Single%20Example%20Detailed.png)\n",
    "\n",
    "Sequential data can appear in many datasets that we want to model.\n",
    "These datasets typically contain complex input-output relationships, so we want to use neural networks to model them.\n",
    "\n",
    "The time dimension makes the features of each example at least 2-D, with the first axis being for each timestep, and the second axis being for each feature. The problem: Feedforward neural networks can only process vectors. The naive thing to do would be to flatten out all of this data into a vector. But that, again causes issues.\n",
    "\n",
    "The problem with flattening sequential data: \n",
    "- Different input sequences can have different lengths, so the flattened vectors vary in size, but feedforward networks can only process fixed size vectors.\n",
    "    - They cannont be resized like images can without losing potentially important information.\n",
    "- The model should look for the same thing in different parts of the input sequence.\n",
    "    - This is known as parameter sharing across time.\n",
    "\n",
    "Recurrent Neural Networks (RNNs) can tackle these problems.\n",
    "- They process the data sequentially.\n",
    "- They use the same weights at different time steps.\n",
    "- They keep an internal hidden state, that contains information from the sequence that has been processed so far.\n",
    "\n",
    "![](./images/RNN%20Hidden%20State%20Equation.png)\n",
    "\n",
    "RNNs work as follows:\n",
    "- An initial hidden state of the network is initialised as a vector of zeros in each hidden layer\n",
    "- At each timestep, an input vector (data from a single timestep, such as a word embedding, or a X-Y-Z coordinate) is fed to the RNN\n",
    "- The new hidden state of the first recurrent layer is computed by concatenating the outputs of:\n",
    "    - A linear layer takes a number of weighted combinations of that input vector's raw input features \n",
    "    - A linear layer takes a number of weighted combinations of the previous hidden state\n",
    "- Other deeper hidden layers combine their previous hidden state\n",
    "- A model head (such as a classification head) combines the \n",
    "\n",
    "![](./images/RNN%20Predictions.png)\n",
    "\n",
    "> Recurrent networks can process sequences of varying lengths\n",
    "\n",
    "> Recurrent networks can handle many different types of situations\n",
    "\n",
    "![](./images/X-to-X%20RNNs.png)\n",
    "\n",
    "> Recurrent networks look for the same things at different points in time, by using the same weights across different timesteps\n",
    "\n",
    "> Recurrent networks process inputs sequentially\n",
    "\n",
    "## Limitations of RNNs\n",
    "\n",
    "1. Difficulty in training: RNNs can be difficult to train, especially for long sequences. This is due to the vanishing and exploding gradient problem, where the gradients either become very small or very large as they are backpropagated through the network. This can make it difficult for the network to learn long-term dependencies.\n",
    "\n",
    "1. Limited ability to process long-term dependencies: While RNNs are able to capture some long-term dependencies, they may struggle with very long sequences or dependencies that are separated by a large number of elements because the hidden state is manipulated so much by every sequential item processed between the dependencies.\n",
    "\n",
    "1. Sensitivity to initialization: Like other neural networks, RNNs can be sensitive to the values chosen for their initial hidden states. It is not yet well understood how parameter initialisation affects optimisation or generalisation.\n",
    "\n",
    "1. Computational complexity: RNNs can be computationally expensive, especially for large sequences, because they cannot be parallelised in the time dimension. The cost of each parameter update step is $O(T)$. This can be a limitation when working with large datasets or when real-time processing is required.\n",
    "\n",
    "1. Difficulty in interpreting results: RNNs can be difficult to interpret, as it is not clear exactly what is represented by their hidden states just by looking at them. This can make it difficult to understand the decisions made by the network and how it is using the input data.\n",
    "\n",
    "## Training RNNs\n",
    "\n",
    "### Teacher Forcing\n",
    "\n",
    "\"Teacher forcing\" is a technique used when training a sequence-to-sequence model, such as a recurrent neural network (RNN) with attention. It refers to the use of the true target sequence as the input to the decoder at each time step, rather than the predicted output of the model.\n",
    "\n",
    "In other words, when teacher forcing is used, the decoder is \"forced\" to generate the next output based on the true target sequence, rather than its own predicted output at the previous time step. This can help the model learn faster and more accurately, but can also make the model more dependent on the teacher forcing, and may reduce its ability to generate reasonable output when teacher forcing is not used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
