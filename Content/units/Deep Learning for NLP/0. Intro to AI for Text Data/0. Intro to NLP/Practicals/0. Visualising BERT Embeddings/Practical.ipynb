{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Embeddings with BERT\n",
    "\n",
    "BERT is one of the most influential machine learning models. It is a pre-trained language model developed by Google that can be fine-tuned for a wide range of natural language processing tasks. BERT uses a transformer architecture and is trained on massive amounts of text data, allowing it to understand the context and meaning of words in a sentence.\n",
    "\n",
    "It's not important to understand BERT in detail at this point, but for now:\n",
    "\n",
    "- BERT stands for Bidirectional Encoding Representations using Transformers.\n",
    "- It is trained to fill in the missing word in text.\n",
    "- It contains the word embeddings within its first layer's parameters. These BERT embeddings are widely used as a good starting point for word embeddings.\n",
    "\n",
    "We will talk about BERT more later, but we can already start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import the BERT model\n",
    "from transformers import BertModel\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the parameters that the model contains by printing its modules attribute. Run the code in the cell below and look at the output. \n",
    "- Can you find out how many embeddings the `BERT` model contains?\n",
    "- Can you find the size of the output layer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.modules)\n",
    "\n",
    "# TODO - How many embeddings does BERT contain?\n",
    "# TODO - What is the size of the output layer?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the very first layer is the embedding layer. The parameters of this layer are the embeddings for the thousands of words which BERT recognises.\n",
    "\n",
    "Now, let's get those embeddings. In the code block below:\n",
    "\n",
    "- Create a variable called `n_embeddings`, equal to the number of embeddings that you found in the model's `modules` attribute.\n",
    "- Add a statement to print the shape of the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Create a variable called n_embeddings, equal to the number of embeddings in the BERT model.\n",
    "\n",
    "n_embeddings=30000\n",
    "embedding_matrix = model.embeddings.word_embeddings.weight.detach()\n",
    "embedding_matrix = embedding_matrix[:n_embeddings]\n",
    "\n",
    "\n",
    "# TODO  - print the shape of the word embeddings matrix.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the embedding matrix, but we don't know which word each of those embeddings correspond to. This is where we need to use the vocab to map from the index of the word (its row in the embedding matrix) to the word itself.\n",
    "\n",
    "In `HuggingFace` `transformers` models, the vocab is accessible through the tokeniser. In the same way that we loaded in a pre-trained `BERT` model, we can load in the corresponding tokeniser.\n",
    "\n",
    "Check out the docs [here](https://huggingface.co/docs/transformers/main_classes/tokenizer).\n",
    "\n",
    "In the codeblock below, we can explore the tokenisation process:\n",
    "\n",
    "- Define a variable called 'sentence', consisting of a sentence of your choice inside a string.\n",
    "- Encode the sentence using the tokenizer's `encode` method, assigning the output to a variable called `tokens`.\n",
    "- Print the tokens.\n",
    "- Print the number of words in your original sentence.\n",
    "- Print the length of the `tokens` variable. How does it compare to the sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# TODO Define a variable called 'sentence', consisting of a sentence of your choice inside a string.\n",
    "# TODO Encode the sentence using the tokenizer's `encode` method.\n",
    "# TODO Print the tokens.\n",
    "# TODO Print the number of words in the sentence.\n",
    "# TODO Print the length of the tokens. How does it compare to the sentence?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also decode a string of tokens using the tokeniser's `decode` method. \n",
    "\n",
    "In the cell below:\n",
    "- Create a for-loop that steps through the tokens in `tokens` and prints each on a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO Create a for-loop that steps through the tokens in `tokens` and prints each on a new line.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your sentence, you might see a variety of different token values, Most of them will be full words, but some will be word fragments, with prefixes and suffixes denoted by leading or trailing `#` symbols.\n",
    "\n",
    "Regardless of the sentence you chose, you can see that it starts and ends with a pair of special tokens:\n",
    "\n",
    "`[CLS]` - Is a special token that is used to represent the entire input sequence in a single vector. It is used for classification tasks, and always appears at the start of a body of text.\n",
    "\n",
    "`[SEP]` - Is a separator token. This token allows the model to distinguish between the two sequences and learn their relationships separately. For example it could be placed between two sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have examined how a sentence is split up into different tokens with the tokeniser, and we know that the relationships between tokens are parsed into embeddings. Now let's attempt to visualise these embeddings. The embeddings themselves are of very high dimensionality (equal to the size of the output layer of `BERT`), but we can attempt to visualise a 3D projection of them. \n",
    "\n",
    "Run the code block below to label the embeddings according to three feature dimensions:\n",
    "- Length\n",
    "- Number of vowels\n",
    "- Whether the token is a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from time import time\n",
    "import tensorboard as tb\n",
    "import tensorflow as tf\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "n_embeddings=30000\n",
    "embedding_matrix=embedding_matrix[:n_embeddings]\n",
    "def create_embedding_labels():\n",
    "    # ADD NEW COLS\n",
    "    label_functions = {\n",
    "        \"Length\": lambda word: len(word),\n",
    "        \"# vowels\": lambda word: len([char for char in word if char in \"aeiou\"]),\n",
    "        \"is number\": lambda word: word.isdigit(), # boolean label for numbers\n",
    "        # \"is preposition\": lambda word: word in prepositions\n",
    "    }\n",
    "    labels = [\n",
    "        [\n",
    "            word,\n",
    "            *[label_function(word) for label_function in label_functions.values()]\n",
    "        ]\n",
    "        for word in list(bert_tokenizer.ids_to_tokens.values())[:n_embeddings]\n",
    "    ]\n",
    "\n",
    "    label_names = [\"Word\", *list(label_functions.keys())]\n",
    "\n",
    "    return labels, label_names\n",
    "\n",
    "\n",
    "def visualise_embeddings(embeddings, labels=None, label_names=\"Label\"):\n",
    "    print(\"Embedding\")\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    start = time()\n",
    "    writer.add_embedding(\n",
    "        mat=embeddings,\n",
    "        metadata=labels,\n",
    "        metadata_header=label_names\n",
    "    )\n",
    "    print(f\"Total time:\", time() - start)\n",
    "\n",
    "    print(\"Embedding done\")\n",
    "\n",
    "labels, label_names = create_embedding_labels()\n",
    "visualise_embeddings(embedding_matrix, labels, label_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, open tensorboard by running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note that this is a 3D projection of much higher dimensional embeddings, so most information is lost when we visualise it._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e36d4b688d7e3685ae8ad6703c0e99019531dd9f05b6e8f8c82292a1f759bcdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
