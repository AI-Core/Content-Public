{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to NLP\n",
    "\n",
    "## What is NLP?\n",
    "\n",
    "> Natural language processing, or NLP, is the use of human languages by a computer\n",
    "\n",
    "With language being the primary mode of human communication, there are an abundance of use-cases for NLP.\n",
    "\n",
    "Examples include:\n",
    "- Sentiment analysis\n",
    "- Question answering\n",
    "- Translation\n",
    "- Generating text\n",
    "\n",
    "_Note that speech data is not language, it is numerical audio waveforms that represent language data. Audio can be turned into language (text data by transcription) or processed directly by an end-to-end model, but that's out of scope here_\n",
    "\n",
    "## What makes text processing different and hard?\n",
    "\n",
    "- Language has lots of nuance\n",
    "    - Words can have different meanings in different contexts\n",
    "- Language data is not naturally numerical, and so can't immediately be processed mathematically\n",
    "\n",
    "## Essential Tools and Concepts within NLP\n",
    "\n",
    "### What is a corpus?\n",
    "\n",
    "A corpus is a body of text that represents your data. One classic example would be the [Gutenberg corpus](https://zenodo.org/record/2422561#.Y8NpV-zP06E), which contains the text of over 50000 books.\n",
    "\n",
    "### What is a token?\n",
    "\n",
    "A token is an atomic unit of text. In most cases, you can think of tokens as individual words, but in many cases tokens may be something like a common part of a word, like a suffix, in other cases a token might be an individual character.\n",
    "\n",
    "Example tokens:\n",
    "- the word \"probable\"\n",
    "- the character \"h\"\n",
    "    - You won't typically see this, as most NLP is done on the word level\n",
    "- the sequence of tokens \"ing\", which is a common word suffix\n",
    "    - You will commonly see this represented as a token \"##ing\", where the \"##\" indicates that this token is preceded by other characters\n",
    "    - Similarly, you can have tokens which appear at the start of words, like \"pre##\", or that appear in the middle of words like \"##ab##\"\n",
    "    - The tokens you end up with, depend on how you turn your raw text into these tokens, through the process known as _tokenisation_\n",
    "\n",
    "Going forward, you can think about tokens as individual words, which is what they are in most cases. Note that you will probably see the words \"token\" and \"word\" used interchangably.\n",
    "\n",
    "### What is a tokeniser?\n",
    "\n",
    "A tokeniser is a function that takes in raw text and turns it into a sequence of tokens.\n",
    "A tokeniser performs tokenisation on raw text to produce tokens.\n",
    "\n",
    "![](./images/Tokeniser.png)\n",
    "\n",
    "### What is a vocab?\n",
    "\n",
    "A vocab is an assignment of an integer index to each token. \n",
    "If you imagine a list of tokens, the index of each token is the position of that token in the list.\n",
    "\n",
    "![](./images/Vocab.png)\n",
    "\n",
    "## How do we represent words?\n",
    "\n",
    "### Word indexes\n",
    "\n",
    "As indicated by the vocab, we can represent each token mathematically by assigning it an integer index.\n",
    "\n",
    "An alternative way to represent that index is using a vector that is as long as the number of tokens in your corpus, which contains zeros everywhere except in the position of the index corresponding to the word.\n",
    "\n",
    "![](./images/One-hot%20Vector.png)\n",
    "\n",
    "We call this vector a _1-hot vector_, or a _one-hot encoding_ of the token.\n",
    "\n",
    "### Why does the 1-hot representation make more sense mathematically than the index?\n",
    "\n",
    "The one-hot vector makes more sense mathematically than the index, because the index indicates that the words are somehow on the same number line, which they are not. The token with index=2 is not necessarily between tokens 1 & 3. The token represented by the index 100 is not bigger than token 1.\n",
    "\n",
    "### The problem with 1-hot encodings\n",
    "\n",
    "#### Similar words do not have similar representations\n",
    "\n",
    "- Similar words do not have similar representations\n",
    "- in fact, all vectors are orthogonal\n",
    "\n",
    "#### The length of your 1-hot encodings increases with every new token\n",
    "- 1-hot encodings contain an element for every possible word, so for larger corpora, with more tokens, they are longer\n",
    "\n",
    "![](./images/One-hot%20Word%20Embeddings.png)\n",
    "\n",
    "> Overall, we want to avoid using 1-hot encodings to represent our words and try something else... word embeddings\n",
    "\n",
    "### Word embeddings\n",
    "\n",
    "Word embeddings are vector representations of tokens that contain a meaningful representation of what the word means. \n",
    "\n",
    "![](./images/Dense%20Word%20Embeddings.png)\n",
    "\n",
    "Where 1-hot encodings are \"sparse\", containing mostly zeros, word embeddings are \"dense\".\n",
    "\n",
    "![](./images/One-hot%20vs%20Dense%20Word%20Embeddings.png)\n",
    "\n",
    "## Word embeddings can be learnt in a number of ways\n",
    "\n",
    "### Directly maximise the vector similarity between words that appear in similar context - The Word2Vec algorithm\n",
    "\n",
    "Word2vec was the original algorithm used to create meaningful vector representations of words.\n",
    "\n",
    "It is based on the assumption that similar words appear in similar contexts. \n",
    "\n",
    "The famous quote from 1957 that highlights this assumption was: \"you shall know a word by the company it keeps\".\n",
    "\n",
    "At a high level, this is how it works:\n",
    "- Initialise random embeddings\n",
    "- Take pairs of words that appear close together (within a threshold distance)\n",
    "- Calculare their cosine similarity\n",
    "- Maximise this objective using gradient descent\n",
    "\n",
    "### They can be learnt for a specific problem\n",
    "\n",
    "Alternatively, word representations can be learnt from scratch by solving a specific downstream task, such as sentiment analysis.\n",
    "\n",
    "In this setup, the embeddings are simply a part of the model parameters, like the other weights and biases. The input to the model is the integer indexes of the tokens, the first layer of the model is an embedding layer which indexes out the row to use as a word embedding, and the output is whatever is required for the task, such as a classification for sentiment analysis.\n",
    "\n",
    "Learning word representations in this way can produce problem-specific representations. For example, words might have different representations in a translation task compared to those in a sentiment analysis task.\n",
    "\n",
    "Note that however you learn embeddings, what they represent will be determined by the data they are learnt from.\n",
    "\n",
    "The most common of these is BERT, which learns representations of words based on a domain agnostic language modelling problem.\n",
    "\n",
    "## Pre-trained word embeddings\n",
    "\n",
    "Learning meaningful word representations can take a lot of time and compute. \n",
    "Thankfully, we can take the embeddings learnt by others straight off the shelf.\n",
    "\n",
    "One of the most influential machine learning models \n",
    "\n",
    "It's not important to understand BERT at this point, but for now:\n",
    "- BERT stands for Bidirectional Encoding Representations using Transformers\n",
    "- It is trained to fill in the missing word in text\n",
    "- It contains the word embeddings within its first layer's parameters. These BERT embeddings are widely used as a good starting point for word embeddings.\n",
    "\n",
    "We will talk about BERT more later, but we can already start using it.\n",
    "\n",
    "Open the practical [notebbook](https://colab.research.google.com/github/AI-Core/Practicals-Public-dev/blob/main/Content/units/Deep%20Learning%20for%20NLP/0.%20Intro%20to%20AI%20for%20Text%20Data/0.%20Intro%20to%20NLP/Practicals/0.%20Visualising%20BERT%20Embeddings/Practical.ipynb) associated with this lesson to explore embeddings in BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
