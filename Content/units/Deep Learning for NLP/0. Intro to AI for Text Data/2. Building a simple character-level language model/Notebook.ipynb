{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is language modelling?\n",
    "\n",
    "Language modeling is the process of predicting the next word in a sequence of words based on the context provided by the previous words. It is a core task in natural language processing (NLP) and is used in a wide range of applications, including speech recognition, machine translation, and chatbots.\n",
    "\n",
    "In language modeling, the goal is to learn the probability distribution over sequences of words in a language. Given a sequence of words, the language model assigns a probability to each possible word that might come next in the sequence. This can be used to predict the next word in a sequence, generate text that is similar to a given input, or to evaluate the quality of a translation or a summary by comparing the probability of the generated text to the probability of the original text.\n",
    "\n",
    "Language models are typically trained on large corpora of text, such as books, articles, and websites, in order to learn the statistical properties of the language and the dependencies between words. They can be implemented using various types of neural networks, such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, or transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tokeniser\n",
    "\n",
    "The first thing we need to do, is to create a tokeniser that can take in our raw text and split it into a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser:\n",
    "    def __init__(self, txt):\n",
    "        txt = self.preprocess(txt) # Preprocess the text\n",
    "        unique_chars = set(txt) # Create a set of unique characters in the input text\n",
    "        self.id_to_token = dict(enumerate(unique_chars)) # Create a dictionary that maps character IDs to characters\n",
    "        self.token_to_id = {v: k for k, v in self.id_to_token.items()} # Create a dictionary that maps characters to character IDs\n",
    "\n",
    "    def preprocess(self, str):\n",
    "        txt = txt.lower() # Convert the lyrics to lowercase\n",
    "\n",
    "    def encode(self, txt):\n",
    "        txt = self.preprocess(txt) # Preprocess\n",
    "        return [self.token_to_id[char] for char in str.strip(txt)] # Encode the input string by mapping its characters to character IDs\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return \"\".join([self.id_to_token[id] for id in token_ids]) # Decode the input list of character IDs by mapping them to characters\n",
    "\n",
    "def get_tokeniser():\n",
    "    with open('lyrics.txt', 'r') as file:\n",
    "        txt = file.read()\n",
    "\n",
    "    # Create a Tokeniser object to encode and decode the text\n",
    "    return Tokeniser(txt)\n",
    "\n",
    "get_tokeniser()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple character-level language modelling dataset\n",
    "\n",
    "A language modelling dataset consists of:\n",
    "- features which are the sequential words/tokens in a body of text\n",
    "- targets for each position in time which are the features shifted one step forward in time\n",
    "\n",
    "Implementation details:\n",
    "- Like all PyTorch datasets, our dataset needs a `__len__` method.\n",
    "- In this case, we will use the `__iter__` method that allows our dataset to be iterated through\n",
    "    - Usually datasets implement the `__getitem__` method, but in this case, we will just return a random slice of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LyricDataset():\n",
    "    def __init__(self, tokeniser, chunk_size=100):\n",
    "        \"\"\"\n",
    "        Initialize a LyricDataset object.\n",
    "        \n",
    "        Parameters:\n",
    "        chunk_size (int): The size of each chunk of data to be returned by the iterator.\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size  # The size of each chunk of data to be returned by the iterator\n",
    "        self.tokeniser = tokeniser\n",
    "\n",
    "        # Read in the lyrics from the file\n",
    "        with open('lyrics.txt', 'r') as file:\n",
    "            txt = file.read()\n",
    "\n",
    "        self.X = torch.tensor(self.tokeniser.encode(txt)) # Encode the text and store it in a tensor\n",
    "        self.Y = torch.tensor(np.roll(self.X, -1, axis=0)) # Shift the encoded text by one character and store it in a tensor\n",
    "\n",
    "        self.vocab_size = len(set(txt)) # Store the size of the vocabulary (i.e. the number of unique characters in the text)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of chunks in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.X) // self.chunk_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterate through random chunks of the dataset and yield them.\n",
    "        \"\"\"\n",
    "        for idx in range(len(self)):\n",
    "            # Randomly select a start index for the chunk\n",
    "            k = np.random.randint(0, len(self.X)-self.chunk_size)\n",
    "            # Select the chunk using a slice object\n",
    "            slc = slice(k, k+self.chunk_size)\n",
    "            yield self.X[slc], self.Y[slc]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        # store input parameters in the object so we can use them later on\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # required functions for model\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = torch.nn.RNN(hidden_size, hidden_size,\n",
    "                                n_layers, batch_first=True)  # TODO remove batch first\n",
    "        self.decoder = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.init_hidden(X.shape[0])\n",
    "        embedding = self.embedding(X)\n",
    "        outputs, final_hidden = self.rnn(embedding, self.hidden)\n",
    "        # print(hidden.shape)\n",
    "        # print(outputs.shape)\n",
    "        predictions = self.decoder(outputs)\n",
    "        # print(\"final output shape:\", predictions.shape)\n",
    "        return predictions\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new text\n",
    "\n",
    "Now we need to implement a method of our model that takes what it knows and uses it to generate new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        # store input parameters in the object so we can use them later on\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # required functions for model\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = torch.nn.RNN(hidden_size, hidden_size,\n",
    "                                n_layers, batch_first=True)  # TODO remove batch first\n",
    "        self.decoder = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.init_hidden(X.shape[0])\n",
    "        embedding = self.embedding(X)\n",
    "        outputs, final_hidden = self.rnn(embedding, self.hidden)\n",
    "        # print(hidden.shape)\n",
    "        # print(outputs.shape)\n",
    "        predictions = self.decoder(outputs)\n",
    "        # print(\"final output shape:\", predictions.shape)\n",
    "        return predictions\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "\n",
    "    def generate(self):\n",
    "        self.init_hidden(batch_size=1)\n",
    "        initial_token_id = random.randint(0, 49-1)\n",
    "        generated_token_ids = [initial_token_id]\n",
    "        initial_token_batch = torch.tensor(initial_token_id).unsqueeze(\n",
    "            0).unsqueeze(0)  # TODO SOS token\n",
    "        embedding = self.embedding(initial_token_batch)\n",
    "        for idx in range(100):  # generate 100 character sequence\n",
    "            outputs, self.hidden = self.rnn(embedding, self.hidden)\n",
    "            predictions = self.decoder(outputs)\n",
    "            # outputs has shape BxLxN=1x1xN\n",
    "            predictions = predictions.squeeze()  # remove 1-dims\n",
    "            chosen_token_id = torch.argmax(predictions)\n",
    "            generated_token_ids.append(int(chosen_token_id))\n",
    "            embedding = self.embedding(\n",
    "                chosen_token_id).unsqueeze(0).unsqueeze(0)\n",
    "        return generated_token_ids\n",
    "\n",
    "\n",
    "rnn = RNN()\n",
    "rnn.generate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training loop\n",
    "\n",
    "Now we have the model and the dataset, we need to pass the model through the dataset repeatedly and iteratively optimise the model parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "def train(model, dataset, tokeniser, epochs=1):\n",
    "    writer = SummaryWriter()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # choose optimiser\n",
    "    n_steps = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0  # stored the loss per epoch\n",
    "\n",
    "        for seq_inputs, seq_targets in tqdm(dataset):\n",
    "            loss = 0\n",
    "\n",
    "            # add batch dim TODO remove once using dataloader\n",
    "            seq_inputs = seq_inputs.unsqueeze(0)\n",
    "            predictions = model(seq_inputs)\n",
    "            seq_targets = seq_targets.unsqueeze(0)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            seq_targets = seq_targets.view(-1)  # BxT targets all in a line\n",
    "            loss = F.cross_entropy(predictions, seq_targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), n_steps)\n",
    "            n_steps += 1\n",
    "\n",
    "            epoch_loss += loss  # add the loss of this sequence to the loss of this epoch\n",
    "\n",
    "        epoch_loss /= len(dataset)  # avg loss per chunk\n",
    "\n",
    "        print('Epoch ', epoch, ' Avg loss/chunk: ', epoch_loss.item())\n",
    "        generated_token_ids = model.generate()\n",
    "        writer.add_text(\"Generated Text\", tokeniser.decode(\n",
    "            generated_token_ids)[:300], epoch)\n",
    "            # TODO stop on EOS token\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # HYPER-PARAMS\n",
    "    lr = 0.0005\n",
    "    epochs = 500\n",
    "    chunk_size = 100  # the length of the sequences which we will optimize over\n",
    "\n",
    "    hidden_size = 256\n",
    "    n_layers = 2\n",
    "\n",
    "    tokeniser = get_tokeniser()\n",
    "\n",
    "    dataset = LyricDataset(chunk_size=chunk_size)\n",
    "    n_tokens = len(dataset.tokeniser.id_to_token)\n",
    "    # instantiate our model from the class defined earlier\n",
    "    myrnn = RNN(n_tokens, hidden_size, n_layers)\n",
    "    train(myrnn, dataset, epochs)\n",
    "    # myrnn = RNN(n_tokens, hidden_size, n_layers)\n",
    "    # train(myrnn, dataset, epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
