{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**E**xploratory **D**ata **A**nalysis (EDA) is a critical precursor to model application. As the name implies, it is all about exploring data and validating that a certain dataset is clean and without missing values. Perhaps most interestingly, however, is the ability to use various visualisation techniques in our data to understand the underlying trends between the variables provided.\n",
    "\n",
    "Notably, _all_ the problems you will come across require the use of a model. Perhaps the task at hand is to 'simply' provide visualisations and identify interesting facts, which cannot be done through non-visual analyses.\n",
    "To use a model, it is important to formulate a hypothesis. This is because the identification of the intended target will be relevant in determining what parts of the data you explore. Nonetheless, here, we will be visualising data. To expand on the importance of this, see the image below, which demonstrates something known as *Anscombe's Quartet*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center><img src=\"https://www.researchgate.net/profile/Arch_Woodside2/publication/285672900/figure/fig4/AS:305089983074309@1449750528742/Anscombes-quartet-of-different-XY-plots-of-four-data-sets-having-identical-averages.png\">\n",
    "\n",
    "([source](https://www.researchgate.net/publication/285672900_The_general_theory_of_culture_entrepreneurship_innovation_and_quality-of-life_Comparing_nurturing_versus_thwarting_enterprise_start-ups_in_BRIC_Denmark_Germany_and_the_United_States)) </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Case in Point\n",
    "As a case in point, Anscombe's Quartet exemplifies the utmost importance of data visualisation. The image exhibits that the summary statistics (e.g. mean and variance) for all the data is the same. However, as can be observed, the distributions from which the data originate are considerably different. Had we not visualised our data, we would not have been able to trivially identify the data relationships.\n",
    "\n",
    "Data cleaning and addressing the problem of missing data fall under the EDA umbrella, which is a cyclical process comprising data exploration, issue identification, data cleaning and data exploration once more.\n",
    "\n",
    "## Data-Visualisation Demonstration\n",
    "We will be working with the 'multiple_choice_responses.csv' file from the [2019 Kaggle ML & DS Survey](https://www.kaggle.com/c/kaggle-survey-2019/data?select=multiple_choice_responses.csv), which is a 35-question survey performed on Kaggle users regarding the state of data science and ML. As indicated in their abstract, this survey received 19,717 usable respondents from 171 countries and territories. Countries or territories with less than 50 respondents were classified into a group named 'Other' for anonymity. Our task with this dataset is to identify what factors significantly impact the annual salary of those in DSML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Load the dataset, and return the first few rows.\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "df = pd.read_csv(\"https://aicore-files.s3.amazonaws.com/Data-Science/multiple_choice_responses.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There's a file in the DATA folder called 'questions_only.csv'. Load the dataset, and print all the questions.\n",
    "q_df = pd.read_csv(\"https://aicore-files.s3.amazonaws.com/Data-Science/questions_only.csv\")\n",
    "for i, question in enumerate(q_df.iloc[0]):\n",
    "    print(i, \"\\t\", question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this preview, we can note the following:\n",
    "- There are many questions (a considerable volume of data to analyse).\n",
    "- Some of the questions allow for multiple inputs. For these questions, the header row/column names have `_` appended to them, followed by some text.\n",
    "  - If the text is `OTHER_TEXT`, then it appears that following a categorical question, a text field that affords the recipient the option to expand is provided. It appears as though -1 indicates that the user did not write anything.\n",
    "  - If the text is `PART_N`, then it appears that it is a checkbox question (i.e. tick all that apply).\n",
    "  - They are not mutually exclusive.\n",
    "  \n",
    "## Conducting Analyses\n",
    "Conducting a column-by-column analysis of this data will be time-intensive. Therefore, we will determine the factors that may influence the salary and extract the relevant questions that meet this criterion from the list. This is partially why data science is considered an art; you may have a big dataset but are unsure of where to initiate the analysis. Based on hypotheses and the identification of the intended target for modeling, you need to intuitively guess (i.e. hypothesise) the factors that may exert significant influence. This is why domain expertise is important. However, the more you explore your data with the initial concepts you hypothesised, the more you will learn about the wider dataset.\n",
    "- Salary (target)\n",
    "- Age\n",
    "- Gender\n",
    "- Residence\n",
    "- Education\n",
    "- Job role/experience\n",
    "- Programming languages\n",
    "- ML frameworks\n",
    "\n",
    "From this list, we will extract these questions:\n",
    "**Q:** 1, 2, 3, 4, 5, 9, **10**, 15, 18, 24, 28.\n",
    "\n",
    "There are a couple of others that are relevant to the analysis; ideally, we would analyse them as well. However, we have limited time here, and we prioritise teaching you various visualisation techniques while building your intuition as to what to look for in the data.\n",
    "\n",
    "Some of these questions encompass multiple columns in our dataframe. Extracting the desired, relevant columns is not the most straightforward task. To improve your understanding, attempt to implement something that returns a new dataframe containing the relevant columns. If you are unsure of how to proceed after a couple of minutes, follow the steps below.\n",
    "<details>\n",
    "    <summary><b>> Click here for guidance.</b></summary>\n",
    "    <ul>\n",
    "        <li>Define a function which loops over a list of integers of the questions we intend to keep.</li>\n",
    "        <li>For every iteration, determine the number of columns from the current question to the next question in the dataframe (__not the next question we intend to extract__).</li>\n",
    "        <li>Extract/concatenate from the current column position to the current column position + 'distance' (probably using the <code>range()</code> function from Python).</li> \n",
    "    </ul>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_keep = [1,2,3,4,5,9,10,15,18,24,28]\n",
    "\n",
    "def extract_columns(df, idx_to_keep):\n",
    "    \n",
    "    new_df = pd.DataFrame() # empty dataframe\n",
    "    df_col_list = df.columns.tolist()\n",
    "    \n",
    "    for i in idx_to_keep:\n",
    "        column_name_base = \"Q{}\".format(i)\n",
    "        column_index = [df_col_list.index(col_name) for col_name in df_col_list if col_name.startswith(column_name_base)][0]\n",
    "               \n",
    "        next_column_name_base = \"Q{}\".format(i+1)\n",
    "        next_column_index = [df_col_list.index(col_name) for col_name in df_col_list if col_name.startswith(next_column_name_base)][0]\n",
    "         \n",
    "        col_idxs_to_extract = range(column_index, next_column_index)\n",
    "        relevant_cols_df = df.iloc[:, col_idxs_to_extract]\n",
    "        \n",
    "        new_df = pd.concat([new_df, relevant_cols_df], axis=1)\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "\n",
    "df_orig = df.copy(deep=True)\n",
    "df = extract_columns(df_orig, idx_to_keep)\n",
    "df = df[1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender analysis\n",
    "As the first step, we make an arbitrary choice: we start with Gender (Q2). We see that the data here are meant to be categorical; thus, after ensuring that that is the case, we simply plot the frequency of each of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Q2\"] = df[\"Q2\"].astype(\"category\")\n",
    "set(df[\"Q2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.histogram(df, \"Q2\", labels={\"value\": \"Gender\"}, title=\"Counts of Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country analysis\n",
    "Next, we plot the residencies of the individuals on a world map, heating them based on the number of respondents by country. This is known as a [choropleth map](https://plotly.com/python/choropleth-maps/) and will require us to change our country names to [three-letter ISO codes](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3).\n",
    "\n",
    "As the first step, we examine the country column, i.e. Q3, and note that some values require updating to something more conventional.\n",
    "\n",
    "Subsequently, we load a package to which we can pass a country and have it return the ISO code. Thereafter, we will utilise the new column to plot the choropleth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[\"Q3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the values that we think require updating:\n",
    "- Hong Kong (S.A.R.)\n",
    "- Iran, Islamic Republic of ...\n",
    "- United Kingdom of Great Britain and Northern Ireland\n",
    "- Viet Nam\n",
    "- South Korea\n",
    "\n",
    "Additionally, notice that there is an 'Other' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of 'Other':\", df[\"Q3\"].value_counts()[\"Other\"]/len(df) * 100)\n",
    "\n",
    "values_to_update = {\"Q3\": \n",
    "                    {\"Hong Kong (S.A.R.)\": \"Hong Kong\",\n",
    "                     \"Iran, Islamic Republic of...\": \"Iran\",\n",
    "                     \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "                     \"South Korea\": \"Republic of Korea\",\n",
    "                     \"Viet Nam\": \"Vietnam\"}}\n",
    "\n",
    "## Using the replace method, update the values in the relevant column.\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\n",
    "df.replace(values_to_update, inplace=True)\n",
    "set(df[\"Q3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "## Create a new dataframe which will hold only the unique countries, their country codes, and the number of instances of this country (WITHOUT 'Other').\n",
    "countries = df[\"Q3\"][df[\"Q3\"]!= \"Other\"].unique()\n",
    "countries_df = pd.DataFrame(countries, columns=[\"Country\"])\n",
    "countries_df[\"Count\"] = countries_df[\"Country\"].map(df[\"Q3\"].value_counts())\n",
    "\n",
    "## Create a new column in the dataframe containing the ISO country codes.\n",
    "country_codes = []\n",
    "for country in countries_df[\"Country\"]:\n",
    "    country_code = pycountry.countries.search_fuzzy(country)[0] # Take the first element returned from the search\n",
    "    country_codes.append(country_code.alpha_3)\n",
    "\n",
    "countries_df[\"Country Code\"] = country_codes\n",
    "countries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.choropleth(countries_df, locations=\"Country Code\", hover_name=\"Country\", color=\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age by gender analysis\n",
    "To consider age by gender, variable grouping is required as the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_gender_df = df[[\"Q1\", \"Q2\"]]\n",
    "age_gender_groups = age_gender_df.groupby([\"Q1\", \"Q2\"]).size().unstack()\n",
    "fig = px.bar(age_gender_groups, title=\"Count of Age per Gender\", labels={\"Q1\": \"Age\", \"value\": \"Count\"})\n",
    "fig.update_layout(legend_title_text='Gender')\n",
    "# fig.update_layout(barmode=\"group\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the ages with the highest occurrence frequency among the DSML employees are between 25 and 29. There are two probable reasons why these values are considerably higher than the others:\n",
    "1. Data science and ML are relatively new disciplines. However, direct education paths to these fields exist currently, with improved accessibility to young people.\n",
    "2. Think about _where_ the data were collected from. Older people are perhaps less likely to use 'resource' sites such as Kaggle because 1) they do not feel the need for such a learning experience and 2) they do not frequent social sites as much as young people.\n",
    "\n",
    "Thus far, we have simply arbitrarily produced plots. Perhaps a better plan is to perform a slightly more investigative analysis of the categories we outlined earlier. Let's do this with education analysis.\n",
    "\n",
    "### Education analysis\n",
    "\n",
    "Produce two plots:\n",
    "1. The participants' formal education.\n",
    "2. The count of formal education per gender. \n",
    "\n",
    "Display these as a grouped bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, \"Q4\", height=800, title=\"Count of Education\", labels={\"value\": \"Education level\"})\n",
    "fig.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_gender_df = df[[\"Q2\", \"Q4\"]]\n",
    "edu_gender_groups = edu_gender_df.groupby([\"Q4\", \"Q2\"]).size().unstack()\n",
    "fig = px.bar(edu_gender_groups, title=\"Education level count per Gender\",\n",
    "             labels={\"Q2\": \"Education\", \"value\": \"Count\"},\n",
    "             height=800)\n",
    "fig.update_layout(legend_title_text='Gender', xaxis={'categoryorder':'total descending'})\n",
    "# fig.update_layout(barmode=\"group\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create another diagram showing the same information but across four different plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, \"Q4\", \n",
    "                   facet_col=\"Q2\", \n",
    "                   color=\"Q2\",\n",
    "                   title=\"Counts of Education level per Gender\",\n",
    "                   labels={\"Q4\": \"Education Level\"},\n",
    "                   height=1000, \n",
    "                   facet_col_wrap=2, \n",
    "                   facet_col_spacing=0.1,\n",
    "                   )\n",
    "fig.update_layout(showlegend=False, xaxis={'categoryorder':'total descending'})\n",
    "fig.update_yaxes(matches=None, showticklabels=True)\n",
    "# fig.update_xaxes(showticklabels=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can establish the following:\n",
    "1. Those who choose to self-describe their gender are more likely to have a doctorate than a bachelor's. This is contrary to every other category, the members of which are more likely to have a bachelor's degree than a doctorate. Although if we note the counts, we can observe that we are working with single-digit figures, not something we can statistically extrapolate.\n",
    "2. Those who preferred not to give their gender also preferred not to give their education level (relative to the other categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Sankey Diagrams](https://en.wikipedia.org/wiki/Sankey_diagram)\n",
    "\n",
    "Sankey diagrams are vital for plot analysis and data visualisation. The easiest way to start working with the Sankey diagram is to identify the terminating column. In this case, we will use the education level as the final column. Additionally, we will need 'counts', which refers to the total number of people in each education level. To save space on the diagram, we will generalise some of the levels.\n",
    "\n",
    "In this Sankey diagram, we will visualise the paths of gender, age and country to the education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want five levels of education: Bachelor's, Master's, Doctoral, Professional and Other.\n",
    "## Create a new dataframe with only the education level of the respondents, where their education information has been mapped to the level above.\n",
    "education_df = pd.DataFrame(df[\"Q4\"])\n",
    "education_df.rename(columns={\"Q4\": \"Education Level\"}, inplace=True)\n",
    "\n",
    "values_to_update = {\"Education Level\": \n",
    "                    {\"Some college/university study without earning a bachelor’s degree\": \"Other\",\n",
    "                     \"No formal education past high school\": \"Other\",\n",
    "                     \"I prefer not to answer\": \"Other\"}}\n",
    "\n",
    "education_df = education_df.replace(values_to_update)\n",
    "set(education_df[\"Education Level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop na's from Education Level\n",
    "education_df.isna().sum()\n",
    "education_df = education_df.dropna(subset=[\"Education Level\"])\n",
    "education_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the gender, age and region columns to the new dataframe. Name the columns appropriately\n",
    "cols_to_join = [\"Q1\", \"Q2\", \"Q3\"]\n",
    "desired_col_names = [\"Age\", \"Gender\", \"Region\"]\n",
    "for col, name in zip(cols_to_join, desired_col_names):\n",
    "    education_df[name] = df[col]\n",
    "    \n",
    "education_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualisation purposes let's create:\n",
    "# 1. wider age bins as 18-29, 30-49, 50-69 and 70+\n",
    "# 2. group genders as \"Male\", \"Female\", \"Other\"\n",
    "# 3. Convert countries to continents - apart from 'India', 'The United States of America' and 'Other'\n",
    "\n",
    "## Overwrite the age and gender columns so that ages are now: 18-29, 30-49, 50-69 and 70+ and genders are \"Male\", \"Female\" and \"Other\"\n",
    "values_to_update = {\n",
    "    \"Age\": {\"18-21\": \"18-29\", \"22-24\": \"18-29\", \"25-29\": \"18-29\",\n",
    "            \"30-34\": \"30-49\", \"35-39\": \"30-49\", \"40-44\": \"30-49\", \"45-49\": \"30-49\",\n",
    "            \"50-54\": \"50-69\", \"55-59\": \"50-69\", \"60-69\": \"50-69\"\n",
    "           },\n",
    "    \"Gender\": {\"Prefer not to say\": \"Other\", \"Prefer to self-describe\": \"Other\"}\n",
    "}\n",
    "\n",
    "education_df = education_df.replace(values_to_update)\n",
    "education_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycountry_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry_convert as pc\n",
    "## Map countries to their relevant continents, unless the country is India, The United States of America, or Other\n",
    "countries_to_not_map = [\"India\", \"United States of America\", \"Other\"]\n",
    "countries_to_map_to_continents = set(education_df[\"Region\"])\n",
    "for country in countries_to_not_map:\n",
    "    countries_to_map_to_continents.discard(country)\n",
    "\n",
    "countries_continent_dict = dict()\n",
    "for country in countries_to_map_to_continents:\n",
    "    country_alpha2 = pycountry.countries.search_fuzzy(country)[0].alpha_2\n",
    "    continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "    continent_name = pc.convert_continent_code_to_continent_name(continent_code)\n",
    "    countries_continent_dict[country] = continent_name\n",
    "\n",
    "to_update = {\"Region\": countries_continent_dict}\n",
    "education_df = education_df.replace(to_update)\n",
    "education_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-index the columns in our desired order for the diagram for convenience.\n",
    "education_df = education_df.reindex([\"Gender\", \"Age\", \"Region\", \"Education Level\"], axis=1)\n",
    "\n",
    "col_names = education_df.columns.tolist()\n",
    "node_labels = []\n",
    "num_categorical_vals_per_col = []\n",
    "for col in col_names:\n",
    "    uniques = education_df[col].unique().tolist()\n",
    "    node_labels.extend(uniques)\n",
    "    num_categorical_vals_per_col.append(len(uniques))\n",
    "    \n",
    "node_labels, num_categorical_vals_per_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `num_categorical_vals_per_col` parameter provides information on the values from the previous level that we need to map to the next.\n",
    "\n",
    "Now, we need to construct a `link` dictionary. This is slightly less straightforward than that above. Our `link` dictionary will contain three lists: `source`, `target` and `value`. The `source` and `target` indicate the nodes that we intend to interconnect, and `value` indicates the quantity with which we intend to 'fill' the connection. The `source` and `target` are numerical indexes of the `node_labels` list we created above.\n",
    "\n",
    "We will link each category per column (source category) to all the other categories of the next column (target category) with the `size` of the number of the source categories that are mapped to the target categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_df.groupby([\"Gender\", \"Age\"]).size()[\"Female\"][\"18-29\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "value = []\n",
    "colors = []\n",
    "for i, num_categories in enumerate(num_categorical_vals_per_col):\n",
    "    \n",
    "    if i == len(num_categorical_vals_per_col)-1:\n",
    "        break\n",
    "    \n",
    "    # index allows us to refer to the categories by index from the `node_labels` list\n",
    "    start_index = sum(num_categorical_vals_per_col[:i])\n",
    "    start_index_next = sum(num_categorical_vals_per_col[:i+1])\n",
    "    end_index_next = sum(num_categorical_vals_per_col[:i+2])\n",
    "#     print(start_index, start_index_next, end_index_next)\n",
    "    \n",
    "    # i can also provide the category column to refer to\n",
    "    col_name = col_names[i]\n",
    "    next_col_name = col_names[i+1]\n",
    "    \n",
    "    grouped_df = education_df.groupby([col_name, next_col_name]).size()\n",
    "#     print(grouped_df)\n",
    "    \n",
    "    for source_i in range(start_index, start_index_next):\n",
    "        for target_i in range(start_index_next, end_index_next):\n",
    "            source.append(source_i)\n",
    "            target.append(target_i)\n",
    "            source_label = node_labels[source_i]\n",
    "            target_label = node_labels[target_i]\n",
    "            # if the index does not exist in the grouped_df, then the value is 0\n",
    "            try:\n",
    "                value.append(grouped_df[source_label][target_label])\n",
    "            except:\n",
    "                value.append(0)\n",
    "            \n",
    "            random_color = list(np.random.randint(256, size=3)) + [random.random()]\n",
    "            random_color_string = ','.join(map(str, random_color))\n",
    "            colors.append('rgba({})'.format(random_color_string))\n",
    "\n",
    "print(source)\n",
    "print(target)\n",
    "print(value)\n",
    "\n",
    "link = dict(source=source, target=target, value=value, color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = node_labels,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = link)])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram (Gender, Age, Region, Education)\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age\n",
    "\n",
    "Earlier, we examined age briefly. Here, we explore age more in depth. Using the original `df`, produce the following plots:\n",
    "1. A facet plot of the count of education levels by age.\n",
    "2. A facet plot of the different roles by age.\n",
    "\n",
    "Next, we will cover the following:\n",
    "1. A plot of the count of the different languages. \n",
    "2. Subplots/facet plots of the count of different languages per age.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will sort the df by Age so that our plot displays based on the age.\n",
    "df = df.sort_values(by=[\"Q1\"])\n",
    "\n",
    "fig = px.histogram(df, \"Q4\", facet_col=\"Q1\",\n",
    "             color=\"Q1\",\n",
    "             title=\"Counts of Education level per Age\",\n",
    "             labels={\"Q1\": \"Age\", \"Q4\": \"Education Level\"},\n",
    "             height=1000, \n",
    "             facet_col_wrap=4, \n",
    "             facet_col_spacing=0.1)\n",
    "\n",
    "fig.update_layout(showlegend=False, xaxis={'categoryorder':'total descending'})\n",
    "fig.update_yaxes(matches=None, showticklabels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the results for the younger age groups are expected: 18-21 year olds typically are not old enough to do master's degrees; hence, the number of bachelor's is higher for that group. However, for almost every other age group, Master's degrees are prominent. Curiously, those over 70 are more likely to have a doctorate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[\"Q5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(df, \"Q5\", facet_col=\"Q1\",\n",
    "             color=\"Q1\",\n",
    "             title=\"Counts of Education level per Age\",\n",
    "             labels={\"Q1\": \"Age\", \"Q5\": \"Job Role\"},\n",
    "             height=2000, \n",
    "             facet_col_wrap=2, \n",
    "             facet_col_spacing=0.1)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(showticklabels=True, tickangle=45)\n",
    "fig.update_yaxes(matches=None, showticklabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data are in columns 18_p1 and 18_p12.\n",
    "# The first step is to create a new column called 'Known programming languages', and per row, create a comma-separated list containing the programming languages they know (certainly excluding NaNs).\n",
    "programming_cols = [\"Q18_Part_{}\".format(str(i)) for i in range(1, 13)]\n",
    "programming_df = df[programming_cols]\n",
    "programming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_col = []\n",
    "for row in programming_df.itertuples(index=False):\n",
    "    languages_known = [language for language in row if isinstance(language, str)]\n",
    "    programming_col.append(\",\".join(languages_known))\n",
    "    \n",
    "programming_df[\"languages_known\"] = programming_col\n",
    "programming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we trim the new df so it only has our new column.\n",
    "programming_df.drop(labels=programming_cols, axis=1, inplace=True)\n",
    "programming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume blanks mean they don't know a language and replace both the blanks and \"None\" with \"None/NA\"\n",
    "values_to_update = {\"languages_known\": {\"\": \"None/NA\", \"None\": \"None/NA\"}}\n",
    "programming_df = programming_df.replace(values_to_update)\n",
    "programming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the get_dummies method to create new columns. \n",
    "language_dummies = programming_df['languages_known'].str.get_dummies(sep=',')\n",
    "language_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(language_dummies.sum(), labels={\"index\": \"Programming Language\"}, title=\"Count of Programming Languages\")\n",
    "fig.update_layout(showlegend=False, xaxis={'categoryorder':'total descending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 4, we retrieve the ages from the original dataframe and join them to this new dataframe.\n",
    "ages = df[\"Q1\"]\n",
    "language_dummies_with_age = language_dummies.join(ages).rename(columns={\"Q1\": \"Age\"})\n",
    "language_dummies_with_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_languages_by_age = language_dummies_with_age.groupby([\"Age\"]).sum()\n",
    "px.bar(programming_languages_by_age)\n",
    "# px.bar(programming_languages_by_age.T)\n",
    "programming_languages_by_age = programming_languages_by_age.reindex(\n",
    "    programming_languages_by_age.mean().sort_values().index, axis=1)\n",
    "programming_languages_by_age = programming_languages_by_age.iloc[:, ::-1]\n",
    "programming_languages_by_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_languages_by_age_row_norm = programming_languages_by_age.div(programming_languages_by_age.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# programming_languages_by_age.index\n",
    "programming_languages = programming_languages_by_age_row_norm.columns.tolist()\n",
    "fig = make_subplots(4, 3, subplot_titles=programming_languages_by_age_row_norm.index)\n",
    "for i, age_range in enumerate(programming_languages_by_age_row_norm.index):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=programming_languages, y=programming_languages_by_age_row_norm.iloc[i]),\n",
    "        row=row, col=col\n",
    "    )\n",
    "fig.update_layout(showlegend=False, height=1000, title=\"Percent of Known Programming Languages by Age\")\n",
    "fig.update_yaxes(tickformat=\"%\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though everyone likes Python. Younger people (who are most likely to be doing a bachelor's) know more lower-level languages, such as C, C++ and Java. This could be because they are required to study these languages at the university. As data scientists specialise more in their careers, they appear to move away from these languages into more typical DSML-related languages. The 60-69 age group has a high percentage of R users relative to the other age groups, whereas it appears as though most people over 70 do not know any programming language.\n",
    "\n",
    "#### Job roles\n",
    "Perhaps more useful to us is the popularity (occurrence frequency) of programming languages relative to job roles (Q5). Create a plot that demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join the Job Titles question with the language_dummies dataframe.\n",
    "language_dummies_with_job = language_dummies.join(df[\"Q5\"]).rename(columns={\"Q5\": \"Job Title\"})\n",
    "language_dummies_with_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group by Job Title, aggregate the number, normalise each row, and sort the dataframe by the mean of the columns.\n",
    "languages_job_title_grouped = language_dummies_with_job.groupby([\"Job Title\"]).sum()\n",
    "languages_job_title_grouped = languages_job_title_grouped.div(languages_job_title_grouped.sum(axis=1), axis=0)\n",
    "languages_job_title_grouped = languages_job_title_grouped.reindex(\n",
    "    languages_job_title_grouped.mean().sort_values().index, axis=1)\n",
    "languages_job_title_grouped = languages_job_title_grouped.iloc[:, ::-1]\n",
    "languages_job_title_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(languages_job_title_grouped, title=\"Heatmap of Programming Languages and Job Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the data.\n",
    "programming_languages = languages_job_title_grouped.columns.tolist()\n",
    "fig = make_subplots(4, 3, subplot_titles=languages_job_title_grouped.index)\n",
    "for i, role in enumerate(languages_job_title_grouped.index):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "#     numbers = langugages_job_title_grouped.iloc[i]\n",
    "#     as_percent = [number / sum(numbers) for number in numbers]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=programming_languages, y=languages_job_title_grouped.iloc[i]),\n",
    "        row=row, col=col\n",
    "    )\n",
    "fig.update_layout(showlegend=False, height=1000, title=\"Percent of Known Programming Languages Usage per Job Role\")\n",
    "fig.update_yaxes(tickformat=\"%\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "- On average, Python is the most popular language among all job roles.\n",
    "- SQL is the most popular language among database engineers.\n",
    "- MATLAB is relatively more popular among research scientists than other job roles.\n",
    "- Statisticians prefer R over Python.\n",
    "- Students and software engineers prefer C++ over R.\n",
    "\n",
    "Now, we do one more plot: a heatmap of preferred frameworks relative to job roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the top 3 frameworks that each job role likes to use\n",
    "# 28p1 - 28p12\n",
    "## Create a dateframe which contains just the framework columns\n",
    "framework_cols = [\"Q28_Part_{}\".format(str(i)) for i in range(1, 13)]\n",
    "framework_df = df[framework_cols]\n",
    "framework_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a general function, `get_df_for_dummies()`, which accepts a dataframe, a column name prefix string, and an upper range and returns a dataframe populated with the range of columns based on the prefix string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_for_dummies(df, prefix_string, end_range, start_range=1):\n",
    "    ## HINT: use a range function and iteration to generate the column names.\n",
    "    dummies_cols = [prefix_string + str(i) for i in range(start_range, end_range)]\n",
    "    ## HINT: extract and return the column names from the dataframe\n",
    "    return df[dummies_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a column in this dataframe called 'frameworks used', and populate that column with comma-separated frameworks.\n",
    "framework_col = []\n",
    "for row in framework_df.itertuples(index=False):\n",
    "    frameworks_used = [framework for framework in row if isinstance(framework, str)]\n",
    "    framework_col.append(\",\".join(frameworks_used))\n",
    "    \n",
    "framework_df[\"frameworks_used\"] = framework_col\n",
    "framework_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace the blank and None columns with 'None/NA'.\n",
    "values_to_update = {\"frameworks_used\": {\"\": \"None/NA\", \"None\": \"None/NA\"}}\n",
    "framework_df = framework_df.replace(values_to_update)\n",
    "framework_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a general function, `get_dummies()`, which accepts a dataframe and returns a column populated with comma-separated values of each of the individual values over the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies_col(df, sep=\",\"):\n",
    "\n",
    "    ## initialise an empty list to hold the column of strings which will be used to create a dummies dataframe.\n",
    "    dummies_col = []\n",
    "    \n",
    "    ## iterate over each of the rows in the dataframe in a manner that enables access to the individual elements of the cells.\n",
    "    ## get a list of the values of the cells over the row (e.g. a list of programming languages). Make sure that you do not add nan's. \n",
    "    ## join these as a comma-separated string, and append it to the empty list for the column.\n",
    "    for row in df.itertuples(index=False):\n",
    "        values = [item for item in row if isinstance(item, str)]\n",
    "        dummies_col.append(sep.join(values))\n",
    "        \n",
    "    ## create a new column in the dataframe called 'dummies', which accepts the contents of the dummies column.\n",
    "    df[\"dummies\"] = dummies_col\n",
    "    \n",
    "    ## replace all the \"\" and \"None\"s from the dataframe with 'None/NA'.\n",
    "    values_to_update = {\"dummies\": {\"\": \"None/NA\", \"None\": \"None/NA\"}}\n",
    "    df = df.replace(values_to_update)\n",
    "    \n",
    "    ## return the new dataframe.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dummies dataframe for the frameworks.\n",
    "framework_dummies = framework_df['frameworks_used'].str.get_dummies(sep=',')\n",
    "framework_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a general function, `dummies_from_series()`, which accepts a dataframe and a separator argument, and returns dummies for the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies_from_series(series, sep=\",\"):\n",
    "    ## return a dummies dataframe from the dataframe. \n",
    "    # remember which column was used to assign the strings over which we intend to create dummies.\n",
    "    return series[\"dummies\"].str.get_dummies(sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new dataframe which joins this dataframe with job roles.\n",
    "frameworks_for_job_role = framework_dummies.join(df[\"Q5\"]).rename(columns={\"Q5\": \"Job Title\"})\n",
    "frameworks_for_job_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group the dataframe by the job title, and aggregate over the programming languages.\n",
    "frameworks_for_job_role_grouped = frameworks_for_job_role.groupby([\"Job Title\"]).sum()\n",
    "frameworks_for_job_role_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a general function, `group_dummies_by()`, which accepts a dummies dataframe and a Series, and returns the dummies grouped and aggregated by the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_dummies_by(dummies_df, series):\n",
    "    series_name = series.name\n",
    "    to_group = dummies_df.join(series)\n",
    "    grouped = to_group.groupby([series_name]).sum()\n",
    "    \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_df = get_df_for_dummies(df, \"Q28_Part_\", 13)\n",
    "framework_df = get_dummies_col(framework_df)\n",
    "framework_dummies = dummies_from_series(framework_df)\n",
    "frameworks_for_job_role_grouped = group_dummies_by(framework_dummies, df[\"Q5\"])\n",
    "frameworks_for_job_role_grouped = frameworks_for_job_role_grouped.div(\n",
    "    frameworks_for_job_role_grouped.sum(axis=1), axis=0)\n",
    "frameworks_for_job_role_grouped.index.rename(\"Job Role\", inplace=True)\n",
    "frameworks_for_job_role_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Produce a heatmap of the above dataframe!\n",
    "px.imshow(frameworks_for_job_role_grouped, title=\"Heatmap of preferred Frameworks per Job Role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "- None/NA may be slightly misleading. It does not always imply that the role has no need for frameworks because it also incorporates people who may not have answered the question since their tool of use was not provided as an option. For example, a statistician may be using an R framework whose option is not provided here.\n",
    "- Scikit-learn is the most popular listed tool.\n",
    "- There appears to be a high occurrence frequency of random forest among statisticians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly compensation\n",
    "\n",
    "Now, we begin considering our target variable. We will start at the basic level and work our way up. We plot a histogram of salary earnings and sort this plot based on salary ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, \"Q10\", labels={\"Q10\": \"Salary\"}, title=\"Count of salary ranges (Unsorted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we sort the x axis in order of numerical values. In other words, the leftmost column is `$0-999`, and the rightmost is `> $500,000`. \n",
    "\n",
    "Spend some time pondering on how you would solve this problem. If after a few minutes you do not make headway, follow the steps below.\n",
    "<details>\n",
    "<summary><b>> Click here for guidance.</b></summary>\n",
    "<ul>\n",
    "<li>Create a new dataframe with only the salaries.</li>\n",
    "<li>Retrieve the set of salaries.</li>\n",
    "<li>Map the salary categories to an int, where the int is the first numerical part of the string (e.g. <code>{\"$0-999\": 0, \"100,000-124,999\": 100000}</code>). This part will require you to use the `.replace()` and `.split()` methods native to Python.\n",
    "<li>Replace the salaries in the dataframe with the integer value.</li>\n",
    "<li>Sort the dataframe in ascending numerical order.</li>\n",
    "<li>Reverse the mapping, and replace the ints with their string variants.</li>\n",
    "<li>Plot the dataframe, and replace the x labels with the salary strings.</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = pd.DataFrame(df[\"Q10\"])\n",
    "salary_df.rename(columns={\"Q10\": \"Salary\"}, inplace=True)\n",
    "salary_set = set(salary_df[\"Salary\"])\n",
    "salary_string_int_dict = dict()\n",
    "\n",
    "for string_salary in salary_set:\n",
    "    \n",
    "    if isinstance(string_salary, float): continue\n",
    "        \n",
    "    salary = string_salary.replace(\"$\", \"\").replace(\"> \", \"\").replace(\",\", \"\")\n",
    "    salary = salary.split(\"-\")[0]\n",
    "    salary_string_int_dict[string_salary] = int(salary)\n",
    "\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "salary_df = salary_df.replace(values_to_update)\n",
    "salary_df = salary_df.sort_values(\"Salary\")\n",
    "\n",
    "salary_int_string_dict = {v:k for k,v in salary_string_int_dict.items()}\n",
    "values_to_update = {\"Salary\": salary_int_string_dict}\n",
    "salary_df = salary_df.replace(values_to_update)\n",
    "\n",
    "percent_na = np.round(100 * salary_df[\"Salary\"].isna().sum()/len(salary_df), 2)\n",
    "print(\"Percent of users who didn't answer the salary question:\", percent_na)\n",
    "px.histogram(salary_df, \"Salary\", title=\"Count of Salary ranges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 37% of the survey participants explicitly did not answer this question. There appears to be an oddly large amount of people who earn between \\\\$0 and \\\\$999 per annum. Perhaps, this is overly high because many people who did not want to answer the question (and who did not realise that it was optional) ticked this box. We can spot some further interesting facts. There appear to be 'two' peaks at vastly different salaries: at 10,000-14,999 and 100,000-124,999. Additionally, it appears as though there are some very rich kagglers, with 83 of them earning over \\\\$500k per annum.\n",
    "\n",
    "The top three common wage groups (par 0-999) appear to be 10,000-14,999, 100,000-124,999 and 30,000-39,999. Produce a choropleth plot of the median salary of the countries to better discern the correlation between earning expectancy and country of residence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Produce a choropleth plot of the median salary.\n",
    "median_salaries_df = df[[\"Q3\", \"Q10\"]]\n",
    "median_salaries_df.rename(columns={\"Q3\": \"Country\", \"Q10\": \"Salary\"}, inplace=True)\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "median_salaries_df = median_salaries_df.replace(values_to_update)\n",
    "median_salaries_df = median_salaries_df.groupby([\"Country\"]).median()\n",
    "median_salaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = []\n",
    "for country in median_salaries_df.index:\n",
    "    country_code = pycountry.countries.search_fuzzy(country)[0] # Take the first element returned from the search.\n",
    "    country_codes.append(country_code.alpha_3)\n",
    "\n",
    "median_salaries_df[\"Country Code\"] = country_codes\n",
    "median_salaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_series = median_salaries_df[\"Salary\"]\n",
    "values_to_update = {\"Salary\": salary_int_string_dict}\n",
    "median_salaries_df = median_salaries_df.replace(values_to_update)\n",
    "median_salaries_df[\"Salary Values\"] = salaries_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.choropleth(median_salaries_df, locations=\"Country Code\", hover_name=median_salaries_df.index, color=\"Salary Values\", hover_data=[\"Salary\"], title=\"Median Salaries by Country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the number of participants from the USA and India that we discerned earlier, we can take these values to be more correct to the underlying data-generation distribution than those for most other countries. Working under this assumption, in the US and Switzerland, a reasonable figure would be 100,000+, whereas in India, it would most probably be in the 7,500-9,999 range. It appears that Australia has some high-paying jobs as well. We would expect the average salary range in the UK to be higher than the 10,000-14,999 mark. What are some of the factors we could investigate to determine the reason for this salary variance?\n",
    "\n",
    "We consider salaries based on gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_by_gender_df = df[[\"Q2\", \"Q10\"]]\n",
    "salaries_by_gender_df.rename(columns={\"Q2\": \"Gender\", \"Q10\": \"Salary\"}, inplace=True)\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "salaries_by_gender_df = salaries_by_gender_df.replace(values_to_update)\n",
    "salaries_by_gender_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(salaries_by_gender_df, \"Gender\", \"Salary\", labels={\"Salary\": \"Salary (Lower Bound)\"}, title=\"Boxplot of Salary per Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it appears that any of the categories can reach the highest salary bracket, females appear to have the lowest median salary. Those who prefer to self-describe appear to be more likely to have higher average salaries as well.\n",
    "\n",
    "To determine the best-paying jobs, we plot the mean salary of each role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_job_df = df[[\"Q5\", \"Q10\"]]\n",
    "salaries_job_df.rename(columns={\"Q5\": \"Job Title\", \"Q10\": \"Salary\"}, inplace=True)\n",
    "\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "salaries_job_df = salaries_job_df.replace(values_to_update)\n",
    "salaries_job_df\n",
    "\n",
    "# salary_series = salaries_job_df[\"Salary\"]\n",
    "# salaries_job_df[\"Salary Bracket\"] = salary_series\n",
    "# salaries_job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mean_salaries = salaries_job_df.groupby([\"Job Title\"]).mean().reset_index().sort_values(by=\"Salary\", ascending=False)\n",
    "grouped_mean_salaries.dropna(inplace=True)\n",
    "px.bar(grouped_mean_salaries, \"Job Title\", \"Salary\", labels={\"Salary\": \"Mean Salary\"}, title=\"Mean Salary per Job Role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a more informative approach for representing these salaries, we employ a boxplot. We exclude the NAs and '0' salaries and visualise the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_job_df.dropna(inplace=True)\n",
    "salaries_job_df = salaries_job_df[(salaries_job_df[\"Salary\"] != 0)]\n",
    "fig = px.box(salaries_job_df, \"Job Title\", \"Salary\", labels={\"Salary\": \"Salary (Lower Bound of Bracket)\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "- The survey indicates that individuals in all job roles have the potential to gain jobs that pay \\\\$500k+, apart from database engineers.\n",
    "- Globally, software engineers and data analysts have the lowest average salaries, with a software engineer having a lower median salary than a data analyst, but a higher mean (as indicated in the first chart).\n",
    "- Product/project management and data science appear to be the most lucrative job roles, with the former slightly ahead.\n",
    "\n",
    "Next, we determine the percentage of applicable data scientists who earn above \\\\$500k relative to project managers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_scientists_above_500 = len(salaries_job_df[(salaries_job_df[\"Job Title\"] == \"Data Scientist\") & (salaries_job_df[\"Salary\"] == 500000)])\n",
    "num_project_managers_above_500 = len(salaries_job_df[(salaries_job_df[\"Job Title\"] == \"Product/Project Manager\") & (salaries_job_df[\"Salary\"] == 500000)])\n",
    "\n",
    "percent_ds_above_500 = 100 * num_data_scientists_above_500/len(salaries_job_df)\n",
    "percent_pm_above_500 = 100 * num_project_managers_above_500/len(salaries_job_df)\n",
    "\n",
    "print(\"The percent of Data Scientists who earn above $500,000: {}%\".format(np.round(percent_ds_above_500, 2)))\n",
    "print(\"The percent of Project Managers who earn above $500,000: {}%\".format(np.round(percent_pm_above_500, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we determine the effect of years of programming experience (Q11) on salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_experience_salary_df = df[[\"Q10\", \"Q15\"]]\n",
    "programming_experience_salary_df.rename(columns={\"Q10\": \"Salary\", \"Q15\": \"Programming Experience\"}, inplace=True)\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "programming_experience_salary_df = programming_experience_salary_df.replace(values_to_update)\n",
    "programming_experience_salary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_array = [\"I have never written code\", \"< 1 years\", \"1-2 years\", \"3-5 years\", \"5-10 years\", \"10-20 years\", \"20+ years\"]\n",
    "# fig = px.scatter(programming_experience_salary_df, \"Programming Experience\", \"Salary\", title=\"Density of Programming Experience vs Salary\")\n",
    "fig = px.scatter(programming_experience_salary_df, \"Programming Experience\", \"Salary\", facet_col=df[\"Q2\"],title=\"Density of Programming Experience vs Salary\")\n",
    "fig.update_traces(marker=dict(\n",
    "            opacity=0.05,\n",
    "            size=20,\n",
    "            line=dict(\n",
    "                color='MediumPurple',\n",
    "                width=0.5\n",
    "            )))\n",
    "fig.update_layout(xaxis={'categoryorder':'array', 'categoryarray':category_array})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "- There are people earning \\\\$500k+ who have not ever written code. Same with people who have under a year's worth of programming experience.\n",
    "- There is a trend, however, where more experienced programmers earn a higher salary across all salary brackets.\n",
    "- There are not that many experienced females, i.e. females with 10-20 years of experience.\n",
    "\n",
    "Now, we will plot another Sankey Diagram, tracking the effects of gender, age, degree, role and country on salary. To achieve this, generalise the code we wrote earlier for the Sankey diagram into a function, `get_sankey_data`, which accepts a re-indexed dataframe as an argument and returns the node_labels and link dictionary. Subsequently, use these items to plot a Sankey diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functionise the sankey diagram code I wrote earlier\n",
    "def get_sankey_data(reindexed_df):\n",
    "    col_names = reindexed_df.columns.tolist()\n",
    "    node_labels = []\n",
    "    num_categorical_vals_per_col = []\n",
    "    \n",
    "    for col in col_names:\n",
    "        uniques = reindexed_df[col].unique().tolist()\n",
    "        node_labels.extend(uniques)\n",
    "        num_categorical_vals_per_col.append(len(uniques))\n",
    "\n",
    "\n",
    "    source = []\n",
    "    target = []\n",
    "    value = []\n",
    "    for i, num_categories in enumerate(num_categorical_vals_per_col):\n",
    "\n",
    "        if i == len(num_categorical_vals_per_col)-1:\n",
    "            break\n",
    "\n",
    "        # index allows us to refer to the categories by index from the `node_labels` list\n",
    "        start_index = sum(num_categorical_vals_per_col[:i])\n",
    "        start_index_next = sum(num_categorical_vals_per_col[:i+1])\n",
    "        end_index_next = sum(num_categorical_vals_per_col[:i+2])\n",
    "\n",
    "\n",
    "        # i can also give us the category column to refer to\n",
    "        col_name = col_names[i]\n",
    "        next_col_name = col_names[i+1]\n",
    "\n",
    "        grouped_df = reindexed_df.groupby([col_name, next_col_name]).size()\n",
    "\n",
    "        for source_i in range(start_index, start_index_next):\n",
    "            for target_i in range(start_index_next, end_index_next):\n",
    "                source.append(source_i)\n",
    "                target.append(target_i)\n",
    "                source_label = node_labels[source_i]\n",
    "                target_label = node_labels[target_i]\n",
    "                # if the index doesn't exist in the grouped_df, then the value is 0\n",
    "                try:\n",
    "                    value.append(grouped_df[source_label][target_label])\n",
    "                except:\n",
    "                    value.append(0)\n",
    "\n",
    "#                 random_color = list(np.random.randint(256, size=3)) + [random.random()]\n",
    "#                 random_color_string = ','.join(map(str, random_color))\n",
    "#                 colors.append('rgba({})'.format(random_color_string))\n",
    "\n",
    "    link = dict(source=source, target=target, value=value)\n",
    "    return node_labels, link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new dataframe with the relevant variables with which we intend to plot our Sankey diagram, re-index it, and pass it to the get_sankey_data function.\n",
    "salaries_sankey_df = df[[\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\", \"Q10\"]]\n",
    "salaries_sankey_df.rename(columns={\"Q1\": \"Age\", \"Q2\": \"Gender\", \"Q3\": \"Country\", \"Q4\": \"Education\", \"Q5\": \"Role\", \"Q10\": \"Salary\"}, inplace=True)\n",
    "salaries_sankey_df = salaries_sankey_df.reindex([\"Gender\", \"Age\", \"Education\", \"Role\", \"Country\", \"Salary\"], axis=1)\n",
    "node_labels, link = get_sankey_data(salaries_sankey_df)\n",
    "node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = node_labels,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = link)])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram (Gender, Age, Education, Role, Country, Salary)\", font_size=10, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, we clean the above diagram:\n",
    "- Group countries by continent (again we will leave India and USA as is).\n",
    "- Create wider bins for the salaries (maybe five bins), excluding \\\\$0-999.\n",
    "- Add coding experience as a column in the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace countries with continents\n",
    "values_to_update = {\"Country\": countries_continent_dict}\n",
    "salaries_sankey_df = salaries_sankey_df.replace(values_to_update)\n",
    "salaries_sankey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows with $0-999 and create wider bins for salary\n",
    "set(salaries_sankey_df[\"Salary\"])\n",
    "salaries_sankey_df = salaries_sankey_df[salaries_sankey_df[\"Salary\"] != \"$0-999\"]\n",
    "under_30000, under_80000, under_150000, under_300000, under_500000 = \"0 - 29,999\", \"30,000 - 79,999\", \"80,000 - 149,999\", \"150,000 - 299,999\", \"300,000 - 500,000\" \n",
    "salary_wider_bins_dict = dict()\n",
    "for salary_string in set(salaries_sankey_df[\"Salary\"]):\n",
    "    \n",
    "    if salary_string == \"> $500,000\" or isinstance(salary_string, float):\n",
    "        continue\n",
    "    \n",
    "    salary_upper_bound = salary_string.split(\"-\")[-1]\n",
    "    salary_upper_bound = salary_upper_bound.replace(\",\", \"\")\n",
    "    salary_upper_bound = int(salary_upper_bound)\n",
    "    \n",
    "    if salary_upper_bound < 30000:\n",
    "        salary_wider_bins_dict[salary_string] = under_30000\n",
    "    elif salary_upper_bound < 80000:\n",
    "        salary_wider_bins_dict[salary_string] = under_80000\n",
    "    elif salary_upper_bound < 150000:\n",
    "        salary_wider_bins_dict[salary_string] = under_150000\n",
    "    elif salary_upper_bound < 300000:\n",
    "        salary_wider_bins_dict[salary_string] = under_300000\n",
    "    elif salary_upper_bound < 500000:\n",
    "        salary_wider_bins_dict[salary_string] = under_500000\n",
    "\n",
    "values_to_update = {\"Salary\": salary_wider_bins_dict}\n",
    "salaries_sankey_df = salaries_sankey_df.replace(values_to_update)\n",
    "salaries_sankey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column in salaries_sankey_df for the programming experience length.\n",
    "salaries_sankey_df[\"Programming Experience\"] = df[\"Q15\"]\n",
    "salaries_sankey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Re-index, and plot the Sankey diagram.\n",
    "# Exclude the age and gender columns, and reindex the dataframe as\n",
    "# Role, Programming Experience, Region, Education and Salary.\n",
    "salaries_sankey_df = salaries_sankey_df.rename(columns={\"Country\": \"Region\"})\n",
    "salaries_sankey_df = salaries_sankey_df.reindex([\"Role\", \"Programming Experience\", \"Region\", \"Education\", \"Salary\"], axis=1)\n",
    "node_labels, link = get_sankey_data(salaries_sankey_df)\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = node_labels,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = link)])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram (Role, Programming Experience, Region, Education, Salary)\", font_size=10, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information and the paths to the different salary brackets become a lot clearer to interpret. Interestingly, there are some software engineers and data scientists who have never written code. Since this is highly improbable and they are few, we could chalk it up to anomalies. The majority of people earning over 150,000 seem to hold either a master's degree or a doctorate. Experimenting with the order of indexing can allow you to quickly draw insights from different combinations of columns (e.g. by putting Education before Job Role, we can see what roles people are likely to go into based on their education).\n",
    "\n",
    "One last plot! Question 9 asks about the skills and responsibilities that their current job entails. Produce subplots of these skills, aggregated for each salary bracket. Use the six salary brackets we defined previously. Ensure that the columns of the skill dataframes you create are intact, and demonstrate your plot/and any other python output in an interpretable manner, whether this be by the strings on the axis or the ordering of the facets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_df = get_df_for_dummies(df, \"Q9_Part_\", 9)\n",
    "skills_df = get_dummies_col(skills_df, sep=\"::\")\n",
    "skills_dummies = dummies_from_series(skills_df, sep=\"::\")\n",
    "skills_grouped = group_dummies_by(skills_dummies, salaries_sankey_df[\"Salary\"])\n",
    "skills_grouped = skills_grouped.reindex([\"0 - 29,999\", \"30,000 - 79,999\", \"80,000 - 149,999\", \"150,000 - 299,999\", \"300,000-500,000\", \"> $500,000\"])\n",
    "skills_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_df_columns = skills_grouped.columns\n",
    "skills_df_columns_mapping = {col: i for i, col in enumerate(skills_df_columns)}\n",
    "skills_grouped = skills_grouped.rename(columns=skills_df_columns_mapping)\n",
    "[print(i, \"\\t\", col) for col, i in skills_df_columns_mapping.items()]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Produce plots! The x axis should be the top four programming languages, while the y axis should be their count.\n",
    "fig = make_subplots(2, 3, subplot_titles=skills_grouped.index)\n",
    "for i, role in enumerate(skills_grouped.index):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    skills_values = skills_grouped.iloc[i]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=skills_values.index, y=skills_values),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    fig.update_xaxes(type=\"category\")\n",
    "fig.update_layout(showlegend=False, height=800, title=\"Skills/Responsibilities per Salary Bracket\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "- Across all salary brackets, many individuals chose option 0 (analyze and understand data to influence product or business decisions).\n",
    "- Apart from the lowest and highest brackets, it appears that many jobs involve 3 (building prototypes that explore the application of ML to new areas).\n",
    "- The top two salary brackets have a higher relative frequency of answer 4 (do research that advances the state of the art of ML).\n",
    "- The last three brackets appear to have a higher proportion of employees who chose option 5 (experimentation and iteration to improve existing ML models) as a responsibility.\n",
    "- Option 7, None/NA, appears to be more prevalent in the bottom two brackets and the uppermost bracket. This may be because none of the job descriptions apply to these individuals, whereas the other three brackets have job responsibilities focused more on data science and ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of\n",
    "\n",
    "- EDA and its importance.\n",
    "- how to apply various visualisation techniques.\n",
    "- how to determine the suitable plot for different situations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2592652612463181e69ac003232387e3e9a99279aa6b168e76f5df16d5110f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
