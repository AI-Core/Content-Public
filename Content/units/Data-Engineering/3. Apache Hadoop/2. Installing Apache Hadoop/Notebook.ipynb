{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Installation\n",
    "\n",
    "> It's highly recommended to use a Linux system as Hadoop  is known to cause issues when used with Windows\n",
    "\n",
    "Now that we have a high-level understanding of what Hadoop is, let's download and install it to get a better feel of how it operates. \n",
    "\n",
    "At a high-level, to get Hadoop working on your system, the steps involve:\n",
    "\n",
    "- Downloading the appropriate Hadoop package (the version to select depends on several criteria such as compatability with other big data tools)\n",
    "- Extracting (untar) the files and directories into an appropriate folder on your system\n",
    "- Configuring the following files:\n",
    "\n",
    "       .bashrc\n",
    "       hadoop-env.sh\n",
    "       core-site.xml\n",
    "       hdfs-site.xml\n",
    "       mapred-site-xml\n",
    "       yarn-site.xml\n",
    "\n",
    "_Note: We'll be using Linux (Ubuntu) for this tutorial, so if you're on a different operating system the steps might differ._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin:\n",
    "\n",
    "#### 1. Open your terminal and run the following commands to __update__ all existing applications on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt update\n",
    "sudo apt -y upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the update completes successfully, let's go ahead and reboot our system to ensure all the updates are applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reboot the system\n",
    "sudo reboot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ensure that __Java__ is installed on your system. \n",
    "\n",
    "_Note: It is recommended to use `Java 8` as this is the version is quite popular and is compatible with Hadoop and other tools that integrate with Hadoop._\n",
    "\n",
    "To do this, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Java version\n",
    "java -version\n",
    "\n",
    "# Check the Java copmiler version\n",
    "javac -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Java is installed correctly, you'll see an output showing the Java and compiler version.  \n",
    "\n",
    "Otherwise, you will need to download and install Java by running the following commands (sometimes both the JRE and JDK are required, so let's go ahead and install both just in case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java 8 runtime environment\n",
    "sudo apt-get install openjdk-8-jre \n",
    "\n",
    "# Install Java 8 development kit\n",
    "sudo apt-get install openjdk-8-jdk\n",
    "\n",
    "# Add Java to the Linux repository\n",
    "sudo add-apt-repository ppa:openjdk-r/ppa\n",
    "sudo apt-get update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that both the JDK and the JRE have installed correctly.  If all is well, run the below command and you should see the Java version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ java -version\n",
    "\n",
    "# Output should be similar to this\n",
    "java version \"1.8.0_201\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_201-b09)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javac -version\n",
    "\n",
    "# Output should be similar to this\n",
    "javac 1.8.0_312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If you have multiple Java versions installed, it's possible to switch between them.  To do this, check what available Java versions currently exist by running the below command:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Java versions already installed\n",
    "sudo update-alternatives --config java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a menu similar to the below one.  You can select the desired version directly by typing in its number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo update-alternatives --config java\n",
    "[sudo] password for hadoop:\n",
    "\n",
    "# # Output should be similar to this\n",
    "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
    "\n",
    "  Selection\tPath                                        \tPriority   Status\n",
    "------------------------------------------------------------\n",
    "* 0        \t/usr/lib/jvm/java-11-openjdk-amd64/bin/java  \t1111  \tauto mode\n",
    "  1        \t/usr/lib/jvm/java-11-openjdk-amd64/bin/java  \t1111  \tmanual mode\n",
    "  2        \t/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081  \tmanual mode\n",
    "\n",
    "Press <enter> to keep the current choice[*], or type selection number: 2\n",
    "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same step above for the Java compiler `javac`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo update-alternatives --config javac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Next, we need to ensure that the `JAVA_HOME` variable is correctly set up on your system.  To do that, run the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the variable is correctly set up, you should see a path show up similar to the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME\n",
    "\n",
    "# Expected output should be similar to:\n",
    "/usr/lib/jvm/java-8-openjdk-amd64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get no output, or if the output is simply repeating JAVA_HOME, then the variable is not set up correctly.\n",
    "\n",
    "To fix this, you will need to add the correct Java path to your `.bashrc` file.  To do this, first find the location of the Java installation, then open and update the `.bashrc` file to include the following lines at the end of the file (ensure you use the path to your Java folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the .bashrc file using the Nano editor (you can use any other editor like Vim if you prefer)\n",
    "sudo nano ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the below to the .bashrc file\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "export PATH=$PATH:$JAVA_HOME/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the file, we need to use the `source` command to enforce the changes in the operating system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to make sure everything is set up correctly, check that `JAVA_HOME` is working.  You should now be able to see the Java folder path correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Next, we should create a new user account for Hadoop.  HBase uses Hadoop's HDFS to store the data, so it's recommended to have a seperation of accuonts between the Hadoop file system and the Linux file system to avoid confusion.  Go ahead and type the below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new user called Hadoop\n",
    "sudo adduser hadoop\n",
    "sudo usermod -aG sudo hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then be asked for some additional information including a password for the new user account.  Go ahead and add that information.  You should see output similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output should be similar to the below\n",
    "Adding user `hadoop' ...\n",
    "Adding new group `hadoop' (1001) ...\n",
    "Adding new user `hadoop' (1001) with group `hadoop' ...\n",
    "Creating home directory `/home/hadoop' ...\n",
    "Copying files from `/etc/skel' ...\n",
    "New password:\n",
    "Retype new password:\n",
    "passwd: password updated successfully\n",
    "Changing the user information for hadoop\n",
    "Enter the new value, or press ENTER for the default\n",
    "    Full Name []: \t \n",
    "    Room Number []:\n",
    "    Work Phone []:\n",
    "    Home Phone []:\n",
    "    Other []:\n",
    "Is the information correct? [Y/n] y\n",
    "adiwany@dodz-vm:~$ sudo usermod -aG sudo hadoop\n",
    "adiwany@dodz-vm:~$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Now, we need to generate a __SSH key-pair__ for the new Hadoop user. Run the below commands to switch to the newly created Hadoop user and to create the new SSH key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the Hadoop user\n",
    "sudo su hadoop\n",
    "\n",
    "# Generate the SSH key-pair\n",
    "ssh-keygen -t rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, you should see output similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh-keygen -t rsa\n",
    "\n",
    "# Expected output:\n",
    "Generating public/private rsa key pair.\n",
    "Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):\n",
    "Created directory '/home/hadoop/.ssh'.\n",
    "Enter passphrase (empty for no passphrase):\n",
    "Enter same passphrase again:\n",
    "Your identification has been saved in /home/hadoop/.ssh/id_rsa\n",
    "Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub\n",
    "The key fingerprint is:\n",
    "SHA256:tol1mX4v1GrDLeHHq1Wa/nvKaZUMlEGDCZisTAkWVPc hadoop@dodz-vm\n",
    "The key's randomart image is:\n",
    "+---[RSA 3072]----+\n",
    "|  .=+.o.o.. ++o  |\n",
    "|  .  o.+.  o o.  |\n",
    "|\to .  E  .\t|\n",
    "| \to \to .   |\n",
    "|    \tS +  .o o|\n",
    "|   \t+ =  o .*.|\n",
    "|  \t. o .+.=+. |\n",
    "|       \t.O==..|\n",
    "|       \t..BB=+|\n",
    "+----[SHA256]-----+\n",
    "hadoop@dodz-vm:~$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Next, add this newly generated key to the list of authorized SSH keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "sudo chmod 0600 ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no errors come up, then everything is set up correctly so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Verify that we can use SSH with the newly generated key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything runs smoothly, you should see output similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh localhost\n",
    "\n",
    "# Expected output\n",
    "The authenticity of host 'localhost (127.0.0.1)' can't be established.\n",
    "ECDSA key fingerprint is SHA256:fPYKPrq8VD1pNfI+7EXyKqQFFm4/eWi0+jjADURdHhU.\n",
    "Are you sure you want to continue connecting (yes/no/[fingerprint])? y\n",
    "Please type 'yes', 'no' or the fingerprint: yes\n",
    "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\n",
    "Welcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-41-generic x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management: \thttps://landscape.canonical.com\n",
    " * Support:    \thttps://ubuntu.com/advantage\n",
    "\n",
    "23 updates can be applied immediately.\n",
    "To see these additional updates run: apt list --upgradable\n",
    "\n",
    "Your Hardware Enablement Stack (HWE) is supported until April 2025.\n",
    "\n",
    "The programs included with the Ubuntu system are free software;\n",
    "the exact distribution terms for each program are described in the\n",
    "individual files in /usr/share/doc/*/copyright.\n",
    "\n",
    "Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\n",
    "applicable law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error saying something like __Connection Refused__, then the localhost is not properly set up or SSH is not yet installed.\n",
    "\n",
    "To install SSH, enter the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install ssh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install ssh\n",
    "\n",
    "# Expected output \n",
    "[sudo] password for hadoop:\n",
    "Reading package lists... Done\n",
    "Building dependency tree  \t \n",
    "Reading state information... Done\n",
    "The following packages were automatically installed and are no longer required:\n",
    "  chromium-codecs-ffmpeg-extra gstreamer1.0-vaapi\n",
    "  libgstreamer-plugins-bad1.0-0 libva-wayland2\n",
    "Use 'sudo apt autoremove' to remove them.\n",
    "The following additional packages will be installed:\n",
    "  ncurses-term openssh-server openssh-sftp-server ssh-import-id\n",
    "Suggested packages:\n",
    "  molly-guard monkeysphere ssh-askpass\n",
    "The following NEW packages will be installed:\n",
    "  ncurses-term openssh-server openssh-sftp-server ssh ssh-import-id\n",
    "0 upgraded, 5 newly installed, 0 to remove and 18 not upgraded.\n",
    "Need to get 693 kB of archives.\n",
    "After this operation, 6,130 kB of additional disk space will be used.\n",
    "Do you want to continue? [Y/n] y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Download and install `Hadoop`\n",
    "\n",
    "_Note:_ There are many Hadoop versions available to download and install. However, this tutorial uses version `2.10.1` as it's compatible with other tools that will be integrated with Hadoop in other notebooks. You can use any version you prefer but keep in mind that some bugs you may encounter could be different than what is covered in this notebook.\n",
    "\n",
    "Save the version to a variable as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Version=\"2.10.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then download the corresponding Hadoop version as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo wget https://www-eu.apache.org/dist/hadoop/common/hadoop-$Version/hadoop-$Version.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Extract the files and move the resulting folder to the location of your choice (we'll be using `/usr/local/hadoop`). Create the new directory if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Hadoop files\n",
    "tar -xzvf hadoop-$Version.tar.gz\n",
    "\n",
    "# Create the new folder\n",
    "sudo mkdir /usr/local/hadoop\n",
    "\n",
    "# Remove the downloaded tar file\n",
    "rm hadoop-$Version.tar.gz\n",
    "\n",
    "# Move everything to the new folder\n",
    "sudo mv hadoop-$Version/ /usr/local/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Now, we need to set `HADOOP_HOME` and add the directory containing the Hadoop binaries to your `.bashrc` file.  To do this, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Hadoop environment variables required to configure the tool on your system.  To do this, we need to add the following content to the end of the `.bashrc` file (remember to use your own Java and Hadoop folder paths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "export HADOOP_HOME=/usr/local/hadoop/hadoop-2.10.1\n",
    "export HADOOP_INSTALL=$HADOOP_HOME\n",
    "export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
    "export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
    "export YARN_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n",
    "export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file and exit.  It's vital to run the below command to apply the changes we just added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Next, we'll update the `hadoop-env.sh` file.  This file contains important configurations related to Hadoop's setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the `$JAVA_HOME` variable (i.e., remove the # sign) and _add the full path to the OpenJDK installation on your system without the bin directory_. If you don't know the Java path, run the following command to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readlink -f /usr/bin/javac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your file should look something like this:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hadoop-env3.png\" width=600>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. The `core-site.xml` file defines important HDFS and Hadoop cluster properties.  To set up Hadoop properly, we need to provide the URL of the NameNode (master node).  To do that, run the following command to open the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify that we'll be running Hadoop locally, add the following in between the `<configuration>` and `</configuration>` tags and save the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   <property>\n",
    "      <name>fs.default.name</name>\n",
    "      <value>hdfs://localhost:9000</value>\n",
    "      <description>The default file system URI</description>\n",
    "   </property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Next, we need to edit the `hdfs-site.xml` file.  This file stores the details regarding the location of the metadata, NameNode and DataNode directories. We'll first create those directories, then update the file to contain the new directory paths. Run the below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo mkdir $HADOOP_HOME/dfsdata\n",
    "sudo mkdir $HADOOP_HOME/dfsdata/namenode\n",
    "sudo mkdir $HADOOP_HOME/dfsdata/datanode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's update the `hdfs-site.xml` file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following properties to the file in between the `<configuration>` and `</configuration>` tags. If needed, adjust the NameNode and DataNode directories to reflect your custom locations (if you used directories different than above).  We'll also set the default HDFS data replication factor to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<property>\n",
    "  <name>dfs.data.dir</name>\n",
    "  <value>$HADOOP_HOME/dfsdata/namenode</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>dfs.data.dir</name>\n",
    "  <value>$HADOOP_HOME/dfsdata/datanode</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>dfs.replication</name>\n",
    "  <value>1</value>\n",
    "</property>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Edit the `mapred-site.xml` file to define the MapReduce required values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following property to change the default MapReduce framework value to `yarn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<property> \n",
    "  <name>mapreduce.framework.name</name> \n",
    "  <value>yarn</value> \n",
    "</property> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If you don't find the file, it could be because it hasn't been created yet. You'll find a template to use under the name of `mapred-site.xml.template` file. Go ahead and open that file and save a copy as `mapred-site.xml`_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Edit `yarn-site.xml` which is used to define YARN-specific settings by opening the file and adding the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following property between the `<configuration>` and `</configuration>` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<property>\n",
    "  <name>yarn.nodemanager.aux-services</name>\n",
    "  <value>mapreduce_shuffle</value>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the required Hadoop configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Reboot your system to ensure all new settings are loaded before running Hadoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemctl reboot -i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.  Validate Hadoop settings and configurations\n",
    "\n",
    "After completing the above steps, we want to make sure that Hadoop was properly set up.  To do this, we should do the following to check the Hadoop version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output\n",
    "Hadoop 2.10.1\n",
    "Subversion https://github.com/apache/hadoop -r 1827467c9a56f133025f28557bfc2c562d78e816\n",
    "Compiled by centos on 2020-09-14T13:17Z\n",
    "Compiled with protoc 2.5.0\n",
    "From source with checksum 3114edef868f1f3824e7d0f68be03650\n",
    "This command was run using /usr/lib/hadoop/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18. Now we are ready to start the Hadoop cluster.  To do this, we need to run a number of commands:\n",
    "\n",
    "- `start-dfs.sh` \n",
    "    -   This command starts HDFS\n",
    "- `start-yarn.sh`\n",
    "    -   This command starts YARN\n",
    "- `jps`\n",
    "    -   This command checks all Java processes to ensure the correct daemons are active\n",
    "\n",
    "__Note__: It's recommended to run these commands from inside your `HADOOP_HOME` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start HDFS \n",
    "bash start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time to start HDFS.  You should see output similar to:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/start-hdfs.png\" width=600>\n",
    "</p>\n",
    "\n",
    "\n",
    "Then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start YARN\n",
    "bash start-yarn.sh\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the below output:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/start-yarn-jps.png\" width=600>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If the `DataNode` process doesn't show up in the jps list, then we probably need to double check the correct HDFS directory is created and that we have the proper persmissions._\n",
    "\n",
    "The commands should be similar to the below (make sure to use your HDFS path as the below is just an example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Hadoop services\n",
    "bash stop-dfs.sh\n",
    "bash stop-yarn.sh\n",
    "\n",
    "# Grant the Hadoop user read/write folder access\n",
    "sudo chmod -R 755 $HADOOP_HOME/dfsdata/*\n",
    "sudo chown -R hadoop:hadoop $HADOOP_HOME/dfsdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above commands, we need to start the `dfs` and `yarn` services again as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the Hadoop services again\n",
    "bash start-dfs.sh\n",
    "bash start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you run `jps`, you should see the DataNode process correctly displayed along with the remaining Hadoop processes:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/datanode-jps.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Another possible error you may encounter is the `incompatible cluster ID error`.  To resolve this, delete both the `datanode` and `namenode` folders and create them again._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If the `NameNode` process doesn't appear in the list of processes, you may have to format it first before starting the HDFS and YARN services. To do this, run the below command:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the NameNode\n",
    "hadoop namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19. Use your preferred browser and navigate to your localhost URL or IP. The default port number 9870 gives you access to the Hadoop NameNode user interface, which allows you to monitor the Hadoop enviornment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see a page similar to this one:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hadoop-ui.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can use port 9864 to access the DataNode user interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:9864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And port 8088 can be used to view the YARN Resource Manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:8088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have Hadoop installed and ready to go!\n",
    "\n",
    "If you face persistant bugs that you can't resolve, check the Hadoop version you used and try another one as many buys are usually due to compatability issues between the Hadoop version, Java version and the operating system you are using. Different operating systems can cause different errors to occur."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
