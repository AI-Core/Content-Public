{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements accurately describes Spark Streaming?\n",
    "\n",
    "- Spark Streaming is a batch processing framework for processing large volumes of data\n",
    "- Spark Streaming is a real-time data processing framework for processing streaming data ***\n",
    "- Spark Streaming is a relational database management system\n",
    "- Spark Streaming is a machine learning framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a DStream in Spark Streaming and how does it differ from a RDD in Apache Spark?\n",
    "\n",
    "- A DStream is a sequence of static datasets, while a RDD is a continuous stream of data\n",
    "- A DStream is a continuous stream of data, while a RDD represents static datasets ***\n",
    "- A DStream and a RDD are both continuous streams of data, but a DStream is optimized for real-time processing\n",
    "- A DStream and a RDD are both static datasets, but a DStream is optimized for parallel processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the typical process of Spark Streaming?\n",
    "\n",
    "- Spark Streaming reads data from a file, processes it in real-time, and writes the output to a file\n",
    "- Spark Streaming reads data from a database, processes it in real-time, and writes the output to a database\n",
    "- Spark Streaming reads data from a streaming source, processes it in real-time, and writes the output to a streaming sink ***\n",
    "- Spark Streaming reads data from a batch source, processes it in near-real-time, and writes the output to a batch sink"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `StreamingContext` in Spark Streaming and what is its role in the streaming process?\n",
    "\n",
    "- `StreamingContext` is a data structure that represents the input data in Spark Streaming, and it is responsible for processing the data streams in real-time\n",
    "- `StreamingContext` is the entry point for Spark Streaming applications, and it represents the main interface for creating DStreams and configuring the streaming process ***\n",
    "- `StreamingContext` is a component that is responsible for writing the output data to external storage systems, such as HDFS or S3\n",
    "- `StreamingContext` is a machine learning library in Spark that is used for training models on real-time data streams\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following code snippets correctly creates a `StreamingContext` in Spark Streaming?\n",
    "\n",
    "- This code\n",
    "``` python\n",
    "from pyspark.streaming import StreamingContext\n",
    "ssc = StreamingContext(sparkContext, 10)\n",
    "```\n",
    "\n",
    "- This code\n",
    "``` python\n",
    "from pyspark import StreamingContext\n",
    "ssc = StreamingContext(\"local[2]\", \"MyStreamingApp\", 10)\n",
    "```\n",
    "\n",
    "- This code\n",
    "``` python\n",
    "from pyspark.streaming import StreamingContext\n",
    "ssc = StreamingContext(sparkConf, 10)\n",
    "```\n",
    "\n",
    "- This code ***\n",
    "``` python\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "sc = SparkContext(\"local[2]\", \"MyStreamingApp\")\n",
    "ssc = StreamingContext(sc, 10) \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does `awaitTermination()` method do in Spark Streaming?\n",
    "\n",
    "- `awaitTermination()` method waits for the completion of all batch processing tasks before stopping the streaming context\n",
    "- `awaitTermination()` method waits indefinitely until the streaming context is stopped or terminated by a user interrupt signal ***\n",
    "- `awaitTermination()` method waits for a specified amount of time for the streaming context to process the incoming data streams, and then stops the context\n",
    "- `awaitTermination()` method waits for the availability of new data streams and continuously processes the data until the streaming context is explicitly stopped\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are window operations in Spark Streaming?\n",
    "\n",
    "- A way to create subsets of data which are more performant\n",
    "- A way to perform aggregation on a window of data over a specified period of time ***\n",
    "- A way to filter out irrelevant data points from the incoming data stream\n",
    "- A way to perform machine learning algorithms on the incoming data streams\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the output of the following code that performs a window operation on a DStream?\n",
    "\n",
    "``` python\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sparkContext, 1)\n",
    "ssc.checkpoint(\"checkpoint_dir\")\n",
    "\n",
    "input_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "numbers_stream = input_stream.map(lambda x: int(x))\n",
    "windowed_stream = numbers_stream.window(10, 5)\n",
    "count_stream = windowed_stream.count()\n",
    "\n",
    "count_stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "```\n",
    "\n",
    "- The code will not execute as there is no data source defined\n",
    "- The code will calculate the count of numbers over a sliding window of 10 seconds and print the results every 5 seconds ***\n",
    "- The code will calculate the count of numbers over a sliding window of 5 seconds and print the results every 10 seconds\n",
    "- The code will calculate the sum of numbers over a sliding window of 10 seconds and print the results every 5 seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following methods can be used to save a DStream in Spark Streaming?\n",
    "\n",
    "- `dstream.saveAsTextFiles(\"output_dir\")`\n",
    "- `dstream.saveAsParquetFile(\"output_dir\")`\n",
    "- `dstream.saveAsSequenceFile(\"output_dir\")`\n",
    "- All of the above ***\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about `foreachRDD` in Spark Streaming?\n",
    "\n",
    "- `foreachRDD` is a transformation that applies a function to each RDD in a DStream ***\n",
    "- `foreachRDD` is an action that applies a function to each element in a DStream\n",
    "- `foreachRDD` is a method that writes each RDD in a DStream to a file system or external storage\n",
    "- `foreachRDD` is a method that allows custom processing of some Dataframes in a DStream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about `persist()` in Spark Streaming?\n",
    "\n",
    "- `persist()` is a method that saves a DStream to a file system or external storage\n",
    "- `persist()` is a transformation that applies a function to each RDD in a DStream\n",
    "- `persist()` is a method that caches the RDDs of a DStream in memory or disk for faster processing ***\n",
    "- `persist()` is an action that returns the first element of a DStream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about checkpointing in Spark Streaming?\n",
    "\n",
    "- Checkpointing is a method that saves a DStream to a file system or external storage\n",
    "- Checkpointing is a transformation that applies a function to each RDD in a DStream\n",
    "- Checkpointing is a mechanism that stores metadata about the state of a Spark Streaming application for fault tolerance ***\n",
    "- Checkpointing is an action that returns the first element of a DStream"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2592652612463181e69ac003232387e3e9a99279aa6b168e76f5df16d5110f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
