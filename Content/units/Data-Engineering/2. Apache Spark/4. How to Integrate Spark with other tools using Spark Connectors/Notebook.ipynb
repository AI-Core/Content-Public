{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Connectors\n",
    "\n",
    "## Apache Maven\n",
    "\n",
    "> Apache Maven is a build tool designed for managing and uniformly building any Java-based project. By defining your project in a __project object model(POM)__ Maven can build the project into __Java Archive(JAR)__ files which are easily distributed on Mavens central repository. \n",
    "\n",
    "### Maven as a build tool\n",
    "\n",
    "Maven is a __build tool__. So what does this mean and how can this help us? __Build tools__ are programs which help in the automation of __building, testing, compiling, and deployment__ of __source code__ into a useable application.\n",
    "\n",
    "Maven was defined to meet these objectives:\n",
    "- Make the build process easy \n",
    "- Provide a uniform build process\n",
    "- Provide quality project information\n",
    "- Encourage better development practices\n",
    "\n",
    "Maven as a build tool:\n",
    "\n",
    "- Normally used for Java based projects, complies code usually into `JAR` files but other types can be specified. \n",
    "- Keeps project build consistent through the use of __POM__ file which describes the __project structure, dependencies, configuration__ and __licenses__ when build your project.\n",
    "- Automates testing of the file during the build process.\n",
    "- Encourages sharing built projects on the Maven central repository https://mvnrepository.com/repos/central to share with other users.\n",
    "\n",
    "### The POM file\n",
    "\n",
    "When building a project it can rely on many different dependencies critical to its function. \n",
    "\n",
    "Navigating between many different websites can be tedious to find the correct versions and many different dependencies required for your project. This is what the __POM file__ is designed to solve, it gives you a uniform structure to define your project in code when building it.\n",
    "\n",
    "POM files are a representation of your Maven project written in `XML` and stored in a `pom.xml` file. \n",
    "\n",
    "This is where you will define dependencies, build configuration, licensing, and even the URL of where the project lives. The general layout and configurable tags are shown here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n",
    "  <modelVersion>4.0.0</modelVersion>\n",
    " \n",
    "  <!-- The Basics -->\n",
    "  <groupId>...</groupId>\n",
    "  <artifactId>...</artifactId>\n",
    "  <version>...</version>\n",
    "  <packaging>...</packaging>\n",
    "  <dependencies>...</dependencies>\n",
    "  <parent>...</parent>\n",
    "  <dependencyManagement>...</dependencyManagement>\n",
    "  <modules>...</modules>\n",
    "  <properties>...</properties>\n",
    " \n",
    "  <!-- Build Settings -->\n",
    "  <build>...</build>\n",
    "  <reporting>...</reporting>\n",
    " \n",
    "  <!-- More Project Information -->\n",
    "  <name>...</name>\n",
    "  <description>...</description>\n",
    "  <url>...</url>\n",
    "  <inceptionYear>...</inceptionYear>\n",
    "  <licenses>...</licenses>\n",
    "  <organization>...</organization>\n",
    "  <developers>...</developers>\n",
    "  <contributors>...</contributors>\n",
    " \n",
    "  <!-- Environment Settings -->\n",
    "  <issueManagement>...</issueManagement>Setup \n",
    "  <ciManagement>...</ciManagement>\n",
    "  <mailingLists>...</mailingLists>\n",
    "  <scm>...</scm>\n",
    "  <prerequisites>...</prerequisites>\n",
    "  <repositories>...</repositories>\n",
    "  <pluginRepositories>...</pluginRepositories>\n",
    "  <distributionManagement>...</distributionManagement>\n",
    "  <profiles>...</profiles>\n",
    "</project>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bare minimum which should be included in your project is outline here, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n",
    "  <modelVersion>4.0.0</modelVersion>\n",
    " \n",
    "  <groupId>org.apache.spark</groupId> \n",
    "  <artifactId>spark-core</artifactId> \n",
    "  <version>3.2.1</version>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go into detail about building your own projects and connectors because they will normally be available for you on the Maven repository. When using a Maven project with Spark you will just need to specify the Maven coordinates to download the package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maven Coordinates\n",
    "\n",
    "The required fields are `groupId:artifactId:version`. This acts like an address to your project and timestamp in one. It tells you where in the Maven repository to find the project, this is known as it's Maven Coordiantes. \n",
    "\n",
    "- __GroupId__ - Generally unique to an orgainisation or project. For instance all official Apache Spark projects will live under `org.apache.spark`.\n",
    "- __artifactId__ - This is generally the name your project is known by, `spark-sql`, `kafka-clients` etc. By combining this with the groupId should create a unique location in the Maven repository to house your project.\n",
    "- __version__ - Used to specify the version of your project keeping them separate on the repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Maven central repository\n",
    "\n",
    "Maven solves the issue of searching multiple sites for dependencies through the use of the Maven central repository https://mvnrepository.com/repos/central where projects build in Maven can be hosted for other users use. \n",
    "\n",
    "We can use the Maven coordinates to find the specific piece of software required. \n",
    "\n",
    "Let's take a look at an example, search for Apache Spark you will be met with the following.\n",
    "\n",
    "<img src=\"images/maven_spark_search.png?modified=232132453\">\n",
    "\n",
    "You can see that there are many different artifacts for Spark, `spark-streaming`, `spark-sql` etc. Clicking on `spark-sql` will show you the many different versions of this artifact hosted on the Maven repository which were built using Maven.\n",
    "\n",
    "<img src=\"images/spark_sql_maven.png?modified=232132453\">\n",
    "\n",
    "You can see the historical versioning of this connector, which versions of spark it is compatible with and for which versions of Scala. Selecting the version for Spark 3.2.1 will bring you to the following page. \n",
    "\n",
    "<img src=\"images/spark_sql_download_page.png?modified=232132453\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the Maven coordinates on the bottom panel this information will be useful when using the package with PySpark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting packages to Spark\n",
    "\n",
    "You can add these packages to your PySpark environment through the use of `[PYSPARK_SUBMIT_ARGS]` environment variable. This allows you to submit additional arguments to the PySpark shell at runtime.\n",
    "\n",
    "Should of the available arguments are so follows:\n",
    "\n",
    "- `--packages` - Comma separated list of maven coordinates of jars to include on the driver and executor classpaths. This command will search the Maven repository and local Maven installation for the JARs.\n",
    "- `--repositories` - Comma-separated list of additional remote repositories to search Maven coordinates.\n",
    "- `--jars` - Comma-separated list of local jars to include with the driver or classpath.\n",
    "- `--py-files` - Comma-separated list of `.egg`, .`zip` or `.py` files to include with the driver or classpath.\n",
    "\n",
    "You can find all available options by running `pyspark --help` in your terminal.\n",
    "\n",
    "### S3 to Spark\n",
    "\n",
    "Let's add the additional Spark streaming package to `PYSPARK_SUBMIT_ARGS` to enable spark streaming with PySpark. Go to the Maven repository and find the package for your version of Spark.\n",
    "\n",
    "You can check your version of spark by running `spark-shell` in the terminal. \n",
    "\n",
    "<img src=\"images/spark_streaming.png?modified=232132453\">\n",
    "\n",
    "In this case you can see that Spark `3.2.1` is being used and Scala `2.12.15`. \n",
    "\n",
    "When submitting the Package to the `PYSPARK_SUBMIT_ARGS` environment variable it will be in the form of Maven coordinates, i.e. `groupId:artifactId:version`.\n",
    "\n",
    "Let's get the required packages to read some data from an S3 bucket. To read from the S3 bucket we will need the additional packages `aws-java-sdk` and `hadoop-aws` find them on Maven central and add them as packages to your `PYSPARK_SUBMIT_ARGS`.\n",
    "\n",
    "Replace the bucket name, AWS access keys and AWS secret keys with your own to allow S3 to connect to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "\n",
    "# Adding the packages required to get data from S3  \n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.amazonaws:aws-java-sdk-s3:1.12.196,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell\"\n",
    "\n",
    "# Creating our Spark configuration\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('S3toSpark') \\\n",
    "    .setMaster('local[*]')\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "\n",
    "# Configure the setting to read from the S3 bucket\n",
    "accessKeyId=\"ENTER YOUR AWS ACCESS KEY HERE\"\n",
    "secretAccessKey=\"ENTER YOUR SECRET AWS ACCESS KEY\"\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', accessKeyId)\n",
    "hadoopConf.set('fs.s3a.secret.key', secretAccessKey)\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') # Allows the package to authenticate with AWS\n",
    "\n",
    "# Create our Spark session\n",
    "spark=SparkSession(sc)\n",
    "\n",
    "# Read from the S3 bucket\n",
    "df = spark.read.json(\"PATH TO YOUR AWS FILE IN THE BUCKET\") # You may want to change this to read csv depending on the files your reading from the bucket\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what data source you want to read/write to in Spark you will need to find the required package from Maven central before submitting it to Spark. \n",
    "\n",
    "You may want to connect many different data sources to Spark to leverage it's in-memory processing. Spark has many prebuilt connectors to enable you to do so, often the process will be finding the correct connector for the task and implementing it using the process defined above. \n",
    "\n",
    "The documentation for your chosen data source often will indicate which package to use when connecting with Spark so always check the documentation for Maven coordinates. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e77633671b156abbec93a3a64d3ecff44bbbffdabcb44bf362be1210d7b4ba9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
