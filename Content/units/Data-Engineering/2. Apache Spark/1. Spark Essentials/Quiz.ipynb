{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Essentials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PySpark in Apache Spark and how is it useful?\n",
    "\n",
    "- PySpark is a Python library for parallel analysis of data, and it is useful because it provides an easy-to-use programming interface for data analysis\n",
    "- PySpark is a Python package for distributed computing, and it is useful because it allows Python developers to write Spark jobs in Python instead of Java or Scala ***\n",
    "- PySpark is a Python-based cluster manager for Apache Spark, and it is useful because it simplifies the deployment and management of Spark clusters\n",
    "- PySpark is a Python-based machine learning library, and it is useful because it provides a powerful set of tools for machine learning and data analysis in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of `SparkContext` in PySpark?\n",
    "\n",
    "- To provide a connection to a Spark cluster and coordinate the execution of Spark jobs ***\n",
    "- To define a Spark application's configuration settings, such as the application name and the location of input data\n",
    "- To provide a high-level API for working with distributed data in PySpark\n",
    "- To manage the execution of tasks within a Spark job and handle failures and retries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In PySpark, what is the purpose of the `getOrCreate()` method of `SparkSession`?\n",
    "\n",
    "- To create a new `SparkSession` instance if one does not already exist, or to return an existing `SparkSession` instance if one has already been created ***\n",
    "- To create a new RDD from a given data source, such as a file or database table\n",
    "- To create a new DataFrame by applying a schema to a given data source, such as a file or database table\n",
    "- To create a new `StreamingContext` that can consume data from various sources, such as Kafka or HDFS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the main purpose of `SparkSession` in PySpark?\n",
    "\n",
    "- To create a connection to a Spark cluster and coordinate the execution of Spark jobs\n",
    "- To define a Spark application's configuration settings, such as the application name and the location of input data\n",
    "- To provide a high-level API for working with distributed data in PySpark ***\n",
    "- To manage the execution of tasks within a Spark job and handle failures and retries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Which of the following data structures are available in Apache Spark?\n",
    "\n",
    "- RDDs\n",
    "- DataFrames\n",
    "- Datasets\n",
    "- All of the above ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is a RDD in Apache Spark?\n",
    "\n",
    "- A type of database that stores structured data in a distributed manner\n",
    "- A distributed collection of objects that can be processed in parallel ***\n",
    "- A type of machine learning algorithm used in Spark\n",
    "- A tool used to manage Spark clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a dataframe in Apache Spark?\n",
    "\n",
    "- A database management system used for storing and querying large-scale data\n",
    "- A distributed collection of objects that can be processed in parallel\n",
    "- A tabular view of data with named columns, similar to a relational database table ***\n",
    "- A machine learning algorithm used for clustering data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a dataset in Apache Spark?\n",
    "\n",
    "- A distributed collection of objects that can be processed in parallel\n",
    "- A collection of data organized into named columns, similar to a relational database table\n",
    "- A distributed collection of objects that are strongly typed, providing a more efficient and convenient API for working with structured data ***\n",
    "- A type of machine learning algorithm used in Spark\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is lazy evaluation in PySpark?\n",
    "\n",
    "- Lazy evaluation is a feature in PySpark that allows us to chain transformations on an RDD or DataFrame without actually executing them until an action is called ***\n",
    "- Lazy evaluation is a feature in PySpark that allows us to execute transformations on an RDD or DataFrame without chaining them\n",
    "- Lazy evaluation is a feature in PySpark that allows us to execute actions on an RDD or DataFrame without transforming them\n",
    "- Lazy evaluation is a feature in PySpark that automatically optimizes transformations and actions for faster execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `persist()` method in PySpark do?\n",
    "\n",
    "- The `persist()` method allows you to store a PySpark RDD or DataFrame in-memory or on disk for faster access ***\n",
    "- The `persist()` method allows you to delete a PySpark RDD or DataFrame from memory or disk\n",
    "- The `persist()` method allows you to rename a PySpark RDD or DataFrame\n",
    "- The `persist()` method allows you to filter a PySpark RDD or DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following code snippets can be used to create a dataframe using Spark SQL in PySpark?\n",
    "\n",
    "- This code ***\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show() \n",
    "```\n",
    "\n",
    "- This code\n",
    "``` python \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = spark.createDataFrame(rdd, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "- This code\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.sql(\"SELECT _1 as Name, _2 as Age FROM VALUES %s\" % str(data))\n",
    "df.show()\n",
    "```\n",
    "\n",
    "- This code\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = rdd.toDF([\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about Spark SQL?\n",
    "\n",
    "- Spark SQL allows you to execute SQL queries and perform analysis on structured data within Spark ***\n",
    "- Spark SQL is only compatible with relational databases like MySQL and PostgreSQL\n",
    "- Spark SQL is a separate engine from Spark and requires separate installation\n",
    "- Spark SQL is used only for processing unstructured data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `map` function in PySpark do?\n",
    "\n",
    "- It applies a function to each element of a RDD and returns a new RDD ***\n",
    "- It returns the first element of a RDD\n",
    "- It removes all the duplicates from a RDD\n",
    "- It sorts the elements of a RDD in descending order"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `filter` function in PySpark do?\n",
    "\n",
    "- It removes all the duplicates from a RDD\n",
    "- It returns the first element of a RDD\n",
    "- It applies a function to each element of a RDD and returns a new RDD\n",
    "- It selects the elements of a RDD that satisfy a given condition and returns a new RDD ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `sortBy` function in PySpark do?\n",
    "\n",
    "- It sorts a RDD in ascending order based on a key function ***\n",
    "- It returns the first element of a RDD\n",
    "- It applies a function to each element of a RDD and returns a new RDD\n",
    "- It selects the elements of a RDD that satisfy a given condition and returns a new RDD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2592652612463181e69ac003232387e3e9a99279aa6b168e76f5df16d5110f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
