{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Lesson 3 - Kafka Streams and API's </h1>\n",
    "\n",
    "\n",
    "\n",
    "## __1.\tWhat is Kafka Streams and Why is it Important__\n",
    "\n",
    " Kafka Streams is a client Library for building applications and microservices, where the input and output data are stored in Kafka clusters. \n",
    " \n",
    " It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka's server-side cluster technology.\n",
    "\n",
    "It lets you achieve this with concise code in a way that is distributed and fault-tolerant. \n",
    "\n",
    "Stream processing is a computer programming paradigm, equivalent to data-flow programming, event stream processing, and reactive programming, that allows some applications to more easily exploit a limited form of parallel processing.\n",
    "\n",
    "Apache Kafka, Kafka Streams and Kafka Connect are all tools that form part of the Kafka ecosystem of event streaming. These tools have some similarities, but there are also some key differences that set them apart:\n",
    "\n",
    "\n",
    "- __Apache Kafka__ is an Event streaming application. Applications publish a Stream of events or messages to a topic on Kafka. The stream can be consumed independently by many Consumers, and messages in the Topic can even be replayed if needed. Kafka is massively scalable and fault-tolerant.\n",
    "\n",
    "- __Kafka Streams__ is an API for writing applications that transform and enrich data in Apache Kafka, usually by publishing the transformed data onto a new Topic. The data processing itself happens within your application, not on a Kafka Broker.\n",
    " \n",
    "- __Kafka Connect__ is an API for moving data into and out of Kafka. It standardises the integration of other applications into Kafka, by letting you write and share connectors for moving data to/from popular applications like databases.\n",
    "\n",
    "Here is a visual representation of these components:\n",
    "\n",
    "  <img src=\"./images/Kafka_Streams_Connect.png\" width=600>\n",
    "\n",
    "\n",
    "  Kafka Streams builds upon important Stream Processing concepts such as properly distinguishing between Event time and Processing time, windowing support, and simple yet efficient management and real-time querying of application state.\n",
    "\n",
    "Kafka Streams has a low barrier to entry - One can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafka's parallelism model.\n",
    "\n",
    "_Some of the important highlights of Kafka Streams include:_\n",
    "\n",
    "- Designed as a simple and lightweight client Library, which can be easily embedded in any Java application and integrated with any existing packaging, deployment and operational tools that users have for their Streaming applications.\n",
    "\n",
    "- Has no external dependencies on systems other than Apache Kafka itself as the internal messaging layer; notably, it uses Kafka's partitioning model to horizontally scale processing while maintaining strong ordering guarantees.\n",
    "\n",
    "- Supports fault-tolerant local state, which enables very fast and efficient stateful operations like windowed joins and aggregations.\n",
    "\n",
    "- Supports exactly-once processing semantics to guarantee that each record will be processed once and only once even when there is a failure on either Streams clients or Kafka Brokers in the middle of processing.\n",
    "\n",
    "- Employs one-record-at-a-time processing to achieve millisecond processing latency, and supports event-time based windowing operations with out-of-order arrival of records.\n",
    "\n",
    "- Offers necessary stream processing primitives, along with a high-level Streams DSL and a low-level Processor API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __2. Kafka Stream Topology__\n",
    "\n",
    "- A __Stream__ is the most important abstraction provided by Kafka Streams: it represents an unbounded, continuously updating data set. A Stream is an ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a Key-Value pair.\n",
    "- A __Stream Processing Application__ is any program that makes use of the Kafka Streams library. It defines its computational logic through one or more processor topologies, where a processor topology is a graph of stream processors (nodes) that are connected by Streams (edges).\n",
    "- A Stream Processor is a node in the processor topology; it represents a processing step to transform data in streams by receiving one input record at a time from its upstream processors in the topology, applying its operation to it, and may subsequently produce one or more output records to its downstream processors.\n",
    "\n",
    "_There are two special processors in the topology:_\n",
    "\n",
    "- __Source Processor:__ A Source Processor is a special type of Stream Processor that does not have any upstream processors. It produces an input Stream to its topology from one or multiple Kafka topics by consuming records from these topics and forwarding them to its down-stream processors.\n",
    "\n",
    "- __Sink Processor:__ A Sink Processor is a special type of Stream Processor that does not have down-stream processors. It sends any received records from its up-stream processors to a specified Kafka Topic.\n",
    "\n",
    "Below is a high-level overview of the Stream Topology:\n",
    "\n",
    "  <img src=\"./images/streams-architecture-topology.jpg\" width=400>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a video explanation of [__Kafka Streams__](https://www.youtube.com/embed/Z3JKCLG3VP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kafka Streams offers two ways to define the Stream processing topology:__ \n",
    "\n",
    "- Kafka __Streams DSL__ provides the most common data transformation operations such as _map_, _filter_, _join_ and _aggregations_ out of the box\n",
    "\n",
    "For a more detailed explanation of the Streams DSL, read the [Streams DSL guide here](https://kafka.apache.org/25/documentation/streams/developer-guide/dsl-api.html)\n",
    "\n",
    "- __Lower-level Processor API__ allows developers define and connect custom processors as well as to interact with state stores.\n",
    "\n",
    "For a more detailed explanation of the Processor API, read the [Processor API guide here](https://kafka.apache.org/25/documentation/streams/developer-guide/processor-api.html)\n",
    "\n",
    "\n",
    "A Processor topology is merely a logical abstraction for your Stream processing code. At runtime, the logical topology is instantiated and replicated inside the application for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
