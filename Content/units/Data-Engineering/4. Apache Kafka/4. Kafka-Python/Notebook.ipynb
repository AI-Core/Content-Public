{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka-Python\n",
    "\n",
    "> kafka-python is a Python client which allows you to interact with Apache Kafka in a pythonic way. It allows you to write Python code to perform many of the same Kafka tasks, create topics, produce data for those topics that are available to you through the terminal.\n",
    "\n",
    "The client has 6 main API classes some of which will be familiar to you if you've used Kafka before. Below is their main use and some useful methods. \n",
    "\n",
    "- __KafkaConsumer__ - Allows the consumption of records from a Kafka Cluster:\n",
    "    - `assign()` - Assign consumers to partitions\n",
    "    - `subscribe()` - Subscribe to a list of Kafka topics\n",
    "    - `topics()` - Return all topics this consumer is authorised to view<br><br>\n",
    "\n",
    "- __KafakProducer__ - Client that publishes records to a Kafka cluster:\n",
    "    - `send()` - Publish a message to a topic\n",
    "    - `metrics()` - Get metrics about the producers performance<br><br>\n",
    "- __KafkaAdminClient__ - The class to perform any administration of your Kafka cluster:\n",
    "    - `create_topics()` - From a list of topics create new topics in the cluster\n",
    "    - `list_consumer_groups()` - List all consumer groups known to the cluster\n",
    "    - `create_partitions()` - Create additional partitions for an existing topic<br><br>\n",
    "\n",
    "- __KafkaClient__ - Used to check the Kafka network input/output responses/requests:\n",
    "    - `add_topic()` - Add a topic to a list of topics tracked by metadata\n",
    "    - `set_topics()` - Set topics to track for metadata\n",
    "    - `bootstrap_connected()` - Returns true if the bootstrap server is connected<br><br>\n",
    "\n",
    "- __BrokerConnection__ - Initialise a connection to a Kafka Broker:\n",
    "    - `connect()` - Attempt to connect to the broker and return the connection state\n",
    "    - `check_version()` - Try to guess the version of the broker<br><br>    \n",
    " \n",
    "- __ClusterMetadata__ - Manage the Kafka cluster metadata:\n",
    "    - `leader_for_partition()` - Return the node id of the partition leader\n",
    "    - `topics()` - Get a list of known topics<br><br>\n",
    "\n",
    "There are many other methods available to you, they are available to view on kafak-python documentation [here](https://kafka-python.readthedocs.io/en/master/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Kafka\n",
    "\n",
    "Kafka-Python can be simply installed using pip just run `pip install kafka-python`. Once installed you will need to start your Kafka Broker and Zookeeper to begin interacting with Kafka. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both commands in the terminal inside you Kafka folder\n",
    "# Remember to start Zookeeper first as it orchestrates Kafka Brokers\n",
    "# Starting Zookeeper\n",
    "\n",
    "# Starting Kafka\n",
    "./bin/kafka-server-start.sh ./config/server.properties\n",
    "\n",
    "# Starting Zookeeper\n",
    "./bin/zookeeper-server-start.sh ./config/zookeeper.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Kafka now successfully running and `kafka-python` installed lets try to establish a connection to the Kafka broker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaClient\n",
    "from kafka.cluster import ClusterMetadata\n",
    "\n",
    "# Create a connection to retrieve metadata\n",
    "meta_cluster_conn = ClusterMetadata(\n",
    "    bootstrap_servers=\"localhost:9092\", # Specific the broker address to connect to\n",
    ")\n",
    "\n",
    "# retrieve metadata about the cluster\n",
    "print(meta_cluster_conn.brokers())\n",
    "\n",
    "\n",
    "# Create a connection to our KafkaBroker to check if it is running\n",
    "client_conn = KafkaClient(\n",
    "    bootstrap_servers=\"localhost:9092\", # Specific the broker address to connect to\n",
    "    client_id=\"Broker test\" # Create an id from this client for reference\n",
    ")\n",
    "\n",
    "# Check that the server is connected and running\n",
    "print(client_conn.bootstrap_connected())\n",
    "# Check our Kafka version number\n",
    "print(client_conn.check_version())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well you should see that `bootstrap_connected` returned `True` to signify that we can establish a connection the broker. Additionally `check_version` should have made an attempt to return the Kafka broker version. This gives you a method to check individual Kafka brokers though Python without having to interact with the terminal. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Administration\n",
    "\n",
    "Now that we know we can establish a connection to the broker we should create a Kafka admin client to create some topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.cluster import ClusterMetadata\n",
    "\n",
    "# Create a new Kafka client to adminstrate our Kafka broker\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=\"localhost:9092\", \n",
    "    client_id=\"Kafka Administrator\"\n",
    ")\n",
    "\n",
    "# topics must be pass as a list to the create_topics method\n",
    "topics = []\n",
    "topics.append(NewTopic(name=\"MLdata\", num_partitions=3, replication_factor=1))\n",
    "topics.append(NewTopic(name=\"Retaildata\", num_partitions=2, replication_factor=1))\n",
    "\n",
    "# Topics to create must be passed as a list\n",
    "admin_client.create_topics(new_topics=topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our topics are created we can list them to check their creation using the `admin_client` once more. Notice if you run the above code again you will get a `TopicAlreadyExistsError` signifying that then topics already created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get more detailed information about our topics with the `describe_topics` method of the `admin_client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass in a list to topics to describe or describe all topics without the topics keyword argument\n",
    "admin_client.describe_topics(topics=[\"MLdata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details listed by our `describe_topics` method are as follows:\n",
    "\n",
    "- __error_code__: \n",
    "    - Displays if there is an error currently with the topic. \n",
    "      - __0__ - signifies the broker is running correctly. \n",
    "      - __1__ - __OFFSET_OUT_OF_RANGE__ - The requests offset is not in range of the offsets on the server.\n",
    "      - __2__ - __CORRUPT_MESSAGE__ - The message produced by the server is corrupt.\n",
    "      - __3__ - __UNKNOWN_TOPIC_OR_PARTITION__ - The server doesn't host this topic or partition.\n",
    "      - Other error codes can be found in the Kafka documentation [here](https://kafka.apache.org/11/protocol.html)\n",
    "- __is_internal__:\n",
    "    - Boolean value indicates if this is an internal stream used by Kafka. For instance the `__consumer__offsets` topic is used internally by Kafka to manage offsets.\n",
    "- __partitions__:\n",
    "    - List of all partitions within the displayed topic and details for each.\n",
    "        - __partition__ - partition number within the topic\n",
    "        - __leader__ - This number represents the Broker ID the leader topic is hosted on. All other replica topics will copy from this leader on this broker.\n",
    "        - __replicas__ - List brokers replicas of this topic are hosted on\n",
    "        - __isr(in-sync replica)__ - List of all replicas that are currently in-sync with the leader broker. By default this means a replica has replicated all messages from the leader in the last 10 seconds.\n",
    "        - __offline_replicas__ - List of any current replica brokers that are offline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing Data\n",
    "\n",
    "With the topics created we can now send data to the Kafka topics by creating a Kafka producer from the `KafkaProducer` class. Let's create some test data we can send to our topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create some test data to send using our kafka producer\n",
    "ml_models = [\n",
    "    {\n",
    "        \"Model_name\": \"ResNet-50\",\n",
    "        \"Accuracy\": \"92.1\",\n",
    "        \"Framework_used\": \"Pytorch\"\n",
    "    },\n",
    "    {\n",
    "        \"Model_name\": \"Random Forest\",\n",
    "        \"Accuracy\": \"82.7\",\n",
    "        \"Framework_used\": \"SKLearn\"\n",
    "    }\n",
    "] \n",
    "\n",
    "retail_data = [\n",
    "    {\n",
    "        \"Item\": \"42 LCD TV\",\n",
    "        \"Price\": \"209.99\",\n",
    "        \"Quantity\": \"1\"\n",
    "    },\n",
    "    {\n",
    "        \"Item\": \"Large Sofa\",\n",
    "        \"Price\": \"259.99\",\n",
    "        \"Quantity\": 2\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create two producers each sending data to their respective topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "\n",
    "# Configure our producer which will send data to  the MLdata topic\n",
    "ml_producer = KafkaProducer(\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    client_id=\"ML data producer\",\n",
    "    value_serializer=lambda mlmessage: dumps(mlmessage).encode(\"ascii\")\n",
    ") \n",
    "\n",
    "# Configure our producer which will send data to the Retaildata topic\n",
    "retail_producer = KafkaProducer(\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    client_id=\"ML data producer\",\n",
    "    value_serializer=lambda retailmessage: dumps(retailmessage).encode(\"ascii\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `value_serializer` parameter is one we haven't seen before. This method is used to convert each message sent to the topic into bytes. Kafka transports messages as `bytes` so we need to serialise our data into a format which is convertible to `bytes`.\n",
    "\n",
    "We used the `json` modules `dumps` method to serialise the message as a JSON formatted string and encoded it. Which is now in a format which can be converted to bytes. \n",
    "\n",
    "With the two producers setup we can now send the data to our Kafka topics with the `send` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send our ml data to the MLData topic\n",
    "for mlmessage in ml_models:\n",
    "    ml_producer.send(topic=\"MLData\", value=mlmessage)\n",
    "\n",
    "# Send retail data to the Retaildata topic\n",
    "for retail_message in retail_data:\n",
    "    retail_producer.send(topic=\"Retaildata\", value=retail_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the code cell ran without error then our messages should have been serialised correctly and sent to their topics. We can now try and consume these messages from the topics to check the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming Messages\n",
    "\n",
    "We now need to consume the messages stored in our topics, we have the option of subscribing to both topics or create individual consumers for each topic. \n",
    "\n",
    "For this example we will got with the former option and consume all the messages at once in one data stream. Begin by creating your consumer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "\n",
    "# create our consumer to retrieve the message from the topics\n",
    "data_stream_consumer = KafkaConsumer(\n",
    "    bootstrap_servers=\"localhost:9092\",    \n",
    "    value_deserializer=lambda message: loads(message),\n",
    "    auto_offset_reset=\"earliest\" # This value ensures the messages are read from the beginning \n",
    ")\n",
    "\n",
    "data_stream_consumer.subscribe(topics=[\"MLData\", \"Retaildata\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the producer we had to encode our data before being converted to bytes. On the consumer end we now need to decode the message into a readable format. Instead of using the `dumps` method from the `json` module we can use the `loads` method decoding the message into a Python dictionary. \n",
    "\n",
    "With the messages in this format we can then print the consumer messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through all messages in the consumer and prints them out individually\n",
    "for message in data_stream_consumer:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice when printing out the message it is of type `tuple` with all associated metadata included e.g timestamps, partition number, topic name etc. We may only be interested in the `value` part of the message, where the true message lies. We can access individual parts of the message in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in data_stream_consumer:\n",
    "    print(message.value)\n",
    "    print(message.topic)\n",
    "    print(message.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, we can now access from the consumer the components of the message we're interested in receiving. \n",
    "\n",
    "This covers the main tasks you will perform when interacting with Kafka through Python showing the whole process of generating and receiving data from Kafka. Further would recommend browsing the [documentation](https://kafka-python.readthedocs.io/en/master/) to see some of the others tasks you can perform when managing your Kafka Cluster. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e77633671b156abbec93a3a64d3ecff44bbbffdabcb44bf362be1210d7b4ba9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
