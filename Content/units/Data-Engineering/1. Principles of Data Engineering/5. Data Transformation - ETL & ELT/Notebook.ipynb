{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "\n",
    "## What is Data Transformation?\n",
    "\n",
    "> Transformation is the process of converting data from one state to another\n",
    "\n",
    "In order to transform data into more useful information, we may need to clean that data, compute new metrics, filter out only the important stuff, or manipulate it in some other way. We call this _data transformation_. \n",
    "\n",
    "Some examples of transformations could include processes such as: \n",
    "- Data integration (combining different datasets together)\n",
    "- Data ingestion/migration (moving data from one location to another)\n",
    "- Data warehousing (storing data in a warehouse)\n",
    "- Data wrangling (cleaning and organizing the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why Transform Data?\n",
    "\n",
    "There are a number of reasons why an organisation might want to transform its data.\n",
    "\n",
    "Raw (source) data is often:\n",
    "\n",
    "- Partly irrelevant: It contains both relevant and irrelevant data\n",
    "- Inaccurate: It contains incorrectly entered information or missing values\n",
    "- Repetitive: It can contain duplicates of the same data, or of data that we already have\n",
    "\n",
    "There are different categories to data transformation organisations use which include:\n",
    "\n",
    "- Constructive transformation: Such as adding, copying, and replicating data\n",
    "- Destructive transformation:  Like deleting fields or specific records \n",
    "- Aesthetic transformations:   Such as standardising salutations or street names \n",
    "- Structural transformations:  Such as renaming, moving, and combining columns in a database\n",
    "\n",
    "Accordingly, proper data checking, cleaning and transformation activities are required before organisations can unlock the value stored in this data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Data Transformation\n",
    "\n",
    "> Data transformation typically includes two primary stages: data discovery and data mapping.\n",
    "\n",
    "### 1. Data Discovery\n",
    "\n",
    "The aim of data discovery is to clarify the format of the data, and the types of transformations required.\n",
    "\n",
    "It consists of:\n",
    "- Performing data exploration, where we identify the sources, data types, and their locations\n",
    "- Determine the structure and data transformations that need to occur\n",
    "\n",
    "Some questions to ask during this step include:\n",
    "- In structured data, what do the columns and rows look like?\n",
    "- In unstructured files, how does the data look? And how is it organised (for example, is it a nested JSON?)\n",
    "- What kind of information do the different datasets contain?\n",
    "- How does the information in one data source relate to another source? Are there common fields we can use to join the files?\n",
    "\n",
    "### 2. Data Mapping\n",
    "\n",
    "The next goal is to determine how to perform data mapping. The aim of data mapping is to define what types of data transformations are required, and then implement them. \n",
    "\n",
    "It consists of:\n",
    "- Defining how individual fields are mapped, modified, joined, filtered, and aggregated to fit the target data model\n",
    "- Implementing these transformations\n",
    "\n",
    "There are several strategies for doing this: \n",
    "\n",
    "#### Manual Scripting: \n",
    "- Traditionally, this was implemented by hand-writing code in languages such as Python, Scala or SQL\n",
    "- Common transformations include aggregating data or converting date formats, editing text strings, or joining rows and columns\n",
    "- This step also includes sending the data to the target temporary or permanent store (which could be a database, data warehouse or a data lake) \n",
    "- This approach provides several benefits such as having a customised solution specifically tailored for the company's needs\n",
    "- However, it may be time-consuming and more costly than other options\n",
    "\n",
    "#### On-premise Software \n",
    "- Out-of-the-box 3rd party tools are available to do transformation work. These tools are quite mature.\n",
    "- Compared to offsite (3rd party vendor) scripting solutions, onsite tooling offers the benefit of more oversight by the end-user\n",
    "- The downside however, is that it may need hiring additional expert staff to manage it\n",
    "\n",
    "#### Cloud-Based Solutions\n",
    "- These tools are hosted in the cloud, where they can leverage the expertise and infrastructure of the vendor (such as the Azure Data Factory)\n",
    "- Many of these tools automate major parts of the data transformation process using graphical user interfaces\n",
    "- These solutions are particularly useful when linking cloud-based solutions together, such as integrating software as a service (SaaS) platforms like Salesforce to a cloud-based data warehouse like Amazon Redshift\n",
    "- The benefit from using such tools is that they are mature and compatible with the cloud provider's infrastructure\n",
    "- The downside could be the long-term running costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Data Transformation\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"width:auto;text-align:center\"></th>\n",
    "            <th style=\"width:auto;text-align:center\">Description</th>\n",
    "            <th style=\"width:auto;text-align:center\">Examples</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Bucketing/Binning</th>\n",
    "            <td>Used to change a continuous numeric series into fixed, categorical ranges</td>\n",
    "            <td>Converting data from {2,5,8…} to {2-5, 6-9, 10-13…} </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Data Aggregation</th>\n",
    "            <td>Data aggregation is a process that searches, gathers, summarises, and presents data at an aggregate level</td>\n",
    "            <td>Used mainy for reporting. For example, summing the total weekly sales of a product</td>\n",
    "        </tr>  \n",
    "\t\t<tr>\n",
    "            <th>Data Cleaning</th>\n",
    "            <td> Involves deleting out-of-date, inaccurate, or incomplete information to increase the accuracy of data</td>\n",
    "            <td>Replacing NULL values with 0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Data Deduplication</th>\n",
    "            <td>Data deduplication is a data wrangling process where we identify and remove duplicate records to store a unique \"golden record\" of data</td>\n",
    "            <td>Duplicates can occur during data integration tasks. Removing exact duplicate records helps increase data quality</td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Data Derivation</th>\n",
    "            <td>Involves the creation of special rules to “derive” specific information from available data to create new, derived fields</td>\n",
    "            <td>Creating a new \"Year\" column after extracting the year value from a \"Date\" column</td>\n",
    "        </tr>\t\n",
    "\t\t<tr>\n",
    "            <th>Data Filtering</th>\n",
    "            <td>Includes techniques used to refine datasets. The goal is to remove irrelavent data</td>\n",
    "            <td>Data filters, such as the SQL WHERE command, can be used to select only \"Male\" employees</td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Data Integration</th>\n",
    "            <td>Taking different data sources (such as different database tables, or file formats) and merging them into one table/file which has the same structure</td>\n",
    "            <td>SQL JOIN operations can be used to combine multiple tables into one larger table</td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Data Splitting</th>\n",
    "            <td>Refers to dividing a single dataset (or column) into multiple datasets (or columns)</td>\n",
    "            <td>Splitting a large, historical dataset into smaller datasets split by year and month</td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Data Validation</th>\n",
    "            <td>Process of creating automated rules that activate when the system encounters pre-determined data issues. This helps maintain data quality</td>\n",
    "            <td>Certain fields that require data come as NULL. This triggers an automatic alert to the data engineers to notify them</td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Indexing and Ordering</th>\n",
    "            <td>Data can be transformed so that it’s ordered logically to speed up data retrieval or to suit a pre-determined data storage schema</td>\n",
    "            <td>In RDBMSs, creating indexes can improve data querying performance</td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Anonymisation and Encryption</th>\n",
    "            <td>Data containing private details should be anonymised to avoid potential legal issues</td>\n",
    "            <td>Anonymising date of birth or certain patient health information</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    " </table>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Data Transformation\n",
    "\n",
    "### Provides useful information:\n",
    "- Data is transformed to make it better-organised. Transformed data may be easier for humans to read and for automated reports to derive value from.\n",
    "\n",
    "### Data Integration\n",
    "- Data transformation enables organisations to integrate various datasets together to get a holistic view of various aspects of the company\n",
    "\n",
    "### Data Quality\n",
    "- Properly formatted and validated data improves data quality and protects applications from potential issues that could break the system such as null values, unexpected duplicates, incorrect indexing, and incompatible formats\n",
    "\n",
    "### Easier Storage\n",
    "- Transformed data could be easier to track, store and maintain. For example, compressing  data will save disk stroage space.\n",
    "\n",
    "### Enhance Data Science Models \n",
    "- Transformed data can be more easily stored over time and integrated with older datasets to increase the efficiency of data science algorithms by providing a larger and more accurate dataset for model training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of Data Transformation\n",
    "\n",
    "### Time Consuming\n",
    "- You may need to extensively cleanse the data so you can transform or migrate it\n",
    "- This can be extremely time-consuming, and is a common complaint amongst data engineers and data scientists working with unstructured data in particular\n",
    "- According to a 2017 Crowdflower report, data scientists spend 51% of their time compiling, cleaning, and organising data. They also spend 30% of their time collecting datasets and mining data to identify patterns.\n",
    "\n",
    "### Costly\n",
    "- Depending on the infrastructure, transforming data may require a team of experts and substantial infrastructure (hardware and software) costs.\n",
    "\n",
    "### Slow Processing\n",
    "- Because the process of extracting and transforming data can be a burden on a system, it is often done in batches, which means you may have to wait for hours for the next batch to be processed\n",
    "- This can cost time in making time-sensitive business decisions\n",
    "\n",
    "### Resource Intensive\n",
    "- Performing transformations in an on-premise data warehouse after loading, or transforming data before feeding it into applications, can create a computational burden that slows down other operations\n",
    "\n",
    "### Lack of Expertise \n",
    "- The ability to wield big data technologies successfully requires both knowledge and talent, and there currently is a lack of talent in the data engineering and data science domains \n",
    "- This makes it more difficult to find, recruit and retain talented experts\n",
    "\n",
    "### Changing Business Requirements\n",
    "- The format of source data may change suddenly, causing issues in the system\n",
    "\n",
    "### Risk of Project Failure\n",
    "- According to a recent survey, companies are falling behind in their data-driven goals: 72% of survey participants have yet to forge an internal data culture, while 52% say they have not leveraged data and analytics to remain competitive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Transform, and Load\n",
    "\n",
    "> Extract, Transform and Load are 3 steps required in order to prepare raw or isolated data into useful, integrated information that the business can analyse\n",
    "\n",
    "The reason businesses need to implement these steps is because data is usually generated in different formats from various sources. Therefore, we have to clean, enrich, and transform the data sources before integrating them into an analysable form. That way, business intelligence and visualisation platforms (like Microstrategy or Tableau) can understand the data to derive insights.\n",
    "\n",
    "### Extract: \n",
    "- _Extract_ refers to ingesting the data either from the original creation source or other systems storing data\n",
    "- An example of extraction is reading raw data arriving from a mobile application (such as Uber) \n",
    "\n",
    "### Transform: \n",
    "- _Transform_ refers to the process of changing the structure of the data in order to prepare it for loading into a storage location\n",
    "- An example of transformation is changing the file format from CSV to JSON\n",
    "\n",
    "### Load: \n",
    "- _Loading_ refers to the process of depositing the data into a data storage system\n",
    "- An example of loading is moving all transformed data into a Postgres database\n",
    "\n",
    "The transformation step is by far the most complex one. Transformations for ETL or ELT differ based on:\n",
    "\n",
    "- _When_ the transformation takes place\n",
    "- _Where_ the transformation occurs\n",
    "\n",
    "In order to implement the above steps in an organisation, there are 2 main approaches:\n",
    "\n",
    "### ETL\n",
    "\n",
    "> Extract, Transform _then_ Load. Transformations are applied before loading into storage.\n",
    "\n",
    "Transformation happens before loading so that the data can be fit into an existing schema. This makes it easy to do analytics, but it means that you might be throwing away valuable data. ETL has been the traditional data handling paradigm for the past several decades. The tools used are mature, and data storage is usually in a centralised database.\n",
    "\n",
    "### ELT\n",
    "\n",
    "> Extract, Load _then_ Transform. Data is loaded into storage in the raw format it arrives in. Transformations are applied later.\n",
    "\n",
    "Loading happens before the transformations so that all of the data can be stored in the format which it arrives. This means that you are never throwing away potentially valuable data, but it makes it harder to do analytics on messy data. The data is usually stored in distributed file system storage such as AWS S3, which can store any raw data format, including complex unstructured data types. This is the more modern approach which became popular after the advent of big data, especially as more complex data types like images and video proliferated. \n",
    "\n",
    "In ELT systems, data transformation is still necessary - to do analytics you need to cleanse, enrich, and integrate data (amongst other transformations).\n",
    "\n",
    "> Both ELT and ETL are popular approaches to data wrangling in organisations\n",
    "\n",
    "The diagram below shows how both approaches compare:\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src= \"images/etl-elt-details.png\" width=900>\n",
    "\n",
    "</p>\n",
    "\n",
    "## ETL vs ELT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"width:auto;text-align:center\"></th>\n",
    "            <th style=\"width:auto;text-align:center\">ETL</th>\n",
    "            <th style=\"width:auto;text-align:center\">ELT</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "\t\t<tr>\n",
    "            <th>Transformation</th>\n",
    "            <td><li>Transformations are done in an ETL temporary server/staging area before loading the data into a final, permanent repository\n",
    "                <li>As data size grows, transformation time increases\n",
    "            </td>\n",
    "            <td><li>All raw and incoming data is stored in a data like first, then transformations are performed on the data\n",
    "                <li>ELT only transforms the required data (not the full data set), so transformation time could be less\n",
    "            </td>\n",
    "        </tr>\n",
    "\t    <tr>\n",
    "            <th>Data Load Time</th>\n",
    "            <td><li>Data first loaded into a temporary staging area and later loaded into a target system\n",
    "                <li>Data must be first transformed before loading, which needs significant amounts of time initially\n",
    "            <td><li>Data loaded into a data lake only once as the data arrives\n",
    "                <li>Faster to implement as there is no requirement to first transform the data\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Cloud Support</th>\n",
    "            <td><li>Historically, ETL solutions were hosted on-premise which made them expensive and difficult to maintain\n",
    "                <li>More modern ETL solutions are now cloud-native\n",
    "                <li>To scale up or down, the server needs to increase its resources (increasing/decreasing the node CPU or disk storage)\n",
    "            </td>\n",
    "            <td><li>ELT uses a distributed compute and data storage model which was designed to leverage cloud features\n",
    "                <li>Can easily scale vertically (up or down) and horizontally (by adding additional nodes to the cluster)\n",
    "            </td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Maintenance Effort</th>\n",
    "            <td><li>It needs higher maintenance as you need to select which high-value data to load and transform</td>\n",
    "            <td><li>Low maintenance as data is always loaded in full as-is, and is available anytime</td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Support for Data warehouses</th>\n",
    "            <td><li>ETL model used mainly for relational and structured data which is designed for data warehouses</td>\n",
    "            <td><li>Used more commonly in scalable distributed infrastructure which supports both structured and unstructured data</td>\n",
    "        </tr>  \n",
    "\t\t<tr>\n",
    "            <th>Data Lake Support</th>\n",
    "            <td><li>Does not support the concept of a data lake\n",
    "                <li>Only supports structured data\n",
    "            </td>\n",
    "            <td><li>Designed to leverage data lakes\n",
    "                <li>Supports both structured and unstructured data at any volumes\n",
    "                <li>Can store raw and transformed data easily\n",
    "            </td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Data Aggregations</th>\n",
    "            <td><li>Due to the centralised architecture, complexity grows with the additional amount of data in the dataset</td>\n",
    "            <td><li>Due to the distributed architecture, processing time for aggregations depends more on the resources of the platform</td>\n",
    "        </tr>\n",
    "\t\t<tr>\n",
    "            <th>Maturity</th>\n",
    "            <td><li>The technology has been used for several decades\n",
    "                <li>Best practices are well documented and talent is readily available\n",
    "            </td>\n",
    "            <td><li>Relatively new technologies which are still evolving\n",
    "                <li>Tools can be complex to deploy and integrate with legacy systems\n",
    "                <li>Talent supply is still lagging behind industry demand\n",
    "            </td>\n",
    "        </tr>  \n",
    "    </tbody>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL vs ELT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Now that we have a better understanding of what ETL and ELT are, which approach should we be using for our data engineering tasks? The answer is: that depends on the use case and business requirementS.  Below are some sample use cases highlighting when it's advisable to use each approach:\n",
    "\n",
    "### Use case #1\n",
    "-   A company has massive amounts of data being ingested in real-time from multiple sources and in different file formats\n",
    "-   In this example, _ELT_ works best with huge quantities of data, both structured and unstructured. As long as the target system is cloud-based or a data lake, you will likely be able to process those huge amounts of data more quickly with an ELT solution.\n",
    "\n",
    "### Use case #2\n",
    "-   An organisation has its entire dataset organized into structured rows and columns stored in Postgres\n",
    "-   Reporting on the data is usually done in batches at regular intervals (daily, weekly and monthly)\n",
    "-   In this case, ETL is the preferred option as the data is structured and stored in a relational database system\n",
    "\n",
    "### Use case #3 \n",
    "-   A company that needs all it's data in one place as soon as possible\n",
    "-   When the transformations take place at the end of the process, ELT prioritizes the speed of transfer over almost everything else, which means that all data - good, bad, and otherwise - ends up in the data lake for later transformation\n",
    "\n",
    "\n",
    "Continuing with the Emirates Airlines example discussed earlier, the data transformation activities in this system include the Apache Spark steps in the below diagram:\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src= \"images/emirates-batch2.png\" width=1000>\n",
    "\n",
    "</p>\n",
    "\n",
    "Some examples of data transformation activities that the Emirates check-in appication performs include:\n",
    "-   Combining all incoming XML files for a particular hour into one folder on HDFS.\n",
    "-   Joining together all the data files for a particular day by using an end-of-day batch job.\n",
    "-   The combined daily file is checked for duplicate data.  Any duplicate records are removed.\n",
    "-   The daily cleaned file from the above step is transformed into Parquet file format and stored in another HDFS storage location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Data transformation is a critical part of the data engineering process at organisations. The reason being that data in modern systems exists in various forms and in ever increasing volumes.\n",
    "- Raw data generated from different sources is often inconsistent, imprecise and could have duplicates. Hence, data cleansing and transformation are important to create a reliable analyatics data platform.\n",
    "- _ETL_ is an acronym for Extract, Transform then Load. It should not be confused with _ELT_, which has the same steps but in a different order: Extract, Load then Transform.\n",
    "- Data transformation is important to change data into useful information. Some common types of transformations include constructive (like adding columns), destructive (like removing incomplete records, filtering certain rows), structural (such as integrating different files) or altering file formats.\n",
    "- Data transformation can be implemented manually by scripting, using automated tools or by cloud-based software\n",
    "- Despite the benefits of data transformation such as increasing reporting accuracy, it's also a challenging task due to the complexity, time and costs involved among other factors\n",
    "- ETL has been around for decades, and is better equiped to handle structured data (data that's arranged in rows and columns) efficiently and cannot handle unstructured data.\n",
    "- ELT is better tailored for big data that can be both structured or unstructured (such as image and video files) in batch or real-time ingestion\n",
    "- The modern trend in global companies is to migrate data extraction, loading and transformation to cloud-based tools\n",
    "- The decision to use ETL or ELT in companies depends on various criteria such as the business requirements and the type of data being ingested"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
