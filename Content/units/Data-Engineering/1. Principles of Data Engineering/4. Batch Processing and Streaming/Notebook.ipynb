{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing and Streaming\n",
    "\n",
    "> Data processing is the process of changing data somehow, perhaps by cleaning it or computing more meaningful information from it\n",
    "\n",
    "As mentioned earlier, there are 2 main approaches to data processing: batch and streaming\n",
    "\n",
    "- _Batch processing_ is where data is aggregated, before being processed all at once in bulk\n",
    "- _Streaming (or stream processing)_ is where data is processed as soon as it is ingested into the system\n",
    "\n",
    "### Other types of processing\n",
    "\n",
    "- _Micro-batch processing_ is where streamed data is batched into small groups to achieve some of the performance advantages of batch processing. It is somewhere between batch processing and streaming\n",
    "- _Real Real-time processing_ (yes, there are supposed to be two \"real\"s there) is where a system has a requirement to process data in not just _near real-time_, but _real real-time_. In streaming, data is processed as soon as possible, but the processing may not be complete quickly enough to use predictions in real time. With real real-time processing, the results of processing are required right now, not just as soon as possible. For example, in a selfie filter there can't be any noticable delay.\n",
    "\n",
    "Let's take a closer look at batch processing and streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch data processing \n",
    "\n",
    "Batch processing is widely used in organisations, as many of the legacy systems were built upon this philosophy of data engineering where:\n",
    "- Data is collected over time\n",
    "- The collected data is sent for processing either at regular intervals, when a certain criteria is met, or when manually performed\n",
    "- Datasets could be huge (terabytes or petabytes in size) and processing this data can be time-consuming. Hence, it's meant for information that isn't very time-sensitive.\n",
    "\n",
    "### When does it make sense to do batch processing?\n",
    "\n",
    "- You already have all of the data in storage\n",
    "- Results are not time-sensitive\n",
    "- Data migrations are required from one storage system to another (such as from on-premise hardware to cloud-based storage)\n",
    "\n",
    "### When does batch processing not make sense?\n",
    "- When results are required instantly or in (near) real-time \n",
    "- When data is constantly flowing in, and operations depend on up-to date results being shown as they arrive (for instance, Google maps)\n",
    "\n",
    "### Example use-cases of batch processing\n",
    "\n",
    "#### Pricing products for the next quarter\n",
    "If you want to set the price of 1 million new products for the upcoming season, it would make no sense to do that one by one as they are added to the system. Firstly it would be inefficient to spin up and down compute resources for processing each item, secondly you would not be able to price them in the _context_ of all other items which is an important factor, and thirdly streaming simply adds no benefit because the product prices will not be used until the new season drops, at which point they will all be released at once.\n",
    "\n",
    "#### Performing some historical analysis\n",
    "You may want to ask questions about a huge amount of data that has been collected over the lifetime of your organisation in storage. \"What is our most popular product?\", \"What is the most common way we acquire users?\", \"What is the average spend of men, based in London, aged between 20-25?\".\n",
    "\n",
    "Continuing with our example from the Emirates Airlines check-in system, below is a high-level diagram demonstrating the batch processing components within that system:\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src= \"images/emirates-batch2.png\" width=1000>\n",
    "\n",
    "</p>\n",
    "\n",
    "__In the above example, the data flow would be as follows__:\n",
    "\n",
    "1. XML data is ingested in batches into the HDFS raw-layer data lake every 30 minutes (this is called the _incremental data_)\n",
    "2. The XML data is accumulated throughout the day, and saved in folders arranged by day and sub-folders by time of arrival \n",
    "3. At the end of the work day, the newly collected incremental data, in addition to the previously stored historical (full) dataset are processed using a Spark Scala application. This application is a batch job that runs once daily.\n",
    "4. The Spark Scala application performs basic data integrity and quality checks (such as checking for duplicates), and transforms all available files into a file format often used in big data environments called Parquet format. The process can take several hours to complete.\n",
    "5. The transformed Parquet files are then loaded into a new HDFS intermediate storage location called the _decomposed layer_. This layer is used as a staging layer to perform more advanced transformations.\n",
    "6. The Parquet files stored in the decomposed layer are then further processed by a more detailed Spark Scala application that implements more complex transformations as per the business requirements (for example, linking check-in event data to a particular passenger). This is another batch job that runs once per day and requires several hours to complete.\n",
    "7. The final cleaned and processed data is stored in a seperate HDFS location called the _modelled layer_, which is the layer that contains the final dataset that can be used by the various stakeholders.\n",
    "8. Data in the modelled layer is then exposed to the approved stakeholders who will query and analyse that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch data processing tools:\n",
    "\n",
    "### Hadoop and HDFS\n",
    "\n",
    "<p></p>\n",
    "<p align=\"left\">\n",
    "<img src= \"images/hdfs2.png\" width=100>\n",
    "</p>\n",
    "\n",
    "> Apache Hadoop is among the most popular tools in the big data industry. An open-source framework developed by Apache, it runs solely on commodity hardware and is used for big data storage, processing, and analysis.\n",
    "\n",
    "Hadoop, which was the first big data framework introduced, it enables parallel data processing on multiple machines simultaneously - we call this a _distributed cluster_.  The tool was extremely popular for various big data processing activities during the past 10 years, but now it seems Spark is replacing it.\n",
    "\n",
    "#### Hadoop consists of three main parts\n",
    "- The Hadoop Distributed File System (HDFS), which is the data storage layer\n",
    "- MapReduce, which handles data processing\n",
    "- YARN (Yet Another Resource Negotiator), which is the software layer designed for resource management\n",
    "\n",
    "#### Features of Hadoop\n",
    "- It can authenticate with an HTTP proxy server\n",
    "- Supports the POSIX-style file system with extended attributes\n",
    "- Offers a robust ecosystem for analytics capable of meeting most needs\n",
    "- Makes data processing faster and more flexible\n",
    "\n",
    "#### Main Limitations\n",
    "- Hadoop itself lacks real-time processing capabilities\n",
    "- Inability to do in-memory calculations, which are much faster\n",
    "\n",
    "#### Example Hadoop use cases\n",
    "- Building and running applications that rely on analytics to assess risks and create investment models\n",
    "- Cleaning and transforming the entire dataset of customers in an airline company\n",
    "- Creating a data analytics infrastructure for improving customer experience\n",
    "- Predictive maintenance of IoT devices and other infrastructure machines\n",
    "\n",
    "For more details, [check out Hadoop's homepage](https://hadoop.apache.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Hive\n",
    "\n",
    "<p></p>\n",
    "<p align=\"left\">\n",
    "<img src= \"images/hive.png\" width=100>\n",
    "</p>\n",
    "\n",
    "\n",
    "> Apache Hive is an open-source, big data warehouse for reading, writing and managing large data files that are stored directly in either HDFS or other big data systems like HBase.\n",
    "\n",
    "#### Features of Hive:\n",
    "-   Hive enables SQL developers to write Hive Query Language (HQL) statements that are similar to standard SQL statements for querying and analysing data\n",
    "-   It is designed to make MapReduce programming easier because you don’t have to write lengthy Java code. Instead, you can write queries more simply in HQL, and Hive can then create the map and reduce the functions automatically under the hood.\n",
    "-   Hive also includes the Hive _metastore_, which enables developers to apply a table structure onto large amounts of unstructured data. Once a table is created, details defining the columns, rows, data types, etc., are all stored in the metastore and becomes part of the Hive architecture. \n",
    "    -   Other tools such as Apache Spark and Apache Pig can then access the data in the metastore\n",
    "\n",
    "#### Hive vs SQL\n",
    "- Hive uses a query language very similar to that of traditional SQL databases. However, Hive is based on Apache Hadoop and uses MapReduce operations, resulting in key differences:\n",
    "    -   Hadoop is intended for long full-table scans and, because Hive is based on Hadoop, queries have a higher latency compared to SQL, making Hive less appropriate for applications that need very fast response times. Why are the queries slower?\n",
    "        - Network latency between distributed machines which store the data\n",
    "        - Hive and Hadoop do not cache data in memory like relational databases do\n",
    "- Hive is better suited than relational databases (RDBMS) for big data warehousing tasks. This is because:\n",
    "    -   Hive can scale up or down as needed using the power of distributed computing\n",
    "    -   Hive is better at handling complicated data than RDBMSs (which are designed to handle less complicated data faster)\n",
    "    -   Hive's schema can be more flexible than in relational databases (which require a fixed schema)\n",
    "    -   Hive can support a wider variety of input data types \n",
    "\n",
    "For more details, [check Hive's homepage](https://hive.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "<p></p>\n",
    "<p align=\"left\">\n",
    "<img src= \"images/spark.png\" width=100>\n",
    "</p>\n",
    "\n",
    "\n",
    "> Apache Spark is an open-source unified analytics engine for large-scale data processing \n",
    "\n",
    "Spark is often considered to be the successor of Hadoop, as it fills the gaps of Hadoop's drawbacks. For instance, Spark supports both batch and real-time data processing. It also supports in-memory calculations, thus yielding results faster than Hadoop, thanks to a reduction in the number of read and write processes. Spark is also a more versatile and flexible tool for big data crunching, capable of working with an array of data stores such as Apache Cassandra, OpenStack, and HDFS.\n",
    "\n",
    "#### Spark features:\n",
    "- Fast processing for both batch and streaming data\n",
    "- Easy to use and provides support for multiple programming languages, including Python\n",
    "- Supports complex analytics as it comes with several built-in components\n",
    "- Flexibility to connect to various big data tools\n",
    "- In-memory computation makes Spark much faster than Hadoop for many use cases\n",
    "\n",
    "#### Spark use cases:\n",
    "- With Spark, data can be cleaned and aggregated continuously before being transferred to data stores\n",
    "- Combines live data with static data, enabling powerful real-time analysis\n",
    "- Detects and addresses unusual behaviors quickly, thus eliminating potential serious threats\n",
    "- Its interactive analysis capabilities are used for processing and interactively visualising complex data sets\n",
    "\n",
    "#### Spark Components\n",
    "<p></p>\n",
    "<p align=\"center\">\n",
    "<img src= \"images/spark-ecosystem.png\" width=600>\n",
    "</p>\n",
    "\n",
    "\n",
    "Spark consists of 5 main components, 4 of which can be used for batch processing:\n",
    "1. Spark Core:\n",
    "    - This is the main, general purpose data processing engine for Spark\n",
    "    - It provides the in-memory computing model that enables code execution\n",
    "    - Supports a wide variety of API's for different programming languages like Python and Scala\n",
    "<p></p>\n",
    "\n",
    "2. Spark SQL: \n",
    "    - Is one of the main componets of Spark which is designed to handle structured data\n",
    "    - Uses a concept called _Dataframes_, which is the abstraction framework used to store and organise data into rows and columns. For those who are familiar with Python, they are similar to Panda's dataframes.\n",
    "    - Enables running SQL queries on top of the data stored in dataframes, which makes it easier than coding for various data transformation tasks like joining and aggregations\n",
    "\n",
    "<p></p>\n",
    "\n",
    "3. Spark MLLib: \n",
    "    - Stands for Machine Learning Library \n",
    "    - This component is for performing common machine learning tasks such as customer segmentation, predictive intelligence, and sentiment analysis\n",
    "<p></p>\n",
    "    \n",
    "4. Spark GraphX: \n",
    "    - This component is mainly used to store and analyse data that is in graph format\n",
    "    - Graph data is that which consists of entities and their relationships to each other. One example is social media data regarding \"friends of friends\" of a particular user.\n",
    "    \n",
    "For more details, [check Spark's homepage](https://spark.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Batch Processing Tools\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "             <th style=\"width:auto;text-align:center\">Tool</th>\n",
    "             <th style=\"width:auto;text-align:center\">Maturity</th>\n",
    "\t\t     <th style=\"width:auto;text-align:center\">Key Features</th>\n",
    "             <th style=\"width:auto;text-align:center\">Limitations</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Hadoop</th>\n",
    "            <td>High, becoming outdated</td>\n",
    "            <td>Strong for batch processing</td>\n",
    "            <td>Does not natively support streaming data. Also, difficult to code with MapReduce</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Hive</th>\n",
    "            <td>High, becoming outdated</td>\n",
    "            <td>Easy to use HQL queries which are similar to SQL. Uses HDFS as a the data storage layer</td>\n",
    "            <td>Can only handle structured data</td>\n",
    "        </tr>  \n",
    "\t\t<tr>\n",
    "            <th>Spark</th>\n",
    "            <td>Still maturing</td>\n",
    "            <td>Faster in-memory computation. Support both batch and real-time. Easier to code than Hadoop MapReduce</td>\n",
    "            <td>Does not have its own file management/storage system. Requires more RAM than Hadoop</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing\n",
    "\n",
    "> Streaming, or stream data processing, involves processing data as soon as it is ingested into the software system\n",
    "\n",
    "The main characteristics of stream data processing include:\n",
    "- Data is processed individually as the records arrive (not in big batches)\n",
    "- Data is processed as soon as possible, which means that it's useful for applications where live updates are key such as progressively updating metrics, reports, and summary statistics in response to every data record that becomes available\n",
    "- Modern data processing has progressed from batch processing of data towards working with stream processing, although streaming is not appropriate for every use-case\n",
    "\n",
    "Here's an analogy: Historically, you would have to download the whole movie before you could watch it (batch processing). Nowadays you can stream the movie by downloading small pieces at a time (streaming).\n",
    "\n",
    "### When does it make sense to do stream processing?\n",
    "- When the data ingestion source provides data in streaming mode (one data point at a time)\n",
    "- No historical data anaysis is required\n",
    "- When the data is time-sensitive. For example, while using an application like Google Maps which requires constant location data\n",
    "\n",
    "### When does stream processing not make sense?\n",
    "- When any type of historical analysis is required\n",
    "- When the data is not time-sensitive\n",
    "- When complex transformations are required before deriving insights from the data\n",
    "\n",
    "### Example use-cases of stream processing\n",
    "\n",
    "#### Stock Trading\n",
    "Most trading platforms require analysing data in real-time to execute all orders as soon as they are entered and to provide second-by-second recommendations on which stocks to buy, sell or hold.\n",
    "\n",
    "#### Multiplayer Gaming\n",
    "When playing online multiplayer games, the system must be able to handle all communication and interactions in real-time as they occur.\n",
    "\n",
    "#### Location Applications\n",
    "Any platform that requires location information, such as Uber or Google Maps, must be able to support stream data processing to provide accurate and immediate information to its users.\n",
    "\n",
    "A more specific example of a stream data processing system can be seen in the below image. This system is one of Pintrest's main data engineering pipelines that is designed to provide real-time metrics and alerts regarding the state of the system:\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src= \"images/pintrest-streaming.png\" width=1000>\n",
    "\n",
    "</p>\n",
    "\n",
    "The data flow is as follows:\n",
    "\n",
    "1. Stream data is created by the API and mobile application\n",
    "2. The produced data is ingested into Kafka as it arrives in real-time\n",
    "3. Kafka pushes the data to Spark Streaming, which processes all incoming records as they arrive\n",
    "4. The processed metrics data is then stored into a relational database (MemSQL)\n",
    "5. The database feeds a Grafana dashboard which provides real-time alerts to users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming data processing tools\n",
    "\n",
    "There are a wide variety of modern tools that are designed to handle stream data processing. Most of these tools provide more-or-less similar functionality, although they may differ on some of the technical details. For instance, some tools can handle both batch and streaming data, while others are designed to specifically process streaming data. It's worth mentioning that, out of all tools currently available, Apache Spark is outshining its rivals and is becoming the most popular tool used by global companies.\n",
    "\n",
    "### Spark Streaming\n",
    "<p align=\"left\">\n",
    "<img src= \"images/spark-streaming.jpg\" width=100>\n",
    "</p>\n",
    "\n",
    "> Spark Streaming is an extension of the core Spark API that allows users to process data in real-time from various sources including (but not limited to) Kafka and Flume \n",
    "\n",
    "This processed data can be pushed out to various file systems, databases, and live dashboards. This allows Spark Streaming to seamlessly integrate with any other Spark components like MLLib and Spark SQL. \n",
    "\n",
    "Spark Streaming is different from other systems which have a processing engine designed only for streaming, or have similar batch and streaming APIs but compile internally to different engines. Spark’s single execution engine and unified programming model for batch and streaming lead to some unique benefits over other traditional streaming systems.\n",
    "\n",
    "#### Features of Spark Streaming (over Hadoop)\n",
    "- More user-friendly than Hadoop and supports more programming languages\n",
    "- Faster, in-memory data processing (a feature that isn't present in Hadoop which uses disks for data storage)\n",
    "- Ability to handle both batch and streaming data simultaneously\n",
    "- Provides ready to use application-specific libraries (such as MLLib, GraphX and SQL)\n",
    "\n",
    "#### Use Cases for Spark Streaming\n",
    "- Global companies use Spark Streaming to perform sentiment analysis on data ingested from Facebook, Twitter and customer reviews on websites\n",
    "- Uber uses Spark Streaming as part of its data pipeline to ingest and process event data generated from the mobile application for real time telemtry analysis\n",
    "- eBay leverages Spark Streaming as part of its platform to deliver real-time targeted product recommendations and offers to customers \n",
    "\n",
    "\n",
    "For more details, [check Spark Streaming's homepage](https://spark.apache.org/docs/latest/streaming-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Storm\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src= \"images/storm.png\" width=100>\n",
    "</p>\n",
    "\n",
    "> Apache Storm is an open-source tool for big data used for processing streaming data\n",
    "\n",
    "Storm is also a fault-tolerant data processing system. It's distributed design enables it to easily process data in real-time. Moreover, it's compatible with all major programming languages and also supports JSON-based protocols. Storm is easily scalable and highly user-friendly. Storm was the first popular streaming platform introduced which was compatible with Hadoop. Although it was popular some time ago, it's popularity has declined since Apache Spark was released.\n",
    "\n",
    "#### Features of Storm\n",
    "- High throughput as it can process up to one million 100-byte messages per second per node\n",
    "- Can scale easily as it uses parallel data processing across a cluster of distributed machines\n",
    "- Self-healing network. In case of node failure, the system automatically restarts and transfers work to another node\n",
    "- Relatively easy to setup and configure compared to Hadoop\n",
    "\n",
    "#### Use Cases for Storm\n",
    "- Real-time analytics (Twitter uses Storm to manage some of the analytics around tweets)\n",
    "- Fraud detection for online transactions (such as credit card payments)\n",
    "- Machine learning applications that process a continuous flow of data for real-time predictions (real-time product recommendations)\n",
    "\n",
    "For more details, [check Storm's homepage](https://storm.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Flink\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src= \"images/flink.png\" width=100>\n",
    "</p>\n",
    "\n",
    "\n",
    "> Flink is another open-source distributed processing framework for big data that can manage both real-tme and batch data processing\n",
    "\n",
    "Flink is designed to run on commodity (inexpensive) hardware, and can perform fast computations in-memory at massive scale. Flink can run in a stand-alone mode, or it can integrate with popular big data frameworks like Hadoop. \n",
    "\n",
    "#### Features of Flink\n",
    "- Fault-tolerant infrastructure that does not have a single point of failure\n",
    "- High scalability due to its distributed architecture\n",
    "- Low latency and high throughput\n",
    "- Support wide array of connectors to third-party resources such as data sources and targets such as Elasticsearch, Kinesis Kafka, and JDBC database systems\n",
    "- Flink’s SQL interface (Table API) can perform data enrichment and transformation tasks and supports user-defined functions\n",
    "- Its Gelly library offers building blocks and algorithms for high-performance, large-scale graph analytics on data batches\n",
    "\n",
    "#### Use Cases for Flink\n",
    "- Large telecom companies (such as [Bouygues Telecom](https://2016.flink-forward.org/kb_sessions/a-brief-history-of-time-with-apache-flink-real-time-monitoring-and-analysis-with-flink-kafka-hb/)) use Flink to monitor the network and quality in real-time\n",
    "- Uber uses Flink as part of their open-source AthenaX analytics platform (more details can be found  [here](https://eng.uber.com/athenax/)\n",
    "- Alibaba is using Flink as part of its online search engine. More details on this use case can be found [here](https://www.ververica.com/blog/blink-flink-alibaba-search)\n",
    "\n",
    "\n",
    "For more details, [check Flink's homepage](https://flink.apache.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Samza\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src= \"images/samza.png\" width=100>\n",
    "</p>\n",
    "\n",
    "> Apache Samza is another distributed data processing framework for streaming data originally developed by Linkedin\n",
    "\n",
    "It enables building applications that process data in real-time from multiple sources. It can run indepedently or within a Hadoop YARN cluster and can also integrate with other tools such as ElasticSearch and AWS Kinesis. \n",
    "\n",
    "#### Features of Samza \n",
    "- Samza offers extremely low latency and high throughput to analyze data in real-time\n",
    "- Samza uses Kafka to guarantee that messages are processed in the order they were written to a partition, and that no messages are lost\n",
    "- Samza is partitioned, distributed and is easy to scale up or down as required\n",
    "- Though Samza works out of the box with Kafka and YARN, Samza provides a pluggable API that lets it run with other messaging systems and execution environments\n",
    "- It has flexible deployment options which allow its applications to run on-premise, in the cloud, or in containerised enviornments\n",
    "\n",
    "#### Use Cases for Samza\n",
    "\n",
    "- Ebay uses Samza to power low-latency fraud prevention in real time. You can read more about this use case [here](https://samza.apache.org/case-studies/ebay)\n",
    "- Slack leverages Samza to provide near real-timne alerting and to process billions of events such as metrics and logs data. More on this use case [here](https://samza.apache.org/case-studies/slack)\n",
    "- LinkedIn uses Samza as part of its new email and notification platform. You can read more about this use case [here](https://samza.apache.org/case-studies/linkedin)\n",
    "\n",
    "For more details on the tool itself, check Samza's [homepage](https://samza.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Stream Processing Tools\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "             <th style=\"width:auto;text-align:center\">Tool</th>\n",
    "             <th style=\"width:auto;text-align:center\">Latency</th>\n",
    "\t\t     <th style=\"width:auto;text-align:center\">Processing Approach</th>\n",
    "             <th style=\"width:auto;text-align:center\">Key Features</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Spark Streaming</th>\n",
    "            <td>Seconds</td>\n",
    "            <td>Tiny micro-batch</td>\n",
    "            <td>Currently most popular tool for streaming. Also has a mature support community</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Storm</th>\n",
    "            <td>Sub-second</td>\n",
    "            <td>True record-by-record streaming</td>\n",
    "            <td>Support a wide variety of processing modes (exactly once, at least once, and at most once)</td>\n",
    "        </tr>  \n",
    "\t\t<tr>\n",
    "            <th>Flink</th>\n",
    "            <td>Sub-second</td>\n",
    "            <td>True record-by-record streaming</td>\n",
    "            <td>Can also support batch data processing</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Samza</th>\n",
    "            <td>Sub-second</td>\n",
    "            <td>True record-by-record streaming</td>\n",
    "            <td>Easily integrates with Kafka and Hadoop</td>\n",
    "        </tr>  \n",
    "    </tbody>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Data processing is a critical component of an enterprise's technology foundation as it involves converting raw data into more meaningful information that businesses can use to help enhance decision making\n",
    "- Batch data processing is the more traditional approach organisations have used for data processing. Batch is leveraged for handling large datasets that contain historical data, while stream data processing focuses on processing small amounts of newly created data in real-time.\n",
    "- Hadoop, Hive, and Spark Core are the most common tools organisations leverage for big data batch processing at scale. This is because they can handle both structured and unstructured data and can address a wide variety of use cases.\n",
    "- For streaming data, Spark Streaming, Storm, Flink, and Samza are the popular data processing frameworks used in industry. Each tool has its strengths, although Spark is quickly becoming the dominant technology."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
