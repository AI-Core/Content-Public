{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "## What is Data Ingestion? \n",
    "\n",
    "> Data ingestion is the process of moving data into a software system. It is one of the main steps of a data pipeline.\n",
    "\n",
    "Data has become a very valuable asset in modern organisations as they become more data-driven. Data ingestion is a critical step in enabling companies to capture and analyse their data. \n",
    "\n",
    "Data ingestion typically do the following things:\n",
    "- Consumes raw data from a creation source (such as a mobile app)\n",
    "- Performs processing (such as cleaning the data to remove empty records)\n",
    "- Writes the cleaned data to a target (which could be another tool, a downstream system or a storage location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to Data Ingestion \n",
    "\n",
    "> In industry, two main approaches for data ingestion are commonly used: Batch and Streaming.\n",
    "\n",
    "Let's take a closer look at each:\n",
    "\n",
    "### 1. Batch Ingestion\n",
    "\n",
    "> Batch is when data is grouped together over a period of time and then ingested in bulk (on a regularly scheduled or ad-hoc basis)\n",
    "\n",
    "Sometimes you need to process the _entire dataset_ to calculate some metrics, because they depend on both new and historical data. That's one place where batch ingestion can be required.\n",
    "\n",
    "Some other typical examples of batch data ingestion include:\n",
    "<p></p>\n",
    "\n",
    "#### Database migration\n",
    "- A company has an on-premise relational database that is being migrated to the cloud\n",
    "- The database stores all customer and product information\n",
    "- A full data dump followed by a data migration is required to move everything to the new cloud database\n",
    "- This would be done as one large batch data ingestion activity  \n",
    "<p></p>\n",
    "\n",
    "#### Data ingestion into data lake\n",
    "- A company has data arriving regularly to different folders and database systems which it wants to integrate together\n",
    "- Every night at 9pm, the data engineers run a scheduled job to extract all the data available from each storage location and ingest that data into an HDFS data lake\n",
    "- The data stored in HDFS can then be processed, integrated and transformed at a later stage\n",
    "<p></p>\n",
    "\n",
    "#### Payroll\n",
    "- Once a month, a bank runs a number of automated batch jobs to ingest payroll data for all of its staff from various data sources such as a timtesheet system or various database tables\n",
    "- This data can then be integrated and used to do monthly payroll calculations\n",
    "\n",
    "Batch data ingestion is the traditional way organisations have performed data ingestion. It's been used for several decades and the tools implementing it are mature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Streaming\n",
    "\n",
    "> Streaming is when data is ingested as soon as it's created from the source\n",
    "\n",
    "Some business use cases require ingesting data as soon as it is produced. There are several reasons for this. For example the source might not able to store data and can only create a single copy of a data point at regular time intervals. \n",
    "\n",
    "Some of the typical use cases requiring streaming ingestion include:\n",
    "\n",
    "#### Google maps\n",
    "- Location data created from the mobile device must be sent and ingested instantly\n",
    "- This is required to provide accurate location information even if one is driving at high speeds\n",
    "\n",
    "#### Self-driving vehicles\n",
    "- Autonomous vehicles have many systems and sensors producing non-stop data\n",
    "- This data must be ingested as it's created to be sent for immediate processing \n",
    "- A few seconds of delay could be life threatening (for example, a car is at an intersection and the traffic light switches to red)\n",
    "\n",
    "Previously, almost all processing was done in batches - It's pretty easy to write a script that runs every few hours. Streaming however, is considered tougher as it raises new challenges such as having to keep a system running constantly. In the past, tools to ingest and process data as it was produced didn't yet exist. Having said that, batch processing can have it's own strengths in certain use cases, and raise it's own problems when pushed to it's limits. Although streaming is the more modern approach, it's not the right approach to every problem. We will explore this in more detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Batch and Streaming \n",
    "\n",
    "> There is a spectrum between streaming and batch data ingestion. The more frequently you run the batch ingestion job, the closer you get to streaming. \n",
    "\n",
    "Near the pure batch batch end of the spectrum of data ingestion approaches, there is a technique called _micro-batching_ which is where data is ingested very frequently but not immediately. In micro-batching, data is collected into small groups (batches) before the data is ingestion into a system. An example of this type of data ingestion is to send airline check-in data every 5 minutes to a mainframe computer. This approach has been commonly used in situations where batching can be used but where lower response times are required and there is no need to ingest a full historical dataset.\n",
    "\n",
    "Here is how we can visualise a micro-batch of data which will ben sent to the mainframe every 5 minutes:\n",
    "\n",
    "<p></p>\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src= \"images/micro-batch_2.png\" width=600>\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "Now that we've discussed at a high-level the different tools and frameworks available to ingest data in batch and streaming, it's important to understand the similarities and differences between each approach and also to clarify when to use each one.\n",
    "\n",
    "Below is a table summarizing the key criteria defining each of the data ingestion approaches, assesed by the hardware, performance, data size and analysis complexity:\n",
    "<p></p>\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src= \"images/batch-micro-stream.png\" width=600>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for Data Ingestion\n",
    "\n",
    "> Batch processing data tools are mature and have been used for a several years. They can handle data ingestion, data processing or sometimes both\n",
    "\n",
    "Tools commonly used for ingesting batch data and loading it into a storage location (such as a data lake) include:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Description</th>\n",
    "            <th>Use Cases</th>\n",
    "\t\t    <th>Key Features</th>\n",
    "            <th>Limitations</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>TIBCO</th>\n",
    "            <td>Specialised connector that can ingest both batch and streaming data</td>\n",
    "            <td>Can directly use the pre-built connector to ingest data from over 100 types of source systems</td>\n",
    "            <td>Industry grade, mature and has out-of-the-box compatibility with the top data sources</td>\n",
    "            <td>Close-sourced, relatively old and requires paid licesnses</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Flume</th>\n",
    "            <td>Tool for efficiently collecting, aggregating and moving large amounts of data</td>\n",
    "            <td>Batch and streaming data pipelines mainly in big data environments</td>\n",
    "            <td>Reliable, fault-tolerant, distributed architecture, flexible</td>\n",
    "            <td>Relatively outdated, doesn't guarantee ordering, sometimes causes duplicates</td>\n",
    "        </tr>  \n",
    "\t\t<tr>\n",
    "            <th>Sqoop</th>\n",
    "            <td>Specialised tool for transferring data between Hadoop and relational databases</td>\n",
    "            <td>Integrating Hadoop with relational databases</td>\n",
    "            <td>Easy to use, fast, supports parallel data transfer</td>\n",
    "            <td>Deprecated, can't pause or resume a transfer, failures need special handling</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Kafka</th>\n",
    "            <td>Event streaming platform capable of handling big data</td>\n",
    "            <td>Streaming data pipelines and analytics (although it can also support Batch)</td>\n",
    "            <td>Distributed, highly scalable, reliable, guarantees data ordering, low-latency</td>\n",
    "            <td>Lacks monitoring tools, not yet fully mature, can be difficult to configure</td>\n",
    "        </tr>  \n",
    "    </tbody>\n",
    " </table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIBCO\n",
    "<p></p>\n",
    "<p align=\"left\">\n",
    "<img src= \"images/tibco-white.png\" width=100>\n",
    "</p>\n",
    "\n",
    "- Although a legacy tool, it provides a data pipeline capable of ingesting batch and streaming data\n",
    "- For more details, [check TIBCO's homepage](https://www.tibco.com/)\n",
    "\n",
    "### Flume\n",
    "<p></p>\n",
    "<p align=\"left\">\n",
    "<img src= \"images/flume.png\" width=100>\n",
    "</p>\n",
    "\n",
    "- An open-source, distributed service that collects logs from several sources and takes them to a destination for aggregation and analysis. It is highly fault tolerant. \n",
    "- Flume allows data collection in batch (processing data in batches) as well as in streaming (processing data in real-time) mode\n",
    "- For more details, [check Flume's homepage](https://flume.apache.org/)\n",
    "\n",
    "### Sqoop\n",
    "<p></p>\n",
    "<p align=\"left\">\n",
    "<img src= \"images/sqoop_white2.png\" width=100>\n",
    "</p>\n",
    "\n",
    "- Sqoop is shorthand for SQL to Hadoop. It was popular during the past 10 years but has now become a legacy component, although it's still used in many systems.\n",
    "- It's a big data tool that offers the capability to extract data from non-Hadoop data stores, transform the data into a form usable by Hadoop, and then load the data into HDFS. This process is called ETL, for extract, transform, and load.\n",
    "- For more details, [check Sqoop's homepage](https://sqoop.apache.org/)\n",
    "\n",
    "### Kafka\n",
    "<p></p>\n",
    "<p align=\"left\">\n",
    "<img src= \"images/kafka.png\" width=100>\n",
    "</p>\n",
    "\n",
    "- Kafka is a popular tool used for moving big data between a source and a destination\n",
    "- The tool provides configurable features that give data engineers the ability to handle both batch and streaming data\n",
    "- For more details, [check Kafka's homepage](https://kafka.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Best Practices \n",
    "\n",
    "Some of the best practices to focus on while building industry-grade data ingestion pipelines include:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"width:auto;text-align:center\">Best Practice Criteria</th>\n",
    "            <th style=\"width:auto;text-align:center\">Description</th>\n",
    "\t\t    <th style=\"width:auto;text-align:center\">Examples</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "            <th>1. Automating data ingestion</th>\n",
    "            <td>\n",
    "                <li> Workflow scheduling tools are used to automate certain steps or to trigger specific parts of the ingestion process based on a schedule\n",
    "                <li> Instead of manually starting the job, it can be initiated automatically using a workflow scheduler\n",
    "                <li> Workflow scheduling is also known as orchestration\n",
    "            </td>\n",
    "            <td>\n",
    "                <li> Most pipelines use automated workflow schedulers like Apache Oozie (now somewhat outdated) or Apache Airflow (which is newer) to manage the data movement throughout the system\n",
    "                <li> These tools sometimes provide graphical user interfaces (GUI) to monitor the status of the data pipeline\n",
    "            </td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "            <th>2. Scrubbing sensitive data early </th>\n",
    "            <td>\n",
    "                <li> Sensitive data should be scrubbed as early as possible (ideally while it's being ingested), to avoid storing it in the data lake\n",
    "                <li> Storing such data may potentially expose it accidentally and lead to fines\n",
    "            </td>\n",
    "            <td>\n",
    "                <li> Some ingestion tools allow basic checks on the data while it's in-motion. Some logic can be used to check for sensitive data\n",
    "                <li> This is to avoid paying hefty fines such as those mandated by GDPR regulations\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>3. Using heterogeneous/compatible technologies</th>\n",
    "            <td>\n",
    "                <li> Tools used in the data ingestion process must be designed and implemented with backward (legacy) and forward (future) compatability in mind\n",
    "                <li> This is to avoid future re-work and to have a fully heterogeneous data system\n",
    "            </td>\n",
    "            <td>\n",
    "                <li>One way to support this best practice is by using containerisation technologies such as Kubernetes and Docker\n",
    "            </td>\n",
    "        </tr>\n",
    "           <tr>\n",
    "            <th>4. Flexibility regarding data formats </th>\n",
    "            <td>\n",
    "                <li> Data ingestion tools must be flexible to support reading and transforming a wide variety of data formats (such as JSON, CSV and so on)\n",
    "                <li> In the long-term, this helps to avoid having re-work done as new data file formats are introduced into the system\n",
    "            </td>\n",
    "            <td>\n",
    "                <li> The design and setup should anticipate potential future data types as much as possible\n",
    "            </td>\n",
    "        </tr>            \n",
    "     </tbody>\n",
    " </table>\n",
    "\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Ingestion is one of the critical components of an enterprise data pipeline as it is necessary to introduce new data into a software platform\n",
    "- Data ingestion usually involves 3 steps:  consume raw data from a creation source, processing the data, then writing the processed data to a target \n",
    "- There are 2 main approaches to data ingestion: streaming and batch. The approaches differ based on how fast data is ingested. In streaming, data ingestion is almost instant, while in batch data is collected over a period of time and then ingested all together in one big bulk (batch).\n",
    "- Streaming and micro-batch data ingestion methods are similar but not identical to each other. Streaming requires almost instant data ingestion and processing while micro-batch does not.\n",
    "- There are a few industry best practices that should be following when designing and deploying data ingestion solutions. These include using automated ingestion workflows, scrubbing sensitive data early, using compatible tools/technologies and providing future flexibility regarding new file formats."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
