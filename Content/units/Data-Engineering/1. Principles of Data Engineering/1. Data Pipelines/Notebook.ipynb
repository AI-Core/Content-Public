{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipelines\n",
    "\n",
    "In order to be able to handle the increasing volume, variety and velocity of data in the modern age, the data foundation in modern organisations is becoming more sophisticated. One critical element of a data infrastructure is the data pipeline.\n",
    "\n",
    "## What is a Data Pipeline?\n",
    "\n",
    "> A data pipeline is a set of tools and processes that involve moving, processing and storing data in a similar way that a water pipeline moves, processes, and stores water. \n",
    "\n",
    "A data pipeline might be as simple as moving raw data from a single source (point A) to a single target location (point B), or as complex as gathering data from multiple sources, transforming it then storing it in multiple destinations. \n",
    "\n",
    "Constructing data pipelines is the core responsibility of data engineering. It requires advanced programming skills to design a program for continuous and automated data exchange and sophisticated technical knowledge to enable engineers to integrate various tools and technologies together.\n",
    "\n",
    "A data pipeline is commonly used for:\n",
    "\n",
    "- Moving data from the source of creation to a target storage destination\n",
    "- Wrangling/transforming the data, integrating various datasets and moving the output into a single centralised location (often called a data lake)\n",
    "- Integrating data generated from various connected devices and systems \n",
    "- Migrating/copying databases from on-premise hardware into a cloud-based data warehouse\n",
    "- Connecting a data lake to advanced business intelligence reporting dashboards which are used by the business executives\n",
    "\n",
    "## How is Data Actually Moved? \n",
    "\n",
    "An example of a real-world data pipeline is in the below image. This is part of the Emirates Airlines check-in system mentioned earlier.  \n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src= \"images/emirates-data-pipeline.png\" width=900>\n",
    "\n",
    "</p>\n",
    "\n",
    "The above data pipeline consists of the following:\n",
    "\n",
    "- Data generated at the source (by check-in agents and the Emirates mobile app) is sent to a mainframe computer and also to a data warehouse for permanent storage. This setup was the original legacy system\n",
    "- Data is extracted in XML format every 5 minutes from the data warehouse and sent to TIBCO, which is a data pipeline tool that has tradtionally been used for some time\n",
    "- TIBCO forwards the data to Flume every 30 minutes. Flume was a new component added to integrate TIBCO with big data tools.\n",
    "- Flume moves the data to Kafka, where it's buffered until specific data criteria are met (time and size related criteria). Kafka was added at a later step to increase address some of the issues Flume faced\n",
    "- Once the data criteria is met, Kafka send the XML files to a big data lake (Hadoop HDFS)\n",
    "\n",
    "In the past, data pipelines were usually manually triggered and monitored. More recenty however, tools have come out to enable automated scheduling, running and monitoring of pipelines. Some of the benefits of automation include:\n",
    "\n",
    "- Ensuring consistency of the data by re-using the same data ingestion and storing code\n",
    "- Enhancing productivity as data engineers don't need to manually ingest the data on a regular basis and can spend more of their time on other tasks\n",
    "- Providing automatic scaling of the network bandwidth to enable the data pipeline to handle different data speeds throughout the day\n",
    "- Making the code re-usable across the various teams and systems which helps to standardise best pratices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a Data Pipeline \n",
    "\n",
    "Below is the flight check-in system used by Emirates Airlines. This system displays how check-in data is created, transported, stored and processed in a real production enviornment.  \n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src= \"images/emirates-color.png\" width=900>\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "Data pipelines almost always start by ingesting data. We'll get into more details regarding data ingestion in a seperate notebook.\n",
    "\n",
    "\n",
    "\n",
    "In the above system, we have _7 component groups_ (which can be on-premise hardware/software, cloud based tools or a combination of both):\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"width:auto;text-align:center\">Component</th>\n",
    "            <th style=\"width:auto;text-align:center\">Description</th>\n",
    "\t\t    <th style=\"width:auto;text-align:center\">Diagram Examples/Color</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>1. Data Source/Origin</th>\n",
    "            <td><li> Is a component in the system which creates/produces the raw data\n",
    "                <li> A data source can be human or machine generated data\n",
    "                <li> Flight check-in data can be manually entered by agents at the airport counter, or via mobile devices if passengers use the online check-in feature provided by the Emirates app\n",
    "                <li> A data source can also be a data storage location which will export/produce data for another system\n",
    "            </td>\n",
    "            <td>\n",
    "                <li>Top left corner of the diagram\n",
    "                <li><span style=\"color:blue\">Blue</span>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>2. Data Destination</th>\n",
    "            <td><li> Is the target location to which data is moved to\n",
    "                <li> The destination is relative to the source, and can be another data pipeline, a data repository, or an entirely different system\n",
    "                <li> It can be a temporary or permanent target\n",
    "            </td>\n",
    "            <td>\n",
    "                <li> The destination can be any step that recieves the data\n",
    "                <li> For example, Flume is the destination for TIBCO, while Kafka is the destination for Flume\n",
    "            </td>\n",
    "        </tr>  \n",
    "\t\t<tr>\n",
    "            <th>3. Data Ingestion</th>\n",
    "            <td>\n",
    "                <li> Consists of the tools and processes of moving data from a source to a temporary or permanent target\n",
    "                <li> Some of the more modern tools (such as Kafka) allow basic data processing (such as running counters) while the data is in-motion\n",
    "            </td>\n",
    "            <td> \n",
    "                <li> Examples include TIBCO, Flume and Kafka\n",
    "                <li><span style=\"color:#FFD580\">Light Orange</span>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>4. Data Transformation</th>\n",
    "            <td>\n",
    "                <li> Is the process of changing the state of the data \n",
    "                <li> Involves integrating the raw data which is produced and then manipulating it to become useful information\n",
    "                <li> Some examples of pre-processing and transformation include:\n",
    "                <ul> \n",
    "                    <li> Simple data cleaning tasks (such as removing empty records)</li>\n",
    "                    <li> Removing certain unwanted columns from a table</li>\n",
    "                    <li> Data de-duplication (removing duplicate records)</li>\n",
    "                    <li> Changing the data file format (from JSON to Parquet)</li>\n",
    "                </ul> \n",
    "            </td>\n",
    "            <td>\n",
    "                <li> Apache Spark steps at the bottom of the diagram\n",
    "                <li> <span style=\"color:#BABDBA\">Grey</span> \n",
    "            </td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "            <th>5. Data Wrangling</th>\n",
    "            <td>\n",
    "                <li>Wrangling involves more advanced data cleaning and integrating  complex datasets for easy access and analysis\n",
    "            </td>\n",
    "            <td>\n",
    "                <li> The Apache Spark step at the bottom of the diagram\n",
    "                <li> <span style=\"color:#CF9FFF\">Purple</span>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>6. Workflow Scheduling</th>\n",
    "            <td>\n",
    "                <li> Workflow scheduling tools are used to automate certain steps or to trigger specific parts of the pipeline based on a schedule\n",
    "                <li> For example, you might want to run a data cleaning program once per day at 9pm. Instead of manually starting the job, it can be initiated automatically using a workflow scheduler\n",
    "                <li> Workflow scheduling is also known as orchestration\n",
    "                <li> Most pipelines use automated workflow schedulers like Apache Oozie (now somewhat outdated) or Apache Airflow (which is newer) to manage the data movement throughout the system\n",
    "                <li> These tools sometimes provide graphical user interfaces (GUI) to monitor the status of the data pipeline\n",
    "            </td>\n",
    "            <td>\n",
    "                <li> All arrows connecting the various components\n",
    "                <li><span style=\"color:#FF8C00\">Orange arrows</span>   \n",
    "            </td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "            <th>7. Data Storage</th>\n",
    "            <td>\n",
    "                <li> Storage tools can vary and include several options such: as databases, cloud storage or phyiscal commodity (cheap) disk storage  \n",
    "                <li> Is often a centralised big data repository mainly used by large organisations to capture raw data for processing and ETL purposes\n",
    "                <li> Data is normally preserved in various storage environments at different stages of the data pipeline\n",
    "            </td>\n",
    "            <td>\n",
    "                <li>Note that there are several landing zones throughout the data pipeline\n",
    "                <li><span style=\"color:#90EE90\">Green</span>\n",
    "            </td>\n",
    "        </tr>    \n",
    "    </tbody>\n",
    " </table>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "Once the raw data is captured in the landing zone, it can then be manipulated and transformed into more meaningful information that can be used for various downstream tasks.  \n",
    "\n",
    "Some of these downstream tasks may include:\n",
    "\n",
    "-   Ad-hoc queries - such as doing aggregate calculations to retrieve the total sales for the week\n",
    "-   Advanced analytics - such as creating sophisticated graphs and charts showing various metrics over time\n",
    "-   Data science - such as creating algorithms to predict product prices for next year\n",
    "\n",
    "Depending on the business use case, data engineering projects could have different demands for the latency (delay) and speed of the data acquisition process, amongst other things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challanges Building Data Pipelines\n",
    "\n",
    "Creating data pipelines in organisations doesn't come without its challenges.  For instance, the data infrastructure can be complex and diversified both in terms of the technologies used and geographic location of the various tools.  Moreover, systems are usually built incrementally over time, so there are a variety of legacy systems and tools that must be accounted for as well.\n",
    "\n",
    "Setting up secure and reliable data flow pipelines can be a complex task. There are so many things that can go wrong.  \n",
    "\n",
    "Some of the main challenges are highlighted below:\n",
    "    \n",
    "### Complexity\n",
    "-   Creating data ingestion processes can be complex due to the increasing speed (velocity) of data generation and the growing size (volume) of data files\n",
    "-   Development times can be costly in time and resources (data engineers are expensive)\n",
    "-   Building data pipelines from scratch every time a new data source or business requirement comes up can be time consuming\n",
    "<p></p>\n",
    "\n",
    "### Data security\n",
    "-   Keeping sensitive details private while transferring data from one point to another is a key concern \n",
    "<p></p>\n",
    "\n",
    "### Unreliability\n",
    "-   During data movement, the reliability of the data may be compromised and thus cause incorrect decisions based on untrue or corrupted data. This may occur because:\n",
    "    -   Data can be corrupted\n",
    "    -   Data can be lost\n",
    "    -   Networks can be overloaded during peak times causing delays (latency)\n",
    "    -   Data sources may conflict, generating duplicate or incorrect data\n",
    "<p></p>\n",
    "\n",
    "### Technology evolution\n",
    "- Modern data sources, tools, and consuming applications evolve rapidly. Thus we are faced with constant changes while deploying and maintaining data pipelines.\n",
    "- Additionally, integrating newer tools with older legacy ones can be a daunting and a time-consuming challenge\n",
    "<p></p>\n",
    "\n",
    "### Data changes\n",
    "- The structure of the data produced from origin could change without prior notice, causing issues to consuming applications\n",
    "- For instance, a new column might have been added that the code isn't designed to handle yet. This will lead to a system error or crash.\n",
    "<p></p>\n",
    "\n",
    "### Maintenance and rework\n",
    "- Implementing changes to a data pipeline can be time-consuming and complex\n",
    "- Debugging and maintenance can take away time from developing new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Data pipelines are a critical component in the data infrastructure of the modern day corporation as they are used to manage the data flow throughout entire systems\n",
    "- A data pipeline consists of the tools and processes involved in moving, processing and storing data\n",
    "- A typical enterprise data pipeline consists of 7 components which include: a data source, a data target, data ingestion process, data transformation, data wrangling/cleaning, workflow scheduling and data storage  \n",
    "- Automating data pipelines is important as it provides numerous benefits which include: ensuring consistency of the data, freeing up data engineer's time for other tasks, automatic scaling (up or down) of resources based on the data load and helping to standardise best practices by making the code re-usable across the organisation\n",
    "- Data pipeline design and implementation is a core role of data engineers in global companies\n",
    "- Data pipeline creation has several challenges. The top challanges include: complexity, time required, data security, data reliability, technology evolution and system maintenance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
