{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount an AWS S3 bucket to Databricks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of mounting an S3 bucket to Databricks?\n",
    "\n",
    "- To copy data from S3 to Databricks for faster processing\n",
    "- To enable Databricks to access data stored in an S3 bucket as if it were a local file system ***\n",
    "- To synchronize data between Databricks and an S3 bucket in real-time\n",
    "- To enable Databricks to run Spark jobs directly on data stored in an S3 bucket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following commands can be used to mount an S3 bucket to Databricks?\n",
    "\n",
    "- `dbutils.fs.mount(\"s3://my-bucket\", \"/mnt/my-bucket\")` ***\n",
    "- `databricks.fs.mount(\"s3://my-bucket\", \"/mnt/my-bucket\")`\n",
    "- `spark.fs.mount(\"s3://my-bucket\", \"/mnt/my-bucket\")`\n",
    "-  `aws.fs.mount(\"s3://my-bucket\", \"/mnt/my-bucket\")`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the steps to mount an S3 bucket to Databricks?\n",
    "\n",
    "- Create an IAM role for Databricks, configure the S3 bucket policy, and run the mount command in a Databricks notebook\n",
    "- Create a Databricks secret with the S3 bucket access key and secret key, upload the credentials to Databricks, and run the mount command in a Databricks notebook ***\n",
    "- Create an S3 bucket policy, configure the Databricks workspace IAM role, and run the mount command in a Databricks notebook\n",
    "- Configure Databricks to use AWS credentials, configure the S3 bucket policy, and run the mount command in a Databricks notebook"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
