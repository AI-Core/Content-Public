{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The [*Databricks Lakehouse Platform*](https://www.databricks.com/product/data-lakehouse) is a unified platform that provides tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. Databricks integrates with cloud storage and security in your cloud account, and can manage and deploy cloud infrastructure on your behalf.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Databricks work with AWS?\n",
    "\n",
    "The Databricks Lakehouse platform architecture is composed of two primary parts: \n",
    "- the infrastructure used by Databricks to deploy, configure, and manage the platform\n",
    "- the customer-owned infrastructure managed in collaboration by Databricks and your company\n",
    "\n",
    "Databricks does not force you to migrate your data into its own storage systems in order to use the platform. Instead, you can configure a Databricks workspace by configuring a secure integration between the Databricks and your cloud account. Then, Databricks will deploy the compute clusters using cloud resources in your account to process and store data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks fundamental concepts\n",
    "\n",
    "### Accounts, Workspaces and Users\n",
    "\n",
    "A Databricks *Account* represents a single entity for purpose of billing and support, but can include multiple workspaces.\n",
    "\n",
    "A *Workspace* can have two meanings:\n",
    "- A Databricks deployment in the cloud that functions as an unified environment for all your Databricks assets. \n",
    "- The UI for the Databricks persona-based environments, as seen below:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Databricks Workspace.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "A workspace organizes objects (notebooks, libraries, dashboards and experiments) into directories and provides access to data and other computational resources, such as clusters and jobs. The objects you will be working with the most are *Notebooks*, which are documents that contain runnable commands and visualizations.\n",
    "\n",
    "A *User* is an unique individual who has access to the system. User identities are represented by email addresses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "\n",
    "A set of computation resources on which you run notebooks and jobs. There are two types of clusters: *all-purpose* and *job*.\n",
    "- All-purpose cluster: you create them using the UI, CLI, or REST API. You can manually terminate and restart them, and they can be shared across multiple users.\n",
    "- Job cluster: The Databricks job scheduler creates a job cluster when you run a job and terminates the cluster when the job is complete. You cannot restart an job cluster.\n",
    "\n",
    "You can check out active clusters, under the **Compute** panel on your Databricks account:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Databricks Cluster.png\" width=\"1000\" height=\"250\"/>\n",
    "</p>\n",
    "\n",
    "### Databricks runtime\n",
    "\n",
    "The set of core components that run on the clusters managed by Databricks. Databricks offers several types of runtimes:\n",
    "\n",
    "- *Databricks Runtime* includes Apache Spark but also adds a number of components and updates that substantially improve the usability, performance, and security.\n",
    "\n",
    "- *Databricks Runtime for Machine Learning* is built on Databricks Runtime and provides a ready-to-go environment for machine learning and data science. It contains libraries such as TensorFlow and PyTorch.\n",
    "\n",
    "- *Databricks Light* is the Databricks packaging of the open source Apache Spark runtime. It provides a runtime option for jobs that donâ€™t need the advanced performance, reliability, or autoscaling benefits. You can select Databricks Light only when you create a cluster to run a JAR, Python, or spark-submit job; you cannot select this runtime for clusters on which you run interactive notebooks, jobs or workloads.\n",
    "\n",
    "### Workflows\n",
    "\n",
    "Workflows are frameworks to develop and run data processing pipelines, including create, run, and manage Databricks Jobs. Workflows represent a non-interactive mechanism for running a notebook or library either immediately or on a scheduled basis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Lakehouse components\n",
    "\n",
    "The main components of the Databricks Lakehouse are:\n",
    "#### *1.Delta Table*\n",
    "Tables created on Databricks use the Delta Lake protocol by default. When you create a new Delta table:\n",
    "- Metadata used to reference the table is added to the metastore in the declared schema or database.\n",
    "- Data and table metadata are saved to a directory in cloud object storage.\n",
    "\n",
    "#### *2.Unity Catalog*\n",
    "\n",
    "The Unity Catalog ensures you have complete control over who gains access to which data and provides a centralised mechanisms for managing all data governance and access controls without needing to replicate data.\n",
    "\n",
    "- Account-level management of the Unity Catalog metastore means databases, data objects, and permissions can be shared across Databricks workspaces.\n",
    "- You can leverage three tier namespacing (`<catalog>.<database>.<table>`) for organizing and granting access to data.\n",
    "- External locations and storage credentials are also securable objects.\n",
    "- The Data Explorer provides a graphical user interface to explore databases and manage permissions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data objects in the Databricks Lakehouse\n",
    "\n",
    "The Databricks Lakehouse architecture combines data stored with the Delta Lake protocol in cloud object storage with metadata registered to a metastore. \n",
    "\n",
    "The metastore contains all the metadata that defines data objects in the lakehouse. The following options exist:\n",
    "- *Unity Catalog*: you can create a metastore to store and share metadata across multiple Databricks workspaces. Unity Catalog is managed at the account level.\n",
    "\n",
    "- *Hive metastore*: Databricks stores all the metadata for the built-in Hive metastore as a managed service. Every Databricks deployment has a central Hive metastore accessible by all clusters to persist table metadata. A Hive metastore is a database that holds metadata about data, such as paths to the data in the data lake and the format of the data.\n",
    "\n",
    "- *External metastore*: you can also bring your own metastore to Databricks.\n",
    "\n",
    "Regardless of the used metastore, Databricks will store all data associated with the table in the object storage configured by the customer in their cloud account.\n",
    "\n",
    "There are five main objects in the Databricks Lakehouse:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Primary Objects.png\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "- *Catalog*: a grouping of databases.\n",
    "- *Database or schema*: a grouping of objects in a catalog. Databases contain tables, views, and functions.\n",
    "- *Table*: a collection of rows and columns stored as data files in object storage.\n",
    "- *View*: a saved query typically against one or more tables or data sources.\n",
    "- *Function*: saved logic that returns a scalar value or set of rows.\n",
    "\n",
    "Most of these objects will be stored in the *Databricks File System (DBFS)*. A filesystem that contains directories, which can contain files (data files, libraries, and images), and other directories. DBFS is automatically populated with some datasets that you can use to learn Databricks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks architecture planes\n",
    "\n",
    "> Databricks architectures have two planes: a *control plane* and a *data plane*.\n",
    "\n",
    "### Control plane\n",
    "\n",
    "The control plane includes the backend services that Databricks manages in its own AWS account. Notebook commands and many other workspace configurations are stored in the control plane and encrypted at rest.\n",
    "\n",
    "### Data plane\n",
    "\n",
    "The data plane, which your AWS account manages, is where the data resides and is processed. By default, Apache Spark clusters are created in a single VPC that Databricks creates and configures in the customer-controlled AWS account.\n",
    "\n",
    "- For most Databricks computation, the compute resources are in your AWS account in what is called the *Classic data plane*. This is the type of data plane Databricks uses for notebooks, jobs, and for Classic Databricks SQL warehouses.\n",
    "- If you enable Serverless compute for Databricks SQL, the compute resources for Databricks SQL are in a *Shared Serverless data plane*. The compute resources for notebooks, jobs and Classic Databricks SQL warehouses still live in the Classic data plane in the customer account. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Databricks Architecture.png\" width=\"500\" height=\"500\"/>\n",
    "</p>\n",
    "\n",
    "### S3 bucket in customer-controlled AWS account\n",
    "\n",
    "An Amazon S3 bucket is created in the customer account when a Databricks cluster is deployed. The Databricks workspace uses this S3 bucket to store some input and output data. It access this data in two ways:\n",
    "\n",
    "- *Databricks-managed directories*. Some data (Spark driver log initial storage, job output, etc) is stored or read by Databricks in hidden directories. These directories are inaccessible to customers using Databricks File System (DBFS).\n",
    "\n",
    "- *DBFS root storage*. This storage can be accessed by customer notebooks through a DBFS path."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of:\n",
    "- What is Databricks\n",
    "- How can Databricks be integrated with a cloud provider\n",
    "- Fundamental concepts of Databricks\n",
    "- The Databricks Lakehouse components\n",
    "- The Databricks Lakehouse architecture\n",
    "- How data objects are stored in the Databricks Lakehouse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2 (default, Mar  3 2021, 11:58:52) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d25a140d753dbbce5959f3e993c340d725ba39c38e4259359e51030082d2593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
