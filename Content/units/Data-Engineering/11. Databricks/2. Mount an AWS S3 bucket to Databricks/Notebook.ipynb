{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount an AWS S3 bucket to Databricks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Databricks is an integrated analytics environment powered by Apache Spark which lets you connect and read from many data sources such as AWS S3, HDFS, MySQL, Cassandra etc. In this notebook, we will learn how to read data from an Amazon S3 bucket.\n",
    "\n",
    "To do so, we will follow these steps:\n",
    "- Create an access key and a secret access key for Databricks in AWS\n",
    "- Mount Databricks to a AWS S3 bucket\n",
    "- Read JSON files from mounted S3 bucket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create AWS Access Key and Secret Access Key for Databricks\n",
    "\n",
    "Once the desired data has been uploaded to a S3 bucket, we will go through the following steps:\n",
    "\n",
    "1. Access the **IAM console** in your AWS account.\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AWS Console IAM.png\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "2. In the IAM console, under **Access management** click on **Users**.\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/IAM Users.png\" width=\"200\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "3. Click on the **Add users** button.\n",
    "\n",
    "4. Enter the desired **User name** and click **Next**.\n",
    "\n",
    "5. On the permission page, select the **Attach existing policies directly** choice. In the search bar type **AmazonS3FullAccess** and check the box.\n",
    "(This will allow full access to S3, meaning Databricks will be able to connect to any existing buckets on the AWS account.)\n",
    "\n",
    "6. Skip the next sections until you reach the **Review page**. Here select the **Create user** button. \n",
    "\n",
    "7. Now that you have created your IAM User, you will need to assign it a programmatic access key:\n",
    "- In the **Security Credentials** tab select **Create Access Key**\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/security_credentials.png\" width=\"700\"/>\n",
    "</p>\n",
    "\n",
    "- On the subsequent page select **Command Line Interface (CLI)**, navigate to the bottom of the page click **I understand**\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/i_understand.png\" width=\"700\"/>\n",
    "</p>\n",
    "\n",
    "- On the next page, give the keypair a description and select **Create Access Key**\n",
    "\n",
    "8. Click the **Download.csv file** button to download the credentials you have just created.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/copy_keypair_2.png\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload credential csv file to Databricks\n",
    "\n",
    "1. In the **Databricks UI**, click the **Data** icon and then click **Create Table**.\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Databricks Credential Upload.png\" width=\"700\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "2. Click on **Drop files to upload, or click to browse** and select the credentials file you have just downloaded from AWS. Once the file has been successfully uploaded, you should see a green checkmark next to it.\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Credentials Uploaded.png\" width=\"500\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "> As we can see the credentials will be uploaded in the following location: */FileStore/tables*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Mount S3 bucket to Databricks\n",
    "\n",
    "1. Select the **New** icon and then select **Notebook**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Create Notebook.png\" width=\"500\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "2. Let's check the contents in FileStore, the location where we uploaded our AWS credentials in the last step, by running the following command:\n",
    "\n",
    "`dbutils.fs.ls(“/FileStore/tables”)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the CSV file you uploaded earlier is now inside the FileStore tables folder.\n",
    "\n",
    "3. Mount the S3 bucket to Databricks.\n",
    "\n",
    "We will need to import the following libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing\n",
    "import urllib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read the csv file containing the AWS keys to Databricks using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the `access key` and `secret access key` from the spark dataframe created above. The secret access key will be encoded using `urllib.parse.quote` for security purposes. `safe=\"\"` means that every character will be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now mount the S3 bucket by passing in the **S3 URL** and the **desired mount name** to `dbutils.fs.mount()`. Make sure to replace the `AWS_S3_BUCKET` with the name of the bucket you have your data stored into, and `MOUNT_NAME` with the desired name inside your Databricks workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"bucket_name\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/mount_name\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will return *True* if the bucket was mounted successfully. You will only need to mount the bucket once, and then you should be able to access it from Databricks at any time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Read data from the mounted S3 bucket\n",
    "\n",
    "1. To check if the S3 bucket was mounted succesfully run the following command:\n",
    "\n",
    "`display(dbutils.fs.ls(\"/mnt/mount_name/../..\"))`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If inside the mounted S3 bucket your data is organised in folders, you can specify the whole path in the above command after `/mnt/mount_name`. With the correct path specified, you should be able to see the contents of the S3 bucket when running the above command.\n",
    "\n",
    "2. Read the JSON format dataset from S3 into Databricks using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/mount_name/filepath_to_data_objects/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 (Optional): Unmount S3 bucket\n",
    "\n",
    "If you want to unmount the S3 bucket, run the following code:\n",
    "\n",
    "`dbutils.fs.unmount(\"/mnt/mount_name\")`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of:\n",
    "- How to create AWS access keys for Databricks and how to upload them\n",
    "- How to mount/unmount an Amazon S3 bucket to Databricks\n",
    "- How to read data from mounted buckets "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2 (default, Mar  3 2021, 11:58:52) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d25a140d753dbbce5959f3e993c340d725ba39c38e4259359e51030082d2593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
