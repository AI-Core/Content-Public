{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Spark Streaming* is an extension of the Apache Spark ecosystem designed for processing and analyzing real-time data. It enables developers to build scalable, fault-tolerant, and high-throughput stream processing applications using the familiar programming model of Apache Spark. Rather than processing large batches of data at once, Spark Streaming breaks the data into small, manageable chunks known as *micro-batches*, allowing for near-real-time processing.\n",
    "\n",
    "Common use cases for Spark Streaming include:\n",
    "\n",
    "- **Real-time Analytics**: Spark Streaming is widely used for real-time analytics, providing businesses with insights and intelligence as data is generated\n",
    "- **Fraud Detection**: In financial services, Spark Streaming can be employed to detect fraudulent transactions in real time\n",
    "- **IoT Data Processing**: Analyzing data from Internet of Things (IoT) devices as it streams in, enabling timely decision-making\n",
    "- **Log Analysis**: Processing and analyzing logs and events as they occur, aiding in troubleshooting and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of Spark Streaming Applications\n",
    "\n",
    "### 1. Micro-Batch Processing\n",
    "\n",
    "**Micro-batch processing** is a fundamental concept in Spark Streaming, enabling the continuous handling of data in compact time-based intervals. This approach stands in contrast to traditional batch processing, as it allows for more frequent processing cycles, making it ideal for scenarios demanding low-latency analytics.\n",
    "\n",
    "Spark Streaming operates on micro-batches of data. Each micro-batch represents a discrete unit of work and is processed independently.\n",
    "\n",
    "### 2. DStreams (Discretized Streams)\n",
    "\n",
    "> *DStreams*, or *Discretized Streams*, the fundamental abstraction in Spark Streaming, represent a continuous series of data divided into micro-batches. DStreams are built on the concept of *Resilient Distributed Datasets (RDDs)*, which are fault-tolerant, immutable collections of objects that can be processed in parallel across a distributed computing cluster. Each micro-batch in a DStream corresponds to an RDD, essentially creating a sequence or series of RDDs over time. These RDDs capture the state of the streaming data at each time interval.\n",
    "\n",
    "DStreams provide a high-level API that enables developers to perform transformations and actions on streaming data. This API is similar to the one used in batch processing with Apache Spark.\n",
    "\n",
    "### 3. Transformations and Actions\n",
    "\n",
    "Spark Streaming applications consist of a series of transformations and actions applied to DStreams. Transformations modify the data within each micro-batch, while actions trigger the execution of computations and produce results.\n",
    "\n",
    "### 4. Receivers and Executors\n",
    "\n",
    "The success of a Spark Streaming application relies on the effective coordination between receivers and executors.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/streaming-arch.jpg\" width=\"900\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "*Receivers* collect data from various sources, such as Kafka, Flume, or HDFS, and deliver it to the Spark Streaming application. They act as the entry point for ingesting streaming data.\n",
    "\n",
    "*Executors* process the received data in parallel across the Spark cluster. They perform the defined transformations and actions on micro-batches, ensuring efficient and scalable stream processing.\n",
    "\n",
    "The combination of receivers and executors provides a balance between parallel processing and fault tolerance. Receivers collect data in parallel, and executors process micro-batches independently, ensuring resilience to node failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize the process end-to-end:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/streaming-flow.jpg\" width=\"900\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "1. **We hook up to a Data Stream Source**:\n",
    "\n",
    "   - Spark Streaming integrates with various streaming sources, such as: Apache Kafka, Amazon Kinesis, Flume, etc. <br><br>\n",
    "\n",
    "2. **Incoming Data is Divided into Batches**:\n",
    "\n",
    "   - The streaming data is received in continuous streams and divided into micro-batches\n",
    "   - These micro-batches represent the fundamental unit of processing in Spark Streaming <br><br>\n",
    "\n",
    "3. **Batches are Processed by Spark Engine**:\n",
    "\n",
    "   - The Spark engine processes each micro-batch independently using the same powerful transformations and actions available in batch processing. This ensures a consistent and scalable processing model. <br><br>\n",
    "\n",
    "4. **Resulting Data Batches Created**:\n",
    "\n",
    "    - The outcome of the processing is a series of result batches, each reflecting the analysis performed on the corresponding input micro-batch\n",
    "    - These result batches can then be used for further analysis\n",
    "\n",
    "> In the Databricks Lakehouse platform, Spark Streaming seamlessly integrates with Databricks notebooks. Databricks clusters support streaming workloads, allowing data engineers to create, experiment, and deploy Spark Streaming applications effortlessly. Through Databricks, developers can connect with streaming sources, define micro-batch processing intervals, and visualize streaming data output directly within notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a Stream\n",
    "\n",
    "In Structured Streaming, a **data stream** is treated as a table that is continuously appended to. Consider the input data stream as the input table. Every data item arriving in the stream is like a new row being appended to the input table. At every trigger interval, let's say every 1 second, new rows are appended to the input table, which eventually updates the result table. Whenever the result table is updated, the changed results rows are written to an external sink.\n",
    "\n",
    "Databricks provides default datasets that can be used to emulate streaming scenarios. The default Databricks datasets can be found at `/databricks-datasets/`.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DatabricksDatasets.png\" width=\"900\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "As we can see, there is a dataset called `structured-streaming`. Let's examine the contents of the `/databricks-datasets/structured-streaming/events/` directory.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/StreamingDataset.png\" width=\"900\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "We can see that this directory contains multiple `JSON` files. Let's run the code below to see the structure of one of these `JSON` files:\n",
    "\n",
    "```python\n",
    "# Define the path to the JSON file\n",
    "jsonFilePath = \"dbfs:/databricks-datasets/structured-streaming/events/file-0.json\"\n",
    "\n",
    "# Read the content of the JSON file into a DataFrame\n",
    "jsonContentDF = spark.read.json(jsonFilePath)\n",
    "\n",
    "# Show the content of the DataFrame\n",
    "display(jsonContentDF)\n",
    "```\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/JSONContent.png\" width=\"900\" height=\"450\"/>\n",
    "</p>\n",
    "\n",
    "Each line in this `JSON` (and all the `JSON`s in this directory) contains two fields: `time` and `action`.\n",
    "\n",
    "Now this might be confusing, as Databricks calls this dataset `structured-streaming`, but we can clearly see the sample data contains a set of static files. We can emulate a stream from these files by reading one file at a time, in the chronological order, as such:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "inputPath = \"/databricks-datasets/structured-streaming/events/\"\n",
    "\n",
    "# Define the schema to speed up processing\n",
    "jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n",
    "\n",
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(jsonSchema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .load(inputPath)\n",
    ")\n",
    "\n",
    "display(streamingInputDF)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "- The `readStream` method is used to initialize a streaming DataFrame by reading from a default Databricks dataset for structured streaming events\n",
    "- The option`(\"maxFilesPerTrigger\", 1)` ensures that each file is treated as an individual stream entry, allowing for one-file-at-a-time processing and maintaining the chronological order of file creation during data ingestion\n",
    "- The schema of the `JSON` data is explicitly specified using `schema` for efficient processing\n",
    "\n",
    "> When you run this code in a Databricks notebook, the `display` command will display the streaming DataFrame as a live updating dashboard. This dashboard updates as new files are read from the specified directory, providing a visual representation of the streaming data.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/LiveDashboard.png\" width=\"800\" height=\"450\"/>\n",
    "</p>\n",
    "\n",
    "As the source directory is a static set of files and no new files are added, the streaming will not automatically stop. It will keep processing the existing files one by one based on the trigger interval. To stop the streaming manually, you can either interrupt the notebook execution, using the **Interrupt** button at the top-right side of the notebook UI, or stop the streaming query explicitly in your code. We will see how to automatically stop a streaming query later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `readStream` method\n",
    "\n",
    "> The `readStream` method can be generalized beyond the example above, and is used to initiate the process of reading streaming data from a specified source.\n",
    "\n",
    "The general syntax of the method is:\n",
    "\n",
    "```python\n",
    "spark.readStream\n",
    "  .format(\"source_format\")         # Specify the format of the streaming source (e.g., \"kafka\", \"json\", \"parquet\")\n",
    "  .option(\"option_key\", \"value\")   # Set specific options for reading from the streaming source\n",
    "  .schema(custom_schema)            # Specify a custom schema for the streaming data\n",
    "  .load(\"stream_source_path\")       # Specify the path or location of the streaming data\n",
    "```\n",
    "\n",
    "In the syntax above:\n",
    "\n",
    "- `spark.readStream`: This initiates the reading of streaming data. It is followed by a chain of methods to specify the details of the streaming source.\n",
    "\n",
    "- `.format(\"source_format\")`: Specifies the format of the streaming source. Examples include `kafka` for Apache Kafka, `json` for `JSON` files, `parquet` for Parquet files, etc.\n",
    "\n",
    "- `.option(\"option_key\", \"value\")`: Sets specific options for reading from the streaming source. These options vary depending on the source format. For example, in Kafka, you might specify the Kafka topic, group id, etc.\n",
    "\n",
    "- `.schema(custom_schema)`: Specifies a custom schema for the streaming data. This is optional. If not specified, Spark will infer the schema from the streaming data. Generally, it is good practice to specify a custom schema, as Spark's automatic schema inference might not capture the intended data types accurately all the time.\n",
    "\n",
    "- `.load(\"stream_source_path\")`: Specifies the path or location of the streaming data. This could be a file path, directory path, or a connection URL depending on the source format.\n",
    "\n",
    "We have seen an example using `JSON` files above. In the another lesson, we will look at another commonly used streaming source: AWS Kinesis data streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Structured Streaming Schemas\n",
    "\n",
    "When working with Structured Streaming, defining a schema is important for processing streaming data. A schema specifies the structure of your streaming data, including the data types of each field. In PySpark, you can define a schema using the `StructType` class, `StructField` class, and related classes from the `pyspark.sql.types` module.\n",
    "\n",
    "Let's break down the process of defining a schema:\n",
    "\n",
    "1. **Import Necessary Classes**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "```\n",
    "\n",
    "2. **Define the Schema**\n",
    "\n",
    "```python\n",
    "# Define a streaming schema using StructType\n",
    "streaming_schema = StructType([\n",
    "    StructField(\"field1\", StringType(), True),\n",
    "    StructField(\"field2\", IntegerType(), True),\n",
    "    StructField(\"field3\", TimestampType(), True),\n",
    "    # Add more fields as needed\n",
    "])\n",
    "```\n",
    "\n",
    "   - `StructType`: Represents the structure of a Spark DataFrame. It is composed of a list of `StructField` objects.\n",
    "\n",
    "   - `StructField`: Represents a field in a DataFrame. It takes three arguments: the name of the field, the data type of the field, and a boolean indicating whether the field can be null.\n",
    "\n",
    "   - `StringType`, `IntegerType`, `TimestampType`: Represent the data types of the fields. PySpark supports various data types such as strings, integers, timestamps, etc.\n",
    "\n",
    "3. **Use the Schema in Streaming Operations**\n",
    "\n",
    "```python\n",
    "# Read streaming data with the custom schema\n",
    "streaming_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(streaming_schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Example option for file-based streaming source\n",
    "    .load(\"/path/to/streaming/data\")\n",
    ")\n",
    "```\n",
    "\n",
    "By defining a schema for your streaming data, you ensure that Spark processes the data correctly and consistently during the streaming operations. This becomes essential when dealing with semi-structured or `JSON` data in a streaming context. Adjust the schema definition based on the structure of your streaming data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Operations\n",
    "\n",
    "> Structured Streaming in PySpark provides powerful mechanisms for time-based operations through *window operations*. Windowing allows you to perform computations on data over specific time intervals, enabling you to gain insights into trends, patterns, and aggregates within those windows.\n",
    "\n",
    "A **window** is a logical unit of time that you define for processing data. It involves grouping data within specified time boundaries, and then applying transformations or aggregations to that grouped data. Windows are particularly useful when dealing with time-series data, enabling you to analyze trends and patterns over fixed intervals.\n",
    "\n",
    "Let's explore some common window operations that you can perform with Structured Streaming.\n",
    "\n",
    "### Sliding Windows\n",
    "\n",
    "*Sliding windows* capture data within a specified duration and slide forward by a specified interval. For example, you might want to compute the sum of values within a 1-hour window every 30 minutes.\n",
    "\n",
    "```python\n",
    "sliding_window_df = (\n",
    "    streaming_df\n",
    "    .groupBy(window(\"timestamp\", \"1 hour\", \"30 minutes\"))\n",
    "    .agg(sum(\"value\"))\n",
    ")\n",
    "```\n",
    "\n",
    "In the example above:\n",
    "\n",
    "- `window(\"timestamp\", \"1 hour\", \"30 minutes\")`: This part defines the window specification. It indicates that we want to create windows based on the `timestamp` column. Each window has a duration of `1 hour`, and it slides forward every `30 minutes`. This means that we analyze the sum of value within overlapping 1-hour windows that update every 30 minutes.\n",
    "\n",
    "- `.agg(sum(\"value\"))`: This part specifies the aggregation operation. We use the `agg` function to compute the `sum` of the `value` column within each window. The resulting DataFrame, `sliding_window_df`, provides insights into the total sum of values within each sliding 1-hour window.\n",
    "\n",
    "### Tumbling Windows\n",
    "\n",
    "Tumbling windows are non-overlapping windows of fixed duration. For instance, you might want to compute the average value within a 1-hour window:\n",
    "\n",
    "```python\n",
    "tumbling_window_df = (\n",
    "    streaming_df\n",
    "    .groupBy(window(\"timestamp\", \"1 hour\"))\n",
    "    .agg(avg(\"value\"))\n",
    ")\n",
    "```\n",
    "\n",
    "In the example above, similarly to the sliding window example, the `window(\"timestamp\", \"1 hour\")` part defines the window specification. Here, we just indicate the column `timestamp`, and the fixed duration `1 hour`.\n",
    "\n",
    "> When using window operations, you can apply various types of aggregations to derive meaningful insights from your data. These aggregations include common aggregations also used in batch processing, such as `sum`, `avg`, `count`, etc.\n",
    "\n",
    "Now, let's perform a window operation on our streaming DataFrame `streamingInputDF` from the previous section:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "streamingCountsDF = (\n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.action,\n",
    "      window(streamingInputDF.time, \"1 hour\"))\n",
    "    .count()\n",
    ")\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "- `.groupBy(...)`: The `groupBy` operation is applied to group the streaming data based on two columns:\n",
    "\n",
    "  - `action`: This is the categorical field representing different actions\n",
    "  - `window(streamingInputDF.time, \"1 hour\")`: This specifies the window specification. It indicates that we want to create windows based on the `time` column with a fixed duration of 1 hour. Each window groups data within a distinct 1-hour interval. <br><br>\n",
    "\n",
    "- `.count()`: The `count` aggregation function is used to calculate the number of records within each window. This provides insights into the frequency or occurrence of different actions within each 1-hour window.\n",
    "\n",
    "So, the resulting DataFrame, `streamingCountsDF`, contains the count of occurrences of each unique action within non-overlapping 1-hour windows. It gives a temporal perspective on the distribution of actions over time, offering insights into how the frequency of actions changes within each hour. In the next section, we will learn how to visualize the output of such operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Streams\n",
    "\n",
    "Writing streaming data to *external sinks* is a crucial aspect of real-time data processing.\n",
    " \n",
    "**Sinks** are the destinations where the processed and transformed data is stored for further analysis or retrieval. Spark Streaming supports a variety of sinks, including *Delta Lake*, external databases, and more. The choice of the sink depends on the specific requirements of the application. Key considerations when selecting a sink include reliability, scalability, and the ability to handle real-time data updates.\n",
    "\n",
    "To initiate writing streaming data to a sink, you can use the following general syntax:\n",
    "\n",
    "```python\n",
    "query = (\n",
    "    streamingDataFrame  # Replace with your actual streaming DataFrame\n",
    "    .writeStream\n",
    "    .format(\"sink_format\")      # Specify the sink format (e.g., \"memory\", \"delta\", \"jdbc\", etc.)\n",
    "    .option(\"option_key\", \"value\")   # Set specific options for the chosen sink\n",
    "    .queryName(\"query_name\")    # Name your streaming query\n",
    "    .outputMode(\"output_mode\")  # Specify the output mode (\"complete\", \"append\", or \"update\")\n",
    "    .table(\"table_name\") # Specify table_name if writing to a table\n",
    ")\n",
    "```\n",
    "\n",
    "### Output Modes in Streaming\n",
    "\n",
    "The output of a streaming computation is defined by what gets written to the external storage. Spark Streaming provides three output modes:\n",
    "\n",
    "- **Complete Mode**:\n",
    "\n",
    "  - In this mode, the entire updated result table is written to external storage\n",
    "  - The storage connector determines how to handle writing the complete table\n",
    "  - Useful when the entire result set is relevant for downstream processing <br><br>\n",
    "\n",
    "- **Append Mode**:\n",
    "\n",
    "  - Only new rows appended to the result table since the last trigger are written to external storage\n",
    "  - Applicable when existing rows in the result table are not expected to change\n",
    "  - Efficient for scenarios where only new data is relevant <br><br>\n",
    "\n",
    "- **Update Mode**:\n",
    "\n",
    "  - Only the rows that were updated in the result table since the last trigger are written to external storage\n",
    "  - Different from Complete Mode as it outputs only the changed rows since the last trigger\n",
    "  - Equivalent to Append Mode if the query doesn't contain aggregations\n",
    "\n",
    "### Checkpointing\n",
    "\n",
    "*Checkpoints* are essential for fault-tolerance and ensuring the resiliency of streaming queries. In the context of Spark, a **checkpoint** is a snapshot of the distributed processing state of a streaming application. It includes metadata, configuration settings, and necessary information to recover the application's state in case of failures or restarts. To add a checkpoint location, use the `.option(\"checkpointLocation\", \"path/to/checkpoint\")` method in the general syntax. \n",
    "\n",
    "### Example: Writing to Databricks Delta Lake\n",
    "\n",
    "To illustrate the process of writing streaming data to a Delta Lake, let's continue our previous example using `streamingCountsDF`:\n",
    "\n",
    "```python\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"delta\")          # Delta Lake sink for durable storage\n",
    "    .queryName(\"streaming_query\") # Can give the query a name\n",
    "    .outputMode(\"complete\")   # Complete mode: All counts should be stored in Delta Lake\n",
    "    .option(\"checkpointLocation\", \"tmp/checkpoints\")  # Add checkpoint location\n",
    "    .table(\"delta_lake_table\")  # Specify the Delta Lake table name\n",
    ")\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "- `format(\"delta\")`: Specifies the sink type as Delta Lake for durable and reliable storage\n",
    "\n",
    "- `outputMode(\"complete\")`: Specifies the output mode as complete, ensuring that all counts are stored in Delta Lake\n",
    "\n",
    "- `.option(\"checkpointLocation\", \"tmp/checkpoints\")`: Adds a checkpoint location to ensure fault-tolerance and resiliency\n",
    "\n",
    "The `query` handle represents the streaming query running in the background. This query continuously processes incoming data, updates the windowed counts, and writes the complete result set to Delta table.\n",
    "\n",
    "> IMPORTANT: You will need to run all the commands we have used as an example so far in the same notebook cell, so: reading the stream, performing the window operation and writing the stream should all be contained within the same cell.\n",
    "\n",
    "The cell output will report the status of the stream:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/StreamOutput.png\" width=\"800\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "You can expand this output using the arrow next to the query name, `streaming_query`. You will get a dashboard of the number of records processed, batch statistics and the state of the aggregation:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/QueryOutput.png\" width=\"700\" height=\"500\"/>\n",
    "</p>\n",
    "\n",
    "You can either interrupt the cell for the operation to stop, or you can programmatically determine the time period at which the query should stop using the following syntax:\n",
    "\n",
    "```python\n",
    "import time\n",
    "# .... the code from before\n",
    "\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"delta\")          # Delta Lake sink for durable storage\n",
    "    .queryName(\"streaming_query\") # Can give the query a name\n",
    "    .outputMode(\"complete\")   # Complete mode: All counts should be stored in Delta Lake\n",
    "    .option(\"checkpointLocation\", \"tmp/checkpoints\")  # Add checkpoint location\n",
    "    .table(\"delta_lake_table\")  # Specify the Delta Lake table name\n",
    ")\n",
    "\n",
    "time.sleep(60) # desired time after which the query should stop\n",
    "query.stop()\n",
    "```\n",
    "\n",
    "Once the query has stopped running, you should be able to access this streaming data using the **Data** explorer tab:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DeltaTable.png\" width=\"700\" height=\"350\"/>\n",
    "</p>\n",
    "\n",
    "You can double-click on the table name to access its schema and its sample data:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DeltaData.png\" width=\"800\" height=\"450\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Spark Streaming is designed for real-time data processing, breaking data into micro-batches for scalable and fault-tolerant stream processing applications\n",
    "- Micro-batch processing in Spark Streaming enables continuous handling of data in manageable intervals, providing low-latency analytics compared to traditional batch processing\n",
    "- DStreams, based on Resilient Distributed Datasets (RDDs), serve as the fundamental abstraction in Spark Streaming, allowing high-level transformations and actions on streaming data\n",
    "- The `readStream` method initializes a streaming DataFrame, specifying the source format, options, schema, and location of the streaming data\n",
    "- Schemas are crucial for processing streaming data accurately. Use `StructType`, `StructField`, and related classes to define a schema for structured streaming.\n",
    "- Window operations enable time-based analysis, allowing insights into trends, patterns, and aggregates within specified intervals\n",
    "- Use the `writeStream` method to write streaming data to external sinks. Output modes (Complete, Append, Update) determine what data is written.\n",
    "- Checkpoints are crucial for fault-tolerance, providing snapshots of the distributed processing state to recover the application's state in case of failures or restart"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
