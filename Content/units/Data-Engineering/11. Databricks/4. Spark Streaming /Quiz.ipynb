{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements accurately describes Spark Streaming?\n",
    "\n",
    "- Spark Streaming is a batch processing framework for processing large volumes of data\n",
    "- Spark Streaming is a real-time data processing framework for processing streaming data ***\n",
    "- Spark Streaming is a relational database management system\n",
    "- Spark Streaming is a machine learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a DStream in Spark Streaming and how does it differ from a RDD in Apache Spark?\n",
    "\n",
    "- A DStream is a sequence of static datasets, while a RDD is a continuous stream of data\n",
    "- A DStream is a continuous stream of data, while a RDD represents static datasets ***\n",
    "- A DStream and a RDD are both continuous streams of data, but a DStream is optimized for real-time processing\n",
    "- A DStream and a RDD are both static datasets, but a DStream is optimized for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the typical process of Spark Streaming?\n",
    "\n",
    "- Spark Streaming reads data from a file, processes it in real-time, and writes the output to a file\n",
    "- Spark Streaming reads data from a database, processes it in real-time, and writes the output to a database\n",
    "- Spark Streaming reads data from a streaming source, processes it in real-time, and writes the output to a streaming sink ***\n",
    "- Spark Streaming reads data from a batch source, processes it in near-real-time, and writes the output to a batch sink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are window operations in Spark Streaming?\n",
    "\n",
    "- A way to create subsets of data which are more performant\n",
    "- A way to perform aggregation on a window of data over a specified period of time ***\n",
    "- A way to filter out irrelevant data points from the incoming data stream\n",
    "- A way to perform machine learning algorithms on the incoming data streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about checkpointing in Spark Streaming?\n",
    "\n",
    "- Checkpointing is a method that saves a DStream to a file system or external storage\n",
    "- Checkpointing is a transformation that applies a function to each RDD in a DStream\n",
    "- Checkpointing is a mechanism that stores metadata about the state of a Spark Streaming application for fault tolerance ***\n",
    "- Checkpointing is an action that returns the first element of a DStream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does a sliding window capture in Spark Streaming?\n",
    "\n",
    "- Non-overlapping windows of fixed duration\n",
    "- All data in the streaming source\n",
    "- Data within a specified duration that slides forward by a specified interval ***\n",
    "- Fixed-duration windows with no sliding interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True or False: The `readStream` method in Spark Streaming is used to initiate the process of writing streaming data to an external sink.\n",
    "\n",
    "- True\n",
    "- False ***"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
