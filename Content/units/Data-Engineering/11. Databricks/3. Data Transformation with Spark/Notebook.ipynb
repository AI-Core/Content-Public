{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation is a critical step in the journey from raw data to actionable insights. It involves the process of cleaning, enriching, and structuring data to make it suitable for analysis. The importance of data transformation lies in its ability to:\n",
    "\n",
    "- **Enhance Data Quality**: Transformations help clean and validate data, ensuring that it is accurate and reliable for downstream analysis\n",
    "\n",
    "- **Enable Analysis**: Well-transformed data is easier to analyze, allowing data scientists and analysts to derive meaningful patterns, trends, and insights\n",
    "\n",
    "- **Support Decision-Making**: Businesses rely on high-quality, transformed data to make informed decisions, optimize processes, and gain a competitive edge\n",
    "\n",
    "> Databricks leverages the power of Spark to ensure that data undergoes these transformations efficiently, providing a seamless and collaborative environment where the full potential of transformed data can be realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark Architecture\n",
    "\n",
    "Before we start leveraging Spark to perform data transformations, let's first understand the architecture underlying Apache Spark. Remember, Spark is a unified engine for large-scale distributed data processing on computer clusters.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/SparkArchitecture.png\" width=\"700\" height=\"350\"/>\n",
    "</p>\n",
    "\n",
    "### Cluster Manager\n",
    "\n",
    "> Apache Spark's architecture revolves around a *Cluster Manager*, a central entity that coordinates the distribution of tasks across a computing cluster. The Cluster Manager is responsible for resource allocation and task scheduling.\n",
    " \n",
    "In Databricks, users are abstracted from direct interaction with the Cluster Manager. Users only interact with Databricks to create and configure clusters through an interface. In the background, the platform automatically manages cluster resources, handling tasks like resource allocation and task scheduling.\n",
    "\n",
    "### Spark Application\n",
    "\n",
    "> A *Spark Application* represents the entire computation process performed using Spark. It consists of the driver program (*Spark Driver*) and a set of executor programs (*Spark Executors*). The Spark Application defines the tasks to be executed on the Spark cluster and submit them to the Cluster Manager for execution.\n",
    "\n",
    "Databricks facilitates the submission and management of Spark Applications. Users define and execute Spark Applications through Databricks Notebooks. Databricks also manages the Spark Application lifecycle, including job submission and execution.\n",
    "\n",
    "### Spark Executors\n",
    "\n",
    "> **Spark Executors** are worker nodes within the cluster responsible for executing tasks. Executors manage data partitions in memory and store intermediate results. They enhance performance by processing data close to where it is stored, minimizing data movement across the network.\n",
    "\n",
    "Databricks transparently managed Spark Executors. When users execute Spark Jobs, the Databricks platform dynamically allocates and oversees Spark Executors on the underlying infrastructure. Users do not need to manually configure or monitor individual executors.\n",
    "\n",
    "### Spark Driver\n",
    "\n",
    "> The **Spark Driver** is a central control program that manages the overall execution of a Spark job. It communicates with the Cluster Manager to acquire resources and coordinates tasks across Spark Executors. The Spark Driver is responsible for overseeing the execution flow and collection of final results.\n",
    "\n",
    "Users initiate the Spark Driver through Databricks notebooks or jobs. Code is written and executed in notebooks, or when jobs are triggered, Databricks coordinates with the Spark Driver. This automated process ensures smooth execution of Spark tasks without requiring users to initiate the Spark Driver manually.\n",
    "\n",
    "### Spark Session\n",
    "\n",
    "> The *Spark Session* serves as the entry point for interacting with Spark. It manages configuration settings and provides an unified interface for executing various operations. \n",
    "\n",
    "Databricks abstracts the concept of Spark Session for users. Starting a Spark Session is implicit when running code in a notebook cell. Databricks handles the creation of the Spark Session behind the scenes, ensuring users can switch between Spark SQL, Python, and other components without explicit session management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL vs PySpark\n",
    "\n",
    "So far in the course, we have seen examples of managing relational identities and Delta Lakes using Spark SQL. Now, we will switch our attention to PySpark, the Python API for Spark, but first let's understand the difference between the two flavors of Spark.\n",
    "\n",
    "> **Spark SQL** is a module in Spark designed for structured data processing. It allows users to execute SQL queries on Spark data, providing a high-level interface for working with structure and semi-structured data.\n",
    "\n",
    "Spark SQL is ideal for scenarios where you want to leverage the familiarity and expressiveness of SQL for querying and analyzing data. It's particularly well-suited for structured datasets and situations where SQL-like operations are preferable.\n",
    "\n",
    "> **PySpark** is the Python API for Spark. It enables Python developers to harness the power of Spark for distributed data processing. PySpark provides a programmatic interface for working with Spark, allowing more flexibility in expressing complex data transformations and analytics.\n",
    "\n",
    "PySpark is versatile and can be employed when you need more control and customization in your data processing tasks. It's suitable for scenarios where Python is the preferred language, or when you need to integrate Spark with Python-based libraries and tools.\n",
    "\n",
    "As we've seen before, Databricks provides a unified platform where you can seamlessly switch between Spark SQL and PySpark within the same notebook environment. This is because both Spark SQL and PySpark operate on the underlying concept of *DataFrames*. This serves as a bridge, allowing you to transition between the two of them using the DataFrame API.\n",
    "\n",
    "In the previous lesson, we focus our attention on relational entities in Databricks. While effective, traditional data structures such as databases and tables are more limited to SQL-based operations. With DataFrames, you can perform data manipulations and transformations using Spark SQL operations, benefiting from SQL-like syntax, then switch to PySpark to apply more programmatic and customized transformations, all on the same DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames\n",
    "\n",
    "> A **DataFrame** is a distributed collection of data organized into named columns, similar to a table in a relational database. DataFrames serve as a fundamental abstraction, providing a structured and tabular representation o data.\n",
    "\n",
    "The key features of DataFrames are:\n",
    "\n",
    "- **Distributed Nature**: DataFrames are distributed across a cluster of machines, allowing for parallel processing. This distribution enables Spark to handle large-scale datasets by dividing them into smaller partitions and processing them in parallel.\n",
    "\n",
    "- **Immutable Structure**: DataFrames are immutable, meaning their structure cannot be changed once created. However, you can perform transformations on a DataFrame to create a new DataFrame with the desired changes. This immutability ensures data consistency and facilitates the construction of a lineage of transformations.\n",
    "\n",
    "- **Lazy Evaluation**: Spark employs *lazy evaluation*, meaning that transformations on DataFrames are not executed immediately. Instead, Spark builds a logical execution plan, and the actual computation is deferred until an action is triggered. This optimization enhances performance by minimizing unnecessary computations.\n",
    "\n",
    "- **Schema**: DataFrames have a well-defined schema that specifies the names and types of columns. The schema provides structure to the data, allowing Spark to optimize query execution and enabling users to express complex transformations in a declarative manner.\n",
    "\n",
    "DataFrames in Spark can be created from various data sources, including structured data formats like `CSV` and `JSON`, or external databases. In the data transformation process, reading and loading data plays a pivotal role. In the next section, we will learn how to read and load data from various file types and examine practical examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data into DataBricks\n",
    "\n",
    "Let's firstly begin exploring how to read `JSON` data. We'll cover different scenarios, including reading a single `JSON` file, a directory of `JSON` files, and multiple `JSON` files using a wildcard.\n",
    "\n",
    "### Reading a Single `JSON` File\n",
    "\n",
    "When dealing with a single `JSON` file, PySpark DataFrames provide a straightforward method for reading and interacting with the data. The syntax is as it follows: \n",
    "\n",
    "```python\n",
    "json_data_single_file = spark.read.json(\"path/to/single/json/file\")\n",
    "```\n",
    "\n",
    "Let's see how we would use this in Databricks. Begin by downloading [this example `JSON` file](https://cdn.theaicore.com/content/lessons/14cf5386-4ddc-44c3-a575-bf581a740fec/single_json_file.json). Next, import it into Databricks using the **Data** explorer. In the **Data** explorer tab, use the **Create Table** button. Utilize the drag-and-drop functionality to upload the previously downloaded file. Note and copy the path at which the file will be uploaded.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CreateTable.png\" width=\"700\" height=\"550\"/>\n",
    "</p>\n",
    "\n",
    "Click the **Create Table with UI** button, select a cluster to preview the table, and click **Preview Table**. This should display the table preview and the inferred file type. Finally, click **Create Table** to finish.\n",
    "\n",
    "Now, navigate to a Databricks Notebook and run the following PySpark code to read in the uploaded `JSON` file:\n",
    "\n",
    "`json_data_from_dbfs1 = spark.read.json('/FileStore/tables/single_json_file_1.json')`\n",
    "\n",
    "Replacing the file path with your specific file path.\n",
    "\n",
    "### Checking DataFrame Contents\n",
    "\n",
    "After creating the DataFrame, you might want to inspect its contents. You can use the `show()` method to display the first few rows of the DataFrame: `json_data_from_dbfs1.show()`. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/ShowCommand.png\" width=\"800\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "This command prints a tabular representation of the DataFrame, providing an overview of the data's structure. You can adjust the number of rows displayed by specifying the desired value within the `show()` method (e.g., `show(10)` for the first 10 rows).\n",
    "\n",
    "Alternatively, for a more interactive approach, you can use the `display()` method:\n",
    "\n",
    "`display(json_data_from_dbfs1)`\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DisplayTable.png\" width=\"800\" height=\"250\"/>\n",
    "</p>\n",
    "\n",
    "The `display()` method provides a feature-rich interface for exploring and interacting with the DataFrame, including filtering, sorting, and visualizations. It's a powerful tool for a comprehensive examination of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a Directory of Files\n",
    "\n",
    "To read a directory of `JSON` files, you can use a similar approach but instead of specifying the file path, specify the path of the directory:\n",
    "\n",
    "`data = spark.read.json(\"path/to/files/directory\")`\n",
    "\n",
    "> This command reads all the files in the specified directory into a PySpark DataFrame.\n",
    "\n",
    "Let's look at an example to illustrate the process. Follow these steps:\n",
    "\n",
    "- Begin by downloading [this file](https://cdn.theaicore.com/content/lessons/14cf5386-4ddc-44c3-a575-bf581a740fec/file1.json), and [this file](https://cdn.theaicore.com/content/lessons/14cf5386-4ddc-44c3-a575-bf581a740fec/file2.json), representing two different `JSON` files\n",
    "\n",
    "- Import the files into Databricks using the **Data** explorer. In the **Data** explorer tab, make sure to modify the **DBFS target directory** to create a new folder where you can store your `JSON` files. See the example below for updating the first file:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/NewDirectory.png\" width=\"700\" height=\"550\"/>\n",
    "</p>\n",
    "\n",
    "- Upload both files to the same directory\n",
    "\n",
    "- Run the following command to read in all the files in the directory: `json_data = spark.read.json('/FileStore/tables/json_files')`\n",
    "\n",
    "- Visualize the output of this command use the `display()` command\n",
    "\n",
    "### Reading Multiple Files Using a Wildcard\n",
    "\n",
    "If you have multiple files in a directory, that also contains other data types. For example, you want to read only the `JSON` files you can use the following syntax:\n",
    "\n",
    "`json_data_multiple_files = spark.read.json(\"path/to/json/files/*.json\")`\n",
    "\n",
    "This command reads all `JSON` files matching the wildcard pattern (`*`) into a PySpark DataFrame. The wildcard (`*`) here represents any number of possible characters. Given this `*.json` will match any possible file name ending in `.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading `CSV` Data\n",
    "\n",
    "When dealing with tabular data, especially in scenarios where structured data is stored in `CSV` format, Spark provides efficient methods for reading and loading this data into Spark DataFrames.\n",
    "\n",
    "### Reading a Single `CSV` File\n",
    "\n",
    "To read a single `CSV` file into a DataFrame, you can use the following syntax:\n",
    "\n",
    "```python\n",
    "# Read a single CSV file into a PySpark DataFrame\n",
    "csv_data_single_file = spark.read.csv(\"path/to/single/csv/file.csv\", \n",
    "                                     header=True, \n",
    "                                     inferSchema=True, \n",
    "                                     sep=\";\")\n",
    "```\n",
    "\n",
    "The command above has the following key parameters:\n",
    "\n",
    "- `header=True`: Indicates that the first row contains column headers\n",
    "- `inferSchema=True`: Attempts to infer the schema of the data\n",
    "- `sep=\";\"`: Specifies the column delimiter. The default value is a comma (`,`), but in this example, it's set to a semicolon (`;`). Other possible values for `sep` include, but are not limited to: `\\t` for tab, `\" \"` for space, etc.\n",
    "\n",
    "Let's look at an example to illustrate the process. Follow these steps:\n",
    "\n",
    "- Begin by downloading [this CSV file](https://cdn.theaicore.com/content/lessons/14cf5386-4ddc-44c3-a575-bf581a740fec/username.csv)\n",
    "\n",
    "- Import the file into Databricks using the **Data** explorer. In the preview table tab, make sure to enable **First row is header** and the **Infer schema** options before creating the table.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CreateCSVTable.png\" width=\"800\" height=\"350\"/>\n",
    "</p>\n",
    "\n",
    "- Run the following command in a Databricks Notebook to read in the `CSV` file: `csv_data_single_file = spark.read.csv(\"dbfs:/FileStore/tables/username.csv\", header=True, inferSchema=True, sep=\";\")`\n",
    "\n",
    "- Visualize the output using the `display()` command\n",
    "\n",
    "### Reading `CSV` Files from a Directory\n",
    "\n",
    "If you have multiple `CSV` files in a directory, you can read them all into a DataFrame using a similar approach as for reading `JSON`s files from a directory:\n",
    "\n",
    "`csv_data_directory = spark.read.csv(\"path/to/csv/files/directory\", header=True, inferSchema=True)`\n",
    "\n",
    "### Handling `CSV` Files with Custom Schemas\n",
    "\n",
    "In some cases, you might want to specify a custom schema for your `CSV` data. You can achieve this by defining a schema and using it during the read operation:\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define a custom schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    # Add more fields as needed\n",
    "])\n",
    "\n",
    "# Read CSV data with the custom schema\n",
    "csv_data_custom_schema = spark.read.csv(\"path/to/csv/file.csv\", header=True, schema=custom_schema)\n",
    "\n",
    "````\n",
    "This command reads a `CSV` file into a PySpark DataFrame, applying the specified custom schema. We will look in more detail at defining custom schema in a future lesson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from Cloud Storage\n",
    "\n",
    "Data stored in cloud storage systems such as Amazon S3, Google Cloud Storage (GCS), or Azure Storage can be easily accessed with PySpark. Here's are two general approachs:\n",
    "\n",
    "Legacy approach:\n",
    "\n",
    "- **Store Credentials Safely**: Avoid hardcoding credentials directly in code. Leverage secure methods such as environment variables or secure key storage services.\n",
    "- **Mount Storage to Databricks**: Mount your cloud storage to Databricks, providing a secure way to access data. This involves configuring storage-specific credentials within Databricks.\n",
    "\n",
    "While mounting storage locations still works on Databricks it is **no longer recommended**. \n",
    "\n",
    "Modern approach:\n",
    "\n",
    "- **Configure Access using Cloud Permissions**: Each cloud provider has their own method of configuring Databricks to allow direct access to the cloud storage. Once this is configured, the data can be read directly into Databricks without the need to upload credentials. This reduces the risk that the credentials could be leaked or maliciously used. \n",
    "\n",
    "### Reading Data from S3\n",
    "\n",
    "Once Databricks has been configured for access, you will be able to read data into Databricks directly from an S3 bucket. This task would be the responsibility of the workspace administrator and would require the use of a Databricks AWS account. The steps to achieve this can be found in the Databricks documentation [here](https://docs.databricks.com/en/connect/storage/tutorial-s3-instance-profile.html). For any projects requiring this, this will already be configured for you.  \n",
    "\n",
    "Once configured you can use the following syntax to read your files directly:\n",
    "\n",
    "``` python\n",
    "bucket_name = \"<YOUR S3 BUCKET NAME>\"\n",
    "\n",
    "df = spark.read.load(f\"s3a://{bucket_name}/path/to/your/s3/files/somefile.txt\")\n",
    "display(df)\n",
    "dbutils.fs.ls(f\"s3a://{bucket_name}/\") # List the contents of the S3 directory\n",
    "\n",
    "```\n",
    "\n",
    "This method, will also support the reading of multiple files just like the `JSON` and `CSV` examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL for Reading Data\n",
    "\n",
    "In addition to using PySpark DataFrames, you can leverage Spark SQL to query and interact with your data using SQL commands. Let's explore how you can use Spark SQL to read `JSON` and `CSV` files.\n",
    "\n",
    "You can use Spark SQL to query `JSON` files directly. The following example illustrates how to select all columns from a `JSON` file:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM json.`path/to/json/file`\n",
    "```\n",
    "\n",
    "Similarly, Spark SQL enables you to query `CSV` files using SQL commands. For instance, you can use the following SQL command to select all columns from a `CSV` file:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM csv.`path/to/csv/file`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Transformation\n",
    "\n",
    "In the data analysis process, ensuring that the data is clean and appropriately formatted is crucial. Raw data often comes with inconsistencies, missing values, or formats that are not helpful for analysis. Cleaning and transforming data involves preparing it for further analysis, enhancing its quality, and making it suitable for downstream processes. In this section we will look at different techniques for cleaning and transforming data using PySpark.\n",
    "\n",
    "Let's start by considering the following example DataFrame representing information about individuals:\n",
    "\n",
    "```python\n",
    "# Create an example DataFrame\n",
    "data = [\n",
    "    [\"John\", \"Doe\", 30, \"Male\", \"$500.00\", \"2022-01-01 08:30:00\", [\"Street1\", \"New York\", \"12345\", \"USA\"], \"john.doe@example.com\", \"Married\"],\n",
    "    [\"Alice\", \"Smith\", 25, \"Female\", \"$700.50\", \"2022-01-02 15:45:30\", [\"Street2\", \"San Francisco\", \"54321\", \"USA\"], \"alice.smith@example.com\", \"Single\"],\n",
    "    [\"Bob\", \"Jones\", 30, \"Male\", \"$650.00\", \"2022-01-03 12:15:00\", [\"Street3\", \"Los Angeles\", \"67890\", \"USA\"], \"Unknown\", \"Single\"],\n",
    "    [\"Eve\", \"White\", 25, \"Female\", \"$600.75\", \"2022-01-04 10:00:45\", [\"Street4\", \"New Haven\", \"00000\", \"USA\"], \"eve.white@example.com\", \"User Info Error\"],\n",
    "    [\"Chris\", \"Johnson\", 25, \"Male\", \"$550.00\", \"2022-01-05 09:30:15\", [\"Street5\", \"Chicago\", \"45678\", \"USA\"], \"chris.j@example.com\", \"Single\"],\n",
    "    [\"Emily\", \"Davis\", 30, \"Female\", \"$750.25\", \"2022-01-06 11:20:30\", [\"Street6\", \"Seattle\", \"98765\", \"USA\"], \"emily.d@example.com\", \"Married\"],\n",
    "    [\"Michael\", \"Brown\", 25, \"Male\", \"$620.50\", \"2022-01-07 14:45:00\", [\"Street7\", \"Boston\", \"13579\", \"USA\"], \"michael.b@example.com\", \"Single\"],\n",
    "    [\"Samantha\", \"Clark\", 30, \"Female\", \"$720.75\", \"2022-01-08 16:00:45\", [\"Street8\", \"Denver\", \"24680\", \"USA\"], \"samantha.c@example.com\", \"User Info Error\"]\n",
    "]\n",
    "\n",
    "columns = [\"First Name\", \"Last Name\", \"Age\", \"Gender\", \"Salary\", \"Timestamp\", \"Location\", \"Email\", \"Status\"]\n",
    "\n",
    "# Creating the updated DataFrame\n",
    "example_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "example_df.show()\n",
    "```\n",
    "\n",
    "This DataFrame will serve as our example throughout this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Replacing Missing Values\n",
    "\n",
    "Handling missing values is a common occurrence in the data cleaning process. The `replace` method allows us to replace specific values with designated replacements. In the example DataFrame, suppose we want to replace all occurrences of `'User Info Error'` with `None` in the `Status` column:\n",
    "\n",
    "```python\n",
    "cleaned_df = example_df.replace({'User Info Error': None}, subset=['Status'])\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "  - `replace` initiates the replacement operation.\n",
    "  - `{'User Info Error': None}` defines the replacement rule, indicating that occurrences of `'User Info Error'` should be replaced with `None` \n",
    "  - `subset=['Status']` specifies the column where the replacement should occur\n",
    "\n",
    "### 2. Updating Data Points\n",
    "\n",
    "The `replace` method is not limited to handling null values; it can also be employed to update existing values. For instance, suppose we want to update all occurrences of `Unknown` in the `Email` column to `Pending`:\n",
    "\n",
    "```python\n",
    "cleaned_df = cleaned_df.replace({'Unknown': 'Pending'}, subset=['Email'])\n",
    "```\n",
    "In this example, `{'Unknown': 'Pending'}` defines the replacement rule, indicating that occurrences of `Unknown` should be replaced with `Pending` in the `Email` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using `regexp_replace` for Column Transformations\n",
    "\n",
    "Column transformations are essential for manipulating text-based columns. The `regexp_replace` function enables us to apply regular expression patterns to modify or clean column values.\n",
    "\n",
    "The default syntax is as follows:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"ColumnName\", regexp_replace(\"ColumnName\", \"pattern\", \"replacement\")\n",
    "```\n",
    "Let's break down the components:\n",
    "\n",
    "- `.withColumn(\"ColumnName\", ...)` : This method is used to add or replace a column in the DataFrame. In this case, it specifies that the operation is targeting a column called `ColumnName`.\n",
    "\n",
    "- `regexp_replace(\"ColumnName\", \"pattern\", \"replacement\")`: This is the PySpark `regexp_replace` function. It takes three arguments:\n",
    "  - `ColumnName`: The name of the column to which the replacement will be applied\n",
    "  - `pattern`: The regular expression pattern to search for in the values of the specified column\n",
    "  - `replacement`: The string that will replace the matched pattern in the column values\n",
    "\n",
    "Let's look at an example in our `cleaned_df` DataFrame:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "cleaned_df = cleaned_df.withColumn(\"Salary\", regexp_replace(\"Salary\", \"\\\\$\", \"\")\n",
    "```\n",
    "In the example above, the `regexp_replace` function removes dollar signs (`$`) from the `Salary` column in the DataFrame. The regex pattern `\\\\$` represents the dollar sign, and it is replaced with an empty string `\"\"`.\n",
    "\n",
    "### 4. Casting Columns to Different Data Types\n",
    "\n",
    "Casting columns to different data types is a common operation, especially when the inferred schema needs adjustment. We can cast columns to ensure they are of the correct data type. The default syntax is as it follows:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"ColumnName\", df[\"ColumnName\"].cast(\"<desired_type>\"))\n",
    "```\n",
    "The `cast` function is applied to convert the specified column to the desired data type. The argument, `<desired_type>`, represents the target data type to which you want to cast the column to. There are different types of data casting, including:\n",
    "\n",
    "- **Numeric Types**: Casting to numeric types, such as `integer`, `double`, `float`, etc., is common when dealing with numerical data\n",
    "\n",
    "- **String Type**: You can cast a column to the `string` type if you want to treat it as text\n",
    "\n",
    "- **Boolean Type**: Casting to `boolean` is suitable for columns representing true/false or binary data\n",
    "\n",
    "- **Timestamp Type**: For columns containing timestamp or date data, casting to `timestamp` is useful\n",
    "\n",
    "> Before casting different columns to new data types, it is useful to see exactly which data type is assigned to each column. You can do so using the `printSchema()` command.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/printSchema.png\" width=\"700\" height=\"250\"/>\n",
    "</p>\n",
    "\n",
    "In the example above, we can observe that the `Salary` column in our `cleaned_df` DataFrame is of type `string`, but as we removed the `$` sign from each value in the column in the previous step, we can now change the data type for this column to a numeric one:\n",
    "\n",
    "```python\n",
    "cleaned_df = cleaned_df.withColumn(\"Salary\", cleaned_df[\"Salary\"].cast(\"float\"))\n",
    "```\n",
    "If we now rerun the `printSchema()` command we should see the `Salary` column is now of type `float`:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/NewDataType.png\" width=\"700\" height=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transforming Columns to Timestamp Type\n",
    "\n",
    "Transforming columns to `timestamp` type is crucial for handling temporal data. The `to_timestamp` function is used to convert a `string` representation of a timestamp into the `timestamp` type. \n",
    "\n",
    "While the `cast` function allows casting to different data types, including timestamps, if you need to handle timestamp or date-related data, `to_timestamp` is the appropriate choice. This is because `to_timestamp` is specific to timestamp-related transformations and ensures the correct interpretation of time-related data.\n",
    "\n",
    "The default syntax is:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"NewTimestampColumn\", to_timestamp(\"ExistingTimestampColumn\"))\n",
    "```\n",
    "\n",
    "Let's break down the components of this syntax:\n",
    "\n",
    "- `\"NewTimestampColumn\"`: Specifies the name of the new column that will store the converted timestamps\n",
    "- `to_timestamp(\"ExistingTimestampColumn\")`: This is the PySpark `to_timestamp` function. It takes one argument:\n",
    "  - `\"ExistingTimestampColumn\"`: The name of the existing column containing string representations of timestamps that you want to convert\n",
    "\n",
    "If we take a look at the previously ran `printSchema()` command for our `cleaned_df` DataFrame we can see that the `Timestamp` column is of type string. We can change this using the following command:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleaned_df = cleaned_df.withColumn(\"Timestamp\", to_timestamp(\"Timestamp\"))\n",
    "```\n",
    "In this example, the `Timestamp` column is transformed to a `timestamp` type using the `to_timestamp` function.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/ToTimestamp.png\" width=\"700\" height=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creating New Columns Using Array Functions\n",
    "\n",
    "Generating new insights and integrating data often requires the creation of new columns. In PySpark, we leverage *array functions* and concatenation to derive meaningful information.\n",
    "\n",
    "> **Array functions** operate on arrays, which are ordered collections of elements. These functions become invaluable when working with columns that contain arrays of values. \n",
    "\n",
    "#### `array`: Creating a new Array Column\n",
    "\n",
    "The `array` function creates a new array column. In the example below, we create a column named `new_array_column` by combining values from `column1` and `column2`:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"new_array_column\", array(\"column1\", \"column2\"))\n",
    "```\n",
    "\n",
    "The output of such command would look like this:\n",
    "\n",
    "```markdown\n",
    "| column1 | column2 | new_array_column |\n",
    "|---------|---------|------------------|\n",
    "|   val1  |   val2  | [val1, val2]     |\n",
    "|   val3  |   val4  | [val3, val4]     |\n",
    "|   val5  |   val6  | [val5, val6]     |\n",
    "```\n",
    "\n",
    "We can use the `array` function to create a new column called `Full Name` in our `clean_df` DataFrame from the `First Name` and `Last Name` columns:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import array\n",
    "cleaned_df = cleaned_df.withColumn(\"Full Name Array\", array(\"First Name\", \"Last Name\"))\n",
    "```\n",
    "\n",
    "#### `array_contains`: Checking Array for a Value\n",
    "\n",
    "The `array_contains` function checks if an array contains a specific value. In the example below, we check if the `array_column` contains the value `val3`:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"contains_value\", array_contains(\"array_column\", \"val3\"))\n",
    "```\n",
    "\n",
    "The output of this command would look like this:\n",
    "\n",
    "```markdown\n",
    "| array_column    | contains_value  |\n",
    "|-----------------|-----------------|\n",
    "| [val1, val2]    | False           |\n",
    "| [val3, val4]    | True            |\n",
    "| [val5, val6]    | False           |\n",
    "```\n",
    "\n",
    "#### `size`: Getting Size of an Array\n",
    "\n",
    "The `size` function returns the size (length) of an array. In the example below, we create a column named `array_size` to store the size of the `array_column`:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"array_size\", size(\"array_column\"))\n",
    "```\n",
    "The output of this command would look like this:\n",
    "\n",
    "```markdown\n",
    "| array_column    | array_size |\n",
    "|-----------------|------------|\n",
    "| [val1, val2]    | 2          |\n",
    "| [val3, val4]    | 2          |\n",
    "| [val5, val6]    | 2          |\n",
    "```\n",
    "\n",
    "#### `concat`: Concatenating Multiple Arrays/Values\n",
    "\n",
    "The `concat` function can concatenate multiple arrays into a single array. In the example below, we create a column named `concatenated_arrays` by combining values from `array1` and `array2`:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"concatenated_arrays\", concat(\"array1\", \"array2\"))\n",
    "```\n",
    "The output of this command would look like this:\n",
    "\n",
    "```markdown\n",
    "| array1    | array2    | concatenated_arrays  |\n",
    "|-----------|-----------|-----------------------|\n",
    "| [v1, v2]  | [v3, v4]  | [v1, v2, v3, v4]      |\n",
    "| [v5, v6]  | [v7, v8]  | [v5, v6, v7, v8]      |\n",
    "| [v9, v10] | [v11, v12]| [v9, v10, v11, v12]   |\n",
    "```\n",
    "\n",
    "> The `concat` function in PySpark is not limited to concatenating arrays only; it can be employed to concatenate various elements, including strings and literals. Here's a breakdown of its utility:\n",
    "\n",
    "```python\n",
    "# Example: Concatenating strings from two columns\n",
    "df = df.withColumn(\"concatenated_strings\", concat(\"column1\", \"column2\"))\n",
    "\n",
    "# Example: Concatenating strings and literals\n",
    "df = df.withColumn(\"combined_values\", concat(\"column1\", lit(\" - \"), \"column2\"))\n",
    "```\n",
    "The `lit` function allows you to include a constant or literal value in a DataFrame operation, making it useful for combining columns with fixed strings or values.\n",
    "\n",
    "Let's explore the examples and their corresponding output:\n",
    "\n",
    "- Example 1: Concatenating Strings from Two Columns:\n",
    "\n",
    "```markdown\n",
    "| column1 | column2 | concatenated_strings |\n",
    "|---------|---------|----------------------|\n",
    "|   val1  |   val2  |       val1val2       |\n",
    "|   val3  |   val4  |       val3val4       |\n",
    "|   val5  |   val6  |       val5val6       |\n",
    "```\n",
    "\n",
    "- Example 2: Concatenating Strings and Literals\n",
    "\n",
    "```markdown\n",
    "| column1 | column2 | combined_values  |\n",
    "|---------|---------|-------------------|\n",
    "|   val1  |   val2  |   val1 - val2     |\n",
    "|   val3  |   val4  |   val3 - val4     |\n",
    "|   val5  |   val6  |   val5 - val6     |\n",
    "```\n",
    "\n",
    "In the first example, we concatenate the strings from two columns (`column1` and `column2`). In the second example, we combine strings with a literal hyphen (`\" - \"`) between them. This adds a hyphen between the values of `column1` and `column2` in the newly created `combined_values` column.\n",
    "\n",
    "Earlier we created a new column `Full Name Array` in our `cleaned_df` DataFrame. Alternatively to that approach, we can create a new column called `Full Name` that will use the `concat` function to concatenate the information from the `First Name` and `Last Name` columns. Instead of being a type `array` as with the previous example, this new column will simply be a `string`.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import concat, lit\n",
    "cleaned_df = cleaned_df.withColumn(\"Full Name\", concat(\"First Name\", lit(\" \"), \"Last Name\"))\n",
    "```\n",
    "\n",
    "#### `explode`: Transforming Array into Rows\n",
    "\n",
    "The `explode` function in PySpark is specifically designed for transforming columns containing arrays into rows. It duplicates the other columns for each element in the array, creating a new row for each array element.\n",
    "\n",
    "Let's consider the following DataFrame:\n",
    "\n",
    "```markdown\n",
    "| column1 |       array_column        |\n",
    "|---------|---------------------------|\n",
    "|   val1  |   [\"item1\", \"item2\"]      |\n",
    "|   val2  |   [\"item3\", \"item4\"]      |\n",
    "|   val3  |   [\"item5\", \"item6\"]      |\n",
    "```\n",
    "\n",
    "If we now apply the following transformation to this DataFrame:\n",
    "\n",
    "```python\n",
    "df = df.select(\"column1\", explode(\"array_column\").alias(\"exploded_values\"))\n",
    "```\n",
    "\n",
    "After applying the `explode` function:\n",
    "\n",
    "```markdown\n",
    "| column1 |  exploded_values  |\n",
    "|---------|-------------------|\n",
    "|   val1  |      \"item1\"      |\n",
    "|   val1  |      \"item2\"      |\n",
    "|   val2  |      \"item3\"      |\n",
    "|   val2  |      \"item4\"      |\n",
    "|   val3  |      \"item5\"      |\n",
    "|   val3  |      \"item6\"      |\n",
    "```\n",
    "\n",
    "In this example, the `explode` function creates new rows for each element in the `array_column`, duplicating the values in the `column1` for each exploded row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Unfolding Information from Arrays\n",
    "\n",
    "When working with arrays in PySpark, you may encounter scenarios where information is stored in array columns, and you want to unfold or extract that information into separate columns. The `withColumn` method, combined with array indexing, enables you to achieve this transformation. The general syntax is as follows:\n",
    "\n",
    "```python\n",
    "# Unfolding array information into separate columns\n",
    "df = df.withColumn(\"NewColumn1\", col(\"ArrayColumn\")[index1]) \\\n",
    "       .withColumn(\"NewColumn2\", col(\"ArrayColumn\")[index2]) \\\n",
    "       .withColumn(\"NewColumn3\", col(\"ArrayColumn\")[index3])\n",
    "# Repeat for each desired column\n",
    "```\n",
    "\n",
    "In the syntax above:\n",
    "\n",
    "- `\"NewColumn1\"`, `\"NewColumn2\"`, ...: The names of the new columns you want to create\n",
    "- `\"ArrayColumn\"`: The name of the column containing the array\n",
    "- `index1`, `index2`, ...: The indices indicating which elements from the array should populate the new columns\n",
    "\n",
    "Let's take a look now at our example `cleaned_df`, which has an array column named `\"Location\"` containing street, city, postcode, and country information:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Unfolding the Address array into separate columns\n",
    "cleaned_df = cleaned_df.withColumn(\"Street\", col(\"Location\")[0]) \\\n",
    "            .withColumn(\"City\", col(\"Location\")[1]) \\\n",
    "            .withColumn(\"Postcode\", col(\"Location\")[2]) \\\n",
    "            .withColumn(\"Country\", col(\"Location\")[3])\n",
    "```\n",
    "After applying this transformation, the DataFrame will have new columns: `\"Street\"`, `\"City\"`, `\"Postcode\"`, and `\"Country\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Renaming and Dropping Columns\n",
    "\n",
    "Renaming and dropping columns are common operations to enhance DataFrame clarity and exclude unnecessary information. In PySpark, you can achieve this using the `withColumnRenamed` method for renaming and the `drop` method for dropping columns.\n",
    "\n",
    "The general syntax for renaming columns is:\n",
    "\n",
    "```python\n",
    "# Renaming a single column\n",
    "df = df.withColumnRenamed(\"OldColumnName\", \"NewColumnName\")\n",
    "\n",
    "# Renaming multiple columns\n",
    "df = df.withColumnRenamed(\"OldColumnName1\", \"NewColumnName1\") \\\n",
    "       .withColumnRenamed(\"OldColumnName2\", \"NewColumnName2\")\n",
    "# Repeat for each column pair\n",
    "\n",
    "```\n",
    "The general syntax for dropping columns is:\n",
    "\n",
    "```python\n",
    "# Dropping a single column\n",
    "df = df.drop(\"ColumnName\")\n",
    "\n",
    "# Dropping multiple columns\n",
    "df = df.drop(\"ColumnName1\", \"ColumnName2\", ...)\n",
    "# List all columns you want to drop\n",
    "```\n",
    "\n",
    "Let's drop some redundant columns in our example `cleaned_df` DataFrame:\n",
    "\n",
    "```python\n",
    "cleaned_df = cleaned_df.drop(\"Location\", \"Full Name Array\", \"First Name\", \"Last Name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Reordering Columns\n",
    "\n",
    "Reordering columns can enhance DataFrame structure, providing better organization and simplifying data exploration. The syntax is as follows:\n",
    "\n",
    "```python\n",
    "df = df.select(\"Column1\", \"Column2\", ...)\n",
    "```\n",
    "Let's apply this to our example `cleaned_df` DataFrame. You can begin by observing the current column ordering using the `printSchema()` function, then decide on the desired ordering. For example:\n",
    "\n",
    "```python\n",
    "cleaned_df = cleaned_df.select(\"Full Name\", \"Age\", \"Gender\", \"Email\", \"Salary\", \"Street\", \"City\", \"Postcode\", \"Country\", \"Status\", \"Timestamp\")\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Manipulations\n",
    "\n",
    "In this section, we will look at more advanced data manipulation techniques for combining, grouping, aggregating, sorting, and utilizing window functions to extract deeper insights from your data.\n",
    "\n",
    "Let's begin by creating a second DataFrame that we will use in combination with our previously defined and cleaned, `cleaned_df` DataFrame:\n",
    "\n",
    "```python\n",
    "# Create a second example DataFrame\n",
    "additional_data = [\n",
    "    [\"john.doe@example.com\", \"Engineer\"],\n",
    "    [\"alice.smith@example.com\", \"Designer\"],\n",
    "    [\"bob.jones@example.com\", \"Analyst\"],\n",
    "    [\"eve.white@example.com\", \"Manager\"]\n",
    "]\n",
    "additional_columns = [\"Email\", \"Occupation\"]\n",
    "\n",
    "additional_data_df = spark.createDataFrame(additional_data, additional_columns)\n",
    "\n",
    "# Show the additional DataFrame\n",
    "print(\"Additional DataFrame:\")\n",
    "additional_data_df.show()\n",
    "```\n",
    "\n",
    "### 1. Using Joins to Combine DataFrames\n",
    "\n",
    "Joining DataFrames is a common operation to combine data from different sources. The `join` method is used, specifying the common column and the type of join (e.g., inner, outer, left, right).\n",
    "\n",
    "The general syntax is:\n",
    "\n",
    "```python\n",
    "# Joining two DataFrames based on a common column\n",
    "joined_df = df1.join(df2, df1[\"CommonColumn\"] == df2[\"CommonColumn\"], how=\"join_type\")\n",
    "```\n",
    "The `join` method is used to combine two DataFrames, `df1` and `df2`, based on a common column, in this case, the column named `\"CommonColumn\"`. The condition specified by `df1[\"CommonColumn\"] == df2[\"CommonColumn\"] `ensures that rows are aligned where the values in the common column match between the two DataFrames.\n",
    "\n",
    "The `how` parameter determines the type of join to perform, and it takes values such as `\"inner\"`, `\"outer\"`, `\"left\"`, and `\"right\"`. These join types correspond to standard SQL `JOIN` operations.\n",
    "\n",
    "Let's join our two DataFrames, `cleaned_df` and `additional_data_df` based on the `Email` column:\n",
    "\n",
    "```python\n",
    "# Joining the two DataFrames\n",
    "combined_df = cleaned_df.join(additional_data_df, additional_data_df[\"Email\"] == cleaned_df[\"Email\"], how=\"inner\")\n",
    "```\n",
    "\n",
    "In this example, we've joined `cleaned_df` and `additional_data_df` based on the `Email` column, resulting in a new DataFrame `combined_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Grouping and Aggregating Data\n",
    "\n",
    "Grouping and aggregating data allows us to summarize information based on certain columns. The `groupBy` method is used to define the grouping column, and then *aggregation functions* are applied to obtain summary statistics. Essentially, the aggregation functions will operate on groups of rows defined by the `groupBy` method.\n",
    "\n",
    "> **Aggregation functions** are mathematical operations that perform calculations on a set of values and return a single value as a result. In the context of data manipulation, these functions summarize or transform data within a group. \n",
    "\n",
    "There are various types of aggregation functions, categorized based on the type of summary they provide:\n",
    "\n",
    "- **Counting Functions**:\n",
    "  - `count`: Counts the number of elements in a group <br><br>\n",
    "\n",
    "- **Statistical Functions**:\n",
    "  - `sum`: Calculates the sum of values in a group\n",
    "  - `avg`: Computes the average of values in a group\n",
    "  - `min`: Identifies the minimum value in a group\n",
    "  - `max`: Identifies the maximum value in a group\n",
    "\n",
    "The general syntax here is:\n",
    "\n",
    "```python\n",
    "# Grouping by a column and applying an aggregation function\n",
    "grouped_df = df.groupBy(\"GroupingColumn\").agg({\"AggregatedColumn\": \"aggregation_function\"})\n",
    "```\n",
    "\n",
    "In this syntax:\n",
    "\n",
    "- `\"GroupingColumn\"` is the column by which the DataFrame is grouped\n",
    "- `\"AggregatedColumn\"` is the column for which the aggregation function is applied\n",
    "- `\"aggregation_function\"` is the specific aggregation function to be applied\n",
    "\n",
    "Let's group `combined_df` by the `Gender` column and calculate the average salary for each gender: \n",
    "\n",
    "```python\n",
    "# Grouping and aggregating by gender\n",
    "grouped_df = combined_df.groupBy(\"Gender\").agg({\"Salary\": \"avg\"})\n",
    "```\n",
    "Alternatively, this can also be written in the following way:\n",
    "\n",
    "```python\n",
    "grouped_df = combined_df.groupBy(\"Gender\").agg(avg(\"Salary\"))\n",
    "```\n",
    "\n",
    "Notice the output of this command:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AggregationOutput.png\" width=\"700\" height=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using Aliases\n",
    "\n",
    "Applying aliases to columns enhances clarity, especially when dealing with aggregated or calculated columns. Notice in the above aggregation example that the aggregate column receives quite a random name `avg(Salary)`. In cases like this we might want to replace that with a better formatted or a more descriptive name.\n",
    "\n",
    "The general syntax for aliases is:\n",
    "\n",
    "```python\n",
    "aliased_df = df.select(col(\"OriginalColumn\").alias(\"NewColumnName\"))\n",
    "```\n",
    "Let's calculate the average salary again and apply an alias to the resulting column.\n",
    "\n",
    "```python\n",
    "# Grouping by Gender and calculating average salary with alias\n",
    "grouped_df = df.groupBy(\"Gender\").agg(avg(\"Salary\").alias(\"AverageSalary\"))\n",
    "```\n",
    "\n",
    "Notice the difference in output to the previous command:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Alias.png\" width=\"700\" height=\"250\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "### 4. Sorting Data\n",
    "\n",
    "Sorting data is crucial for better visualization and analysis. The `orderBy` method is used to sort the DataFrame based on one or more columns. The general syntax is:\n",
    "\n",
    "```python\n",
    "# Sorting data based on one or more columns\n",
    "sorted_df = df.orderBy(\"Column1\", ascending=False)\n",
    "```\n",
    "The `ascending` parameter in the `orderBy` method determines the sorting order for the specified columns. It is a Boolean parameter, where `True` represents ascending order (default) and `False` represents descending order.\n",
    "\n",
    "Let's sort `joined_df` by the `Salary` columns in descending order:\n",
    "\n",
    "```python\n",
    "sorted_df = combined_df.orderBy(\"Salary\", ascending=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Utilizing Window Functions\n",
    "\n",
    "> *Window functions* are used for advanced operations that involve calculations over a specified range or \"window\" of rows. Unlike standard aggregation functions, which collapse multiple rows into a single result, window functions perform calculations across a set of related rows defined by a *window specification*.\n",
    "\n",
    "The **window specification** defines the criteria for partitioning and ordering the data before applying the window function. It is created using the `Window` class, specifying partitioning and ordering columns. The window specification is essential for defining the context in which the window function operates.\n",
    "\n",
    "The general syntax of a window function is:\n",
    "\n",
    "```python\n",
    "# Creating a Window specification\n",
    "window_spec = Window.partitionBy(\"PartitionColumn\").orderBy(\"OrderColumn\")\n",
    "\n",
    "# Applying window functions\n",
    "result_df = df.withColumn(\"NewColumn\", function.over(window_spec))\n",
    "\n",
    "```\n",
    "\n",
    "In the above syntax:\n",
    "\n",
    "- **Creating a Window Specification**:\n",
    "\n",
    "  - `Window.partitionBy(\"PartitionColumn\")`: This part of the syntax defines the partitioning column(s) based on which the data will be divided into distinct partitions. The window function will operate independently within each partition.\n",
    "  - `orderBy(\"OrderColumn\")`: This part specifies the ordering of rows within each partition. It defines the sequence in which the window function processes rows. <br><br>\n",
    "\n",
    "- **Applying Window Functions**:\n",
    "\n",
    "  - `df.withColumn(\"NewColumn\", function.over(window_spec))`: Here, the `withColumn` method is used to add a new column, and the `function.over(window_spec)` applies the window function within the specified window specification. The result is stored in the `NewColumn`.\n",
    "\n",
    "Let's consider an example where we want to calculate the average salary for each gender, considering the rows in the same gender partition and ordered by age within each partition:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Creating a Window specification\n",
    "window_spec = Window.partitionBy(\"Gender\").orderBy(\"Age\")\n",
    "\n",
    "# Applying window function to calculate average salary within each gender partition\n",
    "result_df = cleaned_df.withColumn(\"AvgSalaryByGender\", avg(\"Salary\").over(window_spec))\n",
    "\n",
    "# Selecting only the relevant columns\n",
    "result_df = result_df.select(\"Gender\", \"Age\", \"AvgSalaryByGender\").distinct()\n",
    "\n",
    "display(result_df)\n",
    "\n",
    "```\n",
    "\n",
    "Let's break down the example above:\n",
    "\n",
    "- **Creating a Window Specification**:\n",
    "\n",
    "  - `Window.partitionBy(\"Gender\")`: This part of the specification defines the partitioning of the data. It means that the window function will be applied separately for each distinct value in the `Gender` column.\n",
    "  - `.orderBy(\"Age\")`: This specifies the ordering of rows within each partition based on the `Age` column. It influences the window frame over which the window function operates.\n",
    "\n",
    "- **Applying Window Function to Calculate Average Salary**:\n",
    "\n",
    "  - `avg(\"Salary\").over(window_spec)`: This applies the window function to calculate the average salary over the window specified by `window_spec`. It creates a new column called `AvgSalaryByGender` in the DataFrame.\n",
    "\n",
    "- **Selecting Only Relevant Columns**:\n",
    "\n",
    "  - Here we select only the columns `Gender`, `Age`, and `AvgSalaryByGender` from the DataFrame. Including additional columns in the final result, such as `First Name`, `Last Name` or other non-aggregated columns, might lead to inaccurate or nonsensical values. The `distinct()` method ensures that you get unique combinations of `Gender` and `Age`.\n",
    "\n",
    "The output should look like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/WindowFunction3.png\" width=\"700\" height=\"350\"/>\n",
    "</p>\n",
    "\n",
    "In output DataFrame (`result_df`) for each distinct value in the `Gender` column, the `Age` will be ordered, and the `AvgSalaryByGender` column will represent the calculated average salary within each gender and age partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data To Tables\n",
    "\n",
    "Persisting DataFrames as tables is a critical step in data processing, and PySpark provides several write methods to facilitate this operation. The `write` method allows you to save a DataFrame to a table, specifying the storage formation, compression options, and additional properties.\n",
    "\n",
    "```python\n",
    "# Writing a DataFrame to a table in Parquet format\n",
    "df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"my_table\")\n",
    "```\n",
    "\n",
    "Let's break the syntax down:\n",
    "\n",
    "- `format (\"parquet\")`:\n",
    "\n",
    "  - Specifies the storage format for the table. In this example, it's set to `parquet`, a columnar storage format that is efficient for analytics and provides good compression.\n",
    "  - Other notable formats include `orc` (Optimized Row Columnar) and `csv` (Comma-Separated Values), among others. Each format has its advantages and use cases, and you can choose the one that best fits your requirements. <br><br>\n",
    "\n",
    "- `Mode (\"overwrite\")`:\n",
    "\n",
    "  - Defines the behavior when saving the DataFrame to an existing table. In this case, `overwrite` means that if the table already exists, its contents will be replaced with the new DataFrame.\n",
    "  - Other modes include `append` (add data to the existing table), `ignore` (do nothing if the table already exists), and `error` (throw an error if the table already exists). <br><br>\n",
    "\n",
    "- `saveAsTable(\"my_table\")`:\n",
    "\n",
    "  - Indicates the name of the table to which the DataFrame is being saved to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Apache Spark is a distributed computing system that provides high-level APIs for distributed data processing. Apache Spark's architecture includes a **Cluster Manager**, **Spark Application**, **Spark Executors**, and **Spark Driver**:\n",
    "  - Databricks abstracts users from direct interaction with the **Cluster Manager**, managing resources and tasks\n",
    "  - **Spark Executors** process data close to where it's stored, enhancing performance\n",
    "  - The **Spark Driver** oversees the overall execution of a Spark job and communicates with the **Cluster Manager** <br><br>\n",
    "\n",
    "- Spark SQL is designed for structured data processing and allows SQL queries on Spark data. PySpark is the Python API for Spark, offering a programmatic interface for distributed data processing. \n",
    "\n",
    "- A **DataFrame** is a distributed collection of data organized into named columns, akin to a relational database table. DataFrames are distributed, immutable, and follow lazy evaluation in Spark.\n",
    "\n",
    "- Spark DataFrames support various data sources and can be created from existing RDDs, external databases, or various file formats\n",
    "\n",
    "- PySpark allows reading data from various sources, including `JSON` and `CSV` files. The `spark.read` module provides methods like `json()`, `csv()`, etc., to load data into DataFrames.\n",
    "\n",
    "- Data transformations are fundamental operations to clean, preprocess, and reshape data in PySpark:\n",
    "\n",
    "  - Techniques include replacing missing values, using `regexp_replace` for pattern-based string replacements, and casting data types with `cast`\n",
    "  - `withColumn` and `select` are essential for renaming, dropping, and creating new columns\n",
    "  - Array functions, such as `array`, `array_contains`, `size`, and `concat`, facilitate working with columns that contain arrays of values. They are useful for creating, checking, and manipulating arrays, including concatenating multiple arrays or values. <br><br>\n",
    "\n",
    "- Joins are employed to combine data from different DataFrames based on common column\n",
    "\n",
    "- Grouping and aggregating data involve the `groupBy` method and aggregation functions like `avg`, providing summarized insights\n",
    "\n",
    "- Window functions, defined by a `Window` specification, allow advanced operations over a specified range or \"window\" of rows\n",
    "\n",
    "- Persisting DataFrames as tables is crucial for data processing, and PySpark provides the `write` method for this purpose"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
