{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the Apache Spark architecture, what is the role of Spark Executors?\n",
    "\n",
    "- Manage resource allocation\n",
    "- Execute Spark tasks on worker nodes ***\n",
    "- Oversee the Spark Driver\n",
    "- Define the Spark Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True or False: Databricks users directly interact with the Cluster Manager when creating and configuring clusters.\n",
    "\n",
    "- True\n",
    "- False *** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following is a key benefit of using window functions in Spark?\n",
    "\n",
    "- Grouping data by a specific column\n",
    "- Performing advanced operations over a specified range of rows ***\n",
    "- Sorting data in descending order\n",
    "- Joining two DataFrames based on a common column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the primary role of the `groupBy` method in Spark data manipulation?\n",
    "\n",
    "- Sorting data based on a specific column\n",
    "- Aggregating data based on common values ***\n",
    "- Joining two DataFrames\n",
    "- Filtering out rows with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True or False: The `join` method in Spark is used to combine DataFrames based on a common column, and the `how` parameter specifies the type of join.\n",
    "\n",
    "- True ***\n",
    "- False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True or False: In Spark, the `agg` method is used to define the grouping column and apply aggregation functions.\n",
    "\n",
    "- True\n",
    "- False ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `partitionBy` clause in a window specification define?\n",
    "\n",
    "- The number of partitions in the DataFrame\n",
    "- The criteria for partitioning and ordering the data before applying the window function ***\n",
    "- The columns to be included in the final result\n",
    "- The type of join to perform in the window operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of the `orderBy` clause in a window specification?\n",
    "\n",
    "- Sorting the entire DataFrame\n",
    "- Defining the grouping column for the window function\n",
    "- Specifying the sorting order of rows within each partition ***\n",
    "- Filtering out rows with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of applying aliases to columns in Spark?\n",
    "\n",
    "- Enhancing data quality\n",
    "- Improving data consistency\n",
    "- Facilitating column renaming in DataFrames\n",
    "- Providing clarity, especially for aggregated or calculated columns ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following is true about the `mode` parameter in the `write` method when saving a DataFrame to a table?\n",
    "\n",
    "- It specifies the storage format for the table\n",
    "- It determines the type of join to perform\n",
    "- It defines the behavior when saving to an existing table ***\n",
    "- It controls the level of parallelism during the write operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True or False: The Spark Driver is responsible for managing the overall execution of a Spark job and collecting final results.\n",
    "\n",
    "- True ***\n",
    "- False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True or False: DataFrames in Spark are mutable, allowing their structure to be changed after creation.\n",
    "\n",
    "- True \n",
    "- False ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In PySpark, what is the purpose of the `cast` function during data transformation?\n",
    "\n",
    "- To rename a column\n",
    "- To filter rows based on a condition\n",
    "- To convert a column to a different data type ***\n",
    "- To aggregate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When reading a `JSON` file into a PySpark DataFrame, what does the `inferSchema` option do?\n",
    "\n",
    "- Automatically detects the data types of each column ***\n",
    "- Requires explicit specification of the schema\n",
    "- Ignores the schema and reads all columns as strings\n",
    "- Excludes the schema from the resulting DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In PySpark, what is the primary purpose of the `regexp_replace` function during data transformations?\n",
    "\n",
    "- To perform mathematical operations on numeric columns\n",
    "- To filter rows based on a regular expression\n",
    "- To replace substrings in a column based on a regular expression ***\n",
    "- To concatenate multiple columns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
