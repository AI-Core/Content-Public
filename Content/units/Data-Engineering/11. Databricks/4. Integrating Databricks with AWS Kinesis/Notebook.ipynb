{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Databricks with AWS Kinesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lesson you will learn how to stream data from **Kinesis** to **Databricks** using `pyspark`.\n",
    "\n",
    "To be able to follow along you will need to have the following resources setup:\n",
    "- An AWS account\n",
    "- A Databricks account with an **AWS Access Key** and an **AWS Secret Access** key for it\n",
    "- One/multiple Kinesis Data Streams \n",
    "- A preferred method to ingest data into Kinesis Data Stream (such as sending data to an API with a Kinesis proxy integration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read streaming data from Kinesis\n",
    "\n",
    "Using you preferred method start injesting data into Kinesis Data Stream. Once you see the data arriving in the Kinesis data streams, you are ready to read it into Databricks.\n",
    "\n",
    "You will first need to read the csv file containing your AWS **Access Key** and **Secret Access Key**. To do this, you can run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the `ACCESS_KEY` and `SECRET_KEY` from the spark dataframe created above. The secret access key will be encoded using `urllib.parse.quote` for security purposes. `safe=\"\"` means that every character will be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the `ACCESS_KEY` and `SECRET_KEY` we can read the streaming data from Kinesis using the format below (make sure you are sending data to your stream before running the code below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','<KINESIS_STREAM_NAME>') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the streaming data by applying the `display` method on the dataframe (`display(df)`). The data will arrive in the default schema of Kinesis which is shown below:\n",
    "- `partitionKey`\n",
    "- `data`\n",
    "- `stream`\n",
    "- `shardId`\n",
    "- `sequenceNumber`\n",
    "- `approximateArrivalTimestamp`\n",
    "\n",
    "> The display query will run continuously, with the output being updated every few seconds. The rows number is displayed at the bottom of the query output `Showing all <number> rows`. You should see this number increasing as more and more data is being send to your stream. This command will run indefinitely. To stop this from running you need to press the **Interrupt** button at the top of the Databricks Notebook console."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the data contained in your stream, you can explicitly deserialize the `data` column of the dataframe by running the following command:\n",
    "\n",
    "`df = df.selectExpr(\"CAST(data as STRING)\")`\n",
    "\n",
    "If you run `display(df)` again, you should see the data in your stream being displayed to the console."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing streaming data to Databricks\n",
    "\n",
    "After performing any necessary transformations to your streaming data, you are ready to store the transformed streams in Databricks. One way to do this is by writing the streams to Databricks Delta tables, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"<TABLE_NAME>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") ` allows you to recover the previous state of a query in case of failure. Before running the `writeStream` function again, you will need to delete the checkpoint folder using the following command:\n",
    "\n",
    "`dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)`\n",
    "\n",
    "Again, just like the `readStream` query, the `writeStream` query will run indefinitely until interrupting it. To check the data was saved as expected, we will access the **Data** section in the Databricks menu. You should be able to see the created Delta table under **Catalogs** -> **Databases** -> **Tables**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Delta Table.png\" width=\"1000\" height=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the table you have just created, you should be able to see its **Schema** and the **Sample Data** that has been stored inside it.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Example Table.png\" width=\"900\" height=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of:\n",
    "- How to read data from Kinesis Data Streams in Databricks\n",
    "- How save streams in Delta Tables in Databricks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2 (default, Mar  3 2021, 11:58:52) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d25a140d753dbbce5959f3e993c340d725ba39c38e4259359e51030082d2593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
