{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Databricks with AWS Kinesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lesson you will learn how to stream data from **Kinesis** to **Databricks** using `pyspark`.\n",
    "\n",
    "To be able to follow along you will need to have the following resources setup:\n",
    "- An AWS account\n",
    "- A Databricks account with an **AWS Access Key** and an **AWS Secret Access** key for it\n",
    "- One/multiple Kinesis Data Streams \n",
    "- A preferred method to ingest data into Kinesis Data Stream (such as sending data to an API with a Kinesis proxy integration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read streaming data from Kinesis\n",
    "\n",
    "Using you preferred method start ingesting data into Kinesis Data Stream. Once you see the data arriving in the Kinesis data streams, you are ready to read it into Databricks.\n",
    "\n",
    "You will first need to read the table file containing your AWS **Access Key** and **Secret Access Key**. To do this, you can run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the `ACCESS_KEY` and `SECRET_KEY` from the spark dataframe created above. The secret access key will be encoded using `urllib.parse.quote` for security purposes. `safe=\"\"` means that every character will be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the `ACCESS_KEY` and `SECRET_KEY` we can read the streaming data from Kinesis using the format below (make sure you are sending data to your stream before running the code cells below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "# Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','<KINESIS_STREAM_NAME>') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the streaming data by applying the `display` method on the dataframe (`display(df)`). The data will arrive in the default schema of Kinesis which is shown below:\n",
    "- `partitionKey`\n",
    "- `data`\n",
    "- `stream`\n",
    "- `shardId`\n",
    "- `sequenceNumber`\n",
    "- `approximateArrivalTimestamp`\n",
    "\n",
    "The display query will run continuously, with the output being updated every few seconds. The rows number is displayed at the bottom of the query output `Showing all <number> rows`. You should see this number increasing as more and more data is being send to your stream. This command will run indefinitely. To stop this from running you need to press the **Interrupt** button at the top of the Databricks Notebook console.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/pre_partition.png\" width=\"1000\" height=\"500\"/>\n",
    "</p>\n",
    "\n",
    "As the stream recieves all incoming data from all requests, we can utilise the `partitionKey` to filter the stream & only read in the records from our script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.partitionKey == \"desired-partition-key\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data must be sent in serialised byte format when sent by an API request, this is represented by the encoded values in the `data` column.\n",
    "\n",
    "To see the data contained in your stream, you can explicitly deserialise the `data` column of the dataframe and alias it by running the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"CAST(data as STRING) jsonData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run `display(df)` again, you should see the data in your stream being displayed to the console. Here we can note that all the data is combined under one `jsonData` column.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/post_partition.png\" width=\"1000\" height=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Here unlike the `readStream` operation in the Spark Streaming notebook, each row contains one JSON string.\n",
    "\n",
    "\n",
    "\n",
    "We can utilise PySpark's `from_json` method to parse the column containing a JSON string into a `MapType` utilising a `StructType` as the mapping. \n",
    "\n",
    "If you need a refresher on streaming with PySpark the notebook can be found here: [Spark Streaming notebook](https://colab.research.google.com/github/AI-Core/Content-Public/blob/main/Content/units/Data-Engineering/11.%20Databricks/5.%20Spark%20Streaming%20/Notebook.ipynb)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/pre_alias_dataframe.png\" width=\"1000\" height=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By aliasing this column to `data` and selecting all columns from the MapType `select(\"data.*\")` we can view each json as column seperated values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(from_json(\"jsonData\", struct).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"images/schema_applied_final.png\" width=\"1000\" height=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing streaming data to Databricks\n",
    "\n",
    "After performing any necessary transformations to your streaming data, you are ready to store the transformed streams in Databricks. One way to do this is by writing the streams to Databricks Delta tables, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"<TABLE_NAME>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") ` allows you to recover the previous state of a query in case of failure. Before running the `writeStream` function again, you will need to delete the checkpoint folder using the following command:\n",
    "\n",
    "`dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)`\n",
    "\n",
    "Again, just like the `readStream` query, the `writeStream` query will run indefinitely until interrupting it. To check the data was saved as expected, we will access the **Data** section in the Databricks menu. You should be able to see the created Delta table under **Catalogs** -> **Databases** -> **Tables**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Delta Table.png\" width=\"1000\" height=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the table you have just created, you should be able to see its **Schema** and the **Sample Data** that has been stored inside it.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Example Table.png\" width=\"900\" height=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of:\n",
    "- How to read data from Kinesis Data Streams in Databricks\n",
    "- How save streams in Delta Tables in Databricks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2 (default, Mar  3 2021, 11:58:52) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d25a140d753dbbce5959f3e993c340d725ba39c38e4259359e51030082d2593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
