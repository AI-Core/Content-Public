{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating Databricks workloads on AWS MWAA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Job orchestration is a fully integrated feature of Databricks. Customers can use the *Jobs API* or UI to create and manage jobs and features such as monitoring. Databricks orchestration can support both jobs with single or multiple tasks.\n",
    "\n",
    "In this notebook we will use this powerful Job API together with AWS MWAA to monitor a DAG (Directed Acyclic Graph) with Databricks-based tasks. To do so, we will create a simple DAG that connects to a Databricks Cluster and executes a notebook. MWAA will in turn monitor this execution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an API token in Databricks\n",
    "\n",
    "Make sure you have access to a Databricks workspace and that a Databricks cluster that has been configured inside this workspace.\n",
    "\n",
    "In your Databricks account, select your username from top right and then **User Settings**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Databricks User Settings.png\" width=\"700\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "From the User Settings panel select Access tokens and then **Generate new token**.Here, you can add a description to the token and select its lifetime (after how many days it will expire). Once you have created a token, the following window will popup:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Databricks Token.png\" width=\"700\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "Copy the **Token ID** now and make a note of it. Once you press **Done** you won't be able to see its value again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the MWAA to Databricks connection\n",
    "\n",
    "Open the Airflow UI from the MWAA environment. Navigate to **Admin** and then select **Connections**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Airflow UI Connections.png\" width=\"700\" height=\"250\"/>\n",
    "</p>\n",
    "\n",
    "From the list of possible connection select **databricks_default** and click on **Edit record**. \n",
    "- In the **Host** column copy and paste the url of your Databricks account. You can find this by simply navigating your account and copying the URL in the top bar.\n",
    "- In the **Extra** column you should add the following dictionary:\n",
    "\n",
    "`{\"token\": \"<token_from_previous_step>\", \"host\": \"<url_from_host_column>\"}`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the **Connection Type** column you should select **Databricks** from the drop-down menu. If the connection type is missing, you will need to install additional packages using the *Airflow Provider Package*. Follow the **Create & upload requirements.txt file** section below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & upload requirements.txt file\n",
    "\n",
    "To obtain the Databricks connection type we will need to install the corresponding Python dependency in our MWAA environment, by uploading a `requirements.txt` file in the MWAA-designated S3 bucket.\n",
    "\n",
    "To test that we are creating our requirements.txt file correctly, before uploading it to our MWAA environment, we will use a command line interface (CLI) utility that replicates a MWAA environment locally. The CLI will build a Docker image locally, which will allow you to run a local Airflow environment to develop/test DAGs, custom plugins and dependencies before deploying them to the cloud.\n",
    "\n",
    "We will use the following Github repository for that: https://github.com/aws/aws-mwaa-local-runner. \n",
    "\n",
    "1. First we will clone the repository:\n",
    "\n",
    "`git clone https://github.com/aws/aws-mwaa-local-runner.git`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cd aws-mwaa-local-runner`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then we will build the Docker image using the following command (this will take a while):\n",
    "\n",
    "`./mwaa-local-env build-image`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Once the image has been build, we can now run a local Airflow environment that will be a close representation of the MWAA environment:\n",
    "\n",
    "`./mwaa-local-env start`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a while to set up, but once everything is ready to go, you should see the following message in your terminal:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Container Started.png\" width=\"400\" height=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now you're ready to open the Apache Airflow UI at http://localhost:8080/. By default the username will be set to **admin** and the password to **test**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Local UI.png\" width=\"700\" height=\"350\"/>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. To add the desired **Python dependencies** you will need to navigate to `aws-mwaa-local-runner/requirements/`. Inside this folder is where you will create your `requirements.txt file`.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Requirements Folder.png\" width=\"600\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "Inside `requirements.txt` you should add the following line to install the package needed for the **Databricks connection type**:\n",
    "\n",
    "`apache-airflow[databricks]`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. To test the `requirements.txt` without running Airflow, use the following command:\n",
    "\n",
    "`./mwaa-local-env test-requirements`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything ran successfully, you should see the following output:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Requirements Installed.png\" width=\"400\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "Now you are ready to upload this requirements.txt file to your MWAA environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the requirements.txt file to the S3 bucket\n",
    "\n",
    "Once you have uploaded the file to your MWAA S3 bucket, the bucket should look like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/S3 bucket.png\" width=\"600\" height=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the file path in Requirements file field\n",
    "\n",
    "Navigate to the MWAA console and select your **Environment**. Once you're on the environment page select **Edit**.\n",
    "\n",
    "Under the **DAG code in Amazon S3**, update your **Requirements file** field by selecting the path corresponding to the requirements.txt file you have just uploaded to the S3 bucket."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to put everything together. Navigate back to the Airflow UI in your environment and let's go back to our Connections.\n",
    "\n",
    "Click Edit Connection on the `databricks_default`, and now you should be able to find **Databricks** in the Connection Type column. The final connection should look like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Databricks Connection.png\" width=\"700\" height=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Airflow DAG\n",
    "\n",
    "Below you have an example DAG file that will run a Databricks Notebook on a specific schedule. Once you have created your DAG, you should upload it to the MWAA S3 bucket in the `dags` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator, DatabricksRunNowOperator\n",
    "from datetime import datetime, timedelta \n",
    "\n",
    "\n",
    "#Define params for Submit Run Operator\n",
    "notebook_task = {\n",
    "    'notebook_path': '<DATABRICKS_NOTEBOOK_PATH',\n",
    "}\n",
    "\n",
    "\n",
    "#Define params for Run Now Operator\n",
    "notebook_params = {\n",
    "    \"Variable\":5\n",
    "}\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': '<OWNER_NAME>',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': <NUMBER_DESIRED_RETRIES>,\n",
    "    'retry_delay': timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "\n",
    "with DAG('databricks_dag',\n",
    "    # should be a datetime format\n",
    "    start_date=<DESIRED_START_DATE>,\n",
    "    # check out possible intervals, should be a string\n",
    "    schedule_interval='<DESIRED_INTERVAL>',\n",
    "    catchup=False,\n",
    "    default_args=default_args\n",
    "    ) as dag:\n",
    "\n",
    "\n",
    "    opr_submit_run = DatabricksSubmitRunOperator(\n",
    "        task_id='submit_run',\n",
    "        # the connection we set-up previously\n",
    "        databricks_conn_id='databricks_default',\n",
    "        existing_cluster_id='<CLUSTER_ID>',\n",
    "        notebook_task=notebook_task\n",
    "    )\n",
    "    opr_submit_run\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once uploaded to the `dags` folder, you will be able to see the new DAG in the Airflow UI on your MWAA environment, under paused DAGs. In order to manually trigger the DAG, you will first have to unpause it.\n",
    "\n",
    "The DAG might fail if you're notebook contains the commands for mounting the S3 bucket, as this is something that should only be done once. To avoid this, either comment out those commands, or make sure to include the commands to un-mount the S3 bucket in a cell at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of:\n",
    "- How to create an API token in Databricks\n",
    "- How to create a MWAA to Databricks Connection\n",
    "- How to create a DAG that runs a Databricks Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2 (default, Mar  3 2021, 11:58:52) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d25a140d753dbbce5959f3e993c340d725ba39c38e4259359e51030082d2593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
