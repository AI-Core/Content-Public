{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    ">At this point, you have trained a model and utilised it to make predictions. Therefore, you are probably aware of the complications associated with retraining a model with an updated version of the data. This problem is exacerbated when working in a CI/CD environment, where data may be constantly ingested and cleaned.\n",
    "\n",
    "Consider a situation, where a model is trained daily with a new version of data. Thereafter, the model is utilised to make predictions daily, and the predictions are monitored to trigger the retraining of the model. Thus, the model is trained with a different version of the data than that which is used to make predictions.\n",
    "\n",
    "However, if, for whatever reason, you need to revert to a previous version of the model, you must know what version of the data was utilised in training that model and, in turn, making the corresponding prediction. This is where data-version tracking comes in.\n",
    "\n",
    "## Data-Version Tracking in a CI/CD Environment\n",
    "\n",
    "Consider data versioning as the Git (i.e. the version-control system (VCS)) for data. In fact, in many tools, you will use Git along with data versioning.\n",
    "\n",
    "![Data Versioning](images/Data_versioning.png)\n",
    "\n",
    "### Challenges encountered\n",
    "\n",
    "There are a few challenges associated with data versioning.\n",
    "\n",
    "1. A version number must be added to the data. \n",
    "2. When working in teams, team members should have access to the activity history (i.e. who did what to the data), similar to the case when using a VCS, such as [Git](https://git-scm.com/).\n",
    "2. Datasets are usually quite large; consequently, GitHub might be unable to handle them.\n",
    "\n",
    "Therefore, a solution that can keep track of data versions and handle large datasets is needed, i.e. data-versioning tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Versioning Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tools track data versions and manage the storage of the data. Although there are many data-versioning tools, only a few of them will be introduced in this lesson. In particular, Data Version Control (DVC), a popular data-versioning tool, will be explored in depth in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Delta Lake](https://delta.io) is a data-versioning tool that exploits the lakehouse architecture to store and track data. The lakehouse architecture enables the combination of data-warehousing/lake-storage functionalities and supports data analytics.\n",
    "\n",
    "Currently, many data-science projects rely on data lakes that contain raw and cleaned data. An external ETL process is employed to load the processed data to a warehouse and back into the lake. Conversely, lakehouse architectures integrate these ETL processes into the data lake itself.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/Lakehouse.png\" width=600/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on lakehouse architectures, check [here](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf).\n",
    "\n",
    "As shown in the image, metadata is also part of the information contained in the lake, and Delta Lake treats this information as regular data.\n",
    "\n",
    "> Note that Delta Lake treats metadata like regular data, even when working with 'big data'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, Delta Lake can store data versions of your data, and it uses a feature called `Time Travel` to access and revert to earlier versions for audits, rollbacks or experiments. More information about `Time Travel` can be found [here](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitation\n",
    "Delta Lake is a good tool for managing data versioning, and it allows you to focus on the data. However, it was built for Spark and big data; therefore, it might be overly powerful for your needs. Moreover, it uses a dedicated data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neptune](https://neptune.ai) is a metadata store that is intended to be used for running many experiments and keeping track of the metrics. It is composed of three major elements:\n",
    "\n",
    "1. Data versioning\n",
    "2. Experiment tracking\n",
    "3. Model registry\n",
    "\n",
    "They work in conjunction with many other parts of the [MLOps](https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective) ecosystem.\n",
    "\n",
    "![neptune](images/Neptune.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage\n",
    "It is incredibly easy to set up an account and use Neptune. Further, as an added merit, it provides a very user-friendly interface.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/Neptune_UI.png\" width=600/></div>\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/Neptune_UI2.png\" width=600/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitation\n",
    "The main downside (if it can be called that) of Neptune is that it is not focused on data versioning. It is more focused on experiment tracking and model registry. This is not necessarily bad, particularly if you are not working with extremely large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git Large File System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Git Large File System (LFS)](https://www.atlassian.com/git/tutorials/git-lfs) is a Git extension that allows you to store large files in Git repositories. It is utilised to store data-versioning files, as well as model files. It works by applying `pointers` to large files, enabling you keep track of the `deltas` in your repository for each data version.\n",
    "\n",
    "Git LFS is very easy to implement in a data-science project, and it works essentially the same way as a git repository. \n",
    "\n",
    "#### Limitation\n",
    "The main downside of Git LFS is that it is not designed to be used in production, and it does not include any integration tool (unless you are using a custom integration tool). Additionally, it does not offer the analytics capabilities that Delta Lake does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pachyderm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pachyderm](https://www.pachyderm.com) is a platform that integrates a variety of data-science tools and services, including data versioning, experiment tracking, model registry, and data lakes. The platform makes it easy to reproduce the results of ML models by managing the entire data workflow.\n",
    "\n",
    "Pachyderm leverages Docker containers to help you set the execution in a consistent environment across systems, which makes it easy to reproduce the same output. Thus, your team can use any language or library, without any infrastructure overhead. Furthermore, the platform offers many integrated solutions that are very well documented.\n",
    "\n",
    "#### Limitation\n",
    "Despite the good documentation, the learning curve is quite steep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DVC\n",
    "\n",
    "DVC allows you to keep track of the datasets used in different versions of code. It does this by creating small files that reference the datasets and stores these files in GitHub repos.\n",
    "\n",
    "In the next lesson, we will explore DVC in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of \n",
    "\n",
    "- data versioning.\n",
    "- data versioning tracking in a CI/CD environment.\n",
    "- data-versioning tools."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
