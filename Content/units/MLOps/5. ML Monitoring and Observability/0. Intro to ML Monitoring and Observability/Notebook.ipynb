{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Observability and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    " \n",
    ">ML systems are fundamentally different from traditional software systems because they aim to continuously optimise some metrics. However, the best approach for achieving this optimisation can vary over time. For example, music-recommendation systems may be designed to maximise the user's listening time; however, as music trends and the user's taste change over time, the solution becomes invalid.\n",
    "\n",
    "Therefore, it is important to have an adequate understanding of a model's performance and general status at all stages of its lifecycle.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "ML models are frequently retrained using data from the real world, and the distribution of that data can change over time. This gives data scientists the additional concern that the model's predictions degrade when deployed into production. There is a good chance that when your model is deployed into production, it will not perform as well as it did on your training data. This is where monitoring and observability take the cake. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"./images/model_deployment_fail.png\"\n",
    "     alt=\"Example of Data Drift\"\n",
    "     style=\"float: centre; margin-right: 10px;\" \n",
    "     width=\"600\"\n",
    "     height=\"225\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring vs Observability\n",
    " \n",
    "Traditional model monitoring is concerned with creating alerts on key-model performance metrics, which could be altered if the model data start to drift or metrics, such as accuracy, begin to drop. \n",
    "\n",
    "> The key difference between monitoring and observability is that while monitoring alerts you to the problem, it does not provide insights into the root cause. Conversely, observability helps you identify the root cause. \n",
    "\n",
    "Using ML Observability, we can gain a profound understanding of our model's performance throughout all stages of its development lifecycle. This will enable us to pinpoint the root cause of any issue at the correct stage of the life cycle. Observability is concerned with collecting model evaluations during training, validation, and production and applying analytics to these evaluations to gain insights into the reason for the degradation of models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasons for the Degradation of ML Performance with Time\n",
    "\n",
    "### Changes in measure or transformations\n",
    "\n",
    "Feature transformations in code may be inconsistent between the trained model and the production model. This can be a common problem because of the lack of feature-version control. As a result, the knowledge of the exact transformations used between the training model and the production model becomes inadequate or unreliable. If these transformations are inconsistent between the models, the model may perform poorly during deployment.\n",
    "\n",
    "The measure of data can change when the data pipeline upstream that feeds the model changes. This can impact the model's performance. For instance, when working with a model that accepts image data from cameras, if the cameras are updated to a model with a different resolution, this change in the resolution measure can degrade the model's performance.\n",
    "\n",
    "### Model drift\n",
    " \n",
    "Since ML models in production accept data from the real world, model drift can occur due to the change in the data distribution over time. Model drift occurs when the prediction accuracy of the deployed model begins to 'drift' from that achieved during the training stage, due to new input data. This can result in the degradation of the model's predictive capabilities, leading to poor performance.\n",
    "\n",
    "#### Types of model drift\n",
    "\n",
    "Model drift can be classified into two main types: concept and data drifts.<br>\n",
    " \n",
    "<li><b>__Concept drift:__</b> this occurs when the statistical properties of a target (dependent) variable change. This could arise when the meaning of the intended prediction target changes; consequently, the model's prediction accuracy relative to the updated definition will be low. For instance, a model that tracks fraudulent transactions will exhibit significantly low accuracy progressively if the definition of a fraudulent transaction changes over time.</li><br>\n",
    "\n",
    "\n",
    "<li><b>__Data drift:__</b> this occurs when the statistical properties of independent variables change, e.g. if the feature distribution changes or the correlation between variables changes. \n",
    "\n",
    "A classic example of natural data drift is that brought about by seasonality.</li><br>\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"./images/data_drift.png\"\n",
    "     alt=\"Example of Data Drift\"\n",
    "     style=\"float: centre; margin-right: 10px;\" \n",
    "     width=\"400\"\n",
    "     height=\"300\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Monitoring and Observability\n",
    "\n",
    "### Problem detection\n",
    "\n",
    "The first goal of an observability tool is to alert the user to any problems with the model to facilitate a prompt resolution. Since your models will be served to customers in real-time, you do not want weeks, days, or even hours to go by without being alerted. Problem detection varies with the model's development stage. During model creation, the tool may alert you to problems with the model or data. This enables you to quickly identify the problem and fix it, thereby increasing productivity. In production, your tool may alert you to a decrease in performance due to data drift so that the model can be retrained subsequently and the performance issues fixed.\n",
    "\n",
    "### Insight provision\n",
    "\n",
    "A good observability tool will allow you to detect where the problem occurs and give you the steps to resolve the problem. We know that there are many reasons for the degradation of a model's performance. An observability tool guides the user to the data, feature store or transformations housing the issue and provides a resolution path. For example, if the transformations across training and deployment are inconsistent, the tool will alert you to the issue, display the inconsistencies and provide suggestions for resolution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Observability\n",
    "\n",
    "To gain the aforementioned benefits, you must implement an observability platform in your ML pipeline. The platform will be able to detect degradation in your model and guide you to the root cause. Observability platforms employ a <b>model evaluation store</b> to observe models and perform some key tasks that enable the observation of the model's performance, create alerts and provide solutions to the problem(s).<br>\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"./images/ML_pipeline.png\"\n",
    "     alt=\"Example of Data Drift\"\n",
    "     style=\"float: centre; margin-right: 10px;\" \n",
    "     width=\"1000\"\n",
    "     height=\"300\"/></div>\n",
    "\n",
    "### Key features of an evaluation store\n",
    "\n",
    "<li>Notifies teams of any data-drift issues, reduction in data quality or degradation in the model's performance.</li>\n",
    "<li>Creates alerts if the production data differs considerably from the training data during deployment.</li>\n",
    "<li>Designed to analyse and alert you to slices of data that may cause poor performance or have caused poor performance in the past.</li>\n",
    "<li>Enables teams to understand changes in model performance and the reason for the changes.</li>\n",
    "<li>Enables you to run A/B testing to evaluate different versions of your model.</li>\n",
    "<li>Enables the use of a feedback loop to retrain models if the performance decreases below a certain threshold.</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Observability Platforms\n",
    "\n",
    "Here are some observability platforms that you can explore.\n",
    "<li>Fiddler</li>\n",
    "<li>superwise.ai</li>\n",
    "<li>Arize</li>\n",
    "<li>WhyLabs</li>\n",
    "<li>Seldon</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "At this point, you should have a good understanding of \n",
    "- ML observability and monitoring, as well as their benefits.\n",
    "- the reasons for the degradation in the performance of ML models with time.\n",
    "- how to integrate observability into the development lifecycle of ML models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
