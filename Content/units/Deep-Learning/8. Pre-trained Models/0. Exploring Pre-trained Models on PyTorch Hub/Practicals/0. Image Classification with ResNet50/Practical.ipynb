{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with ResNet50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pre-trained image classifier is a quick and efficient way to classify images by content .It involves loading a pretrianed model that has been trained on a large dataset, and then using it to make predictions on new images. Models such as ResNet have already been trained to classify a large range of objects, and so the pretrained model can often be used without any additional training for basic classification tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from google.colab import files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Transforms\n",
    "\n",
    "To use the pre-trained model, it is important to transform the image so as to present it to the model in a format which is compatible with the model's architecture, and also reflects the feature engineering used to train the model. For this we can use  the `torchvision.transforms` module.\n",
    "\n",
    "The following codeblock uses the `transforms.compose` class to compose a sequence of transforms. Important considerations when using transforms for pre-trained images include ensuring that the image is of the correct size to match the input layer, and applying any transforms used on the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #TODO - Add a transform to resize the image to 224x 224 pixels\n",
    "    # TODO - Add a transform to convert the image to a torch tensor\n",
    "    # TODO - Add a Normalisation transform to normalise the image channels. Means should be [0.485, 0.456, 0.406], SD should be [0.229, 0.224, 0.225]   \n",
    "                    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an image from your directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = utils_image.open_user_image(transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sense of the model's output, it is necessary to have a decoder - essentially a dictionary where the keys are the integer labels used by the classifier, and the values are the human-readable class nammes. The codeblock below loads in the classes as a list, with the keys implicit in the index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get imagenet classes\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# TODO - Print the first 20 class labels from the decoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the pretrained model from the `torchvision.models` library, setting `pretrained=True`.\n",
    "\n",
    "- Use the model to get a prediciton. Set the model to evaluation mode and pass the image tensor to the model, remembering to set grad to false, as there is no need for a backward step here, so the 'grad' just adds unnecessary computational load.\n",
    "- Assign the resulting logits to a variable called 'output'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# TODO - Set the model to evaluation mode.\n",
    "# TODO - Pass the image tensor to the model, remembering to set grad to false for speed. pass the result to a variable called `output`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to view the prediction, we need to take the argmax of the output of the model. The final step of the model already applies a softmax step, so we only need to find the maximum of the softmaxed logits.\n",
    "\n",
    "This gives us the class label, which is an integer value, so we also need to use our decoder to find the human-readable predicted class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Get the prediction by finding the argmax of the softmaxed logits. Assign to a variable callled \"prediction\".\n",
    "print(\"Prediction label: \", prediction)\n",
    "#TODO - Use the decoder defined earlier in the script to get the class descriptor for the predicted class.\n",
    "print(\"Prediction category: \", class_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44b49826f70757dd05c2075de036ba047f37ef083b3bf9e82fcf656fa7d561e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
