{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common activation functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following are activation functions? Select all that apply.\n",
    "\n",
    "- Tanh ***\n",
    "- ReLU ***\n",
    "- Ridge\n",
    "- Sigmoid ***\n",
    "- Cross-entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the output of the ReLU activation function for a value of -1?\n",
    "\n",
    "- `-1`\n",
    "- `0` ***\n",
    "- `1`\n",
    "- We can't tell from the information given"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the range of the output of the sigmoid activation function?\n",
    "\n",
    "- `(-1, 1)`\n",
    "- `(0, 1)` ***\n",
    "- `(-1, 0)`\n",
    "- `(-inf, inf)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the range of the output of the tanh activation function?\n",
    "\n",
    "- `(-1, 1)` ***\n",
    "- `(0, 1)`\n",
    "- `(-1, 0)`\n",
    "- `(-inf, inf)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the problem with using the sigmoid or the tanh activation functions for the output layer of a neural network?\n",
    "\n",
    "- They are not actually activation functions\n",
    "- They are not differentiable at 0\n",
    "- They are not differentiable at 1\n",
    "- They become very flat for large inputs, which can cause problems during training ***\n",
    "- Their output is not zero-centered"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
