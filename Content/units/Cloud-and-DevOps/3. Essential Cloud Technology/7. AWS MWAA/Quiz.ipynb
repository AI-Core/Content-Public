{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS MWAA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does AWS MWAA stand for?\n",
    "\n",
    "- Amazon Web Services Managed Web Application Analytics service\n",
    "- Amazon Web Services Managed Workflow Automation service\n",
    "- Amazon Web Services Managed Workflows for Apache Airflow service ***\n",
    "- Amazon Web Services Managed Website Acceleration service\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following is a feature of Amazon Web Services Managed Workflows for Apache Airflow (MWAA)?\n",
    "\n",
    "- Serverless compute and storage\n",
    "- Customizable security policies\n",
    "- Managed Apache Airflow software updates\n",
    "- All of the above ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the role of the scheduler, executor, and workers in the AWS Managed Workflows for Apache Airflow (MWAA) environment?\n",
    "\n",
    "- The scheduler creates and manages DAGs, the executor runs the DAGs, and the workers perform the tasks in the DAGs ***\n",
    "- The scheduler performs the tasks in the DAGs, the executor manages the workers, and the workers create and manage DAGs\n",
    "- The scheduler creates and manages workflows, the executor runs the workflows, and the workers perform the tasks in the workflows\n",
    "- The scheduler creates and manages tasks, the executor runs the tasks, and the workers manage the DAGs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of the metadata database used by Apache Airflow in Amazon Web Services Managed Workflows for Apache Airflow (MWAA)?\n",
    "\n",
    "- To store the variables and connections configurations\n",
    "- To store the user information, role and policies\n",
    "- To store the DAG-related metadata\n",
    "- All of the above ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the steps to create an Apache Airflow environment using the Amazon Web Services Managed Workflows for Apache Airflow (MWAA) Management Console?\n",
    "\n",
    "- Choose **Create environment** from the console, select the MWAA environment type, choose the VPC, security settings, and storage options, and then review and create the environment ***\n",
    "- Choose **Create environment** from the console, select the EC2 instance type, choose the AMI, configure the networking and security, and then launch the environment\n",
    "- Choose **Create environment** from the console, select the Lambda function type, configure the function code and settings, and then deploy the environment\n",
    "- Choose **Create environment** from the console, select the ECS container type, choose the task definition, configure the cluster and networking, and then start the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the required features of an Amazon S3 bucket used for an Amazon Web Services Managed Workflows for Apache Airflow (MWAA) environment?\n",
    "\n",
    "- The bucket must have versioning enabled and an IAM policy granting MWAA access to the bucket ***\n",
    "- The bucket must have public access enabled and a lifecycle policy to delete old files\n",
    "- The bucket must have server-side encryption enabled and a bucket policy allowing public read access\n",
    "- The bucket must have object locking enabled and a bucket policy allowing write access to the MWAA service"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2592652612463181e69ac003232387e3e9a99279aa6b168e76f5df16d5110f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
