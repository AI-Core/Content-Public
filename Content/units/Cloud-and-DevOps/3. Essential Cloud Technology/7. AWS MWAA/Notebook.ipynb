{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Managed Workflows for Apache Airflow (MWAA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "*Amazon Managed Workflows for Apache Airflow (MWAA)* is a managed service that was designed to help you integrate Apache Airflow straight in the cloud, with minimal setup and the quickest time to execution. Apache Airflow is a tool used to schedule and monitor different sequences of processes and tasks, referred to as *workflows*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main features\n",
    "\n",
    "- **Automatic set-up**: quick set-up that allows you to choose the desired Apache Airflow version when creating a MWAA environment. \n",
    "\n",
    "- **Automatic scaling**: automatically scales Airflow workers by setting a minimum and maximum number of workers to run in your environment. Workers can be added to meet workload demand until the maximum number it's reached.\n",
    "\n",
    "- **Built-in authentication and authorisation** through IAM and single sign-on(SSO).\n",
    "\n",
    "- **Built-in security**: Airflow workers and schedulers run inside MWAA's virtual private cloud(VPC). Data is automatically encrypted there, so the environment is secure by default.\n",
    "\n",
    "- **Workflow monitoring**: can monitor logs and metrics using AWS CloudWatch to identify errors or delays in the workflows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MWAA architecture\n",
    "\n",
    "The MWAA environment contains all the components in the pink box of the image below. \n",
    "\n",
    "- The *Scheduler* schedules and delegates tasks on the worker node via the executor. \n",
    "- The *Executor* is the workstation of the tasks. It acts as a middle man to handle resource allocation and distribute tasks. The executors run inside the scheduler.\n",
    "- The *Workers* are the nodes on which the tasks are executed.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Architecture.png\" width=\"700\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "Each MWAA environment has its own Apache Airflow *Metadata Database* managed by AWS that is accessible to the scheduler and workers via a privately-secured VPC endpoint. This DB powers the user interface (UI), as well as acts as the backend for the worker and scheduler. The DB stores configurations, such as variables and connections. It stores user information, roles, and policies. It also stores DAG-related metadata such as schedule intervals, tasks, etc.\n",
    "\n",
    "The *Webserver* provides a control interface for users and maintainers. It lists DAGs, their run history, schedule, pause/start options. This is a central place from where we can manage all the pipelines. The webserver\n",
    "can be accessed either over the Internet by selecting the **Public network** access mode, or within your VPC by selecting the **Private network** mode. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create an Airflow environment with MWAA\n",
    "\n",
    "MWAA orchestrates your workflows using **Directed Acyclic Graphs (DAGs)**. You will have to provide MWAA a S3 bucket where your DAGs, Python requirements and plugins will reside. After that you will be able to run your DAGs from the management console, the CLI or MWAA user interface (UI).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Mechanism.png\" width=\"700\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "#### 1. Create a S3 bucket for MWAA\n",
    "\n",
    "> A S3 bucket for an MWAA environment must be configured to **Block all public access** and have **Bucket Versioning** enabled. \n",
    "\n",
    "The bucket must be located in the same AWS Region as the environment (in our example us-east-1). Create a folder called `dags` in this bucket, this will be where you will later upload your DAGs.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/S3 Bucket.png\" width=\"500\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "#### 2. Create a MWAA environment\n",
    "\n",
    "Open the MWAA console and make sure the AWS Region selector is in the appropriate region. Now we can choose **Create environment**. Specify the desired environment name under **Environment details**.\n",
    "\n",
    "Under **DAG code in Amazon S3**, choose Browse S3 and select the S3 bucket you have created in the previous step.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DAG Code.png\" width=\"450\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "Then click **Next**.\n",
    "\n",
    "On the **Networking** page, you will need to select a VPC to launch your MWAA environment in. Choose Create MWAA VPC to automatically create this VPC with its corresponding private subnets.\n",
    "\n",
    "Choose the preferred **Apache Airflow access mode**. Selecting a *private network* will limit access to the UI to users within the MWAA VPC. Selecting a *public network* will allow the UI to be accessed over the internet.\n",
    "\n",
    "Under **Security groups**, select Create new security group, which will allow MWAA to create a VPC security group within inbound and outbound rules based on the type of web server access selected.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Networking.png\" width=\"500\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "Under **Environment class** select the desired class, as well as the minimum and maximum worker count. It's recommended to choose the smallest environment size that is necessary to support the workload.\n",
    "\n",
    "Leave the rest of default options and choose next. Finally choose **Create environment**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the MWAA Airflow UI\n",
    "\n",
    "In the **MWAA console** look for the environment you have been provided access to and click on **Open Airflow UI**. A new window will open and you will be authenticated with a secure login via IAM. Once the UI is opened you should see something like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Airflow UI.png\" width=\"700\" height=\"250\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise your own DAGs in the UI, you will have to upload the corresponding Python file in the S3 bucket bucket associated with your MWAA environment, under the `dags` folder. Once uploaded, they will appear in the UI and you will be able to run them. Initially, newly added DAGs will appear in the **Paused** section. You will need to unpause them before you can run them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, we should have a good understanding of:\n",
    "- What is AWS MWAA \n",
    "- The key features of MWAA\n",
    "- MWWA architecture\n",
    "- How to create a MWAA environment\n",
    "- How to run DAGs using the MWAA UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2 (default, Mar  3 2021, 11:58:52) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d25a140d753dbbce5959f3e993c340d725ba39c38e4259359e51030082d2593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
