{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSK Connect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "*MSK Connect* is a feature of AWS MSK, that allows users to stream data to and from their MSK-hosted Apache Kafka clusters. With MSK Connect, you can deploy fully managed connectors that move data into or pull data from popular data stores like Amazon S3 and Amazon OpenSearch Service, or that connect Kafka clusters with external systems, such as databases and file systems. \n",
    "\n",
    "*Source connectors* can be used to import data from external systems into your topics, while *Sink connectors* can export data from your topics to external systems.\n",
    "\n",
    "MSK Connect will continuously monitor the connectors health and delivery state, as well as manage the underlying hardware, and autoscale the connectors to match changes in data load."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the required resources\n",
    "\n",
    "As an example, we will create a **sink connector** that will send data from a MSK cluster to a S3 bucket. To achieve this we will need the following resources:\n",
    "\n",
    "- A **MSK cluster** to which we will send the data. The connector will read data from here and send it to the destination bucket.\n",
    "- A **S3 bucket** that will serve as the destination for the data received from the connector.\n",
    "- An **IAM role** that allows the connector to write to the destination.\n",
    "- A **VPC endpoint** that allows data from cluster VPC and connector to be sent to the destination."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the S3 bucket\n",
    "\n",
    "Open the Amazon S3 console and choose **Create bucket**. For the bucket name select your desired name. Make sure to select the same **AWS Region** (in our case us-east-1) as the region in which you created your MSK cluster. Finally, choose **Create bucket**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Create Bucket.png\" width=\"550\" height=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a MSK cluster\n",
    "\n",
    "Check-out the MSK Essentials lesson to see how to create a MSK cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create an IAM role that can write to the destination bucket\n",
    "\n",
    "Navigate to the IAM console, and select Roles under the **Access management** section. Choose **Create role** to create a new IAM role. \n",
    "\n",
    "Under **Trusted entity type**, select AWS service, and under the **Use case** field select S3 in the **Use cases for other AWS services** field.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/IAM Role Trusted Entity.png\" width=\"600\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "In the permission tab, select **Create policy**. This will open a new tab where you can create the desired policy. Select the **JSON** tab. Replace the existing text with the following policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor0\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:GetBucketLocation\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::<DESTINATION_BUCKET>\",\n",
    "                \"arn:aws:s3:::<DESTINATION_BUCKET>/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor1\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucketMultipartUploads\",\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:ListMultipartUploadParts\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor2\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"s3:ListAllMyBuckets\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This policy creates the necessary permissions to write to the destination bucket. Skip the rest of the pages until you reach the **Create policy** button.\n",
    "\n",
    "Now back to the main tab for the IAM role, you should be able to find the policy you have just created, and then select it. Skip the rest of the pages until you reach the **Create role** button. Once you have select it, the new IAM role will be created.\n",
    "\n",
    "In the IAM console, choose the role you have just created, and select the **Trust relationships** tab. In the **Trusted entities** tab you should add the following trust policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"kafkaconnect.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trust relationship allows MSK Connect to assume the role to which we attached the policy created above. Finally select the **Update Trust Policy**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Create a VPC endpoint to S3.\n",
    "\n",
    "In the VPC console, select Endpoints under the **Virtual private cloud** tab and then choose **Create endpoint**.\n",
    "\n",
    "Under **Service Name** choose **com.amazonaws.us-east-1.s3** and the Gateway type. Choose the VPC that corresponds to the MSK cluster's VPC from the drop-down menu in VPC section. Finally, select **Create endpoint**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/VPC Endpoint.png\" width=\"600\" height=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the custom plugin\n",
    "\n",
    "A plugin will contain the code that defines the logic of our connector. We will use the client EC2 machine we have previously used (check out MSK Essentials lesson) to connect to our cluster for this step.\n",
    "\n",
    "First connect to your client EC2 machine. We will download the **Confluent.io Amazon S3 Connector** on our machine, and then copy it to the S3 bucket we have previously created. This connector is a sink connector that exports data from Kafka topics to S3 objects in either JSON, Avro or Bytes format. To do download & copy this connector run the code below inside your client machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# assume admin user privileges\n",
    "sudo -u ec2-user -i\n",
    "# create directory where we will save our connector \n",
    "mkdir kafka-connect-s3 && cd kafka-connect-s3\n",
    "# download connector from Confluent\n",
    "wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip\n",
    "# copy connector to our S3 bucket\n",
    "aws s3 cp ./confluentinc-kafka-connect-s3-10.0.3.zip s3://<BUCKET_NAME>/kafka-connect-s3/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything ran successfully, you should be able to see the following folder and file inside your S3 bucket.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Plugin ZIP.png\" width=\"700\" height=\"450\"/>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, open the MSK console and select *Custom plugins* under the **MSK Connect** section on the left side of the console. Choose **Create custom plugin**.\n",
    "\n",
    "In the list of buckets, find the bucket where you upload the `Confluent connector ZIP file`. Then, in the list of objects in that bucket select the ZIP file and select the **Choose** button. Give the plugin a name and press **Create custom plugin**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Custom plugin.png\" width=\"650\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "Once the plugin has been created you should see the following message at the top of your browser window:\n",
    "\n",
    "`plugin <PLUGIN_NAME> was successfully created. The custom plugin was created. You can now create a connector using this custom plugin`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the connector\n",
    "\n",
    "In the MSK console, select *Connectors* under the **MSK Connect** section on the left side of the console. Choose **Create connector**.\n",
    "\n",
    "In the list of plugin, select the plugin you have just created, and then click **Next**. For the connector name choose the desired name, and then choose your MSK cluster from the cluster list.\n",
    "\n",
    "In the **Connector configuration settings** copy the following configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "connector.class=io.confluent.connect.s3.S3SinkConnector\n",
    "# same region as our bucket and cluster\n",
    "s3.region=us-east-1\n",
    "flush.size=1\n",
    "schema.compatibility=NONE\n",
    "tasks.max=3\n",
    "# include nomeclature of topic name, given here as an example will read all data from topic names starting with msk.topic....\n",
    "topics.regex=<YOUR_UUID>.*\n",
    "format.class=io.confluent.connect.s3.format.json.JsonFormat\n",
    "partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "value.converter.schemas.enable=false\n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "storage.class=io.confluent.connect.s3.storage.S3Storage\n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "s3.bucket.name=<BUCKET_NAME>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave the rest of the configurations as default, except for **Access permissions**, where you should select the IAM role you have created previously. \n",
    "\n",
    "Skip the rest of the pages until you get to **Create connector** button page. Once your connector is up and running you will be able to visualise it in the **Connectors** tab in the MSK console."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data from MSK to S3\n",
    "\n",
    "You are now ready to send data from your MSK cluster to your S3 bucket. Any data that will pass through your MSK cluster will now be automatically uploaded to its designated S3 bucket, in a newly created folder called `topics`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of:\n",
    "- What is MSK Connect\n",
    "- How to set-up the necessary resources for MSK Connect\n",
    "- How to create a custom sink plugin\n",
    "- How to create a connector with MSK connect\n",
    "- How to send data from a MSK cluster to an S3 bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2 (default, Mar  3 2021, 11:58:52) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d25a140d753dbbce5959f3e993c340d725ba39c38e4259359e51030082d2593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
