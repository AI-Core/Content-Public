{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workloads\n",
    "\n",
    "## Introduction\n",
    "> <font size=+1> Workload is an application running on `Kubernetes`. </font>\n",
    "\n",
    "It can be divided into `Pods` and `Workload Resources`.\n",
    "\n",
    "### Pods overview\n",
    "Each `workload` runs within __a set of `Pods`__, and each `Pod` __represents a set of containers running together.__.\n",
    "\n",
    "- `Pod`s are scheduled to run on nodes.\n",
    "- If a `node` fails, all `Pod`s on the node are deleted (__`Pod`s are ephemeral and are easy to (re)create__).\n",
    "- If a `Pod` run fails, it will stay in the inappropriate state.\n",
    "\n",
    "Notably, since there are many failure points, the cluster admin should\n",
    "- constantly verify if the `Pod`s are running well.\n",
    "- manually reschedule `Pod`s to different `Node`s.\n",
    "- manually restart `Pod`s if any container fails.\n",
    "\n",
    "### Workload resources overview\n",
    "\n",
    "> __`Workload resources` were created to address the inefficiency of manual handling.__\n",
    "\n",
    "They can specify, among other things,\n",
    "\n",
    "- when a `Pod` should be recreated.\n",
    "- what to do when a failure occurs.\n",
    "- the number of times to attempt rescheduling before stopping. \n",
    "\n",
    "> `Workload Resources` should be used to manage the lifecycle of `Pod`s. Furthermore, avoid deploying 'bare Pods'.\n",
    "\n",
    "Here are the most common `Workload Resources`:\n",
    "\n",
    "- Deployment\n",
    "- StatefulSet\n",
    "- DaemonSet\n",
    "- Job and CronJob\n",
    "\n",
    "In this lesson, we will learn about Pods and Workload Resources, as well as how to implement them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> <font size=+1>A group of one or more containers, with a shared context and a specification for running the containers.</font>\n",
    "\n",
    "Pods add another layer of abstraction to containers and behave similarly to `docker-compose`, i.e. __they can connect multiple related containers in one logical grouping__.\n",
    "\n",
    "*Shared context*\n",
    "- Shared storage\n",
    "- Shared network resources (e.g. IP)\n",
    "- Linux namespaces\n",
    "\n",
    "__Notably, individual applications can be further isolated within a Pod.__\n",
    "\n",
    "> A pod is a minimal deployment unit in Kubernetes.\n",
    "\n",
    "This indicates that containers cannot be deployed on their own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifecycle\n",
    "\n",
    "> `Pod`s remain on their scheduled `Node` until termination (according to the restart `policy` if a failure occurs) or deletion.\n",
    "\n",
    "__Further, they are never moved across nodes and are eventually recreated.__\n",
    "\n",
    "#### Features\n",
    "- `Pod`s cannot self-heal (i.e. initiate self-restart). The restart is carried out by the appropriate `Workload Resources` or the cluster admin.\n",
    "- __Pods can restart failed containers using `kubelet`.__\n",
    "- Related resources (e.g. `volume`s) are also deleted after `Pod` termination, unless otherwise specified.\n",
    "\n",
    "`Pod`s can be in one of multiple phases:\n",
    "- `pending`: The pod has been accepted by the `k8s` cluster; however,\n",
    "    - one or more containers did not start.\n",
    "    - the `Pod` is waiting for the node schedule.\n",
    "    - the container image is currently downloading.\n",
    "- `running`: At least one `container` within the `Pod` is running (or being restarted).\n",
    "- `succeeded`: All the containers in the `Pod` have succeeded, __and no restart is required.__\n",
    "- `failed`: At least one container failed and was terminated.\n",
    "- `unknown`: The state of the `Pod` could not be obtained (__typically caused by communication error with the `Node`).__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container states\n",
    "\n",
    "> __Kubernetes also watches the state of individual containers within `Pod`s.__\n",
    "\n",
    "Containers can be in one of three states:\n",
    "- `Waiting`: downloading the image or pulling `secrets` (__the reason for this state is provided for monitoring).__\n",
    "- `Running`\n",
    "- `Terminated`: either terminated successfully or not (__the reason and `exit code` are provided for monitoring).__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-container pods\n",
    "\n",
    "Generally, p are run with a single container. A Pod can be considered as a container wrapper.\n",
    "\n",
    "Examples include the following:\n",
    "- A fastAPI server receiving requests and saving them to a shared database.\n",
    "- A docker container receiving images as requests and forwarding the classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-container pods\n",
    "\n",
    "> Multi-container pods are employed in more advanced use cases. They comprise multiple, tightly coupled containers, __affording a cohesive service unit.__\n",
    "\n",
    "<p align=center><img src=images/pod.svg width=350></p>\n",
    "\n",
    "Here are examples of use cases:\n",
    "- Training multiple ML models, where the\n",
    "    - first container accesses the shared storage of raw data and transforms it.\n",
    "    - second container trains the neural network on the presented data.\n",
    "    - third container pushes the trained model to the serving container.\n",
    "    - fourth container serves the model.\n",
    "- One container serves data to the public (`read only` permissions), while another, an internal one, writes data to shared storage.\n",
    "\n",
    "> __Pod containers are scheduled on the same 'logical host' (for cloud, for clusters of servers: same VM or physical computer) because of tight coupling.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in 'Introduction', pods in a cluster can be viewed using the `kubectl get pod` command. However, only the pods in the default namespace will be shown. To observe all the pods in the cluster, add the -A flag.\n",
    "\n",
    "<i>_If you have not done so already, run `minikube start` to create a cluster in your local machine._</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE              NAME                                         READY   STATUS    RESTARTS       AGE\n",
      "default                hello-minikube-6ddfcc9757-mndds              1/1     Running   2 (106s ago)   16h\n",
      "kube-system            coredns-78fcd69978-8j8hz                     1/1     Running   2 (106s ago)   19h\n",
      "kube-system            etcd-minikube                                1/1     Running   2 (106s ago)   19h\n",
      "kube-system            kube-apiserver-minikube                      1/1     Running   2 (106s ago)   19h\n",
      "kube-system            kube-controller-manager-minikube             1/1     Running   2 (106s ago)   19h\n",
      "kube-system            kube-proxy-jb22b                             1/1     Running   2 (106s ago)   16h\n",
      "kube-system            kube-scheduler-minikube                      1/1     Running   2 (106s ago)   19h\n",
      "kube-system            storage-provisioner                          1/1     Running   4 (32s ago)    16h\n",
      "kubernetes-dashboard   dashboard-metrics-scraper-5594458c94-rjg6d   1/1     Running   2 (106s ago)   16h\n",
      "kubernetes-dashboard   kubernetes-dashboard-654cf69797-ggqph        1/1     Running   3 (28s ago)    16h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, we already have some Pods. This is because `minikube` comes with some pods by default. In the next section, we see how to deploy pods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining pods (pod template)\n",
    "\n",
    "As mentioned in the last lesson, Kubernetes objects can be defined imperatively (using defined steps) or declaratively. In this lesson, we will focus on defining the declarative configurations, which are specified using `.yaml` files.\n",
    "\n",
    "A pod, as a basic `kind`, can be specified via the `.yaml` config file; however, __we strongly advise against specifying bare `Pod`s.__\n",
    "\n",
    "It is easy to specify a 'bare `Pod`':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: pod1\n",
    "  labels:\n",
    "    tier: frontend\n",
    "spec:\n",
    "  containers:\n",
    "  - name: hello1\n",
    "    image: gcr.io/google-samples/hello-app:2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we learnt about the API versions and how to find them in the [API docs](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/). \n",
    "\n",
    "Inside `spec`, we added the `container`. This will list the docker containers belonging to the list. Inside `container`, we specified `image`, which corresponds to the Docker image name. In this case, it is a sample image from google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No resources found in default namespace.\n"
     ]
    }
   ],
   "source": [
    "# Observe the pods in the default namespace\n",
    "!kubectl ###Your command here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/pod1 created\n"
     ]
    }
   ],
   "source": [
    "# Spin up the pod corresponding to the single-pod configuration above\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME   READY   STATUS              RESTARTS   AGE\n",
      "pod1   0/1     ContainerCreating   0          23s\n"
     ]
    }
   ],
   "source": [
    "# Observe the pods in the default namespace again\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Delete the pod using the correct `kubectl` command.\n",
    "- Observe the pods in the default namespace once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"pod1\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the pod\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No resources found in default namespace.\n"
     ]
    }
   ],
   "source": [
    "!kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the case in the last notebook, the pod disappears here. This is because, contrary to the case in the last notebook, no instructions were provided here to keep the pod alive. The pod is 'bare', without any resource to keep it running after it fails. \n",
    "\n",
    "Keeping it 'alive' is one of the desired states that can be specified in the config file, and it can be achieved with `Deployment` or `Replica Set`. More options can be found within the workload resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Resources\n",
    "\n",
    "Before diving into specific `Workload Resources`, here are a few important concepts.\n",
    "\n",
    "- Each `workload` presented below uses the `.spec.template` field, which specifies how to create a pod.\n",
    "- The `template` is essentially the same as pod config, except for the `kind` and `apiVersion` fields.\n",
    "- Each `workload` has a `.spec.selector` that specifies which pods are handled by the `workload resource`.\n",
    "- `.spec.selector` employs matches on defined `labels` and may 'handle' pods outside its config file.\n",
    "\n",
    "### Implementation\n",
    "Workload resources can be implemented via\n",
    "- deployment\n",
    "- DaemonSet\n",
    "- jobs\n",
    "\n",
    "In subsequent lessons, we will learn how to implement another type of workload, `StatefulSets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplicaSet\n",
    "\n",
    ">  This maintains a stable set of replicated pods running at any given time.\n",
    "\n",
    "`ReplicaSet`:\n",
    "- creates new `POD`s accordingly to the `.spec.replicas` field value.\n",
    "    (from `.spec.template` config)\n",
    "- deletes `POD`s if overly many of them are scheduled to nodes.\n",
    "\n",
    "#### Acquiring pods\n",
    "\n",
    "> `ReplicaSet` is linked to the pods via `metadata.ownerReferences` and acquired via `.spec.selector` matching.\n",
    "\n",
    "The above works as follows:\n",
    "- Each pod has `metadata.ownerReferences`, which was __automatically added by `k8s`.__\n",
    "- The above specifies who manages the pod, e.g. another controller.\n",
    "- __If the `POD`__has no 'owner' (e.g. bare `POD`) __or__ its owner __is not another controller and__ the `.spec.selector` fields match, then the `POD` is acquired by the `ReplicaSet`.\n",
    "\n",
    "> <font size=+1>The process above works the same for other `workload resources` (or managers).</font>\n",
    "\n",
    "#### Using `ReplicaSet`s\n",
    "\n",
    "> __Generally, it is advised to use relatively high-level `Deployment` `workload resources`.__\n",
    "\n",
    "`Deployment`, a high-level concept, __manages `ReplicaSet`s__ and, in addition, provides __declarative updates to pods.__\n",
    "\n",
    "`ReplicaSet`s may be employed if\n",
    "- custom-update orchestration is to be performed.\n",
    "- the `config` file will never be updated.\n",
    "\n",
    "> <font size=+1>Note, however, that we recommend using Deployment instead.</font>\n",
    "\n",
    "For more detailed information on `ReplicaSet`s, check [here](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "> Refers to the provision of declarative updates for `Pods` and `ReplicaSets`.\n",
    "\n",
    "Consider the example config below, and attempt to decipher the meaning of each field:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: nginx-deployment\n",
    "  labels:\n",
    "    app: nginx\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: nginx\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: nginx\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: nginx\n",
    "        image: nginx:1.14.2\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the resource is being instructed to ensure that at least three pods are running at all times. These three pods can be found using the `selector.matchLabels`, which searches the template and finds the label whose key and value are `app` and `nginx`, respectively. \n",
    "\n",
    "#### Example\n",
    "\n",
    "1. Create a .yaml file with the above configuration.\n",
    "2. Run the right `kubectl` command to run the .yaml file.\n",
    "3. Observe the number of pods.\n",
    "4. Delete one pod.\n",
    "5. Observe the number of pods once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/nginx-deployment created\n"
     ]
    }
   ],
   "source": [
    "!kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                READY   STATUS              RESTARTS   AGE\n",
      "nginx-deployment-66b6c48dd5-7qpvr   0/1     ContainerCreating   0          17s\n",
      "nginx-deployment-66b6c48dd5-gc7z7   0/1     ContainerCreating   0          17s\n",
      "nginx-deployment-66b6c48dd5-vzsx2   0/1     ContainerCreating   0          17s\n"
     ]
    }
   ],
   "source": [
    "# Observe how many pods you have\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can exploit the running `nginx` containers to run some commands manually.\n",
    "\n",
    "In this case, we run the command in the terminal:\n",
    "\n",
    "<p align=center><img src=images/nginx.png width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, after deletion, the pod persists, although with a different name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"nginx-deployment-66b6c48dd5-vzsx2\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete one of the pods\n",
    "!kubectl delete pod nginx-deployment-66b6c48dd5-vzsx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                READY   STATUS    RESTARTS        AGE\n",
      "nginx-deployment-66b6c48dd5-7qpvr   1/1     Running   1 (8m15s ago)   8h\n",
      "nginx-deployment-66b6c48dd5-fsck5   1/1     Running   0               40s\n",
      "nginx-deployment-66b6c48dd5-gc7z7   1/1     Running   1 (8m15s ago)   8h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid confusion, delete the deployment resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"nginx-deployment\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the deployment resource\n",
    "!kubectl delete deployment nginx-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DaemonSet\n",
    "\n",
    "> DaemonSet ensures that a pod is deployed in all Nodes as it is added to the cluster.\n",
    "\n",
    "In the demonstration thus far, we have one pod per node; therefore, if a node is removed, the number of pods will decrease.\n",
    "\n",
    "DaemonSets are generally used to monitor services. A single pod can be used to monitor the health or capture the logs of each node. As you probably can tell, they are quite similar to deployment; however, they do not have replicas.\n",
    "\n",
    "#### Required fields\n",
    "\n",
    "> The required fields are similar to those for `Deployment`, i.e. `.spec.template`, `.spec.selector` and __no `.spec.replicas` (as the same `daemon` is run per node).__\n",
    "\n",
    "- Similarly, `POD`s are acquired via `.spec.selector` matching.\n",
    "- Additionally, __`Node`s are acquired via the `.spec.template.spec.nodeSelector` field.__\n",
    "\n",
    "### Assigning pods to nodes\n",
    "\n",
    "> Generally, we do not have to interfere with `k8s` POD deployment to specific `Node`s.\n",
    "\n",
    "However, interference may be allowed for the following reasons:\n",
    "- To ensure that the `POD` ends up on a `Node` with `SSD` attached.\n",
    "- To co-locate `POD`s from different services in the same zone if they communicate frequently.\n",
    "\n",
    "`k8s` comes with a set of predefined `labels` for `Node`s. \n",
    "\n",
    "Here are a few common labels (the full list can be found [here](https://kubernetes.io/docs/reference/labels-annotations-taints/)):\n",
    "- Region of deployment (for cloud): `topology.kubernetes.io/region=us-east-1`.\n",
    "- IP address of a node: `kubernetes.io/hostname=ip-172-20-114-199.ec2.internal`.\n",
    "- Operating system of the node: `kubernetes.io/os=linux`.\n",
    "\n",
    "> __In the `kubectl` lesson, we will learn how to add custom `label`s to `Node`s.__\n",
    "\n",
    "Now, the `.spec.template.spec.nodeSelector` can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: apps/v1\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: fluentd-elasticsearch\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      name: fluentd-elasticsearch\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        name: fluentd-elasticsearch\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: fluentd-elasticsearch\n",
    "        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n",
    "        resources:\n",
    "          limits:\n",
    "            memory: 200Mi\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 200Mi\n",
    "      terminationGracePeriodSeconds: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we will briefly go over memory resources. As can be observed in the Kubernetes object, there is a new field in the `template.spec.container` field named `resources`. This field, in turn, has two more fields: `limits` and `requests`. \n",
    "\n",
    "- `limits`: These values represent the maximum capacities allotted to a pod. If the process expends more than 200MB of RAM, the pod will be restarted.\n",
    "- `requests`: These values represent the minimum capacities allotted to a pod. In the example above, 200MB of RAM and 100 milicores are allotted to the pod. A _milicore_ is a fraction of a computer's cores, and each core is equivalent to 1000 milicores.\n",
    "\n",
    "For more information on container resources, visit the [link](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, most of the arguments in the DaemonSet are the same as those in the Deployment resource. Observe that the `replicas` key is not used. This is because, as mentioned, there will be a single `DaemonSet` pod per node. For this demonstration, we add a node to the minikube existing cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!minikube node add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jobs\n",
    "\n",
    "`Job`s create one or more pods and makes repetitive attempts to execute them until a specified number successfully terminates.\n",
    "\n",
    "#### Use cases\n",
    "- A `Job` is created to ensure the successful running of a task.\n",
    "- The same `Job` can be run in parallel for `N` times.\n",
    "\n",
    "Below is an example config `Job` workload that calculates the `pi` value:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: pi\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: pi\n",
    "        image: perl\n",
    "        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n",
    "      restartPolicy: Never\n",
    "  backoffLimit: 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.spec`ification\n",
    "\n",
    "For Jobs, the standard fields are necessary, in addition to the following:\n",
    "\n",
    "- `.spec.restartPolicy`: can either be `Never` or `OnFailure` (default).\n",
    "- `.spec.completions`: the number of `Job`s to be completed prior.\n",
    "- `.spec.parallelism`: the number of `pod`s to be scheduled with the job simultaneously.\n",
    "\n",
    "Using `.spec.completions` and `.spec.parallelism`, we can construct different levels of parallelism:\n",
    "\n",
    "- __non-parallel__: specify `.spec.completions=1`, and only one `Job` will be created (__a new one will only start after this one fails__).\n",
    "- __parallel with a fixed completion count__: specify `.spec.completions=N` to run __at most `N` parallel jobs at a given time__ (the controller will reschedule the `Node`s in case of a failure).\n",
    "- __parallel with work queue__: specify `.spec.completions=1` and `.spec.parallelism=N`; `N` pods will run after the first one succeeds; __the execution of the rest will continue until termination__ (implement direct `pod` to `pod` communication for improved efficiency).\n",
    "\n",
    "`non-parallel` is the default mode, while `.spec.completions=1` and `.spec.parallelism=1` are the default values.\n",
    "\n",
    "One could also specify the [completion Mode](https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode), which allows us to modify the behaviour of the pods upon termination.\n",
    "\n",
    "- `.spec.backoffLimit`: the number of times __a single `pod`__ should be restarted before considering the `Job` as failed.\n",
    "\n",
    "Things to note in this case:\n",
    "- Depending on the settings, if `.spec.completions=N` is hit, the __job__ is considered successful.\n",
    "- Until this moment, attempt to recreate `pod` with `Job` `N` times.\n",
    "- Exponential back-off delay is applied:\n",
    "    - first retry after `10s`\n",
    "    - second after `20s`\n",
    "    - third after `40s`\n",
    "    - __capped at `6m` backoff.__\n",
    "    \n",
    "- `.spec.activeDeadlineSeconds`: how long (in seconds) the __whole job__ takes until termination. Once reached, __all the `pod`s are terminated__ (takes precedence over `.spec.backoffLimit`).\n",
    "    \n",
    "#### Cleaning up\n",
    "\n",
    "> Once completed, a `Job` __will not automatically be removed from the cluster.__\n",
    "\n",
    "> This is disadvantageous because `kube-apiserver` will still query `Job` and look for its `pod`s, __applying unnecessary pressure on `k8s`.__\n",
    "\n",
    "This is the default behaviour because the user may need to check\n",
    "- the logs of finished jobs, which are stored within `pod`s or in an external storage volume.\n",
    "- the status of `Job`(s).\n",
    "\n",
    "__`TTL` (time to live) offers an adequate solution to this issue.__\n",
    "\n",
    "> __`TTL` specifies when the `job` should be removed from the cluster, including all of its `pod`s and dependencies__.\n",
    "\n",
    "TTL can be set up via the `.spec.ttlSecondsAfterFinished` field, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: pi-with-ttl\n",
    "spec:\n",
    "  ttlSecondsAfterFinished: 100\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: pi\n",
    "        image: perl\n",
    "        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n",
    "      restartPolicy: Never"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, `Job`s will terminate when they successfully execute a number of Pods. To repeat that operation, you would need to `apply` a Kubernetes object to re-run the job. Fortunately, there is an easier approach to generate `Job`s periodically with the desired frequency: __`Cronjob`__.\n",
    "\n",
    "A `Cronjob` creates `Job`s on a repeating schedule. The desired schedule can be specified in the specs:\n",
    "```\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: hello\n",
    "spec:\n",
    "  schedule: \"*/1 * * * *\"\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: hello\n",
    "            image: busybox\n",
    "            imagePullPolicy: IfNotPresent\n",
    "            command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "            - date; echo Hello from the Kubernetes cluster\n",
    "          restartPolicy: OnFailure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of \n",
    "- workload resources, pods, nodes and jobs.\n",
    "- how to create a .yaml file with the above configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
