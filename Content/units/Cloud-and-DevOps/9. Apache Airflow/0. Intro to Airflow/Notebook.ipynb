{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=https://airflow.apache.org><img src=images/Airflow_logo.png width=400></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, you have learnt a lot of things about data: Extracting the data, Transforming the data, and Loading the data. As mentioned multiple times, this \"ETL\" is the \"core\" of Data Engineering. However, all these operations work in tandem to create a workflow.\n",
    "\n",
    "A workflow is a series of steps that are executed in a specific order. For example, to extract data from a source, transform it, and load it into a target, the following steps are required in order:\n",
    "\n",
    "1. __Step 1:__ __Extract__ data using, for example, the web-scraping skills you have acquired.\n",
    "2. __Step 2:__ __Transform__ the data using, for example, data-cleaning skills in pandas.\n",
    "3. __Step 3:__ __Load__ the data into a target, for example, a database located in your local environment or a remote environment.<br><br>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/WorkFlow1.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflows can also be very helpful when developing a ML model. For example, you might want to train a model with data, but do not exactly know which algorithm to use. In that case, these steps should be followed in order:\n",
    "\n",
    "1. __Extract__ data using, for example, the webscraping skills you learnt\n",
    "2. __Transform__ the data using, for example, the data cleaning skills in pandas\n",
    "3. __Train__ multiple models using the data, and obtain the accuracy of each model\n",
    "4. __Choose__ the model with the highest accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"images/WorkFlow2.png\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow as a workflow manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow is a task-orchestration tool that allows you to define a series of tasks to be executed in a specific order. The tasks can be run in a distributed manner using Airflow's scheduler.\n",
    "\n",
    "In Airflow you use Directed Acyclic Graphs (DAGs) to define a workflow. Each node in the DAG corresponds to a task, and they will be connected to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing airflow would be as simple as running `pip install apache-airflow`, however, that migh cause dependency errors. Thus, in order to prevent those errors, run the following commands in your __`terminal`__ . \n",
    "<details>\n",
    "  <summary><font size=\"+2\">IMPORTANT: For Windows Users</font></summary>\n",
    "  \n",
    "  If you are on Windows make sure to download Ubuntu from the Microsoft store and install it. Then, update everything: `sudo apt update && sudo apt upgrade` and install python3-pip: `sudo apt-get install python3-pip`. Then you can follow the instructions below.\n",
    "\n",
    "</details>\n",
    "\n",
    "At the time of writing, the version of Airflow is 2.1.3, if you are going to use a different version, change it in the following code:\n",
    "\n",
    "```\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "AIRFLOW_VERSION=2.1.3\n",
    "PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are storing the values of Airflow and your Python version in two variables that are going to be used in the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will get the corresponding version of Airflow from their github repo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow is ready for use. It is a good idea to initialise your database at this point. The database will contain metadata and host the DAGs you create:\n",
    "\n",
    "`airflow db init`\n",
    "\n",
    "Enter your credentials:\n",
    "\n",
    "```\n",
    "airflow users create \\\n",
    "    --username <your_username> \\\n",
    "    --firstname <your_firstname> \\\n",
    "    --lastname <your_lastname> \\\n",
    "    --role Admin \\\n",
    "    --email <your_email>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that everything works, run\n",
    "\n",
    "`airflow webserver --port 8080`\n",
    "\n",
    "This will start a new server at your localhost at port 8080. Notably, even if you start the server, your scheduled DAGs will not be monitored. To do this, the scheduler is required. Open a new terminal and run\n",
    "\n",
    "`airflow scheduler`\n",
    "\n",
    "If you receive a Warning message, don't worry, it won't affect your airflow current performance. Now, we are ready to start using the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to your browser and visit `localhost:8080`, your output should be similar to that in the figure:\n",
    "\n",
    "![](images/Airflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above depicts the Airflow UI. Here, you can see the DAGs that have been created, and so far, you will only see some examples and tutorials created by the Airflow team. Let's explore it a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the UI, you can explore the metadata of the DAGs, such as the name (or ID), owner, status of previous runs of the whole DAG or of specific tasks inside the DAG, frequency (in the Schedule column), and run history.\n",
    "\n",
    "For more details, click on the DAG. As an example, we observe the `example_bash_operator` DAG.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow2.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DAG, we can observe the structure, average run time of each task, Gantt chart of the DAG to determine if there are overlapping tasks, details of the DAG, and code that generated the DAG. Since this DAG has not been run, there is no info about previous runs. We can, however, take a look at the code. Before that, we observe the `Graph View` tab, which displays the same information as the `Tree View` tab, but rearranged:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow4.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there are several Nodes, each one representing a task. Additionally, their dependencies, e.g. `run_after_loop`, will not start until all `runme_x` haven't finished. \n",
    "\n",
    "To understand the working mechanism, we run a single task.\n",
    "\n",
    "1. In the Airflow UI, enable the `example_bash_operator` DAG. \n",
    "2. Click the DAG to view its status. You should see two runs, which is because (as we will see later) these examples were set to run 2 days ago, whereas the schedule depicts that it is meant to run once every day. Thus, two runs are appropriate.\n",
    "3. Inside the runs, there are different statuses. In this case, we see 'success' and 'skipped'. Note that they are meant to be skipped.\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow4.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Next, we examine its flow by triggering an event. First, click Auto-refresh to see updates in real-time, and subsequently, click the Play button:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_clip.gif\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, isn't it? We can also see the durantion of each task, and when each run took place. But I will let you explore more on that in the UI. For now, let's take a look at the code. If you click on the `Code` tab, you will see this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example DAG demonstrating the usage of the BashOperator.\"\"\"\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "args = {\n",
    "    'owner': 'airflow',\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='example_bash_operator',\n",
    "    default_args=args,\n",
    "    schedule_interval='0 0 * * *',\n",
    "    start_date=days_ago(2),\n",
    "    dagrun_timeout=timedelta(minutes=60),\n",
    "    tags=['example', 'example2'],\n",
    "    params={\"example_key\": \"example_value\"},\n",
    ") as dag:\n",
    "\n",
    "    run_this_last = DummyOperator(\n",
    "        task_id='run_this_last',\n",
    "    )\n",
    "\n",
    "    # [START howto_operator_bash]\n",
    "    run_this = BashOperator(\n",
    "        task_id='run_after_loop',\n",
    "        bash_command='echo 1',\n",
    "    )\n",
    "    # [END howto_operator_bash]\n",
    "\n",
    "    run_this >> run_this_last\n",
    "\n",
    "    for i in range(3):\n",
    "        task = BashOperator(\n",
    "            task_id='runme_' + str(i),\n",
    "            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\n",
    "        )\n",
    "        task >> run_this\n",
    "\n",
    "    # [START howto_operator_bash_template]\n",
    "    also_run_this = BashOperator(\n",
    "        task_id='also_run_this',\n",
    "        bash_command='echo \"run_id={{ run_id }} | dag_run={{ dag_run }}\"',\n",
    "    )\n",
    "    # [END howto_operator_bash_template]\n",
    "    also_run_this >> run_this_last\n",
    "\n",
    "# [START howto_operator_bash_skip]\n",
    "this_will_skip = BashOperator(\n",
    "    task_id='this_will_skip',\n",
    "    bash_command='echo \"hello world\"; exit 99;',\n",
    "    dag=dag,\n",
    ")\n",
    "# [END howto_operator_bash_skip]\n",
    "this_will_skip >> run_this_last\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dag.cli()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tasks were to print out something to the console, we can confirm that on the Log tab of each task. For example, consider the `also_run_this` task. It is a BashOperator object that will print out `run_id={{ run_id }} | dag_run={{ dag_run }}`. Go to the `Graph View` tab, and click on the `also_run_this` task. In the next window, click `Log`. Observe the output:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_log.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "This was a simple walkthrough of the Airflow UI. \n",
    "We saw that\n",
    "\n",
    "- The workflow is represented by a DAG\n",
    "- Each node in the DAG corresponds to a task\n",
    "- Each DAG has a schedule that sets the frequency of runs\n",
    "- Tasks can be triggered by previous tasks\n",
    "- Each task corresponds to an operator object\n",
    "- We saw BashOperator, which execute a bash script\n",
    "- We saw DummyOperator, which according to the documentation _'Operator that does literally nothing. It can be used to group tasks in a DAG.'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see how to create more operators, for example, a PythonOperator, in the next section. First, let's get some practice defining a DAG with the operators we have seen so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Your First DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, make sure you followed all steps so far. If that's the case, you should have a folder in your home directory named airflow. _Check it by running running the following cell. If no error is thrown, you are good to go_\n",
    "<details>\n",
    "  <summary><font size=\"+1\">IMPORTANT: For Windows Users</font></summary>\n",
    "  \n",
    "  If you are on Windows make sure to check it on the wsl terminal. You can simply type `ls ~` and check if there is a folder\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "import os\n",
    "\n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "assert os.path.isdir(airflow_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside that directory, you have to add a new folder named `dags`. Airflow will look into that directory to check if the DAGs you create through Python. Now, the example DAGs you are using are in your PATH directory, but new DAGs you create should be placed in `~/airflow/dags/`. _You can actually change the path where Airflow will look for new DAGs in the airflow.cfg file_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"+1\">IMPORTANT: For Windows Users</font></summary>\n",
    "  \n",
    "  If you are on Windows, go to the wsl console move to `cd ~/airflow`, and create the dags folder\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "from pathlib import Path\n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "Path(f\"{airflow_dir}/dags\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python files you create must be stored in that folder. The file should contain the DAG with the desired arguments. The arguments can be passed to the context manager and a dictionary.\n",
    "\n",
    "In the context manager, simply define the tasks, don't implement any logical flow. As saw above, tasks are defined by operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from airflow.operators.bash_operator import BashOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "default_args = {\n",
    "    'owner': 'Ivan',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['ivan@theaicore.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'start_date': datetime(2020, 1, 1),\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'end_date': datetime(2022, 1, 1),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'wait_for_downstream': False,\n",
    "    # 'dag': dag,\n",
    "    # 'trigger_rule': 'all_success'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(dag_id='test_dag',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='*/1 * * * *',\n",
    "         catchup=False,\n",
    "         tags=['test']\n",
    "         ) as dag:\n",
    "    # Define the tasks. Here we are going to define only one bash operator\n",
    "    test_task = BashOperator(\n",
    "        task_id='write_date_file',\n",
    "        bash_command='cd ~/Desktop && date >> ai_core.txt',\n",
    "        dag=dag)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example can be found in the `examples` folder, under the name `dag_test.py`. Copy the example to your `dags` folder in your airflow directory.\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"+1\">IMPORTANT: For Windows Users</font></summary>\n",
    "  \n",
    "  If you are on Windows, copy the file using the command line and use the `cp` command to copy the files to `cd ~/airflow/dags`. If you struggle with these commands, and you want to copy everything manually, follow these instructions to find the folder that stores the files from the Ubuntu console.\n",
    "</details>\n",
    "\n",
    "Once the file is in the airflow directory, you can run it by running the following command (if you haven't started the scheduler yet, run `airflow scheduler -D`):\n",
    "\n",
    "`airflow dags unpause test_dag`\n",
    "\n",
    "If you prefer that these DAGs appear in the UI, add them by running the following command:\n",
    "\n",
    "`airflow db init`\n",
    "\n",
    "So you can manage them in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a task; however, since a workflow is composed of more than one task, we add more tasks. If the tasks are specified, they will be executed in sequence, in no specific order (to change how they are executed, change the executor in the airflow.cfg file). However, you can specify their order by setting dependencies between them.\n",
    "\n",
    "Setting dependencies is quite simple. You can specify the tasks and thereafter 'connect' them using the bit-shift operator. For example, to run task `runme_1` after task `runme_0`, do the following:\n",
    "\n",
    "`task_0 >> task_1` or `task_0.set_downstream(task_1)` or `task_1 << task_0` or `task_1.set_upstream(task_0)`.\n",
    "\n",
    "Evidently, there are many ways to set the dependencies. Thus, simply pick the one that works for you.\n",
    "\n",
    "If you intend to run both `task_1` and `task_2` after `task_0`, do the following:\n",
    "\n",
    "`task_0 >> [task_1, task_2]`.\n",
    "\n",
    "If you intend to run `task_2` only after the completion of `task_0` and `task_1`, do the following:\n",
    "```\n",
    "task_0 >> task_2\n",
    "task_1 >> task_2\n",
    "```\n",
    "\n",
    "Finally, it is also possible to set sequential dependencies between tasks. For example, if you intend to run `task_2` after `task_1`, and `task_1` after `task_0`, do the following:\n",
    "\n",
    "`task_0 >> task_1 >> task_2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows a DAG with four tasks:\n",
    "\n",
    "1. date_task: A BashOperator that appends the current date into a file\n",
    "2. add_task: A BashOperator that stages the file created by date_task\n",
    "3. commit_task: A BashOperator that commits the file staged by add_task\n",
    "4. push_task: A BashOperator that pushes the committed file to a remote repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Ivan',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['ivan@theaicore.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'start_date': datetime(2020, 1, 1), # If you set a datetime previous to the curernt date, it will try to backfill\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'end_date': datetime(2022, 1, 1),\n",
    "}\n",
    "with DAG(dag_id='test_dag_dependencies',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='*/1 * * * *',\n",
    "         catchup=False,\n",
    "         tags=['test']\n",
    "         ) as dag:\n",
    "    # Define the tasks. Here we are going to define only one bash operator\n",
    "    date_task = BashOperator(\n",
    "        task_id='write_date',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && date >> date.txt',\n",
    "        dag=dag)\n",
    "    add_task = BashOperator(\n",
    "        task_id='add_files',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git add .',\n",
    "        dag=dag)\n",
    "    commit_task = BashOperator(\n",
    "        task_id='commit_files',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git commit -m \"Update date\"',\n",
    "        dag=dag)\n",
    "    push_task = BashOperator(\n",
    "        task_id='push_files',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git push',\n",
    "        dag=dag)\n",
    "    \n",
    "    date_task >> add_task >> commit_task\n",
    "    add_task >> push_task\n",
    "    commit_task >> push_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the DAG, you can observe the dependencies between the tasks. Definitely, you can set them all in tandem; however, in this case, we will discuss how to set the dependencies in different ways.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Dependencies.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running it, you will find that your repo updates every minute.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_GitHub.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "1. Create a new remote repository in your GitHub account. \n",
    "2. You will eventually use if for storing weather data, so name your repository accordingly.\n",
    "3. Clone the repository to your local machine.\n",
    "4. Copy the DAG file `dag_test_dependencies.py` to the folder `dags` in your local machine.\n",
    "5. Change the file according to the name of your repository and the directory you cloned it to.\n",
    "6. Unpause the DAG by running `airflow dags unpause dag_test_dependencies` or by going to the `DAGS` tab in the UI and clicking on the `dag_test_dependencies` DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you start creating DAGs, you might forget which one are active. Good thing is that airflow has many commands to check your works in the command line. If you type `airflow -h` you can see all comands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: airflow [-h] GROUP_OR_COMMAND ...\n",
      "\n",
      "positional arguments:\n",
      "  GROUP_OR_COMMAND\n",
      "\n",
      "    Groups:\n",
      "      celery         Celery components\n",
      "      config         View configuration\n",
      "      connections    Manage connections\n",
      "      dags           Manage DAGs\n",
      "      db             Database operations\n",
      "      jobs           Manage jobs\n",
      "      kubernetes     Tools to help run the KubernetesExecutor\n",
      "      pools          Manage pools\n",
      "      providers      Display providers\n",
      "      roles          Manage roles\n",
      "      tasks          Manage tasks\n",
      "      users          Manage users\n",
      "      variables      Manage variables\n",
      "\n",
      "    Commands:\n",
      "      cheat-sheet    Display cheat sheet\n",
      "      info           Show information about current Airflow and environment\n",
      "      kerberos       Start a kerberos ticket renewer\n",
      "      plugins        Dump information about loaded plugins\n",
      "      rotate-fernet-key\n",
      "                     Rotate encrypted connection credentials and variables\n",
      "      scheduler      Start a scheduler instance\n",
      "      sync-perm      Update permissions for existing roles and optionally DAGs\n",
      "      version        Show the version\n",
      "      webserver      Start a Airflow webserver instance\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help         show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "airflow -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, you can look at the dags by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'airflow' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "airflow dags list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably observed, in the dependencies, we had to constantly pass the file path to the BashOperator, which is not efficient. To improve efficiency, we define a variable that contains the path to the directory in which the file is stored.\n",
    "\n",
    "Airflow provides a channel to define variables from the UI or the command line. In this case, we will only use the UI. The variables you include in the UI will then be available in the Python code.\n",
    "\n",
    "Hence, open your UI, click on 'Admin' and subsequently on 'Variables'.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_variables.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next window, you can add your variables. You can import a file from your computer or click the `+` sign to add a variable manually.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_variables2.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next window, you can add the name of the variable in the Key and the value in the Value.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_variables3.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking `Save`, the variable will be stored in the Airflow database, and you will be able to use it in your Python code. To access it, you must import the class Variable:\n",
    "```\n",
    "from airflow.models import Variable\n",
    "\n",
    "weather_dir = Variable.get(\"weather_dir\")\n",
    "```\n",
    "\n",
    "Now, you will be able to use that variable in your Python code. If you look at the script in `dag_test_variables.py`, you will see that we are using the variable `weather_dir` to define the path to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, it is possible to run bash commands in each task. However, Airflow is not limited to these commands. You can also create PythonOperators for any task, as long as they are contained in a Python function. As an example, we create a PythonOperator that will extract information about events that occurred 'On this day' in the past.\n",
    "\n",
    "The first thing you have to do is creating a function that uses requests and bs4 to download that information from Wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from os.path import expanduser\n",
    "import requests\n",
    "\n",
    "def get_ul(url: str):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    return soup.find('ul')\n",
    "\n",
    "def get_today_events(url: str, file_dir: str):\n",
    "    ul_soup = get_ul(url)\n",
    "    for li in ul_soup.find_all('li'):\n",
    "        write_file(li.text, file_dir)\n",
    "\n",
    "def write_file(li: str, file_dir: str):\n",
    "    with open(file_dir, 'a') as f:\n",
    "        f.write(li)\n",
    "        f.write('\\n')\n",
    "\n",
    "home = expanduser(\"~\")\n",
    "desktop_dir = os.path.join(home, 'Desktop/test_2.txt')\n",
    "get_today_events('https://en.wikipedia.org/wiki/Wikipedia:On_this_day/Today', desktop_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions can be passed to the PythonOperator as arguments. Here, we will pass the function, `get_today_events`, to the PythonOperator. Note that two fields must be included in the PythonOperator:\n",
    "\n",
    "1. The task id\n",
    "2. The Python function to be executed\n",
    "3. The arguments of the function (optional)\n",
    "\n",
    "Notably, although functions are conventionally situated at the top of code, in this case, they are specified right before the PythonOperator.\n",
    "\n",
    "### Example\n",
    "\n",
    "Create a PythonOperator that will download the events that occurred 'on this day', as shown above. The file will be uploaded to a remote repository.\n",
    "\n",
    "1. Create a new remote repository on GitHub.\n",
    "2. Clone the repository to your local machine.\n",
    "3. Add a variable in the Airflow UI to set the path to the remote repository.\n",
    "4. Create the DAG, where you will call the function. Thereafter, stage the changes, commit them, and push them to the remote repository. The DAG should run daily.\n",
    "5. Move the DAG file to the `dags` folder in your local machine.\n",
    "6. Test the DAG by running `airflow dags test <Name of your DAG>`.\n",
    "\n",
    "You have a small template in the examples folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xcom: Connecting Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a Python script, you may need to pass information from one function to another. When working with tasks in Airflow, you can achieve this using the Xcom feature.\n",
    "\n",
    "Xcom will store the variables in a special database called XCom. You can read more about XCom in the [official documentation](https://airflow.apache.org/concepts.html#xcom). You can store those variables as the tasks are running, and when they are finished, you can retrieve them and pass them to the next tasks. Take a look at the next code (contained in `dag_test_xcom.py`),  passes information between PythonOperators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from random import uniform\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2020, 1, 1)\n",
    "}\n",
    "\n",
    "\n",
    "def training_model(ti):\n",
    "    accuracy = uniform(0.1, 10.0) # In this case, the accuracy is a random number\n",
    "                                  # but when you use it with your ML models, you\n",
    "                                  # can call your real models inside\n",
    "    print(f'model\\'s accuracy: {accuracy}')\n",
    "    ti.xcom_push(key='model_accuracy', value=accuracy)\n",
    "\n",
    "\n",
    "def choose_best_model(ti):\n",
    "    fetched_accuracy = ti.xcom_pull(\n",
    "                            key='model_accuracy',\n",
    "                            task_ids=['training_model_A'])\n",
    "    print(f'choose best model: {fetched_accuracy}')\n",
    "\n",
    "\n",
    "with DAG('test_dag_xcom',\n",
    "         schedule_interval='@daily',\n",
    "         default_args=default_args,\n",
    "         catchup=False) as dag:\n",
    "\n",
    "    downloading_data = BashOperator(\n",
    "        task_id='downloading_data',\n",
    "        bash_command='sleep 3'\n",
    "    )\n",
    "    training_model_task = [\n",
    "        PythonOperator(\n",
    "            task_id=f'training_model_{task}',\n",
    "            python_callable=training_model\n",
    "        ) for task in ['A', 'B', 'C']]\n",
    "\n",
    "    choose_model = PythonOperator(\n",
    "        task_id='choose_model',\n",
    "        python_callable=choose_best_model\n",
    "    )\n",
    "    downloading_data >> training_model_task >> choose_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an example of a use case. Eventually, you will employ it for actual data retrieval. As you can observe, in the functions you create, ti.xcom_push is employed to pass information to the next task, while ti.xcom_pull is employed to retrieve it. As shown in the following graph, the input is passed to each of the models, and their results are passed to a model chooser.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Xcom.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Once you run it, you will see these Xcoms in the UI:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Xcom2.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Finally, in the next window, you should see the results of the Xcoms.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Xcom3.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Observe the names of the Xcoms. There is a name for each of the models that have been run, and there is one that is called `return_value` (In fact, there are many `return_value`s). These Xcoms correspond to the BashOperators that have been created, which _by default_ will push their output, so any task in the DAG can retrieve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When dealing with tasks working in parallel or sequence, you will need to establish a workflow.\n",
    "- Airflow allows you to define your tasks in a way that is easy to understand and maintain.\n",
    "- Airflow orchestrates these tasks using DAGs.\n",
    "- You can define your DAGs in a python script. Each task is defined by an Operator, which can be a PythonOperator, BashOperator, etc.\n",
    "- The Airflow UI allows you to configure how these tasks will run. \n",
    "- The UI can also show the progress of the tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
