{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow and PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow allows you to run SQL queries in your tasks. You can connect to virtually any database, but in this example we will connect to a PosgreSQL database. Connecting to a database is very easy in Airflow, especially because the UI has a nice way to do it.\n",
    "\n",
    "Before you start, you have to install the PosgreSQL client library. To do so, you can run the following command:\n",
    "\n",
    "`pip install apache-airflow-providers-postgres`\n",
    "\n",
    "From now on, Airflow will know how to connect to a postgres database. _If you want to connect to a different database, you can do it by installing a different provider. Check out [this webpage](https://www.astronomer.io/guides/connections) to get more info_\n",
    "\n",
    "As mentioned, the Airflow UI has a nice way to connect to a database. Open your UI, click on 'Admin' and then on 'Connections'.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_SQL.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next window, you can select the connection you want to configure. Take a look at all possibilities Airflow offers! Let's click on the `PostgreSQL` connection. You will see the following:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_SQL2.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the fields with your data, but take into account that `schema` here is the name of your database and `login` is your username. Just for demonstration purposes, we will use the `Pagila` database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are ready to use PostgreSQL from Airflow. In the next example, we are creating a new table with animal names. You will find the same code in `dag_test_sql.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"test_dag_postgre\",\n",
    "    start_date=datetime.datetime(2020, 2, 2),\n",
    "    schedule_interval=\"@once\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    create_pet_table = PostgresOperator(\n",
    "        task_id=\"create_pet_table\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS pet (\n",
    "            pet_id SERIAL PRIMARY KEY,\n",
    "            name VARCHAR NOT NULL,\n",
    "            pet_type VARCHAR NOT NULL,\n",
    "            birth_date DATE NOT NULL,\n",
    "            OWNER VARCHAR NOT NULL);\n",
    "          \"\"\",\n",
    "    )\n",
    "\n",
    "    populate_pet_table = PostgresOperator(\n",
    "        task_id=\"populate_pet_table\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "            INSERT INTO pet VALUES ( 1, 'Max', 'Dog', '2018-07-05', 'Jane');\n",
    "            INSERT INTO pet VALUES ( 2, 'Susie', 'Cat', '2019-05-01', 'Phil');\n",
    "            INSERT INTO pet VALUES ( 3, 'Lester', 'Hamster', '2020-06-23', 'Lily');\n",
    "            INSERT INTO pet VALUES ( 4, 'Quincy', 'Parrot', '2013-08-11', 'Anne');\n",
    "            \"\"\",\n",
    "    )\n",
    "\n",
    "    get_all_pets = PostgresOperator(\n",
    "        task_id=\"get_all_pets\", postgres_conn_id=\"postgres_default\", sql=\"SELECT * FROM pet;\"\n",
    "    )\n",
    "\n",
    "    get_birth_date = PostgresOperator(\n",
    "        task_id=\"get_birth_date\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "            SELECT * FROM pet\n",
    "            WHERE birth_date\n",
    "            BETWEEN {{ params.begin_date }} AND {{ params.end_date }};\n",
    "            \"\"\",\n",
    "        params={'begin_date': '2020-01-01', 'end_date': '2020-12-31'},\n",
    "    )\n",
    "\n",
    "\n",
    "    create_pet_table >> populate_pet_table >> get_all_pets >> get_birth_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the tasks is very similar to defining PythonOperators. The only difference is that you have to include the `postgres_conn_id` in the task definition. Then, the `sql` argument will contain the query, and the params argument will contain the variables you might want to add to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you copy that code to the `dags` folder, you can test it by running `airflow dags test dag_test_sql`. Then, in your pgAdmin, you can see the table `pets`:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_SQL3.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same way you connected to your localhost, you can also connect to your AWS RDS, we are slowly integrating everything from Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything we have done so far can be also done using the command line in an easy manner as well. For example, for changing the configuration of a connection you can run the following command:\n",
    "\n",
    "`airflow connections add <connection_name> \\ --conn-uri \"conn-type://<user>:<password>@<host>:<port>/<database>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another example you can do in the command line is to create a new variable:\n",
    "\n",
    "`airflow variables -s <var_name> <var_value>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the official documentation for more information. \n",
    "\n",
    "With this in mind, you can now start creating your own Airflow DAGs in your EC2 instances without having to worry about the Airflow UI. Just leave it running as shown in the `Cloud Basics` module and you will have a schedule running nonstop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The configuration of Airflow in the UI can be set to connect to different databases such as Postgresql, or even AWS RDS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
