{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial features & curse of dimensionality\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Gradient based methods\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous chapter we've learnt how to train linear regression and what it's good for.\n",
    "\n",
    "Major limitation of linear/logistic regression is that, as the name suggests, __is linear__.\n",
    "\n",
    "> __Most of real life data correlation is non-linear__\n",
    "\n",
    "So our $ X \\rightarrow Y $ function could be something like:\n",
    "- $f(x, y z) = x^2 + y^3 - \\frac{1}{z}$\n",
    "- $f(x) = (x+8)^3 $\n",
    "- $f(x, y) = e^{x} - e^{(x+y)}$\n",
    "\n",
    "Our `boston` regression task is most probably non-linear also, we're gonna go over that soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling more advanced functions\n",
    "\n",
    "We can make more complex models by assuming that they should contain more, different mathematical terms. \n",
    "\n",
    "For example, instead of our model just having a term for $x$, it could also have a term for $x^2$, $x^3$, and so on (that is, it could include higher order terms). We could make all kinds of models by including any kind of mathematical terms such as (but not limited to):\n",
    "- trigonometric terms\n",
    "- exponential terms\n",
    "- gaussian terms\n",
    "\n",
    "Simple linear model: \n",
    "\n",
    "$$\n",
    "y = b + w_1x_1\n",
    "$$\n",
    "\n",
    "Higher order polynomial model of single variable: \n",
    "\n",
    "$$\n",
    "y = b + w_1x + w_2x^2 + w_3x^3\n",
    "$$\n",
    "\n",
    "To do that, we simply __augment__ our train, validation and test sets with polynomial features.\n",
    "    \n",
    "Our X variable would look like this now:\n",
    "\n",
    "![title](images/X_matrix.jpg)\n",
    "    \n",
    "We will train these more complex polynomial models to learn to perform the same task as we did previously.\n",
    "\n",
    "Lets try fitting more complex curves than just a straight line by using a polynomial model for multivariate linear regression. \n",
    "\n",
    "## Loading data\n",
    "\n",
    "We will use some `aicore` utilities not to repeat ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aicore.ml import data\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "\n",
    "def standard_scaler(*datasets):\n",
    "    scaler = preprocessing.StandardScaler().fit(datasets[0])\n",
    "    return [scaler.transform(dataset) for dataset in datasets]\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_validation, y_validation), (X_test, y_test) = data.split(\n",
    "    datasets.make_classification(\n",
    "        n_samples=10000,\n",
    "        n_features=8,\n",
    "        n_informative=4,\n",
    "        n_redundant=2,\n",
    "        n_repeated=1,\n",
    "        n_classes=5,\n",
    "        n_clusters_per_class=3,\n",
    "    )\n",
    ")\n",
    "\n",
    "X_train, X_validation, X_test = standard_scaler(X_train, X_validation, X_test)\n",
    "\n",
    "X_train_poly, X_validation_poly, X_test_poly = standard_scaler(\n",
    "    X_train_poly, X_validation_poly, X_test_poly\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple polynomial features\n",
    "\n",
    "As we have `13` features, our polynomial will become a function which multiplies each feature column-wise.\n",
    "\n",
    "Let's assume we have two features `a` and `b` only. In this case, polynomial combination of power of `2` would be:\n",
    "\n",
    "$$\n",
    "    [1, a, b, a^2, ab, b^2]\n",
    "$$\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's use `sklearn` to make polynomial features for us ([here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) is the documentation).\n",
    "\n",
    "Create `polynomial_datasets` function taking `(degree, *datasets)` as arguments.\n",
    "\n",
    "- First create `PolynomialFeatures` object with specified degree\n",
    "- Return list (via list comprehension) of transformed datasets using `fit_transform` function of `PolynomialFeatures` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def polynomial_datasets(degree: int, *datasets):\n",
    "    polynomial = preprocessing.PolynomialFeatures(degree=degree)\n",
    "    return [polynomial.fit_transform(dataset) for dataset in datasets]\n",
    "\n",
    "\n",
    "X_train_poly, X_validation_poly, X_test_poly = polynomial_datasets(\n",
    "    2, X_train, X_validation, X_test\n",
    ")\n",
    "\n",
    "X_train_poly, X_validation_poly, X_test_poly = standard_scaler(\n",
    "    X_train_poly, X_validation_poly, X_test_poly\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 45)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how our features expanded __drastically__. The higher the degree, the more features you're gonna have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on polynomial and non-polynomial features\n",
    "\n",
    "\n",
    "As previously, we will try how an algorithm works for polynomial and non-polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON-POLYNOMIAL\n",
      "Train accuracy: 0.4841666666666667\n",
      "Test accuracy: 0.4935\n",
      "POLYNOMIAL\n",
      "Train accuracy: 0.64\n",
      "Test accuracy: 0.648\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def check(X_train, y_train, X_test, y_test):\n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Train accuracy: {accuracy_score(y_train, clf.predict(X_train))}\")\n",
    "    print(f\"Test accuracy: {accuracy_score(y_test, clf.predict(X_test))}\")\n",
    "\n",
    "    \n",
    "print(\"NON-POLYNOMIAL\")\n",
    "check(X_train, y_train, X_test, y_test)\n",
    "print(\"POLYNOMIAL\")\n",
    "check(X_train_poly, y_train, X_test_poly, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened?\n",
    "\n",
    "As this task is a little harder simple linear regression is unable to fit our data. If presented with it's non-linear variation, the task becomes a little easier.\n",
    "\n",
    "### Why not degree `100`?\n",
    "\n",
    "We will get to that in next chapters, but too high degree leads to overfitting on training dataset (where the model remembers `train` but is unable to generalize to `test`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "\n",
    "Another important aspect in `deep learning` is:\n",
    "\n",
    "> the higher dimensional the problem, the harder it is to find the solution\n",
    "\n",
    "It becomes both computationally infeasible & it becomes really hard to __find manifolds in high dimensional space__\n",
    "\n",
    "## Manifolds\n",
    "\n",
    "> Manifolds are lower dimensional spaces which describe the data being in higher dimensional space\n",
    "\n",
    "Easiest way to see what manifold is, is looking at the picture below:\n",
    "\n",
    "![](images/manifold.jpg)\n",
    "\n",
    "\n",
    "## Random search\n",
    "\n",
    "Let's assume we train our model by finding random parameters. If we test enough possibilities we can find good approximation (even perfect one in some cases).\n",
    "\n",
    "> Such approach is named random search and is sometimes used when we have no knowledge or way to solve a task instead of tryin \n",
    "\n",
    "To model more complex functions we'll need more complex models.\n",
    "\n",
    "But the time taken for random search scales **exponentially** with the number of parameters. This is because it has to search the whole parameter space, which has as many dimensions as the number of parameters.\n",
    "\n",
    "Imagine our parameters can only take the integer values $0$ or $1$\"\n",
    "\n",
    "- If we have one parameter, the entire parameter space is ${0, 1}$.\n",
    "That is, we have to check the criterion of $2$ possible parameterisations, which is ${(0, 0), (0, 1), (1, 0), (1, 1)}$, which is $4 \\ (=2^2)$ possible parameterisations.\n",
    "- If we have two parameter, the entire parameter space is ${(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)}$.\n",
    "That is, we have to check the criterion of $8 \\ (2^3)$ possible parameterisations.\n",
    "\n",
    "As you should be able to tell from the pattern, the number of parameterisations that are possible is given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    n^d\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This is in the case where parameters can take only integer values, which massively simplifies the problem, and is optimistic because most models parameters can take continuous values.\n",
    "\n",
    "This space is unsearchable by random and becomes increasingly harder to optimize using heuristics like optimizers (though the limit is unknown and models with billions of parameters were successfully optimized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- Play around with polynomial and find the best degree for the presented task on `test` set\n",
    "- Find tasks where polynomial features do more harm than good (__tip__: those should be quite simple tasks and without a lot of data samples)\n",
    "- Check [RBF kernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html) instead of polynomial features. How does it work and does it work better for our task?\n",
    "- Is there a way to generate such functions automatically (learn them from data)? Does such thing exist?\n",
    "- What is [manifold learning](https://scikit-learn.org/stable/modules/manifold.html)? Build some intuition about it\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Polynomial features can give linear models a way to model non-linear relationships\n",
    "- Too high degree may cause overfitting and learning noise instead of real-life relationships"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
