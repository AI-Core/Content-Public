{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Learning objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Understand how classification can be implemented when there are more than 2 classes\n",
    "- Implement a multiclass classifier from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Binary vs Multiclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In binary classification the output must be either `True` or `False` as we already know.\n",
    "\n",
    "Either the example falls into this class, or it doesn't. We have seen that we can represent this by our model having a single output node whose value is forced between 0 and 1, and as such represents probability that the example belongs to the positive class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multiclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center><img width=1000 src=images/binary-class.jpg></p>\n",
    "\n",
    "In the case where we have two nodes to represent true and false, we can think about it as having trained two separate models.\n",
    "\n",
    "Treating `True` and `False` as separate classes with separate output nodes shows us how we can extend this idea to do multiclass classification; \n",
    "\n",
    "> <font size=+2>We simply add more nodes and ensure that their values are positive and sum to one.</font>\n",
    "\n",
    "__Each node is a single `logit` and all of them combined are later passed to `softmax`__\n",
    "\n",
    "<p align=center><img width=1000 src=images/multiclass.jpg></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multiclass vs Multilabel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this course we will not talk about __multilabel__ case, but:\n",
    "\n",
    "> In multilabel problem, each label can exist simultaneously instead of exclusively like in multiclass\n",
    "\n",
    "This might be a single vector where we have `cat` and `dog` on a picture but not a turtle:\n",
    "\n",
    "$$\n",
    "[1, 0, 1]\n",
    "$$\n",
    "\n",
    "> In multiclass, __there is always a single `1` label__, not less, not more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we will also outputs logits, in case of multiclass the only change is __it will be a vector of values__. Each value in the output vector corresponds to certain class.\n",
    "\n",
    "Let's say we would like to classify our input image into one of threes classes: `{dog=0, cat=1, turtle=2}`. Output of our model might look like this:\n",
    "\n",
    "$$\n",
    "    [-5, -3, 2]\n",
    "$$\n",
    "\n",
    "This would be a prediction of class `turtle` as it's value is highest.\n",
    "When we want to get a label from this operation we use [`argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html):\n",
    "\n",
    "> <font size=+2> `argmax` returns __index__ of array entry with __the highest value__ </font>\n",
    "\n",
    "As before, we can transform them into probabilities using...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> <font size=+2>The **softmax function** exponentiates each value in a vector to make it positive and then divides each of them by their sum to normalise them (make them sum to 1). </font>\n",
    "\n",
    "This ensures that the vector then can be interpreted as a probability distribution.\n",
    "\n",
    "<p align=center><img width=1000 src=images/softmax.jpg></p>\n",
    "\n",
    "So, for example, we can replace each variable with some values\n",
    "\n",
    "<p align=center><img width=1000 src=images/softmax_example.jpg></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Differentiating the softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Softmax derivative is different based on the index of element with respect to which we take derivative. \n",
    "- If it is the same as the index of element we applied softmax, the derivative is the equation at the bottom\n",
    "- Otherwise the one above it. \n",
    "\n",
    "<p align=center><img width=1000 src=images/softmax_deriv.jpg></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Properties of softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- increasing the value of any entry decreases the value of all of the others, because the whole vector must always sum to one. \n",
    "- an increase in one input element increases it's corresponding output element exponentially whilst pushing others down, \n",
    "- this means that __it is very easy for the one largest output element to become dominant__\n",
    "- because of that `softmax` is overconfident and there are ways to combat this like `label smoothing`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What does the name \"softmax\" mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- as explained above, usually one input is near `1`, and all others close to `0`. That is, similar to the `argmax` operation mentioned previously but \"soft\" as it can be differentiated.\n",
    "- `argmax` changes abruptly, small difference between two values make it either `0` or `1`. Softmax on the other hand changes gradually when the maximum changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise\n",
    "\n",
    "Let's implement our own softmax function.\n",
    "\n",
    "It should take `x` and divide by `sum` across `axis=1` (as we are normalizing along features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    exponentials = np.exp(x)\n",
    "    return exponentials / exponentials.sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable softmax\n",
    "\n",
    "As seen in `sigmoid` case this version also suffers from numerical instability, check out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-1a709be775cf>:5: RuntimeWarning: overflow encountered in exp\n",
      "  exponentials = np.exp(x)\n",
      "<ipython-input-1-1a709be775cf>:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return exponentials / exponentials.sum(axis=1).reshape(-1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[       nan, 0.        , 0.        ],\n",
       "       [0.01714783, 0.04661262, 0.93623955]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([[1000, 9, 8], [11, 12, 15]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the result is even worse as we get `np.nan` due to overflow. Solution to the problem is to subtract maximum value from each row.\n",
    "\n",
    "### Subtracting, what?\n",
    "\n",
    "As `softmax` works along the horizontal axis (`1`) and all values sum to `1` the only thing that matters with the numbers in certain row is their absolute distance. \n",
    "\n",
    "This means we can divide __any value__ from them and still get the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.92408247e-01, 9.04959183e-04, 6.68679417e-03]]),\n",
       " array([[9.92408247e-01, 9.04959183e-04, 6.68679417e-03]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = np.array([5, -2, 0]).reshape(1, -1)\n",
    "subtracted = original - 6\n",
    "softmax(original), softmax(subtracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to subtract?\n",
    "\n",
    "There is no way to know what is the right `const` value to remove from each row. What if we have `1000` or `1_000_000`? \n",
    "\n",
    "Fortunately, we can find maximum in whole batch of data and simply subtract that.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Implement `softmax` function again, this time a stable version:\n",
    "- subtract `np.max` from `logits` across `1` axis again\n",
    "- return exponential values calculated this way like previously\n",
    "\n",
    "Here is our `stable softmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits - np.max(logits, axis=1).reshape(-1, 1))\n",
    "    return exps / np.sum(exps, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.01714783, 0.04661262, 0.93623955]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([[1000, 9, 8], [11, 12, 15]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "Our targets can be encoded in multiple ways. Usually, we would simply pass class numbers like this (for `5` samples):\n",
    "\n",
    "$$\n",
    "[0, 3, 1, 1, 4]\n",
    "$$\n",
    "\n",
    "We could also do that using one-hot encoding:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&[1, 0, 0, 0, 0]\\\\\n",
    "&[0, 0, 0, 1, 0]\\\\\n",
    "&[0, 1, 0, 0, 0]\\\\\n",
    "&[0, 1, 0, 0, 0]\\\\\n",
    "&[0, 0, 0, 0, 1]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As most of the data works with the first option, we will now code how to transform `labels` into one-hot-encoding and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_one_hot(labels, max_labels: int = None):\n",
    "    if max_labels is None:\n",
    "        max_labels = np.max(labels) + 1\n",
    "    return np.eye(max_labels)[labels]\n",
    "\n",
    "\n",
    "def to_labels(one_hot):\n",
    "    return np.argmax(one_hot, axis=-1)\n",
    "\n",
    "\n",
    "data = np.array([0, 1, 0, 3, 5])\n",
    "to_one_hot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding can also be applied in the inputs, but be careful with using the whole output directly into the model. What problem can you think about this? \n",
    "\n",
    "One hint: look at this sklearn library called preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that one of the arguments we can pass to it is `drop`. Why do you think we want to drop a column from the One Hot Encoder output?\n",
    "\n",
    "<details>\n",
    "  <summary>Click here to see the answer</summary>\n",
    "\n",
    "  When you perform One-Hot encoding, all the columns in the output sum always one. This is because only one column is 1, the rest are 0. Now, the problem with that is that these columns are correlated. \n",
    "  \n",
    "  The problem with correlation in machine learning models is that it can lead to numerically unstable solutions. Check [this page](https://en.wikipedia.org/wiki/Multicollinearity#Consequences) to know more about it\n",
    "  \n",
    "  Therefore, when you perform One-Hot encoding, you should drop one column from the output.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross entropy loss function\n",
    "\n",
    "An appropriate loss function to use for multiclass classification is the __cross entropy loss function__.\n",
    "\n",
    "- It is a __generalization of binary cross entropy loss__ so it will work in binary case as well\n",
    "- BCE (binary cross-entropy) is faster and more stable for binary case so __it should be created separately__ and __used separately__.\n",
    "\n",
    "Like BCE loss, cross entropy uses the same term: __the negative natural log of the output probability__ to penalise outputs exponentially as they stray from the ground truth.\n",
    "\n",
    "> We don't need to simultaneously push down the incorrect class probabilities and push up the correct class probabilities.\n",
    "\n",
    "So if we focus on increasing the correct class likelihood element, then we will implicitly be decreasing the incorrect class likelihood elements.\n",
    "\n",
    "<p align=center><img width=1000 src=images/cross_entropy_loss.jpg></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to use simple linear models?\n",
    "\n",
    "We have seen how to create and use `linear models` for regression and classification.\n",
    "Soon we will meet more powerful models but here is the rough summary of when one should use it in real life:\n",
    "- as a baseline - gives us an overview and \"starting point\" for improvement\n",
    "- when we want easily explaianble model. Each weight shows the impact of a factor onto our target\n",
    "- when we have a lot of features (even more than data point) and we do not want to overfit on data\n",
    "\n",
    "With experience and more models it will become increasingly clear where we should use each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same way you can implement binary classification in sklearn, you can also implement multiclass classification. You can also use logistic regression for this purpose, however, in this case, instead of loading a dataset with two possible outputs, we are loading the `iris` dataset, which has three possible outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    2\n",
       "146    2\n",
       "147    2\n",
       "148    2\n",
       "149    2\n",
       "Name: target, Length: 150, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "display(X)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the dataset has 150 samples, 4 features, and 3 different classes, each one corresponding to a different species of flower.\n",
    "\n",
    "Let's split the dataset into a training set and a test set, and train the logistic regression. One argument that you can pass to the LogisticRegression class is multi_class. By default, it is set to `auto`, which selects `ovr` for binary classification and `multinomial` for multiclass classification. `ovr` means One-vs-Rest, which calculates the probabilities for each pair of classes, like in a binary classification problem. `multinomial` calculates the probability distribution for all the classes, however, it doesn't mean it's always more suitable for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! The iris dataset is a toy dataset, so it's actually not very difficult to train a good model on it. Be suspicious if you get a score of 1.0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- multiclass classification is multiple linear regression stacked together\n",
    "- multiclass classification requires a different loss function (cross entropy)\n",
    "- we can work directly on logits to take predictions by using `argmax`\n",
    "- softmax is a differentiable function that turns a vector of real numbers into a probability distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
