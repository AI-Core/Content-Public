{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_binary_data(m=50): \n",
    "    X = np.random.randn(m) #sample from a normal distribution\n",
    "    X = np.array(sorted(X))\n",
    "    Y = X > 0.2    # return binary vector with true where X above some threshold and false if below\n",
    "    return X, Y #returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', marker='x')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "X, Y = make_binary_data()\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, the output of our model could be any real number, negative or positive, unbounded in magnitude. Here the thing is a little more involved.\n",
    "\n",
    "> In classification, we can interpret the model output as a confidence that the example belongs to a particular class.\n",
    "\n",
    "Which bring us to...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> vector (in binary case a scalar) of unnormalized probabilities in the range $(-\\infty, \\infty)$\n",
    "\n",
    "Maybe you've noticed it is __exactly what we've been outputting with regression__!\n",
    "\n",
    "You know what probability is, so logits are pretty similar, but:\n",
    "- __50% chance (probability=$0.5$) is equal to logit=$0$ .__\n",
    "- Anything below zero is less probable, anything above more\n",
    "\n",
    "Logits are used for classification in order to:\n",
    "- not making unnecessary operation transforming them into probabilities if we are after label (we will see the transformation shortly)\n",
    "- for binary classification every output from our classifier which is greater than `1` is considered `True`, anything below is `False`\n",
    "\n",
    "__Note:__ mathematically, the term logit refers to the log of the odds of a probability and __transforms probability (p below) back into $(-\\infty, \\infty)$ range__.\n",
    "It can be written by the following formula:\n",
    "\n",
    "$$\n",
    "    L = \\ln \\frac{p}{1 - p}\n",
    "$$\n",
    "\n",
    "However, we will use the term logit later to simply refer to the values outputted by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## From logits to probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can do this by applying a **sigmoid** function to our output:\n",
    "\n",
    "<p align=center><img src=images/sigmoid.jpg width=1000></p>\n",
    "\n",
    "> sigmoid squashes logits from $(-\\infty, \\infty)$ to $(0, 1)$ range which we can easily interpret\n",
    "\n",
    "We can also write a function to compute the derivative of the sigmoid function.\n",
    "We will need this to differentiate our loss with respect to the model parameters, as they only affect the loss through the sigmoid.\n",
    "\n",
    "__Note:__ Sigmoid is the inverse function of logit\n",
    "\n",
    "<p align=center><img src=images/sigmoid_deriv.jpg width=1000></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss function for binary classification-  Binary cross entropy (BCE)\n",
    "\n",
    "Basic formula for BCE is:\n",
    "\n",
    "$$\n",
    "    BCE = - (y\\ln\\hat{y} + (1-y)\\ln(1-\\hat{y}))\n",
    "$$\n",
    "\n",
    "We can break it down into parts exclusively:\n",
    "- if the label is `1` $(1-y)\\ln(1-\\hat{y})$ is `0`\n",
    "- if the label is `0` $y\\ln\\hat{y}$ is `0`\n",
    "\n",
    "<p align=center><img src=images/bce.jpg width=1000></p>\n",
    "\n",
    "### Why not calculate only one part per label?\n",
    "\n",
    "Can't we calculate one part for `1` label and the second for `0` label with some kind of `if label == 0` switch? Yes, we could, but sometimes in machine learning & deep learning we use technique called `label smoothing`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label smoothing\n",
    "\n",
    "> Label smoothing changes `{0, 1}` labels into \"soft targets\", for example `{0.1, 0.9}`\n",
    "\n",
    "### Why would we do that?\n",
    "\n",
    "- It is impossible to reach `1` value for sigmoid in practice. Hence, even if our predicted probability is `0.99999` we will still have some loss left\n",
    "- Same thing happens for `0` as it is really hard for the classifier to reach exactly this value\n",
    "\n",
    "### How to do that?\n",
    "\n",
    "- there are many implementations. Usually we subtract from largest labels (`1`) some constant and add it to `0` labels (for binary case)\n",
    "- $\\alpha$ is our __smoothing hyperparameter__, usually around `0.1`\n",
    "\n",
    "### How does it help?\n",
    "\n",
    "- Our classifier is __less confident__ about it's predictions. Due to inherit noise in data it is often desirable (especially for the more complex and powerful models, yes, neural networks are prime example)\n",
    "- Predictions are more smooth and gradual. Instead of having rough jumps from `0.999` probability to `0.0001` probability in another case, our algorithms try to distribute the probability more evenly\n",
    "- Classifier __does not try to \"be sure\" about hard examples so much__. As it has to reach `0.9` (let's say) instead of `1`, `0.8` will be also fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy implementation\n",
    "\n",
    "Let's start with naive implementation, simply follow the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(prediction, label):\n",
    "    return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47407698 0.47407698 0.69314718]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "prediction = sigmoid(np.array([-0.5, 0.5, 0])) \n",
    "labels = np.array([0, 1, 1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as usual, __numerical instability creeps in__. As a rule of thumb, always look suspicious at `exp` and it's inverse function `ln`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([1])\n",
    "labels = np.array([0])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([0])\n",
    "labels = np.array([1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n",
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([0])\n",
    "labels = np.array([0])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n",
      "<ipython-input-2-f3031382e7cc>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([1])\n",
    "labels = np.array([1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- if our model __is very condfident and IS WRONG__ (first two cases) we are left with  `np.log(0)` (which goes towards $-\\infty$, when we take `-` it is simply $\\infty$ we observe). \n",
    "- if our model __is very condfident and IS RIGHT__ (last two cases) we will be left with $ 0 * \\ln{0} $, hence with $ 0 * \\infty $ which is undefined `NaN` (not a number)\n",
    "\n",
    "Can we improve it somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable BCE\n",
    "\n",
    "To stabilize binary cross entropy loss we can mix activation we saw earlier (`sigmoid`) and `binary cross entropy` itself.\n",
    "\n",
    "### Formulation\n",
    "\n",
    "One can derive numerically stable version. It takes some math so a few steps are skipped. You can see the formula that we will use [here](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    BCE & = - (y\\ln\\hat{y} + (1-y)\\ln(1-\\hat{y})) \\\\\n",
    "        & = - (y\\ln(\\frac{1}{1+e^{-x}}) + (1-y)\\ln(1-\\frac{1}{1+e^{-x}}) \\\\\n",
    "        & ... \\\\\n",
    "        & = x - xy + \\ln(1 + e^{-x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Unfortunately this leaves us with $e^{-x}$ once again. After a few math tricks we come to this formula (really, go through it yourself to see some beautiful math tricks):\n",
    "\n",
    "$$\n",
    "    \\max(0, x) - xy + \\ln(1 + e^{-|x|})\n",
    "$$\n",
    "\n",
    "where `x` are logits, `y` are labels\n",
    "\n",
    "You can see no matter the `x` value it will always be negative so `e` can only underflow which is fine for our case (and $\\ln(1 + 0) = 0$). There are also [other formulas](https://discuss.pytorch.org/t/numerical-stability-of-bcewithlogitsloss/8246) but we won't go into details.\n",
    "\n",
    "### Derivative\n",
    "\n",
    "We also have to calculate derivative of `BCEWithLogits` to backpropagate through our model.\n",
    "\n",
    "It is a little cumbersome, but remember you can always use [Wolfram Alpha](https://www.wolframalpha.com/) for such tasks (or it will get you some idea or direction at least).\n",
    "\n",
    "Given that, the derivative is presented as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial BCE}{\\partial x} &= \\sigma(x) - y \\\\\n",
    "    \\frac{\\partial BCE}{\\partial y} &= -x\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This binary cross entropy will work directly on `logits`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally let's build our binary classifier model class\n",
    "\n",
    "We'll use the chain rule to compute the gradient with our previously defined loss function:\n",
    "\n",
    "<p align=center><img src=images/binary_classification.jpg width=1000></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has several different classification algorithms. We will use the `LogisticRegression` class from `sklearn.linear_model`, which is one of the most popular and simple ones. We will see more algorithms eventually, but the procedure for implementing them is very similar.\n",
    "\n",
    "Also, to test it, we will use the breast cancer dataset from `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have already loaded and split our data, let's build and train our model. As with linear regression, we train the instance of the model on the training set and then predict the labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9532163742690059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivanyingx/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the used solver did not converge. This is because we have a very simple dataset, so, as the sklearn documentation states, we can choose a different solver like this:\n",
    "\n",
    "- For small datasets, `liblinear` is a good choice, whereas `sag` and `saga` are faster for large ones;\n",
    "- For multiclass problems, only `newton-cg`, `sag`, `saga` and `lbfgs` handle multinomial loss;\n",
    "- `liblinear` is limited to one-versus-rest schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's use `liblinear` solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, not only the algorithm converged, but also we obtained a better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- The labels for classification problems should be an integer representing the index of the class which that example belongs to\n",
    "- `logits` are unnormalized probability $(-\\infty, \\infty)$\n",
    "- `sigmoid` transforms `logits` into probabilities\n",
    "- Naive implementation of `sigmoid` is numerically unstable, more stable version exists\n",
    "- Binary cross entropy is a new differentiable loss function that can be optimised to solve classification problems\n",
    "- Binary classification can be implemented by having a single boolean integer label for each example, where 1 represents it being a member of that class and 0 represents it not being a member of that class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
