{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting and Adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Learning objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- implement\n",
    "    - your first boosted model - Adaboost\n",
    "- understand\n",
    "    - the ideas behind boosting\n",
    "    - how to apply boosting to decision trees and implement Adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Intro - What is boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Boosting is an ensemble method that combines a sequence of weak classifiers which are fit on successively modified versions of the data, which increasingly prioritise examples misclassified by the previous model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bagging vs boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Not all ensemble methods are designed to regularise the overall model. Boosting is a case where instead, ensembling is used to increase the capacity of the individual models.\n",
    "\n",
    "Like bagging, boosting makes an ensemble of weak learners (models that do barely better than random guessing) to form a single strong learner.\n",
    "Bagging simply combined the predictions of different models that were fit to the same data independently (trained in parallel).\n",
    "Differently, boosting combines the predictions of different models that were fit dependending on how well the previous model performed (trained in sequence).\n",
    "\n",
    "<p align=center><img width=900 src=images/bagging_vs_boosting.jpg></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# AdaBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Boosting algorithms vary in how they adjust the weights of the examples that are samples into each successive bootstrapped dataset, and in how they weight the contribution of each hypothesis to the final prediction.\n",
    "AdaBoost is the most popular boosting algorithm.\n",
    "It is used for classification problems.\n",
    "The name AdaBoost is short for **adaptive boosting**.\n",
    "\n",
    "AdaBoost is a classification algorithm.\n",
    "\n",
    "In AdaBoost, the example labels are coded as $Y=-1$ for examples in the negative class, and $Y=+1$ for examples in the positive class.\n",
    "\n",
    "In AdaBoost, each model is a very weak classification tree with a depth of 1.\n",
    "We call such limited trees \"stumps\".\n",
    "AdaBoost converts many \"weak learners\" into a single \"strong learner\" by combining these stumps.\n",
    "\n",
    "AdaBoost combines the predictions of all of the classifiers to make a prediction by evaluating **the sign** of:\n",
    "\n",
    "<p align=center><img width=900 src=images/adaboost_hypothesis.jpg></p>\n",
    "\n",
    "This is simply the sign of a weighted combination of predictions.\n",
    "\n",
    "If the sign is positive, then the example will be classified as a member of the positive class, otherwise it will be classified as a member of the negative class. Intuitively, this means that the predictions of the models in the boosting sequence push or pull the hypothesis over the point where the decision boundary lies, at zero, with the prediction from each model being scaled by that model's weight, $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### But where do the weights for each model come from?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The models are applied sequentially, and because of their limited capacity, each of them is likely to make some mistake.\n",
    "\n",
    "The error of a model is calculated as:\n",
    "\n",
    "<p align=center><img width=900 src=images/err_l.jpg></p>\n",
    "\n",
    "The weight of the prediction of each model is then computed based on the error rate given by:\n",
    "\n",
    "<p align=center><img width=900 src=images/boosting_model_weight.PNG></p>\n",
    "\n",
    "Large negative model weight = your model sucks.\n",
    "Large positive model weight = your model rocks.\n",
    "Zero model weight = your model is as good as a random guessing.\n",
    "\n",
    "The weights of each example are increased if they were incorrectly classified by the previous model, and decreased if they were already classified correctly.\n",
    "\n",
    "For the first model in the sequence, the importance of classifying each example correctly is equal.\n",
    "That is, we weight the error contribution for each example in the dataset by the same equal amount, $w_i= \\frac{1}{m}$.\n",
    "For the next weighted sample from the dataset, \n",
    "To sample the bootstrapped dataset for the next model in the sequence to be trained on, we set the weight of each example to:\n",
    "\n",
    "<p align=center><img width=900 src=images/boosting_example_weight.jpg></p>\n",
    "\n",
    "Let's consider what this means for a variety of cases:\n",
    "- positive model weight and correct classification: weight of example pushed down\n",
    "- negative model weight and correct classification: weight of example pushed up\n",
    "- positive model weight and incorrect classification: weight of example pushed up\n",
    "- negative model weight and incorrect classification: weight of example pushed down\n",
    "\n",
    "**Note**: even though a model in a certain position in the boosting sequence might not be fit to every example in the dataset, because they may not all be chosen to be a member of the training sample, the for weights **every** example are updated based on whether this model correctly classifies it. \n",
    "If this were not the case, examples that were predicted correctly by early models (which are unlikely to be sampled as training data for any subsequent model), would not have their weights updated later.\n",
    "This means we don't lose sight of the big picture - performing well on all examples - by focusing further and further on only misclassified examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What does this weight calculation do?\n",
    "Most examples may be correctly classified by our very simple weak classifier stumps.\n",
    "It is the edge cases that need extra attention.\n",
    "So sequentially, the importance of examples which are not able to be classified correctly by the previous model are increased and vice versa.\n",
    "Models later in the sequence hence focus on harder to classify examples.\n",
    "As depth increases, the importance of easy to classify examples dimishes and tends to zero.\n",
    "This effectively removes them from the dataset, leaving less examples for the later models to classify.\n",
    "Less examples are separable with a simpler decision boundary.\n",
    "\n",
    "The weighting of each model prediction serves to increase the influence of models that correctly classify examples from the bootstrapped dataset which they are trained on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adaboost algorithm outline\n",
    "\n",
    "- initialise each example weight as $\\frac{1}{m}$\n",
    "- for however many models in your boosting sequence\n",
    "    - make a bootstrapped dataset by taking a sample from the original dataset, weighted by the example weights\n",
    "    - fit the model on this bootstrapped dataset\n",
    "    - compute the proportion of incorrect predictions weighted by corresponding example weights\n",
    "    - use this to compute the model weight\n",
    "    - increase example weight of poorly predicted examples and decrease example weight of well predicted examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download the necessary package to run the next cells\n",
    "!wget \"https://aicore-files.s3.amazonaws.com/Data-Science/data_utils/get_colors.py\" \"https://aicore-files.s3.amazonaws.com/Data-Science/data_utils/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "from utils import get_classification_data, calc_accuracy, visualise_predictions, show_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def encode_labels(labels):\n",
    "    labels[labels == 0] = -1\n",
    "    labels[labels == 1] = +1\n",
    "    return labels\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_layers=20):\n",
    "        self.n_layers = n_layers\n",
    "        self.models = [] # init empty list of models\n",
    "\n",
    "    def sample(self, X, Y, weights):\n",
    "        idxs = np.random.choice(range(len(X)), size=len(X), replace=True, p=weights)\n",
    "        X = X[idxs]\n",
    "        Y = Y[idxs]\n",
    "        return X, Y\n",
    "\n",
    "    def calc_model_error(self, predictions, labels, example_weights):\n",
    "        \"\"\"Compute classifier error rate\"\"\"\n",
    "        diff = predictions != labels\n",
    "        diff = diff.astype(float)\n",
    "        diff *= example_weights\n",
    "        diff /= np.sum(example_weights)\n",
    "        return np.sum(diff)\n",
    "\n",
    "    def calc_model_weight(self, error, delta=0.01):\n",
    "        z = (1 - error) / (error + delta) + delta\n",
    "        return 0.5 * np.log(z)\n",
    "\n",
    "    def update_weights(self, predictions, labels, model_weight):\n",
    "        weights = np.exp(- model_weight * predictions * labels)\n",
    "        weights /= np.sum(weights)\n",
    "        return weights\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        example_weights = np.full(len(X), 1/len(X)) # assign initial importance of classifying each example as uniform and equal\n",
    "        for layer_idx in range(self.n_layers):\n",
    "            model = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "            bootstrapped_X, bootstrapped_Y = self.sample(X, Y, example_weights)\n",
    "            model.fit(bootstrapped_X, bootstrapped_Y)\n",
    "            predictions = model.predict(X) # make predictions for all examples\n",
    "            model_error = self.calc_model_error(predictions, Y, example_weights)\n",
    "            model_weight = self.calc_model_weight(model_error)\n",
    "            model.weight = model_weight\n",
    "            self.models.append(model)\n",
    "            example_weights = self.update_weights(predictions, Y, model_weight)\n",
    "            # print(f'trained model {layer_idx}')\n",
    "            # print()\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            prediction += model.weight * model.predict(X)\n",
    "        prediction = np.sign(prediction) # comment out this line to visualise the predictions in a more interpretable way\n",
    "        return prediction\n",
    "\n",
    "    def __repr__(self):\n",
    "        return json.dumps([m.weight for m in self.models])\n",
    "        return json.dumps([\n",
    "            {\n",
    "                'weight': model.weight\n",
    "            }\n",
    "            for model in self.models\n",
    "        ], indent=4)\n",
    "\n",
    "X, Y = get_classification_data(sd=1)\n",
    "Y = encode_labels(Y)\n",
    "adaBoost = AdaBoost()\n",
    "adaBoost.fit(X, Y)\n",
    "predictions = adaBoost.predict(X)\n",
    "print(f'accuracy: {calc_accuracy(predictions, Y)}')\n",
    "visualise_predictions(adaBoost.predict, X, Y)\n",
    "show_data(X, Y)\n",
    "print(adaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.add_subplot(211)\n",
    "X, Y = get_classification_data(variant='circles')\n",
    "\n",
    "for i in range(20):\n",
    "    adaBoost = AdaBoost(n_layers=i)\n",
    "    adaBoost.fit(X, Y)\n",
    "    predictions = adaBoost.predict(X)\n",
    "    print(f'model {i}')\n",
    "    print(f'accuracy: {calc_accuracy(predictions, Y)}')\n",
    "    print(f'weights: {[ round(m.weight, 2) for m in adaBoost.models]}')\n",
    "    visualise_predictions(adaBoost.predict, X, Y)\n",
    "    # show_data(X, Y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "adaBoost = sklearn.ensemble.AdaBoostClassifier()\n",
    "adaBoost.fit(X, Y)\n",
    "predictions = adaBoost.predict(X)\n",
    "calc_accuracy(predictions, Y)\n",
    "visualise_predictions(adaBoost.predict, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Boosting is a technique where we sequentially adjust the prediction from a model by shifting it by proportionally to the prediction from the following model in the sequence\n",
    "- AdaBoost is an \"adaptive\" boosting algorithm\n",
    "- The weight for \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('main': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   },
   "name": "Python 3.8.6 64-bit ('main': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
