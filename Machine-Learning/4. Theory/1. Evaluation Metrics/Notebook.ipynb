{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Evaluation metrics: why do we need them?](#Evaluation-metrics:-why-do-we-need-them?)\n",
    "- [Classification Metrics](#Classification-Metrics)\n",
    "- [Regression Metrics](#Regression-Metrics)\n",
    "- [Extra: ROC & AUC](#Extra:-ROC-&-AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are they?\n",
    "\n",
    "> __Evaluation metrics__, as the name suggests, are metrics used to measure the performance of a model or algorithm. \n",
    "\n",
    "There are tons of evaluation metrics out there. Certain metrics can only be used for certain types of models and different metrics can be used to evaluate different aspects of performance. \n",
    "\n",
    "Therefore, the optimal metric will vary depending on:\n",
    "- your model\n",
    "- your data\n",
    "- your end goal\n",
    "\n",
    "## Why do we need them?\n",
    "\n",
    "Why do we need these metrics in the first place? Well, building models works on a constructive feedback principle. \n",
    "\n",
    "We build a model, get feedback from metrics, make improvements and carry on until we have reached our desired performance. \n",
    "\n",
    "By evaluating our model over multiple (appropriate) metrics, we can:\n",
    "- ensure that it is robust\n",
    "- tune it in multiple ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isn't loss enough?\n",
    "\n",
    "> loss is rarely taken as main metric in machine learning tasks\n",
    "\n",
    "Loss allows us to learn things we are really after and often works as a proxy for our real goal.\n",
    "\n",
    "> always check the loss, but do not rely only on it\n",
    "\n",
    "Human assesment of performance is still needed, sometimes loss can be abruplty large and yet the model works great for our task (GANs would be an example)\n",
    "\n",
    "On other occasions loss is minimal but the model is not doing what we wanted (reinforcement learning and wrong reward function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "In any classification problem, we aim to predict the category of a given observation based on the general properties of a training data set. \n",
    "\n",
    "In this context, the simplest way to measure performance, whether with binary or multiclass classification, is to measure the number of correct predictions out of all the whole dataset.\n",
    "\n",
    "> Accuracy - percentage of how many predictions were accurate out of all predictions\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Create `accuracy` function which takes the `reduction` function (set it with default value `= np.mean`) and apply it on `vector` which is `labels` equal to `targets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def accuracy(labels, targets, reduction=np.mean):\n",
    "    return reduction(labels == targets)\n",
    "\n",
    "\n",
    "targets = np.random.randint(0, 10, size=100)\n",
    "labels = np.random.randint(0, 10, size=100)\n",
    "\n",
    "accuracy(labels, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traits of accuracy\n",
    "\n",
    "- can be calculated the same way for any classification task\n",
    "- the higher the better\n",
    "\n",
    "## Problems with accuracy\n",
    "\n",
    "Accuracy, although helpful, is useless in some cases.\n",
    "\n",
    "Let's imagine our dataset has `10` positive labels and `90` negative. If we always predict negative labels, we get `90%` accuracy.\n",
    "\n",
    "Let's go over that in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.concatenate((np.zeros(90), np.ones(10)))\n",
    "predictions = np.zeros(100)\n",
    "\n",
    "accuracy(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us we need to monitor other metrics as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "If we consider out of the two categories one to be 'Positive' and the other to be 'Negative', we can derive evaluation metrics that measure more specific aspects of performance by categorizing the prediction outcome under the following four categories:\n",
    "\n",
    "- __True positive:__ Where the model predicts the label to be 'Positive' and the true label is 'Positive'\n",
    "- __True negative:__ Where the model predicts the label to be 'Negative' and the true label is 'Negative'\n",
    "- __False positive:__ Where the model predicts the label to be 'Positive' and the true label is 'Negative'\n",
    "- __False negative:__ Where the model predicts the label to be 'Negative' and the true label is 'Positive'\n",
    "\n",
    "These outcomes can be displayed in tabular form, in what is known as a __confusion matrix,__ as shown below:\n",
    "\n",
    "<p align=center><img src=images/confusion-matrix.png width=600></p>\n",
    "\n",
    "[Source](https://glassboxmedicine.com/2019/02/17/measuring-performance-the-confusion-matrix/)\n",
    "\n",
    "> The values that go in each cell can either be the absolute frequency of that class (e.g. actual number of false positives), or the normalized value (false positives in `[0, 1]` range)\n",
    "\n",
    "Usually, we go with the first case, though the second one might be useful to compare proportions of each cell.\n",
    "\n",
    "From this grouping of the possible outcomes of a binary prediction, we can come up with useful metrics which we are about to go over. Let's start with `true positive` calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True positive\n",
    "\n",
    "> True positive for binary case is when model predicts `true` and the label is `true`\n",
    "\n",
    "## Example\n",
    "\n",
    "Code `true_positive` function taking `predictions` and `targets` arguments \n",
    "\n",
    "- Use `astype(bool)` on `labels` and `targets`\n",
    "- Negate any of `labels` / `targets` if needed\n",
    "- Return `np.sum` with `boolean` and (`&` for numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We assume they are binary\n",
    "def true_positive(labels, targets):\n",
    "    return np.sum(labels.astype(bool) & targets.astype(bool))\n",
    "\n",
    "true_positive(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True negative\n",
    "\n",
    "> True negative for binary case is when model predicts `false` and the label is `false`\n",
    "\n",
    "## Example\n",
    "\n",
    "Code `true_negative` function taking `predictions` and `targets` arguments \n",
    "\n",
    "- Use `astype(bool)` on `labels` and `targets`\n",
    "- Negate any of `labels` / `targets` if needed\n",
    "- Return `np.sum` with `boolean` and (`&` for numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We assume they are binary\n",
    "def true_negative(labels, targets):\n",
    "    return np.sum(~labels.astype(bool) & ~targets.astype(bool))\n",
    "\n",
    "true_negative(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positive\n",
    "\n",
    "> False positive for binary case is when model predicts `true` and the label is `false`\n",
    "\n",
    "## Example\n",
    "\n",
    "Code `false_positive` function taking `predictions` and `targets` arguments \n",
    "\n",
    "- Use `astype(bool)` on `labels` and `targets`\n",
    "- Negate any of `labels` / `targets` if needed\n",
    "- Return `np.sum` with `boolean` and (`&` for numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We assume they are binary\n",
    "def false_positive(labels, targets):\n",
    "    return np.sum(~labels.astype(bool) & targets.astype(bool))\n",
    "\n",
    "false_positive(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False negative\n",
    "\n",
    "> False positive for binary case is when model predicts `false` and the label is `true`\n",
    "\n",
    "## Example\n",
    "\n",
    "Code `false_negative` function taking `predictions` and `targets` arguments \n",
    "\n",
    "\n",
    "- Use `astype(bool)` on `labels` and `targets`\n",
    "- Negate any of `labels` / `targets` if needed\n",
    "- Return `np.sum` with `boolean` and (`&` for numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We assume they are binary\n",
    "def false_negative(labels, targets):\n",
    "    return np.sum(labels.astype(bool) & ~targets.astype(bool))\n",
    "\n",
    "false_negative(predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing confusion matrix\n",
    "\n",
    "Given the above, we can use `sklearn` `plot_confusion_matrix` to easily see how confusion matrix looks for `breast_cancer` dataset.\n",
    "\n",
    "Split dataset into `train` & `test` only, fit on train, evaluate prediction on test and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# Loading in breast cancer data\n",
    "X, Y = datasets.load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Confusion Matrix:\n",
      "Normalized Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff6a8d011f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "# Fitting logistic regression model to the data\n",
    "myLogisticModel = LogisticRegression(random_state=0, max_iter=10000).fit(X_train, Y_train)\n",
    "y_hat = myLogisticModel.predict(X_test)\n",
    "\n",
    "# Displaying Confusion matrix\n",
    "print(\"True Confusion Matrix:\")\n",
    "plot_confusion_matrix(myLogisticModel, X_test, Y_test, display_labels=[\"benign\", \"malignant\"])\n",
    "print(\"Normalized Confusion Matrix:\")\n",
    "plot_confusion_matrix(myLogisticModel, X_test, Y_test, normalize='pred', display_labels=[\"benign\", \"malignant\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy again\n",
    "\n",
    "One can define accuracy using confusion matrix as well\n",
    "\n",
    "$$ \n",
    "\\text{Acc} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "Let's use `sklearn` to calculate accuracy metric and compare with our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (our): 0.9590643274853801\n",
      "Accuracy (scikit-learn) 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy (our):\", accuracy(Y_test, y_hat))\n",
    "print(\"Accuracy (scikit-learn)\", accuracy_score(Y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our first evaluation metric, our model seems to have a good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Precision__ is another commonly used evaluation metric.\n",
    "\n",
    "> Precision is a ratio of correctly predicted positives to the total number of predicted positives\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}$$\n",
    "\n",
    "> Perfect recall is when we have no false positives. So it is a useful metric to consider when false positives are costly.\n",
    "\n",
    "Precision is the answer to the question: \n",
    "\n",
    "> When our model predicts TRUE, how often is it correct?\n",
    "\n",
    "Or in context:\n",
    "\n",
    "> out of all the times we predicted the brain cancer to be malignant, how many times was it actually malignant? \n",
    "\n",
    "It is an important measure to have to ensure we evaluate the performance of our model appropriately. \n",
    "\n",
    "For instance, if we are building a system to predict if we should decrease the credit limit on a particular account, we want to be very sure about our prediction or it may result in customer dissatisfaction. In this context, precision is a very relevant measure of performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Thought experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider our current dataset, and assume that `560` of the patients have a benign tumour and 9 patients have a malignant tumour. \n",
    "\n",
    "If we create a model that always predicts the tumour to be benign (negative), we get:\n",
    "- accuracy of $\\frac{560}{569} = 98.4\\%$, which suggests that our model performed well. We know that the model designed is poor, which once again reflects the fact that accuracy is a poor metric under a non-symmetric distribution for our data. What would the precision in this case be? Since we have no true positives nor false positives, our precision is undefined (equation divided by zero), which highlights the poor performance of the model. \n",
    "\n",
    "__Just as with accuracy, we want precision to be as close to 1 as possible.__\n",
    "\n",
    "Below, we compute the precision of our model using `numpy` and the in-built function in scikit-learn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Code `precision` function taking `(labels, predictions)` arguments (remember to cache `true_positive` value in order not to calculate them twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (Python): 0.9902912621359223\n",
      "Precision (scikit-learn) 0.9902912621359223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Computing precision\n",
    "def precision(labels, predictions):\n",
    "    tp = true_positive(labels, predictions)\n",
    "    return tp / (tp + false_positive(labels, predictions))\n",
    "\n",
    "print(\"Precision (Python):\", precision(Y_test, y_hat))\n",
    "print(\"Precision (scikit-learn)\", precision_score(Y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall\n",
    "\n",
    "Another useful evaluation metric for classification models is the __recall__, also known as __sensitivity__: \n",
    "\n",
    "> Recall is the ratio of the correctly predicted positives to the total number of positives in the dataset. It is given by the following equation:\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}$$\n",
    "\n",
    "> Perfect recall is when we have no false negatives, so it is a useful metric to consider when false negatives are costly.\n",
    "\n",
    "Recall is the answer to the question:\n",
    "\n",
    "> What proportion of the true labelled examples does the model correctly predict?\n",
    "\n",
    "Or in context:\n",
    "\n",
    "> Of all the people who had a malignant brain tumor, how many did the model correctly identify?\n",
    "\n",
    "While precision measures how well our model deals with false positives, recall measures how well it deals with false negatives! \n",
    "\n",
    "> Together, accuracy, recall and precision are robust metrics for performance evaluation. \n",
    "\n",
    "It is a useful measure to consider when false negatives are more costly than false positives. For instance, it is probably better that a tumour is classified as malignant even though it is not, as it will lead to further examination, than if a malignant tumour were to be classified as benign, leading to a sick person not receiving treatment.\n",
    "\n",
    "## Thought experiment\n",
    "\n",
    "Consider the same extreme example from earlier with 560 benign tumours and 9 malignant tumours. \n",
    "\n",
    "What if now, we create a model that always predicts the tumour to be malignant (positive)?\n",
    "- accuracy is now given by $\\frac{9}{569} = 2\\%$\n",
    "- precision is given by $\\frac{9}{569} = 2\\%$\n",
    "- recall now is given by $\\frac{9}{9} =100\\%$. \n",
    "\n",
    "In the previous though-experiment, going with only accuracy would have led us to misjudge the performance of the model, just as now using only recall would have the same effect.\n",
    "\n",
    "## Example\n",
    "\n",
    "Code `recall` function taking `(labels, predictions)` arguments (remember to cache `true_positive` value in order not to calculate them twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (Python): 0.9444444444444444\n",
      "Recall (scikit-learn) 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def recall(labels, predictions):\n",
    "    tp = true_positive(labels, predictions)\n",
    "    return tp / (tp + false_negative(labels, predictions))\n",
    "\n",
    "print(\"Recall (Python):\", recall(Y_test, y_hat))\n",
    "print(\"Recall (scikit-learn)\", recall_score(Y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which, once again, reflects that our model exhibits a strong performance. Using recall and precision in parallel is already a strong way of evaluation our model on its own. \n",
    "\n",
    "> However, as they are two separate measures, we cannot use them directly to compare the performance of one model with another\n",
    "\n",
    "so instead we must come up with a metric that accounts for both the recall __AND__ precision of our model simultaneously. To do that, we'll introduce a new metric called the F1 score.\n",
    "\n",
    "## But firstly, a recap\n",
    "\n",
    "<p align=center><img src=images/precision-recall-side-by-side.png width=600></p>\n",
    "\n",
    "<p align=center><img src=images/precision_recall.png width=500></p>\n",
    "\n",
    "[Source](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score\n",
    "\n",
    "The $\\mathbf{F_{1}}$ __score__, also known as the __F-score__ or __F-measure,__ is a metric that takes what is known as the harmonic mean of our precision and recall. \n",
    "\n",
    "Therefore by maximising the F1-score, we are accounting for both precision AND recall simultaneously. It is given as follows:\n",
    "\n",
    "$$\n",
    "F_{1} = 2\\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision + recall}} = \n",
    "\\frac{\\text{TP}}{\\text{TP} + \\frac{1}{2}(\\text{FP} + \\text{FN})}\n",
    "$$\n",
    "\n",
    "Given this measure, we have a robust measure of the performance of our model, which we can use to determine what aspects of our model are lacking, and which models outperforms the others. An extension of the $F_{1}$ score is known as the $F_{\\beta}$ score, which for a given value for $\\beta$ is given as follows:\n",
    "\n",
    "$$\n",
    "F_{\\beta} = (1+\\beta^{2})\\frac{\\text{precision} \\cdot \\text{recall}}{(\\beta^{2}\\cdot\\text{precision) + recall}}\n",
    "$$\n",
    "\n",
    "While we will not implement this version of the metric in this class, it can be useful as it enables you to give different weightings to the two metrics depending on the chosen parameter value for $\\beta$. \n",
    "\n",
    "Depending on the context under which the model will be applied, you will need to optimise for different metrics. For example, when aiming to diagnose whether someone's breast cancer is:\n",
    "- malignant (positive)\n",
    "- benign (negative)\n",
    "\n",
    "it is much better to misdiagnose it as malignant (`False Positive`), which would result in further testing, than to misdiagnose it as benign (`False Negative`), and not treat someone with a deadly condition. In this context, you would choose to minimise the number of false negatives, hence give a higher weighting to your recall than your precision.\n",
    "\n",
    "Below we implement the $F_{1}$ score with Python as well as with the in-built scikit-learn function.\n",
    "\n",
    "## Example\n",
    "\n",
    "Code `f1` function taking `(labels, predictions)` arguments (remember to cache `precision` and `recall`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (Python): 0.966824644549763\n",
      "F1 score (scikit-learn) 0.966824644549763\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Computing F1 score\n",
    "def f1(labels, predictions):\n",
    "    p = precision(labels, predictions)\n",
    "    r = recall(labels, predictions)\n",
    "    return 2 * (p*r) / (p+r)\n",
    "\n",
    "print(\"F1 score (Python):\", f1(Y_test, y_hat))\n",
    "print(\"F1 score (scikit-learn)\", f1_score(Y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the four metrics we have used, our model exhibits a relatively high performance across all the different metrics, which reflects the robustness of the model.\n",
    "\n",
    "But now some of you may be thinking: \"How does any of this apply to the case of multiclass classification?\". Well, you are in luck, as the same framework as we have seen above can also be applied to non-binary cases. We will demonstrate this using the iris dataset from scikit-learn, which we upload below and fit to a multiclass logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Loading iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a logistic regression model to the iris dataset\n",
    "model = LogisticRegression(random_state=0, max_iter=10000).fit(X_train, Y_train)\n",
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to be able to expand upon our confusion matrix so that it accounts for multiple classes rather than two. \n",
    "\n",
    "> We do this by treating each individual class as a simple case of binary classification\n",
    "\n",
    "Thanks to the in-built tools in scikit-learn, we are able to plot the confusion matrix of our iris dataset and corresponding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[16  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  0 11]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of confusion matrix of iris dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "class_names = iris.target_names\n",
    "\n",
    "title = \"Confusion matrix\"\n",
    "disp = plot_confusion_matrix(\n",
    "    model, X_test, Y_test, display_labels=class_names, cmap=plt.cm.Blues\n",
    ")\n",
    "disp.ax_.set_title(title)\n",
    "print(title)\n",
    "print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the confusion matrix as a visual-aid, we can __'binarize'__ the problem and treat each class as a binary classification problem. For instance, we can compute the precision when classifying an observation as 'versicolor' or not as $\\frac{17}{17} = 100\\%$, to be $\\frac{16}{16} = 100\\%$ for 'setosa' and to be $\\frac{11}{11 + 1} = 91.7\\%$ for 'virgina'. \n",
    "\n",
    "Using these three metrics we can compute the average of the individual precisions to obtain the precision of the multiclassification model! We can generalize this concept and apply to the other metrics encountered. This is the simplest form of computing the multiclass precision and is known as __macro averaging,__ where all metrics have the same weight. For other forms of metric averaging, look [here](https://scikit-learn.org/stable/modules/model_evaluation.html). If we consider a multiclassification model with $N$ possible classes:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{correct predictions}}{\\text{all predictions}}$$\n",
    "\n",
    "$$\\text{Precision} = \\frac{1}{N}\\sum_{i=i}^{N}(\\text{Binary Precision})_{i} $$\n",
    "\n",
    "$$\\text{Recall} = \\frac{1}{N}\\sum_{i=i}^{N}(\\text{Binary Recall})_{i} $$\n",
    "\n",
    "The $F_{1}$ score metric can still be computed in terms of the recall and precision. Below we compute the four metrics encountered for our 'irisLogisticModel'. So that this does not become a 'black-box' concept for you, we have computed by hand the precision of our model based on the values on our confusion matrix.\n",
    "\n",
    "$$\\text{Precision} = \\frac{1}{N}\\sum_{i=i}^{N}(\\text{Binary Precision})_{i} = \\frac{1 + 1 + 0.917}{3} = 0.972 $$\n",
    "\n",
    "> In `sklearn` multiclass and binary metrics work as a single function/class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n",
      "Precision: 0.9722222222222222\n",
      "Recall: 0.9814814814814815\n",
      "F1 score: 0.975983436853002\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(Y_test, y_hat))\n",
    "print(\"Precision:\", precision_score(Y_test, y_hat, average=\"macro\"))\n",
    "print(\"Recall:\", recall_score(Y_test, y_hat, average=\"macro\"))\n",
    "print(\"F1 score:\", f1_score(Y_test, y_hat, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics\n",
    "\n",
    "Now that you are familiar with classification metrics, we can go over regression problems, which deal with numerical quantities rather than categorical ones.\n",
    "\n",
    "This simple fact means that __the same evaluation metrics used in classification cannot be used for regression models__. In classification metrics, we base all metrics on binary concepts such as 'correct' and 'incorrect', then binarize multiclass problems so that the same framework applies. But this does not work with regression models. \n",
    "\n",
    "## Example\n",
    "\n",
    "For instance, under a classification frame of mind, if we predict an output of 3.29742 and the true label is 3.29741, this would be classified as incorrect. What is even worse is that other predictions that are much further away would have the same categorisation. \n",
    "\n",
    "Therefore, with regression metrics, __we do not focus on whether something is correct or incorrect, but rather focus on how similar to the true label our prediction is.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13) (354,) (152, 13) (152,)\n"
     ]
    }
   ],
   "source": [
    "# Loading boston dataset\n",
    "X, Y = datasets.load_boston(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=0\n",
    ")\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting linear regression model\n",
    "myLinearModel = LinearRegression().fit(X_train, Y_train)\n",
    "y_hat = myLinearModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE), Root Mean Squared Error (RMSE) & Mean Absolute Error (MAE)\n",
    "\n",
    "As seen previously, one metric that can be used to measure the performance of a regression model is the __mean square error (MSE)__.\n",
    "\n",
    "We are interested in how much the prediction deviates from the label, not whether it's larger or smaller than the label, thus by squaring the errors we ensure that all terms have the same sign and the sum of any two terms can only increase our metric value. \n",
    "\n",
    "However, because it is measured in terms of the square of the original unit, a more commonly used evaluation metric is the __root mean square error (RMSE)__, which is the square root of the MSE. This is now measured in the same units as originally and thus becomes easier to interpret. They are given as follows for $N$ examples:\n",
    "\n",
    "$$MSE = \\frac{1}{N}\\sum_{i=1}^{N}(y_{i} - \\hat{y}_{i})^{2}$$\n",
    "$$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_{i} - \\hat{y}_{i})^{2}}$$\n",
    "\n",
    "## Example\n",
    "\n",
    "Code `MSE` and `RMSE` functions taking `(predicted, targets)` arguments.\n",
    "\n",
    "__Remember to use `MSE` when computing `RMSE`!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Python): 0.022222222222222223\n",
      "MSE (scikit-learn): 0.022222222222222223\n",
      "\n",
      "RMSE (Python): 0.14907119849998599\n",
      "RMSE (scikit-learn): 0.14907119849998599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Computing MSE and RMSE\n",
    "def MSE(targets, predicted):\n",
    "    return np.mean(np.square(targets - predicted))\n",
    "\n",
    "\n",
    "def RMSE(targets, predicted):\n",
    "    return np.sqrt(MSE(targets, predicted))\n",
    "\n",
    "\n",
    "print(\"MSE (Python):\", MSE(Y_test, y_hat))\n",
    "print(\"MSE (scikit-learn):\", mean_squared_error(Y_test, y_hat))\n",
    "print()\n",
    "print(\"RMSE (Python):\", RMSE(Y_test, y_hat))\n",
    "print(\"RMSE (scikit-learn):\", mean_squared_error(Y_test, y_hat, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another deviation from the MSE and RMSE is known as the __mean absolute error (MAE)__, and is, simply put, the mean of the absolute value of the differences between every prediction and label.\n",
    "\n",
    "It is the most intuitive metric out of the three because all it means is \n",
    "\n",
    "> on average, what is the size of our deviation from the true values\n",
    "\n",
    "Below, we show how it is computed with both Python and scikit-learn.\n",
    "\n",
    "## Example\n",
    "\n",
    "Code `MAE` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Python): 0.022222222222222223\n",
      "MAE (scikit-learn): 0.022222222222222223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# Computing MAE for our Linear Model\n",
    "def MAE(targets, predicted):\n",
    "    return np.mean(np.abs(targets - predicted))\n",
    "\n",
    "\n",
    "print(\"MAE (Python):\", MAE(Y_test, y_hat))\n",
    "print(\"MAE (scikit-learn):\", mean_absolute_error(Y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE vs MSE\n",
    "\n",
    "So which is better: RMSE or MAE? The answer is that it depends on the context. \n",
    "\n",
    "- Since we square the errors when computing the RMSE, we are automatically giving a higher weight to larger errors\n",
    "- MAE only takes their absolute value (so the difference is the same no matter the magnitude)\n",
    "\n",
    "Consider the dummy example where we are handed a list of 10 errors below, where 2 of them are far larger than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  38.24\n",
      "RMSE:  86.32495583549407\n"
     ]
    }
   ],
   "source": [
    "# Dummy example\n",
    "errors = np.array([1.0, -2.5, -1.2, 253.4, 5.3, -6.9, -3.2, -100.9, 4.7, 3.3])\n",
    "print(\"MAE: \", np.mean(np.abs(errors)))\n",
    "print(\"RMSE: \", np.sqrt(np.mean(np.square(errors))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this dummy example, we can see that larger errors will make the RMSE much larger than MAE. \n",
    "\n",
    "In fact, the larger our errors, the larger the RMSE will be in relation to the MAE.\n",
    "\n",
    "> While the MAE is much more intuitive than the RMSE, the RMSE is quite useful for punishing the model for larger errors. \n",
    "\n",
    "Nowadays, since these calculations are not done by hand, there is often no reason why we cannot use both evaluation metrics in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-squared (Coefficient of Determination)\n",
    "\n",
    "The __R-squared ($\\mathbf{R^{2}}$)__ is the the proportion of variance in the output that we are able to predict from the input with our model. \n",
    "\n",
    "This evaluation metric is important as it tackles a different aspect of performance from its MSE, RMSE and MAE counterparts. \n",
    "\n",
    "> We are interested in how much of the variance in the labels is explained by our model as the variation in our data is representative of the information it contains!\n",
    "\n",
    "If our model cannot account for the variance in our data, it is not painting the whole picture!\n",
    "\n",
    "So how can we calculate the $R^{2}$ of our model? The $R^{2}$ is a percentage of the variance from the data that is explained by our model. \n",
    "1. compute the variance of the model, which is equal to how much much our data deviates from the simplest possible model: the sample mean\n",
    "2. compute the mean squared error, which tells us how much the data deviates from our model. We then use the following relationship:\n",
    "\n",
    "$$\n",
    "R^{2} = 1 - \\frac{MSE}{Var(y)} = 1 - \\frac{\\sum_{i=1}^{N}(y - \\hat{y})^{2}}{\\sum_{i=1}^{N}(y - \\bar{y})^{2}}\n",
    "$$\n",
    "\n",
    "Now we have a measure that will give us 0 if the model explains as much about the variability of the model as the sample mean, and 1 if it explains 100% of the variability in our data. Below we compute the $R^{2}$ using Python as well as with the in-built scikit-learn functions for our linear model.\n",
    "\n",
    "## Example\n",
    "\n",
    "Code `R2` function. \n",
    "\n",
    "__Remember to use `MSE` we came up with previously!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 (Python): 0.9621848739495799\n",
      "R2 (scikit-learn): 0.9621848739495799\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Computing R2\n",
    "def R2(targets, predicted):\n",
    "    return 1 - (MSE(targets, predicted)/np.var(targets))\n",
    "\n",
    "print(\"R2 (Python):\", R2(Y_test, y_hat))\n",
    "print(\"R2 (scikit-learn):\", r2_score(Y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, besides the $R^{2}$, the other three metrics are no longer explained in terms of a percentage. \n",
    "\n",
    "How can we interpret our values for MSE, RMSE and MAE? This will depend on the model and the data you are dealing with. If you deviate by 0.7 on a dataset that varies from 0-1000, this is not a large deviation. However, if your data varies from 0-1, 0.7 is a significant deviation from the truth. Therefore, these metrics are useful for comparing different models on the same data. \n",
    "\n",
    "> If we would like to compare the performance of two different regression models who's data are on different scales, it may be useful to normalize our evaluation metrics by the range of values in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: ROC & AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now discuss one of the most powerful evaluation metrics used for classification: the __Receiver Operator Characteristic (ROC) curve__ and the __Area Under the Curve (AUC).__\n",
    "\n",
    "By 'fitting' a logistic regression model to our binary classification data, we have already found the optimal __threshold__ for the model. \n",
    "\n",
    "> In the case of our example, the threshold is the probability where greater than this probability, we consider the tumour to be malignant, and below it, where the tumour is classified as benign.\n",
    "\n",
    "However, 'optimal' threhsold may not necessarily give us the optimal model for the problem we are tackling, which is why we need evaluation metrics in the first place. \n",
    "\n",
    "Here comes the ROC curve, originaly developed for operators of military radar receivers hence the name, which gives us a visual representation of the performance of our model while operating at different thresholds.\n",
    "\n",
    "The ROC curve uses __recall__, referred to in this context as __sensitivity__ or __true positive rate__, as well as another quantity, known as __specificity,__ which is the ratio of how many observations were correctly predicted negative to how many observations were actually negative:\n",
    "\n",
    "$$ \n",
    "\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "Given this quantity, we can define what is known as the __false positive rate__, which tells you the proportion of observations that were predicted to be malignant when they were actually benign:\n",
    "\n",
    "$$ \n",
    "\\text{FPR} = 1-\\text{Specificity} = \\frac{\\text{FP}}{\\text{TN} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "Using these quantities, we can now plot our own ROC curve for our binary classification model, which is the true positive rate versus the false positive rate for all our threshold values tested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  # doctest: +SKIP\n",
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "\n",
    "X, y = datasets.make_classification(random_state=0)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, random_state=0\n",
    ")\n",
    "\n",
    "clf = svm.SVC(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "metrics.plot_roc_curve(clf, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for FPR and TPR are taken at different thresholds and computed as points.\n",
    "\n",
    "We can also see __Area Under the Curve (AUC)__:\n",
    "\n",
    "> AUC is a measure of area under the rectangle\n",
    "\n",
    "Intuitively,\n",
    "\n",
    "> the larger area under the rectangle, the better\n",
    "\n",
    "This value allows us to easily compare models with maximum value being `1.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessments\n",
    "\n",
    "- How does `true positive` and other from confusion matrix look for multiclass instead of binary?\n",
    "- Check out [other possible options](https://en.wikipedia.org/wiki/Precision_and_recall) mixing confusion matrix. Implement some of them\n",
    "- Why ROC Curve always goes up towards the right side edge?\n",
    "- In our notebook we do multiple passes for precision, recall etc. Can you do that with single pass with `numpy` only?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9ca20eb9903ec819ce8e3057ea2baedc00ab0db33ca8a9c7ce80c5c8a90afde"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('AiCore': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
