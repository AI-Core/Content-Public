{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Validation and Test\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "Understand what terms below are all about:\n",
    "\n",
    "- cross validation\n",
    "- test set\n",
    "- random seeding\n",
    "- data leakage\n",
    "\n",
    "## The test set\n",
    "\n",
    "> Machine learning models are only useful if they can __generalise__ to make good predictions on unseen examples\n",
    "\n",
    "To estimate how well a model will perform on unseen data, we split our initial dataset into two different sets. One is for training on, and the other is for testing on.\n",
    "\n",
    "> The testing set is used for evaluating whether a model meets our requirements and estimating real world performance. That is all. It is not for making choices about our model.\n",
    "\n",
    "Sklearn provides a method `train_test_split()` in it's `model_selection` module to split our data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "\n",
    "print(f\"Number of samples in dataset: {len(X)}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"Number of samples in:\")\n",
    "print(f\"    Training: {len(y_train)}\")\n",
    "print(f\"    Testing: {len(y_test)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The validation set\n",
    "\n",
    "If we want to choose between two models, we can't base those decisions on the testing set. If we did, we would be making decisions that are biased towards doing well on answers that we are expecting. \n",
    "\n",
    "Making choices based on the testing set is like seeing the answers on a test.\n",
    "\n",
    "Instead, we create another set, called the __validation set__. This is used for comparing models or different options for the same model. We call this __cross validation__\n",
    "\n",
    "> We make the validation set by further splitting the training set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test, X_validation, y_test, y_validation = train_test_split(\n",
    "    X_test, y_test, test_size=0.3\n",
    ")\n",
    "\n",
    "print(\"Number of samples in:\")\n",
    "print(f\"    Training: {len(y_train)}\")\n",
    "print(f\"    Validation: {len(y_validation)}\")\n",
    "print(f\"    Testing: {len(y_test)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Pay attention: people commonly fail to understand the difference between the validation set and the testing set.\n",
    "\n",
    "The difference between the validation set and the testing set is that we use the validation set to make choices about our models, but not the test set.\n",
    "\n",
    "Such choices may include:\n",
    "- Should I deploy the linear regression model or the neural network?\n",
    "- Should I use the model which was trained on all the features of just the 3 I picked?\n",
    "- Any choice of hyperparameter (which you will learn about shortly)\n",
    "\n",
    "Let's see that now"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "# ML algorithms you will later know, don't panic\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "models = [\n",
    "    DecisionTreeRegressor(splitter=\"random\"),\n",
    "    SVR(),\n",
    "    LinearRegression()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_loss = mean_squared_error(y_train, y_train_pred)\n",
    "    validation_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    print(\n",
    "        f\"{model.__class__.__name__}: \"\n",
    "        f\"Train Loss: {train_loss} | Validation Loss: {validation_loss} | \"\n",
    "        f\"Test Loss: {test_loss}\"\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis\n",
    "\n",
    "As you can see, best Validation Loss is for Linear Regression. This is the model we should choose. Unfortunately it occurs that on \"real\" (test) data it performs worse than Decision Tree.\n",
    "\n",
    "Once again: usually we won't have information from test loss, but now you should know this technique is imperfect (we will see how to mitigate those effects later on)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Seeding\n",
    "\n",
    "Above we had this line: `np.random.seed(2)`. It is actually really important and we should know what is going on.\n",
    "\n",
    "### Pseudo-random number generators\n",
    "\n",
    "Many machine learning algorithms use random initialization (for example to instantiate the parameters of a linear regression model). Depending on algorithm it might have more or less severe effect on the result.\n",
    "\n",
    "- Each time you run algorithm based on the randomness the result may vary to some degree\n",
    "- Random number generators use so called `seed` which is a numerical value which determines what values will be generated\n",
    "- For each run to be the same (or to show some phenomenon like we did above) we should __always__ seed all functions using random numbers\n",
    "\n",
    "The last one is pretty easy in `numpy` and `sklearn` as it is a single line. Seeding this way is present in most of the frameworks.\n",
    "\n",
    "### Why initialise a seed?\n",
    "\n",
    "- When you want your experiments to be reproducible (especially important in Machine Learning)\n",
    "- To be sure the outcome will not change during each run\n",
    "\n",
    "> Always set a random seed to make sure your results are repeatable when some part of the code involves random numbers being generated."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Leakage\n",
    "\n",
    "__Data leakage__ is where a model has access to information about the testing sets. Of course, in the real world when facing totally new examples, our model will not have known anything about the incoming data. That means in training we need to carefully preserve that separation too.\n",
    "\n",
    "> Never make any decisions about your model design using the test set\n",
    "\n",
    "Of course, data leakage can be caused by bad data splitting. These include\n",
    "- some samples are both in training and validation\n",
    "- model is simply evaluated based on performance on training data\n",
    "\n",
    "But there are less obvious ways to cause data leakage as well. One is to compute some statistics (like the mean) or new features (like difference from the mean) based on the data before splitting it, and then using those values in training, after splitting it. Those statistics contain data about the testing data which has since been split off.\n",
    "\n",
    "Let's see an example in action..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_validation_loss(X_train, y_train, X_validation, y_validation):\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Without data leakage, train on train, validate on validation\n",
    "    model.fit(X_train, y_train)\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    validation_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "\n",
    "    print(f\"Validation loss: {validation_loss}\")\n",
    "    \n",
    "# Without data leakage, train on train, validate on validation\n",
    "calculate_validation_loss(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "# With data leakage, 50 samples from validation added\n",
    "fail_X_train = np.concatenate((X_train, X_validation[:50]))\n",
    "fail_y_train = np.concatenate((y_train, y_validation[:50]))\n",
    "\n",
    "calculate_validation_loss(fail_X_train, fail_y_train, X_validation, y_validation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, as the model saw part of validation data and it __falsely__ performs better on it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "- Validation set is used to find info about best algorithms, best set of arguments to algoirthms etc.\n",
    "- Test set is used to check how our algorithm performs on unseen data\n",
    "- __As we tune algorithms according to `validation` dataset we cannot use it to check performance__\n",
    "- `seed` is used to ensure reproducibility. Also multiple runs for experiments are good if our code depends on random initialization heavily (we can take mean results of experiments)\n",
    "- Data leakage is information from `validation` (or `test`) leaking into training\n",
    "- Data leakage leads to falsely good results and should be avoided\n",
    "- Rule of thumb: imagine you only have training dataset when doing preprocessing. Anything you calculate from it cannot be used in `validation` or `test`"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('main': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "interpreter": {
   "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}