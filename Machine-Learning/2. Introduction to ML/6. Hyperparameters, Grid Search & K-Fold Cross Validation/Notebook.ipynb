{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters, K-Fold Cross Validation & Grid search\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "Understand what is going on with:\n",
    "\n",
    "- Hyperparameters\n",
    "- Grid search and how to use it for hyperparameter finding\n",
    "- K-Fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the steps in the Machine Learning system design is to optimize the model. But wait, we were mentioning that the model learns the parameters by performing an optimization of the loss function, what can we do further than that? Some of the parameters that the model can't learn are called hyperparameters. We can tune the hyperparameters to find the best model for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Up to this point we were only using __parameters__ (for example parameters of linear regression are it's weight(s)).\n",
    "\n",
    "> Hyperparameters control the learning process. In contrast to parameters, they either can not or should not be learned from data. They are usually set and fixed before training process starts.\n",
    "\n",
    "E.g. learning rate should not be learned from the data\n",
    "- to find the best learning rate would require to train the model completely, using many different learning rates.\n",
    "- this would be computationally expensive\n",
    "\n",
    "E.g. the order of polynomial features to pass to the model (e.g. $x^2$, $x^3$) should not be learned from the data\n",
    "- Hyperparameters like this control \"representational capacity\" of the model - the complexity between inputs and outputs which the model can represent. E.g. can the model represent wavy relationships or only straight line relationships?\n",
    "- If this hyperparameter were to be optimised by the model, it would obviously choose to be able to represent more complex input-output relationships so that it can perform best on the data, but this will cause it to overfit to the data and not generalise (more on this later).\n",
    "\n",
    "Other hyperparameters include:\n",
    "- learning rate\n",
    "- batch size\n",
    "- regularisation parameter (more on this later)\n",
    "- order of the polynomial features included \n",
    "\n",
    "Those are essential and prevalent in machine learning, see code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressors = [\n",
    "    RandomForestRegressor(n_estimators=10, criterion=\"mae\"),\n",
    "    RandomForestRegressor(n_estimators=50, min_samples_leaf=2),\n",
    "    RandomForestRegressor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have a single classification machine learning method called Random Forest (we will get to how it works in next module).\n",
    "\n",
    "What interests us are `__init__` parameters we provided (`n_estimators`, `criterion`, `min_samples_leaf`). __They are examples of hyperparameters__ you can set before fitting them to data.\n",
    "\n",
    "What can happen after setting them incorrectly?\n",
    "- our algorithm may __under/overfit__ (more details later)\n",
    "- it might not converge __at all__ in some cases\n",
    "\n",
    "When we do it right (at least more or less) we can observe:\n",
    "- improved convergence & faster training time\n",
    "- lower loss & better performance on test data\n",
    "\n",
    "You can probably tell by now how crucial those things are. The question is how to find them?\n",
    "\n",
    "Other examples of hyperparameters include:\n",
    "- batch size\n",
    "- polynomial for linear/logistic regression (will see more about those)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding hyperparameters\n",
    "\n",
    "There are a couple ways to find those:\n",
    "- experience, after some time you get an idea of what should work and what might not, present especially in deep learning\n",
    "- algorithmic (we will focus on this one) - we try a set of possible hyperparameters\n",
    "and choose the best one\n",
    "- mix of both - you know the boundaries that should yield good results (say `64 < batch_size < 1024`) but you are not certain about exact value so you employ algorithmic approach to find them\n",
    "\n",
    "Later you will get enough info to get you started using the last approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "One __algorithmic__ way to find hyperparameters is __grid search__:\n",
    "\n",
    "> Grid Search consists of grid of possible hyperparameters and each combination is used to learn algorithm of choice and validate the results\n",
    "\n",
    "Examplary grid for RandomForest you saw above might be:\n",
    "\n",
    "| `n_estimators`  | `criterion`  | `min_samples_leaf` |\n",
    "|:--:|:---:|:---:|\n",
    "| 10  | \"mse\"  | 2  |\n",
    "| 50  | \"mae\"  | 1  |\n",
    "| 100  |   |   |\n",
    "\n",
    "Each time we create a row from different values so that all combinations are checked. __Please notice the grid doesn't have to be \"square\"__ so any number of different hyperparameters can be checked together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's implement a simple Grid Search. To outline what we have to do:\n",
    "- Create dictionary containing hyperparameters names and their allowed values (as `list`). Use at most `4` values:\n",
    "    - `n_estimators` with values `[10, 50, 100]` (or others, not more than a `1000`)\n",
    "    - `criterion` (check possible values)\n",
    "    - `min_samples_leaf` with values `[2, 1]`\n",
    "\n",
    "### Code undestanding\n",
    "\n",
    "- What is `yield` for? \n",
    "- When should we use it? \n",
    "- What is the name of the function which ends with `yield`?\n",
    "- Can you explain `grid_search` function? (We will want someone to try and explain as much as he can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import typing\n",
    "\n",
    "\n",
    "def grid_search(hyperparameters: typing.Dict[str, typing.Iterable]):\n",
    "    keys, values = zip(*hyperparameters.items())\n",
    "    yield from (dict(zip(keys, v)) for v in itertools.product(*values))\n",
    "\n",
    "\n",
    "grid = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"criterion\": [\"mse\", \"mae\"],\n",
    "    \"min_samples_leaf\": [2, 1],\n",
    "}\n",
    "\n",
    "for i, hyperparams in enumerate(grid_search(grid)):\n",
    "    print(i, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search evaluation\n",
    "\n",
    "Now that we have our grid creating function we can evaluate RandomForest and see how it works out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "best_hyperparams, best_loss = None, np.inf\n",
    "\n",
    "for hyperparams in grid_search(grid):\n",
    "    model = RandomForestRegressor(**hyperparams)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    validation_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "\n",
    "    print(f\"H-Params: {hyperparams} Validation loss: {validation_loss}\")\n",
    "    if validation_loss < best_loss:\n",
    "        best_loss = validation_loss\n",
    "        best_hyperparams = hyperparams\n",
    "\n",
    "print(f\"Best loss: {best_loss}\")\n",
    "print(f\"Best hyperparameters: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Above you can see how one can find best model based on validation loss. There are some drawbacks to this method though, namely:\n",
    "- there is only one validation set; what would happen if we were to choose different part of data as validation?\n",
    "- every combination has to be checked (which can be costly and semi-effective as __a lot__ of checked hyperparameters will not improve the score)\n",
    "\n",
    "Luckily there are some solutions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold cross validation\n",
    "\n",
    "What is it?\n",
    "\n",
    "> K-Fold splits dataset into multiple parts which are in turn used for training and validation\n",
    "\n",
    "Picture will help you understand what this is about:\n",
    "\n",
    "![](images/k-fold.svg)\n",
    "\n",
    "This is how it goes algorithmically:\n",
    "1. split dataset into `K` parts (predefined, usually `5` or `3`)\n",
    "2. set `i=0`\n",
    "3. take `parts[i]` as validation set and the rest as training data\n",
    "4. train on training part of data\n",
    "5. calculate metrics (loss in our case) on validation set and save it\n",
    "6. increment i and repeat until the last part is used as validation\n",
    "7. take the mean of validation results\n",
    "\n",
    "### But why?\n",
    "\n",
    "K-Fold is a common practice in machine learning and de facto standard. Here are the reasons:\n",
    "- evaluation on single validation set can be really noisy; depending on the data we take into validation the results may vary (sometimes wildly)\n",
    "- this gives us __false impression about model performance__. Usually the model does worse than expected\n",
    "\n",
    "### But why not?\n",
    "\n",
    "... It isn't standard in deep learning community though, because:\n",
    "- K-Fold takes a lot of time. In case of `5` splits we have to fit `5` separate models. For larger models (which may be trained for weeks) it is infeasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "We can implement K-Fold easily from scratch, just as we did with grid search, __BUT THIS TIME YOU WILL DO IT ON YOUR OWN WITH `yield`!__\n",
    "\n",
    "- Define `k_fold` function taking `dataset` and `n_splits` (which is the number of splits). Set `n_splits` default value to `5` and specify the `type` as `int`\n",
    "- Use [`np.array_split`](https://numpy.org/doc/stable/reference/generated/numpy.array_split.html) to split `dataset` into `n_splits` __chunks__e\n",
    "- In a `for` loop:\n",
    "    - Get `i`-th chunk as validation dataset\n",
    "    - Get rest of the chunks (up to `i`) and from `i + 1`\n",
    "    - Concatenate those chunks creating training data\n",
    "    - `yield` a tuple containing `training` and `validation` dataset\n",
    "    \n",
    "Finally, go over the rest of this code and check what is going on (we will ask one of you to explain what you see!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(dataset, n_splits: int = 5):\n",
    "    chunks = np.array_split(dataset, n_splits)\n",
    "    for i in range(n_splits):\n",
    "        training = chunks[:i] + chunks[i + 1 :]\n",
    "        validation = chunks[i]\n",
    "        yield np.concatenate(training), validation\n",
    "\n",
    "# K-Fold evaluation\n",
    "loss = 0\n",
    "n_splits = 5\n",
    "for (X_train, X_validation), (y_train, y_validation) in zip(\n",
    "    k_fold(X, n_splits), k_fold(y, n_splits)\n",
    "):\n",
    "    # What happened to hyperparameters?\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    fold_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "    loss += fold_loss\n",
    "    print(f\"Fold loss: {fold_loss}\")\n",
    "\n",
    "# We divide by number of splits to get the mean\n",
    "print(f\"K-Fold estimated loss: {loss / n_splits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "- You can see that loss for each fold varies __a lot__. `46` and `8` is a giant difference\n",
    "- Thanks to K-Fold we have more reliable statistics\n",
    "\n",
    "But what about hyperparameter search? You've probably guessed it:\n",
    "> Grid Search can be mixed with K-Fold validation. This is standard approach and provided by `sklearn` easily\n",
    "\n",
    "We just need two nested loops. Now you can probably see the downside more clearly (and why it isn't used in deep learning):\n",
    "- Complexity will be `O(n*m)` where `n` is the number of hyperparameter and `m` number of splits we want to use. `60` models in our simple case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search + K-Fold\n",
    "\n",
    "Okay, let's mix those folks together in an...\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Now we will do Grid Search with K-Fold inside. Here is what you should do:\n",
    "- Iterate over `grid_search(grid)` in `for` loop\n",
    "    - Set `loss` to `0`\n",
    "    - Iterate over training and validation dataset via `zip` as  we saw above\n",
    "        - Create `RandomForestRegressor` model with `hyperparams` via __dictionary unpacking__ with `**` (check the internet for more info)\n",
    "        - Fit model to training data\n",
    "        - Predict on validation\n",
    "        - Calculate mean squared error loss for this fold\n",
    "        - Add fold loss to `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold evaluation\n",
    "best_hyperparams, best_loss = None, np.inf\n",
    "n_splits = 5\n",
    "# Grid search goes first\n",
    "for hyperparams in grid_search(grid):\n",
    "    loss = 0\n",
    "    # Instead of validation we use K-Fold\n",
    "    for (X_train, X_validation), (y_train, y_validation) in zip(\n",
    "        k_fold(X, n_splits), k_fold(y, n_splits)\n",
    "    ):\n",
    "        model = RandomForestRegressor(**hyperparams)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_validation_pred = model.predict(X_validation)\n",
    "        fold_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "        loss += fold_loss\n",
    "    # Take mean from all folds as final validation score\n",
    "    total_loss = loss / n_splits\n",
    "    print(f\"H-Params: {hyperparams} Loss: {total_loss}\")\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_hyperparams = hyperparams\n",
    "\n",
    "# And see our final results\n",
    "print(f\"Best loss: {best_loss}\")\n",
    "print(f\"Best hyperparameters: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "- Our results changed drastically. Different criterion is the best and different number of estimators. \n",
    "- The results may vary in this case as well but are way less spurious than before (in magnitude)\n",
    "- Also the results are way worse (more than double)\n",
    "- __But our results better estimate algorithm behviour__. If we didn't do that we would be all happy with loss around `9` just to be negatively surprised much later. __It is good to know before!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold variants\n",
    "\n",
    "There are a lot of variants of K-Fold, as always we urge you to explore more on your own, here are some of them:\n",
    "- [Stratified K-Fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) used when we do __classification__ and our dataset is __unbalanced__ (e.g. `20` examples for class `0` and `500` for class `1`)\n",
    "- [Time Series cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) - should be used when we work with time data (where events follow another)\n",
    "- Leave One Out Cross Validation (LOOCV) - Just like K-Fold but `validation` set is single sample. Very in-efficient as we have to fit as many models as samples (over `500` in our case!). Provides noisy estimates and is overly optimistic about the results\n",
    "\n",
    "Any many, many more, go and search if you are interesed (also see challenges if you want to have some guidance for your own research)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Grid Search\n",
    "\n",
    "### Positives\n",
    "\n",
    "- Can be easily parallelized (each hyperparameter can be run on different core or even computer)\n",
    "- Simple and useful when __search space__ (set of all hyperparameters which might be worth checking out) is small\n",
    "\n",
    "### Negatives\n",
    "\n",
    "- Every combination of hyperparameters is tried and previous results do not inform next trials. Hence bad combinations will be tried (and most are bad, trust me) You can check other versions like genetic algorithms so that each trial gives feedback to hyperparameter optimizing algorithm\n",
    "- Needs discrete values. Some algorithms (e.g. [Random Search](https://en.wikipedia.org/wiki/Random_search)) can work on probability distributions which are in turn sampled to get values\n",
    "- Impossible to use with large hyperparameter space which grows __exponentially__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do with best hyperparameters\n",
    "\n",
    "Now that we have some confidence that our model is the best one, we should take the hyperparameters we've found and __train it on a whole dataset once more__.\n",
    "\n",
    "Why, no more validation? No, because:\n",
    "- We already found a good model and estimated how it performs on left-out set (for more rigorous approach check out [Nested Cross Validation](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/)\n",
    "- __Samples are precious__. We want to use all (or as many as posbbile) samples to train our model so it gathers more knowledge about the task.\n",
    "\n",
    "For some models validation might be required __anyway__, especially for those which tend to overfit on our data (neural networks are the prime example but XGBoost can be considered as such as well). It is task specific and you will get to know more later.\n",
    "\n",
    "> As a sanity check you might always leave some small part of data (`5%` say) and validate on that after you've found your hyperparameters\n",
    "\n",
    "As a last step, we should save our model for later re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Hyperparameters are parameters controlling behaviour of our algorithm __which cannot be learned__ (or cannot be at the current stage of our knowledge)\n",
    "- __Grid Search__ in conjunction with __K-Fold__ can be simply used to find hyperparameters of our model\n",
    "- __K-Fold__ creates many `(train, validation)` sets. Using it with Grid Search helps us being __way more confident__ about the results \n",
    "- __Grid Search__ has it's downsides and it might be worth checking other options out\n",
    "- __Always employ domain knowledge__ - if you know some set of hyperparameters will fail, do not try them out. Check the most promising combinations. Check all if you have no sensible idea or even better ask an expert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
