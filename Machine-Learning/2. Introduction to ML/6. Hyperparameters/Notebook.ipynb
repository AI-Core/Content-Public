{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "Understand what is going on with:\n",
    "\n",
    "- Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the steps in the Machine Learning system design is to optimize the model. But wait, we were mentioning that the model learns the parameters by performing an optimization of the loss function, what can we do further than that? Some of the parameters that the model can't learn are called hyperparameters. We can tune the hyperparameters to find the best model for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Up to this point we were only using __parameters__ (for example parameters of linear regression are it's weight(s)).\n",
    "\n",
    "> Hyperparameters control the learning process. In contrast to parameters, they either can not or should not be learned from data. They are usually set and fixed before training process starts.\n",
    "\n",
    "E.g. learning rate should not be learned from the data\n",
    "- to find the best learning rate would require to train the model completely, using many different learning rates.\n",
    "- this would be computationally expensive\n",
    "\n",
    "E.g. the order of polynomial features to pass to the model (e.g. $x^2$, $x^3$) should not be learned from the data\n",
    "- Hyperparameters like this control \"representational capacity\" of the model - the complexity between inputs and outputs which the model can represent. E.g. can the model represent wavy relationships or only straight line relationships?\n",
    "- If this hyperparameter were to be optimised by the model, it would obviously choose to be able to represent more complex input-output relationships so that it can perform best on the data, but this will cause it to overfit to the data and not generalise (more on this later).\n",
    "\n",
    "Other hyperparameters include:\n",
    "- learning rate\n",
    "- batch size\n",
    "- regularisation parameter (more on this later)\n",
    "- order of the polynomial features included \n",
    "\n",
    "Those are essential and prevalent in machine learning, see code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressors = [\n",
    "    RandomForestRegressor(n_estimators=10, criterion=\"mae\"),\n",
    "    RandomForestRegressor(n_estimators=50, min_samples_leaf=2),\n",
    "    RandomForestRegressor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have a single classification machine learning method called Random Forest (we will get to how it works in next module).\n",
    "\n",
    "What interests us are `__init__` parameters we provided (`n_estimators`, `criterion`, `min_samples_leaf`). __They are examples of hyperparameters__ you can set before fitting them to data.\n",
    "\n",
    "What can happen after setting them incorrectly?\n",
    "- our algorithm may __under/overfit__ (more details later)\n",
    "- it might not converge __at all__ in some cases\n",
    "\n",
    "When we do it right (at least more or less) we can observe:\n",
    "- improved convergence & faster training time\n",
    "- lower loss & better performance on test data\n",
    "\n",
    "You can probably tell by now how crucial those things are. The question is how to find them?\n",
    "\n",
    "Other examples of hyperparameters include:\n",
    "- batch size\n",
    "- polynomial for linear/logistic regression (will see more about those)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding hyperparameters\n",
    "\n",
    "There are a couple ways to find those:\n",
    "- experience, after some time you get an idea of what should work and what might not, present especially in deep learning\n",
    "- algorithmic (we will focus on this one) - we try a set of possible hyperparameters\n",
    "and choose the best one\n",
    "- mix of both - you know the boundaries that should yield good results (say `64 < batch_size < 1024`) but you are not certain about exact value so you employ algorithmic approach to find them\n",
    "\n",
    "Later you will get enough info to get you started using the last approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Hyperparameters are parameters controlling behaviour of our algorithm __which cannot be learned__ (or cannot be at the current stage of our knowledge)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
