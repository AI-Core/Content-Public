{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python\n",
    "- Linear algebra\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "- Know the difference between monovariate and multivariate regressions\n",
    "- Implement your first machine learning algorithm from scratch, in Python\n",
    "- Use analytical solution to solve for it\n",
    "- See how to optimize linear regression using analytical solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "\n",
    "Once again we will use `California` dataset from `sklearn`, we saw it previously, easy stuff by now. Let's also split it into validation and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 8) (14448,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, model_selection\n",
    "\n",
    "# 15% for validation and test, 70% for train in total\n",
    "X, y = datasets.fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_validation, X_test, y_validation, y_test = model_selection.train_test_split(\n",
    "    X_test, y_test, test_size=0.5\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is linear regression?\n",
    "\n",
    "Classic starting point for machine learning adventures, something like `Hello World` but in ML world.\n",
    "\n",
    "__Linear regression predicts continuous outputs__ - hence the regression part of the name.\n",
    "Linear regression makes predictions that are simply a __`w`eighted__ combination (a linear combination) of the inputs (plus some offset called __`b`ias__). It is described by linear function:\n",
    "\n",
    "$$\n",
    "    y = wx + b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p align=center><img width=700 src=images/linear_model.jpg></p>\n",
    "\n",
    "In future we will experience much more complex, nonlinear relationships between features and labels that we wish to model. \n",
    "\n",
    "But __do not underestimate linear regression__ as it is often used in statistics and to explain a lot of phenomenas, at the end of the lesson we will see when it should be used in real world.\n",
    "\n",
    "Functions that a model represent are often referred to as the **hypothesis**.\n",
    "\n",
    "<p align=center><img width=800 src=images/linear_model_example.jpg></p>\n",
    "\n",
    "We will make our model able to make predictions for many examples at a time by expressing the hypothesis in vector form as shown below.\n",
    "\n",
    "<p align=center><img width=700 src=images/linear_model_vector.jpg></p>\n",
    "\n",
    "Here's an example of what that computation might look like numerically.\n",
    "\n",
    "<p align=center><img width=700 src=images/linear_model_vector_example.jpg></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical formula of model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Formula below presents linear regression for single example __but multiple features__:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y = w_1x_1 + w_2x_2 + ... + w_Nx_N + b = \\sum_{i=1}^{N} w_ix_i + b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Essentially:\n",
    "- each feature in our sample is multiplied by a weight\n",
    "Now let's implement our first machine learning model in code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple features\n",
    "\n",
    "We will go for multiple features, so here is how our weights will look like:\n",
    "\n",
    "<p align=center><img width=700 src=images/w_vector.jpg></p>\n",
    "\n",
    "The weights variable (w) becomes a row vector so we need to transpose it when we multiply it by the X matrix (or take a `dot` product of `data` and `weights`).\n",
    "\n",
    "<p align=center><img width=800 src=images/vector_linear_regression.jpg></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monovariate vs multivariate\n",
    "\n",
    "A dychotomy you might sometimes come across:\n",
    "\n",
    "> monovariate linear regression is linear regression done with __one or multiple features__ but __predicting single target__\n",
    "\n",
    "And __multivariate__ (as you may of guessed) would be\n",
    "\n",
    "> linear regression with __one or multiple variables (features)__ but __predicting multiple targets__ (which are correlated with each other)\n",
    "\n",
    "In this notebook we will be doing __monovariate__ only, but we will get to __multivariate__ when we do multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "`LinearRegression` implementation is our task!\n",
    "\n",
    "- Create a class `LinearRegression` which takes a single `n_features` argument during initialization.\n",
    "    - Create `W` and `b` variables inside initialization. One of shape `(n_features, 1)` and `bias` of shape `1` initialized with random normal distribution\n",
    "- Create `__call__` function (what does it do, what is a functor?) which takes `X` (`np.array`). It should return predictions our linear regression should do (see formulas above in the picture, it's two operations only)\n",
    "- Create `update_params` function which takes `W` and `b` and assigns them to appropriate variables in `self`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, n_features: int): # initalize parameters\n",
    "        np.random.seed(10)\n",
    "        self.W = np.random.randn(n_features, 1) ## randomly initialise weight\n",
    "        self.b = np.random.randn(1) ## randomly initialise bias\n",
    "        \n",
    "    def __call__(self, X): # how do we calculate output from an input in our model?\n",
    "        ypred = np.dot(X, self.W) + self.b\n",
    "        return ypred # return prediction\n",
    "    \n",
    "    def update_params(self, W, b):\n",
    "        self.W = W ## set this instance's weights to the new weight value passed to the function\n",
    "        self.b = b ## do the same for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [[1482.14731583]\n",
      " [3169.69576072]\n",
      " [1131.67756962]\n",
      " [ 756.45841866]\n",
      " [ 963.42420785]\n",
      " [ 696.58323643]\n",
      " [ 859.12613003]\n",
      " [ 332.18098164]\n",
      " [ 890.04023052]\n",
      " [ 496.39589958]]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(n_features=8)  # instantiate our linear model\n",
    "y_pred = model(X_test)  # make prediction on data\n",
    "print(\"Predictions:\\n\", y_pred[:10]) # print first 10 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(y_pred, y_true):\n",
    "    samples = len(y_pred)\n",
    "    plt.figure()\n",
    "    plt.scatter(np.arange(samples), y_pred, c='r', label='predictions')\n",
    "    plt.scatter(np.arange(samples), y_true, c='b', label='true labels', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Sample numbers')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(y_pred[:10], y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "As you can see predictions of our model are __way off__. This happens because we initialized our model with random weights and bias.\n",
    "\n",
    "Now, we should learn how we can improve this model to learn from data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss - how do we know how good our model is?\n",
    "\n",
    "As mentioned in the machine learning system design process, we need to set a baseline and try to surpass it as we train more models. But how can we obtain a quantitative measurement of how good the model is?\n",
    "\n",
    "> Our **loss** should measure __how poor our model performs__. \n",
    "\n",
    "The larger the value, the worse so we will later try to __minimize it__ (bring as close to zero as possible). We will use it to give our model feedback about it's performance. \n",
    "\n",
    "> Loss values needs to return a **single number**, not a vector, not a matrix.\n",
    "\n",
    "__NOTE:__ minimising the objective is equivalent to maximising the negative of it. \n",
    "\n",
    "Commonly, loss value is also called __cost function__ though it is not exact. Let's go over the difference now.\n",
    "\n",
    "### Squared Error loss\n",
    "\n",
    "> loss is a function which takes prediction and true label and returns __a positive scalar__\n",
    "\n",
    "- The higher the loss value, the worse our model performs\n",
    "- __Loss is defined on a single data point__\n",
    "\n",
    "Squared error is one of the loss functions __used for regression tasks__ and is simply defined as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    (\\hat{y} - y)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This does exactly what you think: it calculates the error (difference between our model's prediction $\\hat{y}$ and the true value $y$):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\hat{y} - y\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and then squares it to make the value positive. As long as the error is not zero it will increase the value of loss regardles of whether our prediction is below (negative error) or above (positive error) the value of the label.\n",
    "\n",
    "### Mean Squared Error (MSE) cost function\n",
    "\n",
    "> cost function is a generalization of loss functions for many data samples\n",
    "\n",
    "So, __loss__ operates on single sample, while __cost__ operates on multiple of them.\n",
    "In case of __Mean Squared Error__ we calculate squared error for each sample and take the mean of that value:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    L_{mse} = \\frac{1}{N}\\sum_{i}^{N}(\\hat{y_i} - y_i)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "There are many other criterions that are useful for different tasks (e.g. the binary cross entropy (BCE) loss for classification, which we will cover later).\n",
    "\n",
    "Let's write a function to calculate the cost using the mean squared error loss function. It should take in an array of predictions for different example inputs as well as an array of corresponding example labels. It should return a single number (scalar) that represents the MSE loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement `mean_squared_error` function taking `y_pred` and `y_true`. Every formula is above (focusing on the last one is enough ;) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_pred, y_true):  # define our criterion (loss function)\n",
    "    errors = y_pred - y_true  ## calculate errors\n",
    "    squared_errors = errors ** 2  ## square errors\n",
    "    return np.mean(squared_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cost = mean_squared_error(y_pred, y_train)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analytical solution to minimising mean square error\n",
    "\n",
    "Now that we have our __loss__ equation we can calculate it's derivative w.r.t. weights. When we set it to zero we can calculate __weights values (`W`)__ which minimize it.\n",
    "\n",
    "<p align=center><img width=900 src=images/analytical_linear_reg.jpg></p>\n",
    "\n",
    "Now let's implement this analytical solution for least squares regression in code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now that we have mathematical formula we can jump in straight to the implementation.\n",
    "\n",
    "- For matrix inverse, you can use [`np.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) function\n",
    "- Remember to return `weights` part of `matrix` first and `bias` after that (`bias` is the `0` element of the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minimize_loss(X_train, y_train):\n",
    "    X_with_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    optimal_w = np.matmul(\n",
    "        np.linalg.inv(np.matmul(X_with_bias.T, X_with_bias)),\n",
    "        np.matmul(X_with_bias.T, y_train),\n",
    "    )\n",
    "    return optimal_w[1:], optimal_w[0]\n",
    "\n",
    "\n",
    "weights, bias = minimize_loss(X_train, y_train)\n",
    "print(weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you didn't notice, this analytical solution has no mention of the model bias. \n",
    "In fact, we incorporate the model bias into our features matrix by adding an extra column filled with `1`.\n",
    "\n",
    "<p align=center><img width=700 src=images/bias_in_weight_matrix.jpg></p>\n",
    "\n",
    "Doing this makes the analytical solution much clearer and means we have to solve it only for one value $W$, rather than also for $b$.\n",
    "\n",
    "In practice (iterative optimization), we treat them as separate variables (we will later see more about that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameters\n",
    "\n",
    "Now that we have found `optimal_w` we should update our model and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_params(weights, bias)\n",
    "y_pred = model(X_train)\n",
    "cost = mean_squared_error(y_pred, y_train)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of computing the analytical solution\n",
    "\n",
    "This solution involves inverting a matrix of size $R^{n \\times n}$. \n",
    "Here $n$ is the number of features that each example has. \n",
    "\n",
    "With 8 features it is becoming more difficult. Furthermore, here, we only have ~20000 samples, while in real life we can have millions or more.\n",
    "\n",
    "However, as we will see, most problems of practical interest contain examples with many more features. \n",
    "\n",
    "> For example, 1080p images have more than 1,000,000 features each. \n",
    "\n",
    "The time complexity of inverting a matrix of size $n \\times n$ is around $O(n^3)$. \n",
    "This means that computing the analytical solution for these kinds of real world problems is often computationally expensive or even impossible.\n",
    "\n",
    "Analytical solutions however, are not the only approach that we can take (and usually we __even cannot use them__ as the close form cannot be calculated).\n",
    "\n",
    "We will see how to update parameters iteratively soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models in `sklearn`\n",
    "\n",
    "`sklearn` comes with some common machine learning models that you can use out of the box, with a simple to use API.\n",
    "\n",
    "(See the linear regression [documentation](https://scikit-learn.org/stable/modules/classes.html#classical-linear-regressors)).\n",
    "\n",
    "Let's get some data and then load [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Also import appropriate package from `sklearn` to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.fetch_california_housing(return_X_y=True)\n",
    "\n",
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use `sklearn`'s API of models\n",
    "\n",
    "`sklearn` machine learning algorithms are objects which usually follow this general convention:\n",
    "\n",
    "- `__init__(*args, **kwargs)` - here you setup your algorithm (as seen above).\n",
    "- `fit(X, [y])` - train the model on `X` (features) and `y` (targets). In case of unsupervised algorithms there is no `y`\n",
    "- `predict(X)` - pass data (previously unseen) to algorithm after `fit` was called. This gives us predictions (`y_pred`).\n",
    "\n",
    "Given that we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.13164983 3.97660644 3.67657094 3.2415985  2.41358744] \n",
      " [4.526 3.585 3.521 3.413 3.422]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print(y_pred[:5], \"\\n\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model\n",
    "\n",
    "So our model predicts some values, but how well does it actually do? `sklearn` provides performance __metrics__ for us to use.\n",
    "\n",
    "You can see `sklearn`'s metrics [here](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics), in this case we will use [Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error).\n",
    "\n",
    "Below, we import `sklearn.metrics.mean_squared_error` using `from` import syntax and\n",
    "display what is the error between true targets and predicted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5243209861846072"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistance\n",
    "\n",
    "Training (`fitting`) process is often quite expensive (in time and or compute cost), while what we are after is the ability to predict on unseen data.\n",
    "\n",
    "We see our model works okay and we would like to save it for later use without the need to `train` on the data again.\n",
    "\n",
    "> Model persistence means saving your machine learning algorithm currently held in RAM (Random Access Memory) to a storage (usually hard drive) from which it can be reinstantiated at any point in time\n",
    "\n",
    "As per usual it's simple with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, BUT\n",
    "\n",
    "You made your first machine learning model in roughly `5` lines of code.\n",
    "Why would we need anything else?\n",
    "\n",
    "### Downsides\n",
    "\n",
    "As `sklearn` is very high level it doesn't require much knowledge to use as is.\n",
    "But __we have to know more__ in order to do machine learning well. What is missing here:\n",
    "\n",
    "- Why and what for? There are many more ways (and way more correct) to do machine learning\n",
    "- Knowledge of machine learning algorithms; we have to know which one to choose for which kind of problems\n",
    "- Knowledge of possible pitfalls; machine learning can easily go wrong. We have to know more about it in order to improve our model's performance\n",
    "- In-depth knowledge of the ideas; often it might be a good idea to implement major ideas on your own\n",
    "\n",
    "__We will do all of the above__, but hopefully you can see how easy and definitely not scary it can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn` tips\n",
    "\n",
    "- __Always try easiest solution first__. Create a weak baseline algorithm and check how it performs. Do not go straight to the most complicated ones! It is called [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor) in philosophy and machine learning also\n",
    "- Some algorithms have attributes you might be interested in. Those are usually suffixed by `_` underscore, for example `my_algorithm.interesting_attribute_`\n",
    "- Some `__init__` functions have __a lot of possible arguments__. Each of them influences how the algorithm works. But which are the most important and have the most influence? __In `sklearn` those arguments come in order from most influential to least__\n",
    "- Many `sklearn` algorithms provide `n_jobs` argument, which parallelizes `fit`, `predict` and other functions. You can use `n_jobs=-1` to use as many processes as there are virtual cores (it is often a reasonable amount), which improves performance tremendously.\n",
    "- __Use idiomatic `sklearn`__ - search the documentation, use pipelines if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- linear regression is \"hello world\" basic machine learning model\n",
    "- linear regression updates it's weight vector and bias in order to improve on the task\n",
    "- this update can be carried out via analytically calculated formula\n",
    "- the MSE loss is appropriate for many regression problems and is the most common loss function for this task"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('main': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
