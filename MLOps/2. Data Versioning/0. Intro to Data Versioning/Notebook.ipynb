{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Intro to Data Versioning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By this point, you have trained a model and used it to make predictions, so it is likely that you know the complications of retraining the model with an updated version of the data. This problem is exacerbated when you are working in a CI/CD environment, where data might be constantly ingested and cleaned. \n",
    "\n",
    "Imagine the following scenario, you have a model that is trained every day with a new version of the data. The model is then used to make predictions every day, and the predictions are monitored to trigger retraining of the model. Thus, the model is trained with a different version of the data than the one that is used to make predictions.\n",
    "\n",
    "However, how do you keep track of what data version is used to make each prediction? Maybe you want to revert to a previous version of the data because it was updated by a teammate, or maybe you want to revert to a previous version of the model that provided a better result. Whatever the reason, you need to know what version of the data was used to make each model, and in turn each prediction.\n",
    "\n",
    "> ## Keeping track of the data version is crucial in a CI/CD environment\n",
    "\n",
    "You can consider Data Versioning as a Git for data; after all, Git is a Version Control System. In fact, in many tools you will use Git along with Data Versioning.\n",
    "\n",
    "![Data Versioning](images/Data_versioning.png)\n",
    "\n",
    "However, there are some problems when keeping track of the data version:\n",
    "\n",
    "1. You need to add a version number to the data, and additionally, when working in teams, team members should know who did what to the data, just like when using a version control system, such as [Git](https://git-scm.com/).\n",
    "2. Datasets are usually quite large, so GitHub might be unable to handle them.\n",
    "\n",
    "Therefore, we need a solution that is able to keep track of the data version, and that is able to handle large datasets; we need Data Version Control tools.\n",
    "\n",
    "> ## Data Version Control tools will keep track of the data version as well as managing the storage of the data \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Version Control Tools"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are many data version control tools, in this lesson we are going to introduce a few of them, and in next lesson we will focus on a quite popular one called Data Version Control (DVC)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delta Lake"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Delta Lake](https://delta.io) is a data version control tool that uses the advantages of Lakehouse architecture to store data and keep track of it. Lakehouses architectures allow you to combine data warehouse and data lake storage, and then perfom analytics on top of both.\n",
    "\n",
    "Currently, many data science projects rely on data lakes that contain the raw and cleaned data, using an external ETL process to load the processed data to a warehouse and also back into the lake. On the other hand, Lakehouse architectures integrates these ETL processes into the data lake itself.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/Lakehouse.png\" width=600/></div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can check [Lakehouse Architectures here](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)\n",
    "\n",
    "As you can see in the image, metadata is also part of the information contained in the lake, and Delta Lake will treat this information as regular data.\n",
    "\n",
    "> ### In big data, even the metadata itself can be \"big data.\" Delta Lake treats metadata just like data \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As such, Delta Lake can store data versions of your data, and it uses a feature called `Time Travel` to access and revert to earlier versions of data for audits, rollbacks or to reproduce experiments. You can check more information about `Time Travel` [here](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using Delta Lake is a good idea if you want to stop worrying too much about managing the data versioning, and you want to focus on the data itself. However, it was built to be used on Spark and big data, and therefore, it might be unnecessarily powerful for your needs, and additionally, it uses a dedicated data format."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neptune"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Neptune](https://neptune.ai) is a metadata store that is intended to be used for running many experiments and keeping track of the metrics. It is composed of three major elements:\n",
    "\n",
    "1. Data Versioning\n",
    "2. Experiment Tracking\n",
    "3. Model Registry\n",
    "\n",
    "They work in conjuction with many other parts of the [MLOps](https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective) ecosystem.  \n",
    "\n",
    "![neptune](images/Neptune.webp)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up an account and start using Neptune is incredibly easy, and it provides a very user-friendly interface.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/Neptune_UI.png\" width=600/></div>\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/Neptune_UI2.png\" width=600/></div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The main \"issue\" with Neptune is that it is not focused on Data Versioning, and it is more focused on Experiment Tracking and Model Registry. This is of course not something necessarily bad, especially if you are not working with extremely large datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Git LFS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Git LFS](https://www.atlassian.com/git/tutorials/git-lfs) (Large File System) is a Git extension that allows you to store large files in Git repositories. It is used to store the data versioning files, and it is also used to store the model files. It works by using `pointers` to the large files, and as such, you can keep track of the `deltas` in your repository for each version of the data.\n",
    "\n",
    "Git LFS is very easy to implement in a data science project, and it works essentially the same way as a git repository. The main problem about Git LFS is that it is not designed to be used in production, and it doesn't include any integration tool (unless you are using a custom integration tool). It also doesn't bring the analytics capabilities that Delta Lake does."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pachyderm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Pachyderm](https://www.pachyderm.com) is a platform that integrates a variety of data science tools and services, including data versioning, experiment tracking, model registry, and data lake. The platform makes it easy to reproduce the results of machine learning models by managing the entire data workflow.\n",
    "\n",
    "Pachyderm leverages Docker containers to help you set the execution in a consistent environment across systems, which makes it easy to reproduce the same output. Thus, your team can use any languages or libraries you want, without any additional infrastructure overhead\n",
    "\n",
    "The platform has many integrated solutions that are very well documented. Despite the good documentation, the learning curve is quite steep."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DVC\n",
    "\n",
    "DVC is a tool which allows you to keep track of which datasets were used in which versions of the code, by creating small files that reference them, and storing those files in your github repos.\n",
    "\n",
    "In the next class, we will dive into DVC."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}