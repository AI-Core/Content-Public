{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kubeflow Pipelines\n",
    "\n",
    "> Platform in `kubeflow`, __defining end-to-end lifecycle of `ML` project__ and helping with re-using components\n",
    "\n",
    "One can do it using:\n",
    "- `SDK` (`python` in our case) - defines the pipeline\n",
    "- `UI` - visualizing created pipelines easily\n",
    "- `k8s` based engine which transpiles `kubeflow` format to `k8s` specific one (yes, `.yaml` files)\n",
    "\n",
    "## Concepts\n",
    "\n",
    "`Pipeline` consists of the following concepts we will understand before:\n",
    "- `pipeline`\n",
    "- `component`\n",
    "- `graph`\n",
    "- `experiment`\n",
    "- `run` and `recurring run`\n",
    "- `run trigger`\n",
    "- `step`\n",
    "- `output artifact`\n",
    "\n",
    "Along each, we will see how to define them using Python's SDK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation\n",
    "\n",
    "First, start `minikube` cluster, as we saw previously one can do it with `minikube start` (__you should still use it for local development__).\n",
    "\n",
    "> `kubeflow-pipelines` can be installed as a standalone via `k8s` `kustomize`\n",
    "\n",
    "We can do it directly from `github`, see three commands below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!export PIPELINE_VERSION=1.6.0\n",
    "!kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\"\n",
    "!kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io\n",
    "!kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref=$PIPELINE_VERSION\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You might need to wait a little bit for appropriate `POD`s to start.\n",
    "\n",
    "To see them, one can check available `POD`s and their status in `kubeflow` namespace:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!kubectl get pods --namespace kubeflow"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T18:49:17.218047Z",
     "start_time": "2021-08-23T18:49:16.934836Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once all of the above are `Running` (__notice some restarts might happen, don't worry about them__), run the following command to `forward` `80` port of `kubeflow`'s UI `POD` to `localhost:8080`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now open `localhost:8080` in our web browser to see the UI.\n",
    "\n",
    "It should look similar to this:\n",
    "\n",
    "![](./images/kubeflow-ui.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to delete `kubeflow-pipelines` we can use the following:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export PIPELINE_VERSION=1.6.0 # Optional, as we exported envvar previously\n",
    "\n",
    "# kubectl delete -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref=$PIPELINE_VERSION\"\n",
    "# kubectl delete -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing `SDK`\n",
    "\n",
    "> `kubeflow`'s `SDK` is provided as `PyPI` package and named `kfp`\n",
    "\n",
    "It communicates with `kubernetes` Python `SDK` and in-directly by transpiling graph to `.yaml` files (as we will see)\n",
    "\n",
    "As per usual, use `pip`  to install (__you should do it within `AiCore`'s `conda` environment__):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install kfp --upgrade"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `SDK` packages\n",
    "\n",
    "High level overview of provided functionalities after installation:\n",
    "- __[`kfp.compiler`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.compiler.html) - class and methods for compiling `dsl` to `.yaml`__:\n",
    "    - [`kfp.compiler.Compiler`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.compiler.html#module-kfp.compiler) - compiles `pipeline` functions to `yaml` workflows\n",
    "    \n",
    "An example schematic code using it (__we will see all parts together later__):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@kfp.dsl.pipeline(\n",
    "  name='name',\n",
    "  description='description'\n",
    ")\n",
    "def my_pipeline(a: int = 1, b: str = \"default value\"):\n",
    "  ...\n",
    "\n",
    "Compiler().compile(my_pipeline, 'path/to/workflow.yaml')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [`kfp.components`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html) - classes and methods for interacting with pipeline components\n",
    "- [`kfp.dsl`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.dsl.html) - contains the domain-specific language that you can use to define and interact with pipelines and components:\n",
    "    - includes `Pipeline` definition (as seen above)\n",
    "    - __core package of `SDK` we will use the most often__\n",
    "- [`kfp.client`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.client.html) - client libraries for [Kubeflow Pipelines API](https://www.kubeflow.org/docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec/) and allows us to:\n",
    "    - create experiments\n",
    "    - run pipelines\n",
    "    - upload pipelines\n",
    "- [`kfp.cli.diagnose_me`](https://github.com/kubeflow/pipelines/tree/master/sdk/python/kfp/cli/diagnose_me) - ways to debug environment interactively, returning various metadata useful for debugging the setup."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `kfp` in command line\n",
    "\n",
    "After installation `kfp` is also available as command-line tool:\n",
    "- `kfp pipeline <COMMAND>` - managing `pipeline`s, commands include:\n",
    "    - `get` - Gets detailed information about a Kubeflow pipeline from your Kubeflow Pipelines cluster.\n",
    "    - `list` - Lists the pipelines that have been uploaded to your Kubeflow Pipelines cluster\n",
    "    - `upload` - Uploads a pipeline to your Kubeflow Pipelines cluster.\n",
    "- `kfp run <COMMAND>` - manage `kubeflow`'s runs:\n",
    "    - `get` - Displays the details of a pipeline run.\n",
    "    - `list` - Lists recent pipeline runs.\n",
    "    - `submit` - Submits a pipeline run\n",
    "\n",
    "For example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!kfp pipeline --help"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Usage: kfp pipeline [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Manage pipeline resources\r\n",
      "\r\n",
      "Options:\r\n",
      "  --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  delete          Delete an uploaded KFP pipeline\r\n",
      "  get             Get detailed information about an uploaded KFP pipeline\r\n",
      "  list            List uploaded KFP pipelines\r\n",
      "  list-versions   List versions of an uploaded KFP pipeline\r\n",
      "  upload          Upload a KFP pipeline\r\n",
      "  upload-version  Upload a version of the KFP pipeline\r\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T19:32:33.000418Z",
     "start_time": "2021-08-23T19:32:32.053431Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Component\n",
    "\n",
    "> __Self-contained piece of code performing one step in `pipeline`__\n",
    "\n",
    "![](./images/kubeflow-graph.png)\n",
    "\n",
    "In the above image `Xgboost train` is an example of such component.\n",
    "\n",
    "> It is analogous to __larger function__ performing __one semantically valid addition__\n",
    "\n",
    "It also has (just like `functions`):\n",
    "- name\n",
    "- parameters/arguments\n",
    "- return values\n",
    "- body (code)\n",
    "\n",
    "> __Each component must be packaged as `Docker` image as they are standalone units of execution__\n",
    "\n",
    "### Component code\n",
    "\n",
    "There are two parts of such `python` code:\n",
    "- `client` - talks to endpoints to submit jobs, e.g. submitting `spark`'s `job`\n",
    "- `runtime` - actual code, e.g. creating `pyspark.sql.DataFrame` from `SparkSession`.\n",
    "\n",
    "Convention is to keep component's code within a `package` named by it, for example:\n",
    "- `/component` - `client` modules \n",
    "- `/component/component.py` - `client` code\n",
    "\n",
    "### Component definition\n",
    "\n",
    "- `metadata` - name, description etc.\n",
    "- `interface` - input/output specification (name, type, description, default value, etc)\n",
    "- `implementation` - actual code, __also defines how to get `outputs` out of it__\n",
    "\n",
    "> __Component is defined using `python`, hence we don't have to worry about transpiling to `k8s` readable `.yaml` definitions above__\n",
    "\n",
    "> ### In most cases, __make sure to check `Challenges.Mandatory.Components` for alternative!__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Python function-based components\n",
    "\n",
    "> __Python funciton-based components allow us to define any component SOLELY in `python`__\n",
    "\n",
    "This approach alleviates the additional steps for `component` definition, namely:\n",
    "- Defining `Docker` image file\n",
    "- Defining `.yaml` with component's definition\n",
    "\n",
    "__Above will be generated automatically from `python` code__, additional benefits include:\n",
    "- More readable as `component` is defined like function\n",
    "- Quicker to develop (and only `python` knowledge is necessary)\n",
    "\n",
    "On the other hand, __due to automation__:\n",
    "- Harder to customize (__although might be done after transpilation was finished by `kfp`__)\n",
    "- For more complicated use cases (e.g. `Docker` image with custom dependencies) we need to create `Dockerfile` anyway (as a base for automation)\n",
    "\n",
    "> Check `Challenges.Mandatory.Components` for a direct way of creation `component`! __THIS KNOWLEDGE IS MANDATORY AND WILL BE VERIFIED!__\n",
    "\n",
    "Before moving on, let's create a [`kfp.Client`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.client.html) instance which will be used from now on:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import kfp\n",
    "\n",
    "# We are using default values\n",
    "# Host is automatically inferred from within `jupyter notebook`s hence not specified\n",
    "# In our case it would be localhost\n",
    "client = kfp.Client()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T20:51:13.641199Z",
     "start_time": "2021-08-23T20:51:13.013944Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standalone Functions\n",
    "\n",
    "Before we start, a few things to `note` about functions __which differ largely from `Python`'s standard practices__:\n",
    "\n",
    "> __It should not use any code declared outside function__\n",
    "\n",
    "Something like this is prohibited:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = 12\n",
    "\n",
    "def foo():\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> __`import` statements USED WITHIN A FUNCTION__\n",
    "\n",
    "There is no apparent \"best practice\", but we suggest `import`ing every necessary dependency at the top of your function and by dividing them with code blocks, e.g.:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# We can't do that anymore!\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "def bar():\n",
    "    ###########################################################################\n",
    "    #\n",
    "    #                               IMPORTS\n",
    "    #\n",
    "    ###########################################################################\n",
    "\n",
    "    import numpy as np\n",
    "    import this  # Well, now I'm not so sure\n",
    "\n",
    "    ###########################################################################\n",
    "    #\n",
    "    #                                 SRC\n",
    "    #\n",
    "    ###########################################################################\n",
    "\n",
    "    return np.array([1])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T20:59:09.946746Z",
     "start_time": "2021-08-23T20:59:09.940990Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> __Helper functions defined within the function__\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "Drawbacks of this approach should be pretty visible right now (for larger/more complicated functions where transforming them into separate microservices is non-sensible):\n",
    "- Long and semi-readable code (we can only get this far with good practices)\n",
    "\n",
    "Alternative might be better for the following cases:\n",
    "- __Longer `Service` doing a lot of heavy-lifting__\n",
    "- Optimization while preserving code readability (disk `I/O` is expensive, hence we should prohibit it)\n",
    "- __There is an option to `cache` results__ but this still does not resolve readability problems\n",
    "\n",
    "One the other hand:\n",
    "- __One has to create a standalone program and define `.yaml` and `Dockerfile`__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple function-based `component`\n",
    "\n",
    "Example below outlines the necessary steps:\n",
    "\n",
    "1. __Create `standalone` function__ (make it `pure`, without any side-effects)\n",
    "\n",
    "Please note that:\n",
    "- We should use `typing` Python feature for `type` inference\n",
    "- __We will later see repercusions during returning multiple values__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def add(a: float, b: float) -> float:\n",
    "  '''Calculates sum of two arguments'''\n",
    "  return a + b"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T21:10:18.079352Z",
     "start_time": "2021-08-23T21:10:18.066646Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. __Create [`kfp.dsl.ContainerOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.dsl.html#kfp.dsl.ContainerOp) using `kfp.components.create_component_from_func`__\n",
    "\n",
    "Please note that:\n",
    "- Factory function will be created (which we can handle in multiple different ways)\n",
    "- __This `op` should be used within `Pipeline` (described in more detail later)__\n",
    "- Creates `.yaml` component definition automagically for us"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "add_op = kfp.components.create_component_from_func(\n",
    "    add, output_component_file=\"add_component.yaml\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T21:13:41.951459Z",
     "start_time": "2021-08-23T21:13:41.863270Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. __Create `Pipeline` that runs our `op`(s)__\n",
    "\n",
    "Please note `comments` within the code:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "@kfp.dsl.pipeline(\n",
    "  name='Addition pipeline',\n",
    "  description='An example pipeline that performs addition calculations.'\n",
    ")\n",
    "def add_pipeline(\n",
    "  a='1',\n",
    "  b='7',\n",
    "):\n",
    "  # Passes a pipeline parameter and a constant value to the `add_op` factory\n",
    "  # function.\n",
    "  first_add_task = add_op(a, 4)\n",
    "  # Passes an output reference from `first_add_task` and a pipeline parameter\n",
    "  # to the `add_op` factory function. For operations with a single return\n",
    "  # value, the output reference can be accessed as `task.output` or\n",
    "  # `task.outputs['output_name']`.\n",
    "  second_add_task = add_op(first_add_task.output, b)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T21:21:17.472041Z",
     "start_time": "2021-08-23T21:21:17.459209Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. __Create and `run` pipeline from function__\n",
    "\n",
    "> __Please run port forwarding of `kubeflow` as described previously to see the `run`__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Specify argument values for your pipeline run.\n",
    "arguments = {'a': '7', 'b': '8'}\n",
    "\n",
    "# Create a pipeline run, using the client you initialized in a prior step.\n",
    "client.create_run_from_pipeline_func(add_pipeline, arguments=arguments)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/9b376543-1d00-40f9-bafe-55101b7a6061\" target=\"_blank\" >Experiment details</a>."
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/85870662-5aa5-4c56-a73e-3a343237565c\" target=\"_blank\" >Run details</a>."
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=85870662-5aa5-4c56-a73e-3a343237565c)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T21:22:50.504639Z",
     "start_time": "2021-08-23T21:22:50.306215Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should have your run within the `UI`, __take a moment to explore relevant info in different `UI` sections!__\n",
    "\n",
    "![](./images/example-add-run.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using packages in functions\n",
    "\n",
    "> In order to use custom `package`s one has to install them within `Docker` environment\n",
    "\n",
    "There are a few ways to obtain that, in the order \"best\" to \"worst\":\n",
    "1. Contained within `Docker` image - __choose appropriate base image for your `microservice`__ to alleviate any headaches; __use `base_image` argument of `kfp.components.create_component_from_func`__\n",
    "2. Install packages within `Docker` image; __useful when `Docker` image has most of the `packages` and a few additional are required__; __use `packages_to_install` argument__\n",
    "3. Install packages using `subprocess` (from your `function` code); __discouraged__; __only for local packages, SHOULD BE AVOIDED THOUGH__\n",
    "\n",
    "For example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kfp.components.create_component_from_func(\n",
    "    # output_component_file is optional\n",
    "    my_op,\n",
    "    output_component_file=\"add_component.yaml\",\n",
    "    base_image=\"tensorflow/tensorflow:1.11.0-py3\",\n",
    "    packages_to_install=(\"torchdata==0.2.0\", \"torchlayers==0.1.1\"),\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Additional info about `create_component_from_func`\n",
    "\n",
    "- Default image: `python:3.7`\n",
    "- __For larger dependencies create `Docker` base image from scratch and deploy it__ as it:\n",
    "    - Component runs way faster (no need to download packages)\n",
    "    - Not as error prone (less likely to be OS-dependent)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "> __`inputs`, `outputs` and data passing in `kubeflow`__\n",
    "\n",
    "In general:\n",
    "- `inputs` are `CLI` arguments for `Docker` container within `POD`\n",
    "- `outputs` are returned as files\n",
    "\n",
    "Parameters are also passed in different ways:\n",
    "- basic types (e.g. `float`, `int`, short `str`) __are passed by value__\n",
    "- parameters passed by file  include things like:\n",
    "    - `csv` files\n",
    "    - images\n",
    "    - datasets\n",
    "    - __anything larger__\n",
    "- __Larger `parameters` are stored in specified `PersistentVolume`!__\n",
    "\n",
    "## Inferring `types`\n",
    "\n",
    "> `kubeflow`'s components created from functions can infer `dtype`s __via Python's typing features__\n",
    "\n",
    "Let's see generated `add`'s component `.yaml`: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!cat ./add_component.yaml"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "name: Add\r\n",
      "description: Calculates sum of two arguments\r\n",
      "inputs:\r\n",
      "- {name: a, type: Float}\r\n",
      "- {name: b, type: Float}\r\n",
      "outputs:\r\n",
      "- {name: Output, type: Float}\r\n",
      "implementation:\r\n",
      "  container:\r\n",
      "    image: python:3.7\r\n",
      "    command:\r\n",
      "    - sh\r\n",
      "    - -ec\r\n",
      "    - |\r\n",
      "      program_path=$(mktemp)\r\n",
      "      printf \"%s\" \"$0\" > \"$program_path\"\r\n",
      "      python3 -u \"$program_path\" \"$@\"\r\n",
      "    - |\r\n",
      "      def add(a, b):\r\n",
      "        '''Calculates sum of two arguments'''\r\n",
      "        return a + b\r\n",
      "\r\n",
      "      def _serialize_float(float_value: float) -> str:\r\n",
      "          if isinstance(float_value, str):\r\n",
      "              return float_value\r\n",
      "          if not isinstance(float_value, (float, int)):\r\n",
      "              raise TypeError('Value \"{}\" has type \"{}\" instead of float.'.format(str(float_value), str(type(float_value))))\r\n",
      "          return str(float_value)\r\n",
      "\r\n",
      "      import argparse\r\n",
      "      _parser = argparse.ArgumentParser(prog='Add', description='Calculates sum of two arguments')\r\n",
      "      _parser.add_argument(\"--a\", dest=\"a\", type=float, required=True, default=argparse.SUPPRESS)\r\n",
      "      _parser.add_argument(\"--b\", dest=\"b\", type=float, required=True, default=argparse.SUPPRESS)\r\n",
      "      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\r\n",
      "      _parsed_args = vars(_parser.parse_args())\r\n",
      "      _output_files = _parsed_args.pop(\"_output_paths\", [])\r\n",
      "\r\n",
      "      _outputs = add(**_parsed_args)\r\n",
      "\r\n",
      "      _outputs = [_outputs]\r\n",
      "\r\n",
      "      _output_serializers = [\r\n",
      "          _serialize_float,\r\n",
      "\r\n",
      "      ]\r\n",
      "\r\n",
      "      import os\r\n",
      "      for idx, output_file in enumerate(_output_files):\r\n",
      "          try:\r\n",
      "              os.makedirs(os.path.dirname(output_file))\r\n",
      "          except OSError:\r\n",
      "              pass\r\n",
      "          with open(output_file, 'w') as f:\r\n",
      "              f.write(_output_serializers[idx](_outputs[idx]))\r\n",
      "    args:\r\n",
      "    - --a\r\n",
      "    - {inputValue: a}\r\n",
      "    - --b\r\n",
      "    - {inputValue: b}\r\n",
      "    - '----output-paths'\r\n",
      "    - {outputPath: Output}\r\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T22:52:10.717862Z",
     "start_time": "2021-08-23T22:52:10.557505Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the same time, we can see how much code was automatically generated for us by `kubeflow`, which includes, amongst other things:\n",
    "- argument parsing via `argparse` module\n",
    "- outputing values __and serializing them to desired type__\n",
    "- saving data within `POD`s storage (`PersistentVolume`)\n",
    "- whole `.yaml` structure\n",
    "\n",
    "> Above code is not meant to be readable as it's automatically generated!\n",
    "\n",
    "So, in this case, `type`s were inferred based on __function signature__. \n",
    "\n",
    "> __If we don't provide function signature it is assumed that we are passing `str` types!__\n",
    "\n",
    "This function returns one value though, what if we wanted to return multiple values?\n",
    "\n",
    "> __Use `NamedTuple` from `typing` module to decorate the function appropriately!__\n",
    "\n",
    "Let's see a more complicated example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def multiple_return_values_example(a: float, b: float) -> NamedTuple(\n",
    "  'ExampleOutputs',\n",
    "  [\n",
    "    ('sum', float),\n",
    "    ('product', float),\n",
    "    ('mlpipeline_ui_metadata', 'UI_metadata'),\n",
    "    ('mlpipeline_metrics', 'Metrics')\n",
    "  ]):\n",
    "  \"\"\"Example function that demonstrates how to return multiple values.\"\"\"\n",
    "  sum_value = a + b\n",
    "  product_value = a * b\n",
    "\n",
    "  # Export a sample tensorboard\n",
    "  metadata = {\n",
    "    'outputs' : [{\n",
    "      'type': 'tensorboard',\n",
    "      'source': 'gs://ml-pipeline-dataset/tensorboard-train',\n",
    "    }]\n",
    "  }\n",
    "\n",
    "  # Export two metrics\n",
    "  metrics = {\n",
    "    'metrics': [\n",
    "      {\n",
    "        'name': 'sum',\n",
    "        'numberValue':  float(sum_value),\n",
    "      },{\n",
    "        'name': 'product',\n",
    "        'numberValue':  float(product_value),\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "\n",
    "  from collections import namedtuple\n",
    "  example_output = namedtuple(\n",
    "      'ExampleOutputs',\n",
    "      ['sum', 'product', 'mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "  return example_output(sum_value, product_value, metadata, metrics)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This one returns `metadata` for `UI` and `metrics`.\n",
    "\n",
    "> Special `str` values can be used for these two values __in order for them to be readable by `metrics` and `UI` respectively!__\n",
    "\n",
    "We have to also return `namedtuple` __and this is the only way to forward multiple arguments!__\n",
    "\n",
    "> Please note, these values __will be saved to `disk` anyway__\n",
    "\n",
    "### Caching\n",
    "\n",
    "A little sidenote:\n",
    "\n",
    "> ### `kubeflow` provides caching out of the box\n",
    "\n",
    "How does it work?\n",
    "\n",
    "- If `component` was run previously __with the same arguments__ this component __will not run!__\n",
    "- Instead, outputs from `PersistentVolume` of choice will be forwarded to the next `component` within `pipeline`\n",
    "\n",
    "One can disable this feature or __force recalculation after some period of time__. See [here](https://www.kubeflow.org/docs/components/pipelines/caching/) and [here](https://www.kubeflow.org/docs/components/pipelines/caching-v2/) for `V2` SDK."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passing parameters by file\n",
    "\n",
    "> __In most cases `parameters` will be files (e.g. dataset) that we would like to operate on__\n",
    "\n",
    "This raises an obvious issue, namely:\n",
    "\n",
    "> What if we save `data` within the function and __return `None` implicitly__?\n",
    "\n",
    "In this case, __there is no way to infer from function's signature return WHAT IS ACTUALLY RETURNED!__\n",
    "(there is, but it is not Python compliant with static checkers like `mypy`).\n",
    "\n",
    "> ### We can use special `kfp.components` types __to mark what is returned by the function__\n",
    "\n",
    "First, let's see a simple example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_text_lines(\n",
    "    source_path: comp.InputPath(str),\n",
    "    odd_lines_path: comp.OutputPath(str),\n",
    "    even_lines_path: comp.OutputPath(str),\n",
    "):\n",
    "    \"\"\"Splits a text file into two files, with even lines going to one file\n",
    "    and odd lines to the other.\"\"\"\n",
    "\n",
    "    with open(source_path, \"r\") as reader:\n",
    "        with open(odd_lines_path, \"w\") as odd_writer:\n",
    "            with open(even_lines_path, \"w\") as even_writer:\n",
    "                while True:\n",
    "                    line = reader.readline()\n",
    "                    if line == \"\":\n",
    "                        break\n",
    "                    odd_writer.write(line)\n",
    "                    line = reader.readline()\n",
    "                    if line == \"\":\n",
    "                        break\n",
    "                    even_writer.write(line)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is pretty self-explanatory, but:\n",
    "- You can find other `Input` definitions (e.g. binary) [here](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html#kfp.components.InputBinaryFile)\n",
    "- You can find other `Output` definitions (e.g. binary) [here](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html#kfp.components.OutputBinaryFile)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Components Best Practices\n",
    "\n",
    "> Below is a compressed set of best practices (one can find full list [here](https://www.kubeflow.org/docs/components/pipelines/sdk/best-practices/))\n",
    "\n",
    "- Components __should use local files unless not possible__ (Cloud ML Engine and BigQuery require Cloud Storage staging paths)\n",
    "- Aforementioned __pure components__ (e.g. no side-effects and modifying anything without \"telling `kubeflow`\")\n",
    "- __Mix and match programming languages__ - DO NOT STICK TO `python` IF IT IS NOT BENEFICIAL! __How?__:\n",
    "    - Create `Docker` image containing `golang` app which requires some `input`s and outputs\n",
    "    - Define `.yaml` directly\n",
    "    - __Use inter-language formats for data exchange__ (e.g. `JSON`, `CSV`, `ProtoBuf` etc.)\n",
    "    - One can do small pre-processing for different languages with `shell` scripts (__SMALL ONES!__)\n",
    "- __One output == one file__\n",
    "- __Do not pollute with temporary data__. Why? __It might/will be preserved by `PersistentVolume`__\n",
    "- __STAY LOCAL FOR AS LONG AS POSSIBLE__ - it's always easier and less stressful, e.g. unit testing specific component\n",
    "- __Test in isolation__ - single container; if not possible, run `minikube` or a-like before going with `kubeadm` and full-blown cluster (way easier to debug)\n",
    "\n",
    "### Feedback from AiCore\n",
    "\n",
    "- __Do not use function-based components for more complicated workflows__ as it allows you to use packages and `python` best practices more freely\n",
    "- __Do not \"overkill\"__: keep the services within \"reasonable size\", for example:\n",
    "    - `microservice` to transform data instead of:\n",
    "        - `microservice` to load data\n",
    "        - `microservice` to perform one operation on data\n",
    "        - `microservice` to rotate image...\n",
    "    - __Why?__ I/O is expensive and we should try to minimize it if possible"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline\n",
    "\n",
    "> __Description of an ML `workflow`, including all of the `component`s in the `workflow` and how they combine in the form of a `graph`__\n",
    "\n",
    "Usually, it is our code packaged in `Docker` image with some `inputs` and `outputs` (as we will later see)\n",
    "\n",
    "Looking at the part of this pipeline one can notice that:\n",
    "- Some of the components can easily run in parallel (in a different `POD` scheduled on some `Node`)\n",
    "- They are directly dependent on the previous steps (similar to `Airflow`)\n",
    "- Data is shared via `artifacts` __as `POD`s do not share data directly__\n",
    "\n",
    "Above `graph` is defined via `SDK` (`python`) and specifically created `dsl` (or rather pseudo-dsl) for this task.\n",
    "\n",
    "> We will see how to create the whole structure in `python` after all of the concepts are described"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining `Pipeline`\n",
    "\n",
    "Now, let's see how one can define `Pipeline` using `kfp`, but before that, let's define `component`s which:\n",
    "- First one downloads `.tar.gz` file and returns CSV\n",
    "- Another one __is not even defined__ and downloads resource from `url`\n",
    "- Last one will be the `add` component just to showcase the functionality of `conditional` flow in `dsl`\n",
    "\n",
    "Before we do that, just to brush up `python`, let's make `kfp.components.create_component_from_func` a __configurable decorator__\n",
    "\n",
    "> `kfp.components.create_component_from_func` can be used as a decorator __but only with default arguments__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import functools\n",
    "\n",
    "# Wrapper\n",
    "@functools.wraps(kfp.components.create_component_from_func)\n",
    "def our_component_from_func(*args, **kwargs)\n",
    "    def wrapper(function):\n",
    "        return kfp.components.create_component_from_func(function, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@our_component_from_func(\n",
    "    output_component_file=\"component.yaml\",  # This is optional. It saves the component spec for future use.\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=[\"pandas==1.1.4\"],\n",
    ")\n",
    "def merge_csv(file_path: comp.InputPath(\"Tarball\"), output_csv: comp.OutputPath(\"CSV\")):\n",
    "    import glob\n",
    "    import tarfile\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    tarfile.open(name=file_path, mode=\"r|gz\").extractall(\"data\")\n",
    "    df = pd.concat(\n",
    "        [pd.read_csv(csv_file, header=None) for csv_file in glob.glob(\"data/*.csv\")]\n",
    "    )\n",
    "    df.to_csv(output_csv, index=False, header=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can __reuse components__, simply provide the `URI` resource which contains `.yaml` specification:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "web_downloader_op = kfp.components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/web/Download/component.yaml\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With everything in place we can define `pipeline`.\n",
    "\n",
    "In order to do that:\n",
    "1. Define `pipeline` function\n",
    "2. Decorate `pipeline` with `dsl.pipeline` and provide necessary information there\n",
    "3. Pass it to `client.create_run_from_pipeline_func` as seen before __OR__\n",
    "3. Compile it and run from `UI`\n",
    "\n",
    "Let's see all of that together"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@kfp.dsl.pipeline(name=\"Example pipeline\", description=\"Shows basics of pipelines\")\n",
    "# Define a pipeline and create a task from a component:\n",
    "# We don't have to specify types\n",
    "def my_pipeline(url, run_add: bool):\n",
    "  web_downloader_task = web_downloader_op(url=url)\n",
    "  merge_csv_task = create_step_merge_csv(file=web_downloader_task.outputs['data'])\n",
    "  # Only `if`\n",
    "  with kfp.dsl.Condition(run_add):\n",
    "      first_add_task = add_op(a, 4)\n",
    "  # The outputs of the merge_csv_task can be referenced using the\n",
    "  # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the first option to run it (non-interactively):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client.create_run_from_pipeline_func(\n",
    "    my_pipeline,\n",
    "    arguments={\n",
    "        \"url\": \"https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz\",\n",
    "        \"run_add\": False,\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another to run it interactively via `UI` within `pipelines` tab:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path='pipeline.yaml',\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-AiCore] *",
   "language": "python",
   "name": "conda-env-.conda-AiCore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}