{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# What is ML Observability and Monitoring?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Problem\n",
    " \n",
    "Machine Learning systems are fundamentally different from traditional software systems because they aim to continuously optimise some metric and the best way to optimise it can change over time. For example, music recommendation systems might be designed to maximise user listening time, but as music trends and the user's taste change over time, the solution becomes invalid.\n",
    "\n",
    "As such, it's important to have an understanding of a model's performance and general status at all stages of the its lifecycle.\n",
    "\n",
    "Machine learning models might be frequently retrained on data from the real world, and the distribution of that data can change over time. This gives data scientists the additional concern that the model's predictions degrade when deployed into production. There is a good chance when your model is deployed into production it will not perform as well as it did on your training data, this is where monitoring and observability are needed. \n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"./images/model_deployment_fail.png\"\n",
    "     alt=\"Example of Data Drift\"\n",
    "     style=\"float: centre; margin-right: 10px;\" \n",
    "     width=\"600\"\n",
    "     height=\"225\"/></div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monitoring vs Observability\n",
    " \n",
    "Traditional model monitoring is concerned with creating alerts on key model performance metrics, which could be altered if the model data begins to drift, or metrics such as accuracy begin to drop. \n",
    "\n",
    "> The key difference between monitoring and observability is that while monitoring will alert you to the problem it will not give you an insight as to why. Observability helps you identify this root cause. \n",
    "\n",
    "Using ML Observability we can gain a deeper understanding of our model's performance throughout all stages of its development lifecycle to pinpoint the root cause of the issue at the correct stage of its life cycle. Observability is concerned with collecting model evaluations during training, validation, and production and applying analytics to these evaluations to gain insight into the degradation of your model. This can be the difference between a team not being able to correct problems with their model and a team that can apply solutions and improves their model quickly. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Why can performance of ML systems degrade over time?\r\n",
    "\r\n",
    "### Change in Measure or Transformations\r\n",
    "\r\n",
    "One cause of degradation in performance of your model is feature transformations in your code may not be consistent between the trained model and the production model. This can be a common problem due to a lack of feature version control so there is confusion about exactly the transformations used between the training model and production. If these transformations are inconsistent between the models then the performance of the model can perform poorly in deployment.\r\n",
    "\r\n",
    "A change in measure of your data can occur due to a change in the data pipeline upstream that feeds your model which can impact your model's performance. For instance when working with a model that takes in image data from cameras. If the cameras are updated to a model with a different resolution, then this change in resolution measure could degrade the model's performance.\r\n",
    "\r\n",
    "### Model Drift\r\n",
    " \r\n",
    "One of the main causes of a machine learning model not performing to the same standard in production is due to **model drift**. Since machine learning models in production are taking in data from the real world model drift can happen due to your data distribution changing over time. Model drift occurs when the prediction accuracy of our deployed model begins to \"drift\" from it's training performance due to new input data. This can result in a degradation of the model's predictive power leading to your model performing poorly. Model Drift can be classified into two main types.<br>\r\n",
    "\r\n",
    "### Types of Drift\r\n",
    " \r\n",
    "<li><b>Concept drift</b> occurs when the statistical properties of you target (dependent) variable change. This could be due to meaning of what you are trying to predict changes and so the model will not be able to predict well on its updated definition. For instance, if you were tracking fraudulent transactions but what constitutes a fraudulent transaction changes over time then the model will not predict well on the updated definition.</li><br>\r\n",
    "\r\n",
    "\r\n",
    "<li><b>Data drift</b> occurs when the statistical properties of the independent variables change for examples their feature distribution changes or correlation between variables is change. \r\n",
    "\r\n",
    "The classic example of natural drift in data is a change due to seasonality.</li><br>\r\n",
    "\r\n",
    "<div style=\"text-align:center\"><img src=\"./images/data_drift.png\"\r\n",
    "     alt=\"Example of Data Drift\"\r\n",
    "     style=\"float: centre; margin-right: 10px;\" \r\n",
    "     width=\"400\"\r\n",
    "     height=\"300\"/></div>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How can Monitoring and Observability Help?\n",
    "\n",
    "### Problem Detection\n",
    "\n",
    "The first goal of an observability tool is to alert you to any problems with the model so that these can be resolved promptly. Since your models will be served to customers in real-time you don't want, days, weeks, or hours to go by without your tool alerting you of any issues. This can mean many things depending on the model's development stage. During model creation, the tool might alert you to problems with the model or data allowing you to quickly identify the problem and fix it increasing productivity. While in production your tool might alert you to a decrease in performance due to data drift so that the model can be retrained fixing performance issues.\n",
    "\n",
    "### Time to Resolution\n",
    "\n",
    "The first stage is to detect the problem with your model but this isn't where Observability ends. A good Observability tool will allow you to detect where the problem occurs and give you the steps to resolve the problem. From before we know that there are many reasons a model's performance can degrade the observability tool needs to guide the user to the data, feature store or transformations identifying the issue and providing a method for resolution. For example, if the transformations across training and deployment are consistent then the tool will alert you to the issue, display the inconsistencies, and give suggestions to fix.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing Observability\r\n",
    "\r\n",
    "Observability can be gain by implementing an Observability platform in your machine learning pipeline. The platform will be able to analyse degradation in your model and point you to the cause of your model's poor performance. Observability platforms employ a <b>model evaluation store</b> to observe your models and perform some key tasks that allow you to observe your model's performance, alert you to issues and offer solutions to the problems you may be facing.<br>\r\n",
    "\r\n",
    "<div style=\"text-align:center\"><img src=\"./images/ML_pipeline.png\"\r\n",
    "     alt=\"Example of Data Drift\"\r\n",
    "     style=\"float: centre; margin-right: 10px;\" \r\n",
    "     width=\"1000\"\r\n",
    "     height=\"300\"/></div>\r\n",
    "\r\n",
    "### Key Features of an Evaluation Store\r\n",
    "\r\n",
    "<li>Notify your team of any data drift issues, quality of data problems, or degradation of the model's performance.</li>\r\n",
    "<li>Alert you on deployment if your production data differs too much from your training data.</li>\r\n",
    "<li>Designed to analyse and alert you to slices of data that may cause poor performance or have caused poor performance in the past.</li>\r\n",
    "<li>Enable your team to understand changes in model performance and why the change occurred.</li>\r\n",
    "<li>Ability to run A/B testing to evaluate different versions of your model.</li>\r\n",
    "<li>Enable a feedback loop to retrain your model if performance drops below the desired level.</li>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ML Observability Platforms\r\n",
    "\r\n",
    "<li>Fiddler</li>\r\n",
    "<li>superwise.ai</li>\r\n",
    "<li>Arize</li>\r\n",
    "<li>WhyLabs</li>\r\n",
    "<li>Seldon</li>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\r\n",
    "\r\n",
    "Keeping Observability and Monitoring in mind and integrating it into your ML models development process can be the difference between a model failing in production or serving your customers for a long time into the future. It can give you deep insight into how your model is performing and alert you to any performance issues, improving the iterative process allowing for a quicker resolution time. The platform can also help with explanability of your model to stakeholders so they understand the issues and are confident there is a process in place to fix them. These methods will allow you to maintain, explain, develop and more confidently launch your model into productions at all stages of its lifecycle."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}