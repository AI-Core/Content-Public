{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying models\n",
    "\n",
    "Once we have our model saved we can easily deploy it to various services, namely:\n",
    "- [locally](https://www.mlflow.org/docs/latest/models.html#deploy-mlflow-models) with REST API (either inside `docker` container or with `conda` environment)\n",
    "- [Microsoft's Azure ML](https://www.mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-microsoft-azure-ml)\n",
    "- [Amazon SageMaker](https://www.mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker)\n",
    "- [Apache UDF](https://www.mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf)\n",
    "- Others, maintained by community deployment plugins (for example `torchserve`), check out [here](https://www.mlflow.org/docs/latest/plugins.html#deployment-plugins)\n",
    "\n",
    "Let's see `mlflow models` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow models --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models build-docker\n",
    "\n",
    "> This subcommand creates a docker image and places our model inside it\n",
    "\n",
    "After this we can serve the model by running created image (by default port `8080` is exposed so we can easily map it).\n",
    "\n",
    "Let's see this command in more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow models build-docker --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`python_flavor` is the default one and every specific integration is compatible with it__ (see more details [here](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html))\n",
    "\n",
    "### model serve\n",
    "\n",
    "> Runs a basic webserver (created via `flask`) which we can query (e.g. using `curl`)\n",
    "\n",
    "We can specify (amongst other things):\n",
    "- `--model-uri` - model resource (mandatory)\n",
    "- `--workers` - number of parallel workers handling requests\n",
    "- `--port` - on which port the server will listen for requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow models serve --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models predict\n",
    "\n",
    "> Allows us to query model with a file (`.csv` or `.json`) (__useful for testing!__)\n",
    "\n",
    "Let's see the possibilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow models predict --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying deployed model\n",
    "\n",
    "Once we deployed the model (via `docker` or `flask` webserver) we can query it (from other machines or from `localhost` also). \n",
    "\n",
    "Requests are done via sending `json` text strings to `/invocations` endpoint. There are a few possibilities to send the data:\n",
    "- JSON-serialized pandas DataFrames in the split orientation (`data = pandas_df.to_json(orient='split')`)\n",
    "- JSON-serialized pandas DataFrames in the records orientation (discouraged)\n",
    "- CSV-serialized pandas DataFrames (`data = pandas_df.to_csv()`)\n",
    "- Tensor input formatted as described in TF Servingâ€™s API docs where the provided inputs will be cast to Numpy arrays\n",
    "\n",
    "Each of the above can be seen below (please notice `content/type` specification for different versions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split-oriented DataFrame input\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{\n",
    "    \"columns\": [\"a\", \"b\", \"c\"],\n",
    "    \"data\": [[1, 2, 3], [4, 5, 6]]\n",
    "}'\n",
    "\n",
    "# record-oriented DataFrame input (fine for vector rows, loses ordering for JSON records)\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json; format=pandas-records' -d '[\n",
    "    {\"a\": 1,\"b\": 2,\"c\": 3},\n",
    "    {\"a\": 4,\"b\": 5,\"c\": 6}\n",
    "]'\n",
    "\n",
    "# numpy/tensor input using TF serving's \"instances\" format\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{\n",
    "    \"instances\": [\n",
    "        {\"a\": \"s1\", \"b\": 1, \"c\": [1, 2, 3]},\n",
    "        {\"a\": \"s2\", \"b\": 2, \"c\": [4, 5, 6]},\n",
    "        {\"a\": \"s3\", \"b\": 3, \"c\": [7, 8, 9]}\n",
    "    ]\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also encode more complex data before sending the request (e.g. images could be encoded using `base64` and automatically decoded by MLFlow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record-oriented DataFrame input with binary column \"b\"\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json; format=pandas-records' -d '[\n",
    "    {\"a\": 0, \"b\": \"dGVzdCBiaW5hcnkgZGF0YSAw\"},\n",
    "    {\"a\": 1, \"b\": \"dGVzdCBiaW5hcnkgZGF0YSAx\"},\n",
    "    {\"a\": 2, \"b\": \"dGVzdCBiaW5hcnkgZGF0YSAy\"}\n",
    "]'\n",
    "\n",
    "# record-oriented DataFrame input with datetime column \"b\"\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json; format=pandas-records' -d '[\n",
    "    {\"a\": 0, \"b\": \"2020-01-01T00:00:00Z\"},\n",
    "    {\"a\": 1, \"b\": \"2020-02-01T12:34:56Z\"},\n",
    "    {\"a\": 2, \"b\": \"2021-03-01T00:00:00Z\"}\n",
    "]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we've seen how MLFlow can be used to deploy models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
