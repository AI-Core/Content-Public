{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MLFlow\n",
    "\n",
    "> __MLflow is an open source platform for managing the end-to-end machine learning lifecycle.__\n",
    "\n",
    "`mlflow` provides a few components we can use to manage our machine learning experimentation and deployment:\n",
    "\n",
    "- Packaging ML code for reproducibility/sharing (dependencies, models etc.)\n",
    "- Tracking experiments (compare results between runs)\n",
    "- Deploying models to various inference platforms\n",
    "- Central model store for MLFlow models (model versioning, stage transitions, annotations)\n",
    "\n",
    "> __MLFlow provides integrations with popular frameworks like `sklearn`, `pytorch` or `spark`__\n",
    "\n",
    "> __MLFlow provides APIs not only for `python`, but also for `R` or `java`__ (and REST API/CLI interface for other languages)\n",
    "\n",
    "We will focus on Python and shell usage, but keep the above in mind"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation \n",
    "\n",
    "As `mlflow` is written in Python we can use `pip`/`conda` or other package manager to install.\n",
    "\n",
    "If you have `pytorch` or other integration installed, it will be picked up automatically by `mlflow` (no need for extras this time)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!pip install mlflow"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: mlflow in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (1.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (5.4.1)\n",
      "Requirement already satisfied: cloudpickle in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (1.6.0)\n",
      "Requirement already satisfied: pytz in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (2021.1)\n",
      "Requirement already satisfied: sqlalchemy in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (1.4.15)\n",
      "Requirement already satisfied: alembic<=1.4.1 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (1.4.1)\n",
      "Requirement already satisfied: entrypoints in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: pandas in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: databricks-cli>=0.8.7 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (0.14.3)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (3.14.0)\n",
      "Requirement already satisfied: requests>=2.17.3 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (2.25.1)\n",
      "Requirement already satisfied: querystring-parser in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: Flask in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (1.1.2)\n",
      "Requirement already satisfied: packaging in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (20.9)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (0.4.1)\n",
      "Requirement already satisfied: gunicorn in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (20.1.0)\n",
      "Requirement already satisfied: docker>=4.0.0 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (5.0.0)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (3.1.18)\n",
      "Requirement already satisfied: numpy in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (1.20.1)\n",
      "Requirement already satisfied: click>=7.0 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (7.1.2)\n",
      "Requirement already satisfied: prometheus-flask-exporter in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from mlflow) (0.18.2)\n",
      "Requirement already satisfied: Mako in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from alembic<=1.4.1->mlflow) (1.1.4)\n",
      "Requirement already satisfied: python-dateutil in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from alembic<=1.4.1->mlflow) (2.8.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from alembic<=1.4.1->mlflow) (1.0.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.7)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from docker>=4.0.0->mlflow) (1.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from gitpython>=2.1.0->mlflow) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (4.0.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from requests>=2.17.3->mlflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from requests>=2.17.3->mlflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from requests>=2.17.3->mlflow) (2.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from sqlalchemy->mlflow) (1.1.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from Flask->mlflow) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from Flask->mlflow) (1.0.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from Flask->mlflow) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\n",
      "Requirement already satisfied: setuptools>=3.0 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from gunicorn->mlflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from packaging->mlflow) (2.4.7)\n",
      "Requirement already satisfied: prometheus-client in /Users/ice/miniconda3/envs/main/lib/python3.8/site-packages (from prometheus-flask-exporter->mlflow) (0.9.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Projects\n",
    "\n",
    "> __MLFlow Projects are mainly CONVENTION to organize and describe your code to let others (people, automation pipelines) easily run it__\n",
    "\n",
    "Projects are usually `git` repositories and allow you to specify (in varying level of detail) required environment (either `conda` or `docker`, eventually `system` specified but this is discouraged) via:\n",
    "- directory structure\n",
    "- `MLproject` file in git's root directory\n",
    "\n",
    "Note:\n",
    "- the `MLproject` file should be a yaml file, but it should have no extension\n",
    "  - save it as `MLroject`, not `MLroject.yaml`\n",
    "\n",
    "### Directories\n",
    "\n",
    "> Structuring our code via directories is enough to create basic `MLFlow` project, __but specifying `MLproject` is a better option__\n",
    "\n",
    "In case where there is no `MLproject.yaml` the following takes place:\n",
    "- __Name of the project__ - name of the project's root directory (e.g. git's root)\n",
    "- __Conda environment__ - if `conda.yaml` is available in the root\n",
    "- __Any `.py`/`.sh` file in the project can be an entry point__ (more about running projects later)\n",
    "\n",
    "One can obtain `conda.yaml` file via a simple command (provided you are inside the conda environment while running it):\n",
    "\n",
    "```bash\n",
    "conda env export [--from-history] > conda.yaml\n",
    "```\n",
    "\n",
    "`--from-history` requests only packages you have explicitly installed. This has two effects:\n",
    "- Portability across operating systems (as OS specific packages will be installed this way)\n",
    "- Not fully reproducible (due to possibly different dependencies)\n",
    "\n",
    "__In general it should be safe to use the `--from-history` flag for increased portability of projects__\n",
    "\n",
    "### Using MLProject.yaml\n",
    "\n",
    "> Better option is to explicitly specify entry points, structure, parameters etc. via `MLproject` file\n",
    "\n",
    "Here is an example `MLproject`:\n",
    "\n",
    "\n",
    "```yml\n",
    "---\n",
    "name: My Project\n",
    "\n",
    "conda_env: my_env.yaml\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      data_file: path\n",
    "      regularization: {type: float, default: 0.1}\n",
    "    command: \"python train.py -r {regularization} {data_file}\"\n",
    "  validate:\n",
    "    parameters:\n",
    "      data_file: path\n",
    "    command: \"python validate.py {data_file}\"\n",
    "```\n",
    "\n",
    "Check out some examples in the documentation [here](https://github.com/mlflow/mlflow/tree/master/examples)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see one can:\n",
    "\n",
    "1. __specify environment explicitly:__\n",
    "    - `conda` (simply a file with dependencies)\n",
    "    - `docker` environment:\n",
    "        - specify image available on the OS\n",
    "        - if image is not available, try to pull it from `DockerHub`\n",
    "        - if registry containing image is specified it will try to pull it (unless it's already available on the system)\n",
    "\n",
    "\n",
    "For `docker` environment one can also specify:\n",
    "- volumes to be mounted during project running\n",
    "- environment variables passed to the container\n",
    "\n",
    "See an example of `docker_env` below:\n",
    "\n",
    "```yml\n",
    "---\n",
    "name: My Project\n",
    "\n",
    "docker_env:\n",
    "  image:  mlflow-docker-example\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      data_file: path\n",
    "      regularization: {type: float, default: 0.1}\n",
    "      p: float\n",
    "    command: \"python train.py -r {regularization} {data_file}\"\n",
    "  validate:\n",
    "    parameters:\n",
    "      data_file: path\n",
    "    command: \"python validate.py {data_file}\"\n",
    "```\n",
    "\n",
    "[See more here](https://www.mlflow.org/docs/latest/projects.html#mlproject-file)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. __Specify parameters and entrypoints__\n",
    "\n",
    "One can specify:\n",
    "- name of the parameter\n",
    "- type of the parameter (default is `str`, others are `float`, `path`, `uri`)\n",
    "- default value(s)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Those values are latter passed on to `command` field and appropriately substituted.\n",
    "__If we don't specify some parameter, it will be passed to the running command via `--key value` syntax__\n",
    "\n",
    "Two cells above all of the parameter specification are shown."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running projects\n",
    "\n",
    "> `MLFLow` provides command line program `mlflow` which has a subcommand `run` allowing us to run the project\n",
    "\n",
    "\n",
    "Usage is really simple:\n",
    "\n",
    "```bash\n",
    "mlflow run <directory>\n",
    "```\n",
    "\n",
    "but there are a few useful tricks which allow us to run it with even less effort:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "!mlflow run --help"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Usage: mlflow run [OPTIONS] URI\n",
      "\n",
      "  Run an MLflow project from the given URI.\n",
      "\n",
      "  For local runs, the run will block until it completes. Otherwise, the\n",
      "  project will run asynchronously.\n",
      "\n",
      "  If running locally (the default), the URI can be either a Git repository\n",
      "  URI or a local path. If running on Databricks, the URI must be a Git\n",
      "  repository.\n",
      "\n",
      "  By default, Git projects run in a new working directory with the given\n",
      "  parameters, while local projects run from the project's root directory.\n",
      "\n",
      "Options:\n",
      "  -e, --entry-point NAME        Entry point within project. [default: main].\n",
      "                                If the entry point is not found, attempts to\n",
      "                                run the project file with the specified name\n",
      "                                as a script, using 'python' to run .py files\n",
      "                                and the default shell (specified by\n",
      "                                environment variable $SHELL) to run .sh files\n",
      "\n",
      "  -v, --version VERSION         Version of the project to run, as a Git commit\n",
      "                                reference for Git projects.\n",
      "\n",
      "  -P, --param-list NAME=VALUE   A parameter for the run, of the form -P\n",
      "                                name=value. Provided parameters that are not\n",
      "                                in the list of parameters for an entry point\n",
      "                                will be passed to the corresponding entry\n",
      "                                point as command-line arguments in the form\n",
      "                                `--name value`\n",
      "\n",
      "  -A, --docker-args NAME=VALUE  A `docker run` argument or flag, of the form\n",
      "                                -A name=value (e.g. -A gpus=all) or -A name\n",
      "                                (e.g. -A t). The argument will then be passed\n",
      "                                as `docker run --name value` or `docker run\n",
      "                                --name` respectively.\n",
      "\n",
      "  --experiment-name TEXT        Name of the experiment under which to launch\n",
      "                                the run. If not specified, 'experiment-id'\n",
      "                                option will be used to launch run.\n",
      "\n",
      "  --experiment-id TEXT          ID of the experiment under which to launch the\n",
      "                                run.\n",
      "\n",
      "  -b, --backend BACKEND         Execution backend to use for run. Supported\n",
      "                                values: 'local', 'databricks', kubernetes\n",
      "                                (experimental). Defaults to 'local'. If\n",
      "                                running against Databricks, will run against a\n",
      "                                Databricks workspace determined as follows: if\n",
      "                                a Databricks tracking URI of the form\n",
      "                                'databricks://profile' has been set (e.g. by\n",
      "                                setting the MLFLOW_TRACKING_URI environment\n",
      "                                variable), will run against the workspace\n",
      "                                specified by <profile>. Otherwise, runs\n",
      "                                against the workspace specified by the default\n",
      "                                Databricks CLI profile. See\n",
      "                                https://github.com/databricks/databricks-cli\n",
      "                                for more info on configuring a Databricks CLI\n",
      "                                profile.\n",
      "\n",
      "  -c, --backend-config FILE     Path to JSON file (must end in '.json') or\n",
      "                                JSON string which will be passed as config to\n",
      "                                the backend. The exact content which should be\n",
      "                                provided is different for each execution\n",
      "                                backend and is documented at https://www.mlflo\n",
      "                                w.org/docs/latest/projects.html.\n",
      "\n",
      "  --no-conda                    If specified, will assume that\n",
      "                                MLmodel/MLproject is running within a Conda\n",
      "                                environment with the necessary dependencies\n",
      "                                for the current project instead of attempting\n",
      "                                to create a new conda environment.\n",
      "\n",
      "  --storage-dir TEXT            Only valid when ``backend`` is local. MLflow\n",
      "                                downloads artifacts from distributed URIs\n",
      "                                passed to parameters of type 'path' to\n",
      "                                subdirectories of storage_dir.\n",
      "\n",
      "  --run-id RUN_ID               If specified, the given run ID will be used\n",
      "                                instead of creating a new run. Note: this\n",
      "                                argument is used internally by the MLflow\n",
      "                                project APIs and should not be specified.\n",
      "\n",
      "  --help                        Show this message and exit.\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T17:57:19.638058Z",
     "start_time": "2021-04-25T17:57:18.637616Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example we could do this (run in your CLI or in your cell):"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:00:54.960912Z",
     "start_time": "2021-04-25T18:00:54.951128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "!mlflow run https://github.com/mlflow/mlflow-example -P alpha=0.5"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021/07/04 18:41:53 INFO mlflow.projects.utils: === Fetching project from https://github.com/mlflow/mlflow-example into /var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/tmpyo68dla4 ===\n",
      "2021/07/04 18:41:54 INFO mlflow.projects.utils: === Created directory /var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/tmp0f1l27x8 for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2021/07/04 18:41:54 INFO mlflow.projects.backend.local: === Running command 'source /Users/ice/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1abc00771765dd9dd15731cbda4938c765fbb90b 1>&2 && python train.py 0.5 0.1' in run with ID '3fedea0d61ba42ecafd5ed87791d507c' === \n",
      "/Users/ice/miniconda3/envs/mlflow-1abc00771765dd9dd15731cbda4938c765fbb90b/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sequence\n",
      "/Users/ice/miniconda3/envs/mlflow-1abc00771765dd9dd15731cbda4938c765fbb90b/lib/python3.7/site-packages/sklearn/model_selection/_split.py:18: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Iterable\n",
      "/Users/ice/miniconda3/envs/mlflow-1abc00771765dd9dd15731cbda4938c765fbb90b/lib/python3.7/site-packages/sklearn/model_selection/_search.py:16: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, namedtuple, defaultdict, Sequence\n",
      "Elasticnet model (alpha=0.500000, l1_ratio=0.100000):\n",
      "  RMSE: 0.7947931019036529\n",
      "  MAE: 0.6189130834228138\n",
      "  R2: 0.18411668718221819\n",
      "2021/07/04 18:41:56 INFO mlflow.projects: === Run (ID '3fedea0d61ba42ecafd5ed87791d507c') succeeded ===\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:04:18.415562Z",
     "start_time": "2021-04-25T18:02:19.993642Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment tracking\n",
    "\n",
    "> __Tracking is an API and UI which allows us to log experiment's data and later visualizing it__\n",
    "\n",
    "Using it we can log:\n",
    "- model parameters\n",
    "- code versions (git commit hashes)\n",
    "- metrics\n",
    "- generated artifacts\n",
    "\n",
    "__`mlflow` tracking is organized around runs, which is simply some form of execution of our program__.\n",
    "\n",
    "Each run is recorded by `mlflow` either to:\n",
    "- local files\n",
    "- SQLAlchemy database\n",
    "- remote storage (via [`mlflow.set_tracking_uri()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_tracking_uri) function)\n",
    "\n",
    "For more information about storage [check out relevant part of documentation](https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded).\n",
    "\n",
    "> __Via `MLFlow` we can track, version and create comprehensive experiment from everything, starting with ETL and ending with deployment__\n",
    "\n",
    "There are a few main concepts to keep in mind when using it:\n",
    "- __experiment__ - mainly [`mlflow.set_experiment(UNIQUE_NAME_OF_EXPERIMENT)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_experiment) which sets current experiments and optionally creates it if it doesn't exist.\n",
    "- __run__ - single run, experiment can consist of multiple of those. Context manager [`mlflow.start_run()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run)\n",
    "- __logging__ - logging data from an experiment; here are the related function:\n",
    "    - [`mlflow.log_param(key, value)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_param) - logs hyperparameters and other settable parameters under current run\n",
    "    - [`mlflow.log_metric(key, value)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metric)\n",
    "    - [`mlflow.log_artifact(local_path)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact) - logs created file (e.g. models, generated text etc.) under the current run\n",
    "    \n",
    "Given the above, let's see how to run and log __non-flavored__ (e.g. without specific integrations) dummy experiment:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "def create_dummy_file():\n",
    "    features = \"rooms, zipcode, median_price, school_rating, transport\"\n",
    "    with open(\"features.txt\", \"w\") as f:\n",
    "        f.write(features)\n",
    "\n",
    "\n",
    "create_dummy_file()\n",
    "\n",
    "# Create experiment (artifact_location=./ml_runs by default)\n",
    "mlflow.set_experiment(\"Dummy Experiments\")\n",
    "\n",
    "# By default experiment we've set will be used\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_artifact(\"features.txt\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.01)\n",
    "    for i in range(10):\n",
    "        mlflow.log_metric(\"Iteration\", i, step=i)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T21:43:32.171088Z",
     "start_time": "2021-04-25T21:43:32.013188Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To visualize & explore saved data we can use `mlflow ui` command and open web browser under [`http://localhost:5000 `](http://localhost:5000) (__data will be saved inside `./mlruns`__)\n",
    "\n",
    "Run below in the terminal:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# !mlflow ui --help"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T21:31:33.674264Z",
     "start_time": "2021-04-25T21:31:32.591694Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After navigating to the the experiment, we can see the `Iteration` being logged like below:\n",
    "\n",
    "![](images/mlflow_ui.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model format\n",
    "\n",
    "> MLFlow provides standard format for saving machine learning models (from various libraries) which helps us with model usage (e.g. inference on REST API, cloud etc.) \n",
    "\n",
    "MLFlow models consist of:\n",
    "- directory with arbitrary files defined by the model)\n",
    "- `MLmodel` file (written in yaml) which specifies what is contained within the model\n",
    "\n",
    "Let's see how to save our model (in this case `sklearn`) in Python..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "mlflow.sklearn.save_model(model, \"my_model\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a07f86677dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "which creates the following directory in our `cwd`:\n",
    "\n",
    "```bash\n",
    "my_model/\n",
    "├── MLmodel\n",
    "└── model.pkl\n",
    "```\n",
    "\n",
    "Contents of the `MLModel` are equally easy to grasp:\n",
    "\n",
    "```yml\n",
    "---\n",
    "time_created: 2021-04-03T17:28:53.35\n",
    "\n",
    "flavors:\n",
    "  sklearn:\n",
    "    sklearn_version: 0.24.1\n",
    "    pickled_model: model.pkl\n",
    "  python_function:\n",
    "    loader_module: mlflow.sklearn\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model signature\n",
    "\n",
    "In order to deploy (and sometimes even run, like in `tensorflow`) we need to specify __model signature__\n",
    "\n",
    "> __Model signature specifies type and shape of inputs going through the model__\n",
    "\n",
    "- Standard casting rules apply (upcasting is fine, downcasting would raise an error)\n",
    "- Helps reading inputs when those are send using JSON via REST API or a-like\n",
    "\n",
    "We can add it to `MLModel` file, two options to do so below:\n",
    "\n",
    "#### Column signature\n",
    "\n",
    "> Specify input signature by specifying each possible column input\n",
    "\n",
    "This mode is supported by all flavors (frameworks), yet those might not be the easiest in all cases.\n",
    "\n",
    "Example for `iris` dataset:\n",
    "\n",
    "```yaml\n",
    "signature:\n",
    "    inputs: '[{\"name\": \"sepal length (cm)\", \"type\": \"double\"}, {\"name\": \"sepal width\n",
    "      (cm)\", \"type\": \"double\"}, {\"name\": \"petal length (cm)\", \"type\": \"double\"}, {\"name\":\n",
    "      \"petal width (cm)\", \"type\": \"double\"}]'\n",
    "    outputs: '[{\"type\": \"integer\"}]'\n",
    "```\n",
    "\n",
    "#### Tensor signature\n",
    "\n",
    "> Specify input for deep learning inputs (e.g. images) via tensor shape\n",
    "\n",
    "Image oriented example:\n",
    "\n",
    "```yaml\n",
    "signature:\n",
    "    inputs: '[{\"name\": \"images\", \"dtype\": \"uint8\", \"shape\": [-1, 28, 28, 1]}]'\n",
    "    outputs: '[{\"shape\": [-1, 10], \"dtype\": \"float32\"}]'\n",
    "```\n",
    "\n",
    "#### Inferring input shapes\n",
    "\n",
    "Often it is easier (and less error-prone) to infer `dtype` and shape through our code. One can easily do this via [`mlflow.models.infer_signature`](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.infer_signature).\n",
    "\n",
    "Check out code below for an example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_train = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "clf = RandomForestClassifier(max_depth=7, random_state=0)\n",
    "clf.fit(iris_train, iris.target)\n",
    "signature = infer_signature(iris_train, clf.predict(iris_train))\n",
    "\n",
    "mlflow.sklearn.log_model(clf, \"iris_rf\", signature=signature)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`infer_signature` is really simple:\n",
    "- Pass input data (usually `torch.Tensor`, `pd.DataFrame`, `np.ndarray` or other standard types)\n",
    "- Pass data through the model as the second argument - this will create `outputs` automatically\n",
    "\n",
    "\n",
    "`mlflow.sklearn.log_model` saves the model to the file in `cwd` named `iris_rf` with our specified signature.\n",
    "We could later load it from the disk (__it has to be tailored to the flavor we saved it in!__):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load sklearn model\n",
    "\n",
    "sklearn_model = mlflow.sklearn.load_model(\"iris_rf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploying models\n",
    "\n",
    "Once we have our model saved we can easily deploy it to various services, namely:\n",
    "- [locally](https://www.mlflow.org/docs/latest/models.html#deploy-mlflow-models) with REST API (either inside `docker` container or with `conda` environment)\n",
    "- [Microsoft's Azure ML](https://www.mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-microsoft-azure-ml)\n",
    "- [Amazon SageMaker](https://www.mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker)\n",
    "- [Apache UDF](https://www.mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf)\n",
    "- Others, maintained by community deployment plugins (for example `torchserve`), check out [here](https://www.mlflow.org/docs/latest/plugins.html#deployment-plugins)\n",
    "\n",
    "Let's see `mlflow models` command:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "!mlflow models --help"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Usage: mlflow models [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Deploy MLflow models locally.\n",
      "\n",
      "  To deploy a model associated with a run on a tracking server, set the\n",
      "  MLFLOW_TRACKING_URI environment variable to the URL of the desired server.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  build-docker  **EXPERIMENTAL**: Builds a Docker image whose default...\n",
      "  predict       Generate predictions in json format using a saved MLflow...\n",
      "  prepare-env   **EXPERIMENTAL**: Performs any preparation necessary to...\n",
      "  serve         Serve a model saved with MLflow by launching a webserver on...\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T14:36:46.388087Z",
     "start_time": "2021-05-01T14:36:45.003143Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### models build-docker\n",
    "\n",
    "> This subcommand creates a docker image and places our model inside it\n",
    "\n",
    "After this we can serve the model by running created image (by default port `8080` is exposed so we can easily map it).\n",
    "\n",
    "Let's see this command in more details"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!mlflow models build-docker --help"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T14:37:21.282191Z",
     "start_time": "2021-05-01T14:37:20.242857Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__`python_flavor` is the default one and every specific integration is compatible with it__ (see more details [here](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html))\n",
    "\n",
    "### model serve\n",
    "\n",
    "> Runs a basic webserver (created via `flask`) which we can query (e.g. using `curl`)\n",
    "\n",
    "We can specify (amongst other things):\n",
    "- `--model-uri` - model resource (mandatory)\n",
    "- `--workers` - number of parallel workers handling requests\n",
    "- `--port` - on which port the server will listen for requests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!mlflow models serve --help"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T18:33:14.270664Z",
     "start_time": "2021-05-02T18:33:13.286867Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### models predict\n",
    "\n",
    "> Allows us to query model with a file (`.csv` or `.json`) (__useful for testing!__)\n",
    "\n",
    "Let's see the possibilities:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!mlflow models predict --help"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T18:42:23.981463Z",
     "start_time": "2021-05-02T18:42:23.007619Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Querying deployed model\n",
    "\n",
    "Once we deployed the model (via `docker` or `flask` webserver) we can query it (from other machines or from `localhost` also). \n",
    "\n",
    "Requests are done via sending `json` text strings to `/invocations` endpoint. There are a few possibilities to send the data:\n",
    "- JSON-serialized pandas DataFrames in the split orientation (`data = pandas_df.to_json(orient='split')`)\n",
    "- JSON-serialized pandas DataFrames in the records orientation (discouraged)\n",
    "- CSV-serialized pandas DataFrames (`data = pandas_df.to_csv()`)\n",
    "- Tensor input formatted as described in TF Serving’s API docs where the provided inputs will be cast to Numpy arrays\n",
    "\n",
    "Each of the above can be seen below (please notice `content/type` specification for different versions):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# split-oriented DataFrame input\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{\n",
    "    \"columns\": [\"a\", \"b\", \"c\"],\n",
    "    \"data\": [[1, 2, 3], [4, 5, 6]]\n",
    "}'\n",
    "\n",
    "# record-oriented DataFrame input (fine for vector rows, loses ordering for JSON records)\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json; format=pandas-records' -d '[\n",
    "    {\"a\": 1,\"b\": 2,\"c\": 3},\n",
    "    {\"a\": 4,\"b\": 5,\"c\": 6}\n",
    "]'\n",
    "\n",
    "# numpy/tensor input using TF serving's \"instances\" format\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{\n",
    "    \"instances\": [\n",
    "        {\"a\": \"s1\", \"b\": 1, \"c\": [1, 2, 3]},\n",
    "        {\"a\": \"s2\", \"b\": 2, \"c\": [4, 5, 6]},\n",
    "        {\"a\": \"s3\", \"b\": 3, \"c\": [7, 8, 9]}\n",
    "    ]\n",
    "}'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could also encode more complex data before sending the request (e.g. images could be encoded using `base64` and automatically decoded by MLFlow):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# record-oriented DataFrame input with binary column \"b\"\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json; format=pandas-records' -d '[\n",
    "    {\"a\": 0, \"b\": \"dGVzdCBiaW5hcnkgZGF0YSAw\"},\n",
    "    {\"a\": 1, \"b\": \"dGVzdCBiaW5hcnkgZGF0YSAx\"},\n",
    "    {\"a\": 2, \"b\": \"dGVzdCBiaW5hcnkgZGF0YSAy\"}\n",
    "]'\n",
    "\n",
    "# record-oriented DataFrame input with datetime column \"b\"\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json; format=pandas-records' -d '[\n",
    "    {\"a\": 0, \"b\": \"2020-01-01T00:00:00Z\"},\n",
    "    {\"a\": 1, \"b\": \"2020-02-01T12:34:56Z\"},\n",
    "    {\"a\": 2, \"b\": \"2021-03-01T00:00:00Z\"}\n",
    "]'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In summary, we've seen how MLFlow can be used to track experiments and deploy models."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('main': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "interpreter": {
   "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}