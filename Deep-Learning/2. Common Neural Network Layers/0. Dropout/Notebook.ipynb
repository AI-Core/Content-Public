{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dropout\n",
    "\n",
    "> \"Dropout, simply described, is the concept that if you can learn how to do a task repeatedly whilst drunk, you should be able to do the task even better when sober.\"\n",
    "\n",
    "## Seriously, what is it?\n",
    "\n",
    "> Randomly setting some neurons equal to zero __during training and forward pass__.\n",
    "\n",
    "![dropout_gif](images/dropout.gif)\n",
    "\n",
    "- Dropout is a widely spread neural network specific regularization method\n",
    "- Remaining connections have to \"catch up\" for the ones dropped and perform task independently\n",
    "- Different set of neurons are randomly dropped during each pass\n",
    "\n",
    "For single neuron equation would look like this:\n",
    "\n",
    "$$\n",
    "O_i = X_ig(\\sum_{k=1}^{d_i}w_k x_k + b), \\ P(X_i = 0) = p\n",
    "$$\n",
    "\n",
    "In simple terms, `p` specifies probability of zeroing out this specific neuron (and `q=1-p` is a probability of keeping it).\n",
    "\n",
    "## Train vs evaluation behaviour\n",
    "\n",
    "Of course, this approach would be wasteful during test as:\n",
    "- It might produce unreproducible behaviour for single sample\n",
    "- It would not utilize the whole network\n",
    "\n",
    "Because of that, the above equation only applies during training phase.\n",
    "\n",
    "> During evaluation (training or validation) we use all of the neurons \n",
    "\n",
    "But if we were to suddenly give the output layer ALL of the values after it had been trained on some of them being dropped out, then the output is going to be way bigger than it is used to. \n",
    "\n",
    "E.g. If during evaluation, we naively used all outputs when during training p=0.5, then the output is going to be twice as large as what was experienced during training!\n",
    "\n",
    "So we need to do one more __very important thing__\n",
    "\n",
    "> During evaluation (training or validation), we scale every output down BY MULTIPLYING IT BY THE PROBABILITY OF THE NEURON BEING DROPPED OUT\n",
    "\n",
    "![train_vs_evaluation](images/train_vs_evaluation.png)\n",
    "\n",
    "For single neuron testing equation would look like this:\n",
    "\n",
    "$$\n",
    "O_i = qg(\\sum_{k=1}^{d_i}w_k x_k + b), \\ q = 1-p\n",
    "$$\n",
    "\n",
    "## Things to note\n",
    "\n",
    "- Don't apply dropout after your final layer\n",
    "    - You don't want to randomly set your prediction to zero!\n",
    "    - If your model outputs probabilities for a classification problem, a zero probability for the true label class will give you an infinite loss when using a cross entropy loss.\n",
    "- The probability of dropping out a node is usually kept consistent across a model's architecture\n",
    "- The dropout probability must not be trained using gradient descent. It is a hyperparameter. Make sure it is not part of the graph (requires_grad must = False). \n",
    "- You should apply dropout after your activation functions.\n",
    "\n",
    "## Dropout in PyTorch\n",
    "\n",
    "As usual, PyTorch makes using this layer simple.\n",
    "\n",
    "Check out the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "\n",
    "Below, we implement a simple neural network with dropout layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "class MyNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(256, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(64, 10),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `train` vs `eval` mode in PyTorch\n",
    "\n",
    "As mentioned above, there are important differences for how dropout should be used during training and evaluation. But how do we tell PyTorch which mode we are in?\n",
    "\n",
    "`torch.nn.Module` to the rescue!\n",
    "\n",
    "PyTorch provides every class which inherits from `torch.nn.Module` a `.train()` and a `.eval()` method. These methods set the behaviour of layers which do different things depending on the mode you're currently in. Calling either of these methods on your model will apply the change to all layers within your model.\n",
    "\n",
    "Check out the docs for train and eval [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "\n",
    "Note: Dropout is not the only layer which behaves differently between training and evaluation - check out BatchNormalisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mymodel = MyNetwork()\n",
    "\n",
    "mymodel.train() # set model to training mode\n",
    "\n",
    "# do training here\n",
    "\n",
    "mymodel.eval() # set model to evaluation mode\n",
    "\n",
    "# do evaluation here"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Demo: Implementing a custom `Dropout` layer\n",
    "- Inside `__init__`:\n",
    "    - single argument `p` the probability of neuron being dropped\n",
    "    - check whether `p` lies within `(0, 1)` range and if it doesn't raise `ValueError` with appropriate message (e.g. `p (probability) has to lie in (0, 1) range!`)\n",
    "    - Create `self._distribution = torch.distributions.binomial.Binomial` object with specified `p` probability\n",
    "- Inside `forward`:\n",
    "    - Use `self.training` `bool` value in `forward` to differentiate between test and train behaviour\n",
    "    - Use `self._distribution.sample` method to get binary mask with the same shape as `inputs` tensor (training)\n",
    "    - Use `.to(inputs.device)` to cast created tensor to `cuda` (or other device) if needed (training). __Note:__ `torch.distributions` __is not casted to device with the module__ as it's not `torch.nn.Module` instance (see [this issue]() for more on the topic)\n",
    "    - Multiply with the binary mask and return it (training)\n",
    "    - Multiply by keep constant (testing phase)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Dropout(torch.nn.Module):\n",
    "    def __init__(self, p: float):\n",
    "        if not 0 < p < 1:\n",
    "            raise ValueError(f\"p should lie between (0, 1), got: {p}\")\n",
    "            \n",
    "        super().__init__()\n",
    "        self.p = p # float\n",
    "        self._distribution = torch.distributions.binomial.Binomial(probs=self.p) # NOT A PARAMETER\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # (batch, in_features)\n",
    "        if self.training:\n",
    "            # (batch, in_features)\n",
    "            mask = self._distribution.sample(inputs.size()).to(inputs.device)\n",
    "            return inputs * mask\n",
    "        return (1 - self.p) * inputs\n",
    "    \n",
    "module = Dropout(0.5)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test\n",
    "\n",
    "Run the code below to see for eventual errors. \n",
    "\n",
    "You should see some values zeroed out during training and no zeroes during testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def test_my_dropout(module):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    cpu_input = torch.randn(8, 5)\n",
    "    gpu_input = torch.randn(8, 5).to(device)\n",
    "\n",
    "\n",
    "    module(cpu_input)\n",
    "    print(\"\\n\\n------------------- TRAINING -------------------\\n\\n\")\n",
    "    print(module(gpu_input))\n",
    "    print(\"\\n\\n-------------------- TESTING -------------------\\n\\n\")\n",
    "\n",
    "    module.eval()\n",
    "    print(module(gpu_input))\n",
    "\n",
    "test_my_dropout(Dropout(p=0.5))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "------------------- TRAINING -------------------\n",
      "\n",
      "\n",
      "tensor([[-0.0000, -0.7166, -0.0000, -1.8316, -0.0000],\n",
      "        [-0.0000,  0.9460, -0.7197,  0.7440,  0.0000],\n",
      "        [ 0.0000, -0.3741, -2.2701, -1.1797, -0.0000],\n",
      "        [-0.0000,  0.6468, -1.7475, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0693,  1.9605, -0.0000],\n",
      "        [-0.1221,  0.3878, -0.0000,  0.0000, -1.0863],\n",
      "        [-0.0000,  0.0000, -0.0000, -1.6344, -0.0000],\n",
      "        [ 1.1280,  0.0000,  0.3795,  0.0000,  0.5406]], device='cuda:0')\n",
      "\n",
      "\n",
      "-------------------- TESTING -------------------\n",
      "\n",
      "\n",
      "tensor([[-0.8776, -0.3583, -0.0216, -0.9158, -0.2285],\n",
      "        [-0.5360,  0.4730, -0.3598,  0.3720,  0.2497],\n",
      "        [ 0.0518, -0.1870, -1.1350, -0.5899, -0.2168],\n",
      "        [-0.1372,  0.3234, -0.8738, -0.7646, -0.5086],\n",
      "        [-0.7511, -0.1746,  0.0346,  0.9803, -0.5257],\n",
      "        [-0.0611,  0.1939, -0.0632,  0.1635, -0.5431],\n",
      "        [-0.3338,  0.4279, -0.2291, -0.8172, -0.8041],\n",
      "        [ 0.5640,  0.2071,  0.1897,  0.4786,  0.2703]], device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dropout rationale\n",
    "\n",
    "### Ensemble\n",
    "\n",
    "> Dropout works like an ensemble of models\n",
    "\n",
    "- During each `forward` pass different internal routes are used to propagate information\n",
    "- During `testing` phase all of the routes are considered but scaled appropriately\n",
    "\n",
    "![](images/ensemble.png)\n",
    "\n",
    "> __\"Dropout provides an inexpensive way to approximate training and inference of exponentially many neural networks. This prevents co-adaptation of features and dependence on any specific features, which improves generalisation.\"__\n",
    "\n",
    "Instead of having to train many individual networks, we can sample from many different \"subnetworks\" of the model which are generated when certain connections are removed.\n",
    "\n",
    "### Sparsity (most important weights)\n",
    "\n",
    "- Dropout pushes distributions of activations towards zero\n",
    "- Neural network focuses more on the important weights and important output neurons\n",
    "- We get a model that is __easier to reason about__ (not to confuse with easy!)\n",
    "- __Breaks co-adaptation__ (multiple neurons do similar tasks, hence decision boundary is less clear)\n",
    "- Due to above, generalization is likely to improve as the most important features are considered (most important factor according to original authors)\n",
    "\n",
    "### Scaling rationale - Monte Carlo sampling\n",
    "\n",
    "- Save each model created during forward pass (__a lot of models__!) randomly (`k=50` used) during training (preferably after some training passed)\n",
    "- Ask each one to predict on test\n",
    "- Average their results\n",
    "- __Results similar to just multiplying activation by expected value (within one standard deviation)!__\n",
    "\n",
    "### Noise addition\n",
    "\n",
    "- As we randomly generate masks, we create noise during each forward pass\n",
    "- Noise is known to improve generalization as it makes the model more reluctant to follow random/uninformative patterns\n",
    "- __This is called internal representation noise__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "torch.nn.Sequential(torch.nn.Dropout(p=0.2), torch.nn.Linear(100))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Usage tips & tricks\n",
    "\n",
    "> Those are mostly anectodal, always perform validation! It __might be__ worth to check those out\n",
    "\n",
    "- Use layer size of `N/p`. If you think `128` layer size would be good for this problem and you set `p=0.5`, go with `256` neurons instead\n",
    "- __Use `p=0.5` for internal layers__\n",
    "- __Use `p=0.2` if Dropout is applied on input__\n",
    "- __Use with Fully Connected Networks__, that's where this technique is most likely to bring improvements\n",
    "- __It should not be the first technique you go to__ as others are more popular and usually work better in practice\n",
    "- __Increase learning rate when using dropout__, momentum to `0.95-0.99` instead of `0.9`\n",
    "- `L1` regularization should improve sparsity and force the network to keep only the most valuable connections, __might be a good choice__.\n",
    "\n",
    "## When to use?\n",
    "\n",
    "- Fully Connected Networks (without batch normalization)\n",
    "- Between linear layers\n",
    "- Possibly on input data (as long as it's not an image or text)\n",
    "\n",
    "## When not to use?\n",
    "\n",
    "> Possible solutions are outlined in the challenges for you to read!\n",
    "\n",
    "- When using one of the most popular building blocks for neural networks: **Batch Normalization** (more about that during batch normalization explanation)\n",
    "- At least not in the same \"block\", as dropout changes mean and std of activations\n",
    "- Most neural network architectures use Batch Normalization hence Dropout is not as popular anymore (sometimes for input, sometimes for linear layers at the very end of network)\n",
    "- Convolutional neural networks (as weights are highly correlated and the effect is miniscule if any)\n",
    "- Also prediction surface is more smooth and we are \"un-smoothing\" it using standard Dropout\n",
    "- Recurrent Neural networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "- Dropout is well-known & battle tested regularization technique\n",
    "- Randomly switching of neurons after activation layer during training\n",
    "- Leaving all the connections during test phase but scaled\n",
    "- Works like an ensemble\n",
    "- __In practice Inverted Dropout is used__ (test phase is fully untouched)\n",
    "- Should be used with FCNs, rarely other types of layers (or you need a sound rationale for that)\n",
    "- PyTorch provides `torch.distributions` module for random data generation\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- What is `AlphaDropout`?\n",
    "- What is `SpatialDropout`?\n",
    "- What is `DropConnect`? \n",
    "- What is `ShakeShake` regularization (you can do this one after convolutional neural networks also)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}