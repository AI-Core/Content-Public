{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Batch Norm(alization)\n",
    "\n",
    "> Batch Normalization is a technique to improve training time and convergence properties of neural networks (and has some other additional properties)\n",
    "\n",
    "## Normalization step\n",
    "\n",
    "> Batch Norm normalizes incoming data using `mean` and `variance` calculated __on a single batch size__\n",
    "\n",
    "$$\n",
    "\\hat{X} = \\frac{X - \\mu(X)}{\\sqrt{\\sigma^2(X)}}\n",
    "$$\n",
    "\n",
    "After this operation we obtain zero centered data with variance of 1, hence how the layers prefer their inputs (see input data normalization).\n",
    "\n",
    "## Reparametrization step\n",
    "\n",
    "After we normalize inputs we use __per-feature mean and variance changing parameters: `beta` and `gamma`__:\n",
    "\n",
    "$$\n",
    "X_f = \\gamma\\hat{X} + \\beta\n",
    "$$\n",
    "\n",
    "Gamma and beta are both learnable parameters, learned through gradient descent, just like our model weights. Allowing the model to learn these allows it to choose how to spread (gamma) and shift (beta) the incoming data so that the current layer can transform it best.\n",
    "\n",
    "> `gamma` and `beta` have the same size as number of input features!\n",
    "\n",
    "Let me make that even more clear so you don't overlook it...\n",
    "\n",
    "> Gamma and beta are tensors, with the same shape and as many elements as there are in the input to the layer. That's because you want to find the means and standard deviations for each feature... NOT THE MEAN OR STD DEV OVER ALL FEATURES WITHIN EACH EXAMPLE. Gamma and beta are not scalars!\n",
    "\n",
    "Here are all the steps one should take:\n",
    "\n",
    "$$\n",
    "\\hat{X} = \\frac{X - \\mu(X)}{\\sqrt{\\sigma^2(X)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_f = \\gamma\\hat{X} + \\beta\n",
    "$$\n",
    "\n",
    "## Train vs evaluation behaviour\n",
    "\n",
    "> As with dropout, __the behaviour of the batch normalisation layer differs between training and evaluation__\n",
    "\n",
    "Why?\n",
    "\n",
    "- __We might do inference using single sample or small batches (which works worse with batch normalization)__\n",
    "- __Neural network outputs COULD change with the same samples, IF the sample is in different batch__ (we usually go for deterministic outputs during inference phase)\n",
    "\n",
    "In order to do that, __we will keep running mean and running variance__ gather throughout the training. Those values will be used instead of `mean` and `variance` calculated from batch, hence (notice lack of `X`):\n",
    "\n",
    "$$\n",
    "\\hat{X} = \\frac{X - \\mu}{\\sqrt{\\sigma^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_f = \\gamma\\hat{X} + \\beta\n",
    "$$\n",
    "\n",
    "In order to obtain those values, during each forward pass we will keep __running mean__ with specified momentum, given by (for timestep `i`):\n",
    "\n",
    "$$\n",
    "\\mu_i = m \\times \\mu_{i-1} + (1 - m) \\times \\mu(X)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2_i = m \\times \\sigma^2_{i-1} + (1 - m) \\times \\sigma^2(X)\n",
    "$$\n",
    "\n",
    "> __Those values should be kept inside a model, but they shouldn't be trained!__\n",
    "\n",
    "## Things to note\n",
    "- Batch normalisation should be applied after linear (or convolutional) layers, before the activation function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Batch Normalisation in PyTorch\n",
    "\n",
    "To initialise the batch norm layers, you need to specify the number of incoming features and that's it. This is required so that the layer can initialise the parameters beta and gamma with the correct shape (vectors of length = number of features), so that they are ready to use in the first forward pass.\n",
    "\n",
    "Check out the docs [here](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)\n",
    "\n",
    "Below, we implement a simple neural network with batchnorm layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "class MyNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 10),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, make sure your model and it's layers are in the right mode by using your model's `.train()` and `.eval()` methods."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\n",
    "\n",
    "> Create `BatchNorm1d` layer!\n",
    "\n",
    "- `__init__`:\n",
    "    - Save all `__init__` arguments as attributes\n",
    "    - In `__init__` create `gamma` and `beta` as `torch.nn.Parameter` instances\n",
    "    - Those should be, respectively zeros of shape `(1, num_features)` and ones of the same shape (hence default mean and variance)\n",
    "    - Running mean and running variance buffers are created for you (we will talk about it after exercise)\n",
    "- `forward`:\n",
    "    - `if not self.training` (no training phase) - subtract `self.running_mean` and divide by square root of `self.running_var + self.eps`, save result as `X_hat\n",
    "    - For training do the following:\n",
    "        - Calculate mean and variance __across batch (0) dimension__ (you have to unsqueeze it afterwards or `keepdim=True`)\n",
    "        - Update `self.running_mean` with the formula given above (essentially running mean)\n",
    "        - Do the same for variance\n",
    "        - `X_hat` is now equal `X` with subtracted mean (__of this batch!__) and divided by variance (__of this batch!__) (look at the formulas if you are not sure!)\n",
    "    - `return` `X_hat` multiplied by `self.gamma` and with `self.beta` added"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class BatchNorm1d(torch.nn.Module):\n",
    "    def __init__(self, num_features, momentum: float = 0.9, eps: float = 1e-7):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(1, self.num_features))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(1, self.num_features))\n",
    "        \n",
    "        # You can use self.running_mean and self.running_var right now\n",
    "        self.register_buffer(\"running_mean\", torch.ones(1, self.num_features))\n",
    "        self.register_buffer(\"running_var\", torch.ones(1, self.num_features))\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        if not self.training:\n",
    "            X_hat = X - self.running_mean / torch.sqrt(self.running_var + self.eps)\n",
    "        else:\n",
    "            mean = X.mean(dim=0).unsqueeze(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0).unsqueeze(dim=0)\n",
    "\n",
    "            # Update running mean and variance\n",
    "            self.running_mean *= self.momentum\n",
    "            self.running_mean += (1 - self.momentum) * mean\n",
    "\n",
    "            self.running_var *= self.momentum\n",
    "            self.running_var += (1 - self.momentum) * var\n",
    "\n",
    "            X_hat = X - mean / torch.sqrt(var + self.eps)\n",
    "\n",
    "        return X_hat * self.gamma + self.beta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "input_data = torch.randn(5, 5)\n",
    "layer = BatchNorm1d(5)\n",
    "\n",
    "layer(input_data), input_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 1.9229, -0.8777, -0.2607,  1.5660, -0.1610],\n",
       "         [ 1.6854, -0.0881,  0.5912,  0.4814,  1.7144],\n",
       "         [-0.3730,  0.9642,  1.2159, -1.1008, -1.4934],\n",
       "         [-1.9959,  0.3057, -1.5824, -0.5691, -0.3325],\n",
       "         [-0.6004, -0.6081,  0.2965, -0.4376,  0.4075]], grad_fn=<AddBackward0>),\n",
       " tensor([[ 2.1895, -0.7009, -1.2250,  1.7636,  0.4287],\n",
       "         [ 1.9520,  0.0887, -0.3731,  0.6790,  2.3041],\n",
       "         [-0.1064,  1.1410,  0.2516, -0.9033, -0.9037],\n",
       "         [-1.7293,  0.4824, -2.5467, -0.3715,  0.2572],\n",
       "         [-0.3337, -0.4313, -0.6678, -0.2400,  0.9971]]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Why BatchNorm works?\n",
    "\n",
    "## Internal Covariate Shift\n",
    "\n",
    "In original paper (link [here](https://arxiv.org/abs/1502.03167)) authors claimed `BatchNorm` improves neural network properties by removing a, so called, __Internal Covariate Shift__.\n",
    "\n",
    "### What is it?\n",
    "\n",
    "- Change in distribution passed to layers in neural network\n",
    "\n",
    "Imagine we have layers `a->b->c->d->e`:\n",
    "\n",
    "- If we change weights in `b` `c->d->e` is affected\n",
    "- We are constantly changing weights\n",
    "\n",
    "Neural network works on gradient obtained from `backpropagation`, hence:\n",
    "\n",
    "- `e` layer gradients are changed based on `a->b->c->d` values __currently__\n",
    "- `d` layer gradients are changed based on `a->b->c` values __currently__\n",
    "\n",
    "You can see that, except for `a`, we are optimizing using info __from the previous step__.\n",
    "\n",
    "> `BatchNormalization` decouples most important statistics and interactions between layers, making the optimization \"more on point\"\n",
    "\n",
    "### Debunked?\n",
    "\n",
    "To this day, people are not sure if reducing ICS is the main effect.\n",
    "\n",
    "> To reduce ICS it is enough to normalize the data after each pass!\n",
    "\n",
    "Some experiments show that the ICS is not reduced __at all__ (unlikely), while others point to reduction in it (much more likely)\n",
    "\n",
    "## Moving away from oversaturation\n",
    "\n",
    "- Some activations oversaturate (e.g. `sigmoid`)\n",
    "- If we normalize the data, those usually move away from `oversaturation` regime\n",
    "- Once again, motivation for `beta` and `gamma` is not clear in this case\n",
    "- Drastic help when it comes to non oversaturating functions like `ReLU` (though the output is zero-centered once again)\n",
    "\n",
    "\n",
    "## Easier control of statistics\n",
    "\n",
    "Simply removing ICS can be done merely by normalizing input data, __but we also have `gamma` and `beta` parameters__.\n",
    "\n",
    "> What is argued, is that it is easier for neural network to __control first and second moment of output distribution (mean and variance)__ \n",
    "\n",
    "### Thought experiment\n",
    "\n",
    "> Let's see, how many parameters would the neural network need to control in order to change `mean` and `variance` of `nn.Linear(100, 10)` layer:\n",
    "\n",
    "- Each neuron has `100` inputs, hence `100` values\n",
    "- There are `10` output neurons\n",
    "- `1000` values in total to control __BOTH MEAN AND VARIANCE__ simultaneuosly\n",
    "\n",
    "> What happens when we add BatchNorm layer?\n",
    "\n",
    "Now, it is guaranteed, that layer's output will have `mean=0` and `std=1` (we can think of it as \"checkpointing system\" for `mean` and `std`).\n",
    "\n",
    "- To change outputs mean distribution we only need to change `10` values (same for variance, __both are independent__)\n",
    "- That __should be__ `100x` times easier to control (given that first order methods optimization is not the smartest and has no info about function curvature)\n",
    "\n",
    "## Smoothing out optimization landscape\n",
    "\n",
    "Due to above and normalization `BatchNorm` smooths the loss landscape:\n",
    "- In general, loss landscape for neural networks can be sharp, have a lot of flat and problematic regions\n",
    "- This leads to large/miniscule gradients which cause __vanishing gradient (dying gradient) or exploding gradient (inverse of this phenomena)__\n",
    "- __`BatchNorm` makes the loss change at a smaller rate, hence gradient is also more \"stable\"__ (so called Lipschitzness of function)\n",
    "\n",
    "Due to above, we can:\n",
    "- Use larger learning rates\n",
    "- We are not so dependent on the hyperparameters choice\n",
    "- We are not so dependent on initialization of neural network weights\n",
    "- __Leads to faster training (less epochs)__\n",
    "\n",
    "### First order optimization\n",
    "\n",
    "`SGD` is blind to second order interactions like Hessians (derivatives of derivatives).\n",
    "\n",
    "> What does the Hessian tell the optimization algorithm?\n",
    "\n",
    "__How the function curves__:\n",
    "\n",
    "![](./images/curvature.png)\n",
    "\n",
    "- negative hessian values - function curves down, hence you will minimize the function even faster\n",
    "- zero hessian values - going downhill, good for optimization also\n",
    "- positive hessian values - function will curve up, hence you will get to the minima shortly and any step forward will __increase the loss value__\n",
    "\n",
    "> Those values define how much you can learn from first order methods\n",
    "\n",
    "> BatchNorm simplifies this landscape implicitly as the second order derivatives will be closer to zero a lot of time (hence gradient steps are safer)\n",
    "\n",
    "> __One could use those values to approximate how much a certain step will reduce loss function__\n",
    "\n",
    "> __It is hard/infeasible to calculate with neural networks and use this information due to sheer amount of parameters and hardware constraints__\n",
    "\n",
    "__All of that in the nearby region, but still informative!__\n",
    "\n",
    "- At the start of training, first order terms play the major role, hence we quickly move downhill\n",
    "- As the task of further optimization gets progressively harder the better the network is at the task, the second order curvature will be more important than the first one\n",
    "- __Unlucky step__: If you minimize according to gradient with some step size, you may end up in a positive curvature regime and actually increase your loss instead of decreasing it!\n",
    "\n",
    "## BatchNormalization as a regularization\n",
    "\n",
    "- It introduces randomness (noise) into the data, as statistics are calculated as we go on batch basis (mean adds the noise, variance multiplies it)\n",
    "- __Statistics are batch dependent__ - if the batch size is too small, those __might be unreliable__ (see challenges for other normalization schemes to mitigate this problem)\n",
    "- Noise robustness, works a little like Dropout (but is less severe)\n",
    "- This forces neural network to learn different features (as the ones it was dependent on might be too noisy to use for some batches)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tips\n",
    "\n",
    "> Where to apply?\n",
    "\n",
    "- Originally applied before activation\n",
    "- Seems to work better __after__ activation (as the inputs to learnable linear layer are controlled and unaffected by activation)\n",
    "- Difference isn't drastic\n",
    "- __Some people applied BatchNormalization before and after activation!__\n",
    "\n",
    "> What about `Dropout`?\n",
    "\n",
    "- Dropout randomizes statistics gathered by `BatchNorm`\n",
    "- Alternative (so called `AlphaDropout` which preserves the statistics is available), so it is feasible\n",
    "- __We rarely use ANY `Dropout` in tandem with `BatchNorm`!__ (it might actually make your scores worse!)\n",
    "- In general: BatchNorm for convolutional layers and linear layers, __if you need more regularization, apply dropout on `linear` layers ONLY__\n",
    "- Conflicting opinions, but this is the common practice (might do some experimentation)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "before_block = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 10), torch.nn.BatchNorm1d(10), torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "after_block = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 10), torch.nn.ReLU(), torch.nn.BatchNorm1d(10)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "- BatchNormalization is a technique which improves training time and convergence\n",
    "- You normalize your batch during training\n",
    "- Multiply the output by `gamma` and you add `beta` which are parameters controlled by neural network (so it can learn `mean` and `variance` easier)\n",
    "- You gather running mean and running variance statistics as you go and use them at test time, because:\n",
    "    - Every example will be classified independently of the batch it is in (good deterministic behaviour)\n",
    "    - It allows you to use single example during training\n",
    "- BatchNormalization needs many examples in the batch, the more the better and more reliable the statistics are\n",
    "- Exact reasons why it works are unclear but include:\n",
    "    - Internal Covariate Shift\n",
    "    - Easier control over first/second order terms\n",
    "    - Simplifies loss landscape by reducing second order interactions during forward and backward\n",
    "- Is partially regularization technique due to noise it injects\n",
    "- Probably will not work best with Dropout\n",
    "- Should be used after activation function, though the difference is rarely drastic\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- Try BatchNorm in previous notebook (dying/vanishing gradients). What did you observe?\n",
    "\n",
    "__Remember to check those challenges AFTER you know what the convolution is and how it plays with BatchNorm!__\n",
    "\n",
    "- What is [Instance Normalization](https://stackoverflow.com/questions/45463778/instance-normalisation-vs-batch-normalisation)?\n",
    "- What is [Layer Normalization](https://leimao.github.io/blog/Layer-Normalization/)?\n",
    "- What is [Group Normalization](https://towardsdatascience.com/what-is-group-normalization-45fe27307be7)?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}