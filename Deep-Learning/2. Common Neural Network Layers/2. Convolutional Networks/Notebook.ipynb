{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "## Why?\n",
    "\n",
    "- Fully Connected networks look at input vector as an independent features\n",
    "- Images are __clearly__ dependent; one can estimate pixel values from the ones surrounding it (or vice versa)\n",
    "- Convolution takes this interaction an account (relative position of feature is used)\n",
    "- Convolution models spatial relationships between features (not only images, but also soundwaves, 3D models, videos, words in sentences etc.)\n",
    "- Smaller number of parameters allows us to use deeper and more complicated layers\n",
    "- Simpler architecture (less parameters), tailored to spatially structured data\n",
    "- __Convolutional neural networks find higher level features (a.k.a. representations) useful for final classification layers__\n",
    "\n",
    "## What is convolution?\n",
    "\n",
    "> Kernel (vector/matrix/cube) \"sliding\" over input data (multiplying it with kernel values) and summing them together\n",
    "\n",
    "For binary case:\n",
    "\n",
    "![image](images/convolution_animation.gif)\n",
    "\n",
    "- Kernel is applied on first image patch. It's values are multiplied by respective values in the image\n",
    "- After multiplication all of those values are summed __returning single element__ (you can think of it as a new pixel.\n",
    "- This process is repeated until end of image is reached and new matrix is created.\n",
    "\n",
    "Below are change'able arguments of convolution (those are it's __hyperparameters__):\n",
    "\n",
    "# Convolution parameters\n",
    "\n",
    "## Kernel size\n",
    "\n",
    "> Dimensionality of kernel. In case above it was `3x3` kernel (pretty popular choice).\n",
    "\n",
    "- It can be specified as a tuple, e.g. `(3, 3)`\n",
    "- It can be irregular (e.g. `(3, 2)`), __though it is rarely a case__ (if it is, it almost always is `(N, 1)` or `(1, N)`)\n",
    "- The larger the kernel, the larger is it's __receptive field__ but more computations have to be performed\n",
    "\n",
    "## Stride\n",
    "\n",
    "> Number of pixels we shift our kernel in a certain direction. In case above it was `(1, 1)`\n",
    "\n",
    "- It can be specified as a tuple, e.g. `(1, 1)`\n",
    "- It can be irregular (e.g. `(2, 1)`), __though it is almost never a case__ (specific use cases, __we shouldn't be concerned with this possibility__\n",
    "- The larger the stride, the more features from original image we miss\n",
    "- The larger the stride, the smaller output image becomes\n",
    "- The larger the stride, the less operations have to be performed\n",
    "- __Due to above `stride=1` is the most common value__\n",
    "\n",
    "\n",
    "## Padding\n",
    "\n",
    "> Addition values around the image (usually zeros). In case above __there was no padding (a.k.a. \"valid\" padding)__\n",
    "\n",
    "![image](images/CNN_diagram.JPG)\n",
    "\n",
    "- It can be specified as a tuple, e.g. `(1, 1)`\n",
    "- One can choose from a few modes, specifically (see [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)):\n",
    "    - `zeros` - output is padded with zero values; __most common & default__\n",
    "    - `reflect` - output is reflected, same as `replicate` for `1`, after that consecutive neighboring values ared used; __used for medical imaging, sometimes image segmentation etc.__\n",
    "    - `replicate` - last value at the border is used; __rare__\n",
    "    - `circular` - first and consecutive values are used; __rare__\n",
    "\n",
    "### Okay but why?\n",
    "\n",
    "> Without padding image shrinks (when `kernel_size` > 1)\n",
    "\n",
    "Assume we stack a few convolutions, one after another. __After a while our image will be a single pixel!__\n",
    "\n",
    "- If we add padding we can preserve image's size without introducing noise (or at least too much of it)\n",
    "- Pixels on the edge do not contribute as much to the kernel. If we add appropriate padding they contribute the same as the rest.\n",
    "- Acts as a mild regularizer (depending on the mode)\n",
    "\n",
    "### Tips\n",
    "\n",
    "- Specifying `padding` with \"default values\" (like `stride=1`, `dilation=1`) is easy and can be done using:\n",
    "\n",
    "$$\n",
    "\\lfloor\\frac{\\text{kernel_size}}{2}\\rfloor\n",
    "$$\n",
    "\n",
    "- Exact output size dependent on parameters is provided by PyTorch in [Shape](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "- Some libraries provide padding `\"same\"` which calculates padding for you dynamically ([torchlayers](https://github.com/szymonmaszke/torchlayers) for PyTorch, Keras & Tensorflow have it out of the box)\n",
    "\n",
    "## Dilation\n",
    "\n",
    "> Dilation means spacing between kernel elements\n",
    "\n",
    "![dilated](images/dilated.jpg)\n",
    "\n",
    "- Increased receptive field\n",
    "- Some pixels are missed but this information can be approximated\n",
    "- __Also called `atrous convolution`__\n",
    "\n",
    "### Tips\n",
    "\n",
    "- Useful for very large images\n",
    "- Useful when we want to get more global representations of images\n",
    "- Most useful in the early layers\n",
    "- Can be concatenated with standard convolution\n",
    "\n",
    "# Convolutional layer\n",
    "\n",
    "## History\n",
    "\n",
    "> Originally, people created those kernels \"by hand\"\n",
    "\n",
    "For example __Sobel filter__ is used to find edges in the image (read more [here](https://en.wikipedia.org/wiki/Sobel_operator))\n",
    "\n",
    "- Those were very specific (edge detection, face detection, image gradient detection)\n",
    "- Hard to come up with\n",
    "- Non-specific to images\n",
    "\n",
    "In 1989 [Yann LeCun](http://yann.lecun.com/ex/research/index.html) came up with an idea to make convolution a neural network layer to solve above shortcomings.\n",
    "\n",
    "## Modern era\n",
    "\n",
    "> Convolutional filter in Deep Learning has learnable connections instead of hard-fixed values\n",
    "\n",
    "This approach solves all of the problems outlined in `History`, but we need some more nomenclature to fully understand this idea:\n",
    "\n",
    "## Input channels\n",
    "\n",
    "> Number of channels entering convolutional layer\n",
    "\n",
    "![image](images/CNN_RGB.JPG)\n",
    "\n",
    "- __Each input channel has it's own set of kernels (which are now weights)!__\n",
    "- Given above we already have weights of shape `(in_channels, width, height)`\n",
    "\n",
    "__Usually, during first convolutional layer, `in_channels=3` (Red, Green Blue) or `1` (grayscale images)__\n",
    "\n",
    "## Output channels\n",
    "\n",
    "> Number of channels created by convolution operation (number of input channels doesn't matter)\n",
    "\n",
    "- Each output channel (with it's filters) convolves over __all input channels__ and sums the result\n",
    "- Given above, we have weights of shape `(out_channels, in_channels, width, height)`\n",
    "\n",
    "__Finally data is produced of shape `(batch, out_channels, width, height)`__\n",
    "\n",
    "## Filters\n",
    "\n",
    "> Collection of kernels (sometimes named as channels)\n",
    "\n",
    "In case of `Conv2d` it will be `(in_channels * out_channels)`\n",
    "\n",
    "# How this looks together\n",
    "\n",
    "Below is a representation of:\n",
    "- __SINGLE__ output channel creation\n",
    "- __FROM THREE INPUT CHANNELS__\n",
    "- __THOSE ARE MIXED TOGETHER__\n",
    "\n",
    "![](./images/kernels_total.gif)\n",
    "\n",
    "# Benefits of convolution\n",
    "\n",
    "- Much smaller amount of parameters (when compared to `nn.Linear`)\n",
    "- Same parameters go over regions of an image (with `nn.Linear` each parameter would be responsible for one pixel)\n",
    "- Tailored for this specific task (architecture)\n",
    "- __Much__ higher performance on spatial tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does each filter look for?\n",
    "\n",
    "> Here's what some simple, small, 1 channel filters might look like after they've been trained.\n",
    "\n",
    "![](images/filters.png)\n",
    "\n",
    "> Convolutional neural networks are often represented by diagrams like the one below:\n",
    "\n",
    "![](images/cnn.png)\n",
    "\n",
    "# Pooling\n",
    "\n",
    "> Pooling allows us to control when we want to shrink image's width and height\n",
    "\n",
    "![](images/maxpoolfig.gif)\n",
    "\n",
    "## Why?\n",
    "\n",
    "- As the network gets depeer __we increase number of channels__ in order to learn more abstract representations, hence __computational cost increases quickly__\n",
    "- Pooling allows us to control computational cost of operations\n",
    "- Pooling chooses the most important features from the image\n",
    "\n",
    "## Versions\n",
    "\n",
    "There are a couple versions of pooling, most prevalent are:\n",
    "- `MaxPooling`\n",
    "- `AvgPooling` (taking average of kernel values)\n",
    "\n",
    "### MaxPooling\n",
    "\n",
    "- Chooses most important features\n",
    "- Sharper decisions\n",
    "- __Might__ be easier to train but __might__ be worse on validation\n",
    "- __Most popular__\n",
    "- Suitable for large networks with enough capacity (layers) to find most important features\n",
    "\n",
    "### AvgPooling\n",
    "\n",
    "- Takes all features into an account\n",
    "- Smoother decision boundary\n",
    "- __Might__ be harder to train but __might__ be better on validation\n",
    "- __Less popular__\n",
    "- Suitable for smaller networks with lower capacity as it doesn't leave any feature behind\n",
    "\n",
    "## Tips\n",
    "\n",
    "- At the start of neural network __do not use pooling__. Go for a couple layers/blocks (like 3/4) and pool after that\n",
    "- After initial convolution layers you may pool every 2 layers/blocks (though you might go for more)\n",
    "- Provided as [torch.nn.AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) \n",
    "\n",
    "\n",
    "# Global Pooling\n",
    "\n",
    "> At the end of neural network we often need to output `(batch_size, classes)` (for classification\n",
    "\n",
    "Due to above, we need to go from shape `(batch_size, channels, width, height)`. Given that:\n",
    "- Abstract features are gathered in channels, __not width and height__\n",
    "\n",
    "We can use so called `GlobalPooling`, provided in PyTorch as [`AdaptiveMaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html) (specify `1` to get `GlobalPooling`):\n",
    "\n",
    "> Global pooling works like a normal pooling, but __always return image with single pixel__, hence output shape will be `(batch_size, channels, 1, 1)` (__remember to `squeeze` dimensions before passing them to `torch.nn.Linear`!)\n",
    "\n",
    "__Global pooling also comes in a few flavors, including `max` and `avg`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting pieces together\n",
    "\n",
    "After this large dose of theorethical knowledge let's see a convolutional neural network in action:\n",
    "\n",
    "## Feature creator\n",
    "\n",
    "> __Convolutional layers create feature representation for our classifier/regressor__\n",
    "\n",
    "General idea standing behind convolutional architectures:\n",
    "\n",
    "- Create first convolutional layer with `in_channels` (usually `3` for `RGB`) and some `out_channels` __which is larger__ (usually `32` or multiple of it)\n",
    "- Follow that by non-linear activation (usually ReLU)\n",
    "- Create blocks consisting of:\n",
    "    - `torch.nn.Conv2d`\n",
    "    - activation\n",
    "    - `torch.nn.BatchNorm2d()` - __spatial batch normalization__ (mean and variance taken separately __for each channel__)\n",
    "- __Each block has predefined size (e.g. `64` channels)__\n",
    "- __This block can be repeated 2/3 times__\n",
    "- After that pooling layer (__usually dividing size of the image by `2`__)\n",
    "- __Next block starts with larger number of channels__ (usually twice as large)\n",
    "- __Process is repeated for `N` layers__ (`3` at least, usually more than `8` up to hundreds for some architectures)\n",
    "\n",
    "Essentially:\n",
    "- __Make your image smaller while SIMULTANEOUSLY increasing number of channels__\n",
    "- Use similar blocks\n",
    "\n",
    "## Classifier\n",
    "\n",
    "> __Classifier takes `2D` input and performs our classification at the end of the network__\n",
    "\n",
    "For modern architectures:\n",
    "- Use `GlobalPooling` __to obtain only number of output channels__ (usually `512`, `1024` or so)\n",
    "- Above creates a tensor of shape `(batch, out_channels)`\n",
    "- Use `torch.nn.Linear(out_channels, n_classes)` to perform classification\n",
    "\n",
    "Sometimes `Flattening` is used instead of global pooling, but:\n",
    "- Imagine output size `(batch, 512, 4, 4)`\n",
    "- If we flatten the above we obtain `(batch, 512 * 4 * 4)`\n",
    "- This tensor goes into `nn.Linear` layer\n",
    "\n",
    "Above approach has two major downsides:\n",
    "- Size of tensor after flattening __is dependent on the size of image__\n",
    "- Due to above we are limited to a single sized images (__neural networks which are not limited are called FULLY CONVOLUTIONAL NETWORKS__) because `nn.Linear` needs static `in_features`\n",
    "- Usually flattening creates way too large `nn.Linear` layer __which outweighs the cost of THE WHOLE CONVOLUTIONAL PART OF THE NETWORK__ (see first architectures like `AlexNet`)\n",
    "\n",
    "## Tips\n",
    "\n",
    "- Give most of the parameter power for the convolutional part\n",
    "- Use `GlobalPooling`\n",
    "- Use skip connections (we will see it in a second)\n",
    "- Base new architectures on well-known ideas (resnets, squeeze-excitation, inverted residuals in MobileNets etc.) as those were found by a large hyperaprameter search techniques unavailable to us\\\n",
    "\n",
    "## Example\n",
    "\n",
    "Let's see an example neural network based on the above guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class CNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, 64, kernel_size=3),\n",
    "            torch.nn.ReLU(),\n",
    "            CNNClassifier._block(64),\n",
    "            CNNClassifier._block(64),\n",
    "            CNNClassifier._block(64),\n",
    "            CNNClassifier._block(64, 128),\n",
    "            # Each group of blocks shorter as it has more learnable parameters\n",
    "            torch.nn.MaxPool2d(2),  # Size of image smaller by 2\n",
    "            CNNClassifier._block(128),\n",
    "            CNNClassifier._block(128),\n",
    "            CNNClassifier._block(128, 256),\n",
    "            torch.nn.MaxPool2d(2),  # Size of image smaller by 2\n",
    "            # Each group of blocks shorter as it has more learnable parameters\n",
    "            CNNClassifier._block(256, 256),\n",
    "            CNNClassifier._block(256, 512),\n",
    "            # output shape: (batch, 512, img_width, img_height)\n",
    "        )\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.AdaptiveAvgPool2d(1),  # output shape: (batch, 512, 1, 1)\n",
    "            torch.nn.Flatten(),  # output shape: (batch, 512)\n",
    "            torch.nn.Linear(512, n_classes),  # output shape: (batch, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.classifier(self.features(X))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, out_channels=None):\n",
    "        if out_channels is None:\n",
    "            out_channels = in_channels\n",
    "\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNets\n",
    "\n",
    "__One of the most important papers in deep learning [link](https://arxiv.org/abs/1512.03385)__\n",
    "\n",
    "> Instead of learning feature transformations as we add convolutional layers, __we learn corrections__ to the previous layers\n",
    "\n",
    "Let's assume `F(x)` is a block containing two convolutional layers transforming `x` inputs. In ResNets case we would add a __skip connection__:\n",
    "\n",
    "![resnet_equation](images/resnet_equation.png)\n",
    "\n",
    "## Degradation problem\n",
    "\n",
    "> When the layer is too deep (on the order of `100` convolutional layers) __it is too hard to optimize__\n",
    "\n",
    "Due to that:\n",
    "- __Deeper networks have larger train loss than more shallow ones!__ `50` layers would perform better than `1000` (which is not intuitive)\n",
    "\n",
    "> __ResNets main addition is solving degradation problem__, due to that we can easily go with `1000` layer networks (though it's __almost always__ not needed for most tasks)\n",
    "\n",
    "> Anywhere from `18` to `152` layers should be enough for any task (use the smallest one satisfying your needs\n",
    "\n",
    "\n",
    "## Additional benefits\n",
    "\n",
    "- Further (after `BatchNorm`) loss landscape smoothing\n",
    "- Further reduction in vanishing/exploding gradient (though mostly taken care of by `BatchNorm` already)\n",
    "\n",
    "## Usage tips\n",
    "\n",
    "- Use any block and connect it using skip connection\n",
    "- Usability is not limited to `convolution`, same thing applies for `linear` (though rarely for recurrent neural networks)\n",
    "- __Try to keep the same size of `inputs` and `outputs` in order not to use projection__, a few resnet blocks with the same number of channels and up the number two times\n",
    "- __DO NOT USE ACTIVATION AT THE END OF LAST LAYER INSIDE RESNET BLOCK__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Create `ResNet18` (or larger) architecture from scratch:\n",
    "- Check out original research paper: https://arxiv.org/abs/1512.03385\n",
    "- Look at `Figure 3` to see how the whole neural network works\n",
    "- Read `Table 1` to see blocks used for different versions\n",
    "- __Use either pooling or stride `2` (original research paper) to make the images smaller after the network__\n",
    "- __Smoke test the architecture with random input of appropriate shape__\n",
    "\n",
    "Additional challenges, __choose any__ (if you get through the above quickly):\n",
    "- Train your architecture on `CIFAR10` via `torchvision` (or any other dataset) and using the training system we've created\n",
    "- Can you generalize the `ResNet` similarly to how `torchvision` does it (e.g. different `ResNet` sizes without too much code repetition (__DRY principle__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Convolutional layers are used with spatial dependent data (usually images)\n",
    "- Can be used for any task just like `nn.Linear` layers\n",
    "- Different versions exist (`1D`, `2D`, `3D`) for different tasks, but usually, respectively:\n",
    "    - Textual (encoded) data or audio\n",
    "    - Images\n",
    "    - Videos (rarest case due to large amount of data)\n",
    "- __Convolution can work on images of any size__ (only `channels` dimension has to be the same)\n",
    "- Convolution has trainable kernels of specified size which together form filters\n",
    "- We increase number of channels while simultaneuosly reducing image's size in order to learn more abstract features\n",
    "- Neural network usually ends with global pooling (and optionally some task specific Linear layers, though modern architectures tend to go for single `nn.Linear`)\n",
    "- Skip connections are used to combat degradation problem\n",
    "- Skip connections allow us to use way deeper networks and optimize the system easier\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- What is depthwise convolution?\n",
    "- What is pointwise convolution?\n",
    "- If you know above, `Separable` convolution is depthwise followed by pointwise convolution\n",
    "- How does MobileNetV2 work and why is it useful?\n",
    "- What is Squeeze-Excitation block, what are the upsides and downsides of adding it? Where should we add it? Read [Squeeze-Excitation research paper](https://arxiv.org/abs/1709.01507)\n",
    "- Read [EfficientNet research paper ](https://arxiv.org/abs/1905.11946) to know current SOTA architecture on ImageNet\n",
    "- Some additional concepts that one might want to read: shuffle nets, inception blocks, \n",
    "- How does attention on images work? Check attention on your own or __come back after attention classes__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
