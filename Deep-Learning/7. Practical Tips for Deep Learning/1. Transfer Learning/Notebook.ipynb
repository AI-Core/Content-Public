{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "> Transfer Learning is an idea to reuse knowledge learned by models in other tasks\n",
    "\n",
    "This approach allows us to spend less time on:\n",
    "- coding and coming up with neural network architectures \n",
    "- collecting data (as large amounts of data were already used to train these networks)\n",
    "\n",
    "Furthermore, it helps with:\n",
    "- generalization (knowledge from similar domain can be easily transferable)\n",
    "- training time (weights are initialized better)\n",
    "\n",
    "And, maybe even more important, __allows us to use knowledge from datasets which are too large to train on one's machine__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvision\n",
    "\n",
    "> `torchvision` ([documentation](https://pytorch.org/docs/stable/torchvision/models.html)) provides SOTA (or close to State Of The Art) neural network models for computer vision tasks\n",
    "\n",
    "Those models were (usually) trained on a well-known `ImageNet` dataset\n",
    "\n",
    "## ImageNet\n",
    "\n",
    "[ImageNet](http://image-net.org/) is not only a dataset, but also __a yearly held classification competition__.\n",
    "\n",
    "Overview of the dataset:\n",
    "- Over 1 million images\n",
    "- Images are of different sizes (but usually those are cropped to `224x224` - `384x384`)\n",
    "- `1000` classes (a lot, this task is hard!)\n",
    "\n",
    "> __One should keep current best models on ImageNet in mind as those are often used as standalone/part of other models!__\n",
    "\n",
    "- At this moment EfficientNet based architectures are current SOTA (original research paper [here](https://arxiv.org/abs/1905.11946))\n",
    "- __Around 90% Top-1 accuracy achieved__ (and 98% Top-5) which means __we are getting closer to solving this dataset as we have \"solved\" MNIST or CIFAR__\n",
    "\n",
    "## Using models\n",
    "\n",
    "> Loading `torchvision` models is simple, use [source code of model](https://pytorch.org/vision/0.8/models.html#torchvision.models.resnet18) to see all available arguments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.squeezenet1_0(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision models classes\n",
    "\n",
    "Models provided by `torchvision` (and not only) can be divided into a few categories (`torchvision` addition if provided by the package):\n",
    "\n",
    "## Classification\n",
    "\n",
    "> Basic task, most of the models were trained on ImageNet (or sometimes pretrained with even larger datasets beforehand). \n",
    "\n",
    "Accuracy classification looks more or less like below (non comprehensive list and grouped by theme, full list [here](https://paperswithcode.com/sota/image-classification-on-imagenet)), sorted from best to last:\n",
    "\n",
    "- EfficientNet family - [research](https://arxiv.org/abs/1905.11946) | `EfficientNet-BN`, `EfficientNet-LN` and their variations\n",
    "- ResNet family - we saw basic idea standing behind it during convolution classes | `torchvision` | `ResNext`, `ResNet`, `Wide ResNe(X)t`\n",
    "- Inception family | `torchvision` | `InceptionV3`, `Xception`\n",
    "- MobileNets | `torchvision` | `MobileNetV{1, 2, 3}`, used as building block of EfficientNet\n",
    "- Older models of historical importance:\n",
    "    - VGG family | `torchvision` | VGG11, VGG19, large and inefficient in comparison\n",
    "    - AlexNet | `torchvision` | First neural network winning ImageNet competition\n",
    "    \n",
    "> __There are a lot of other interesting ideas presented in ImageNet related papers, read them if you are curious!__\n",
    "\n",
    "### Which model should I choose?\n",
    "\n",
    "As always, that depends on your use case, but rough guidelines could be:\n",
    "\n",
    "- __ResNets__:\n",
    "    - battle tested\n",
    "    - work really well in many tasks\n",
    "    - fast and well optimized in many frameworks (perfect for GPUs)\n",
    "    - __may not be the most efficient parameter-wise__\n",
    "    - __go to for initial runs__\n",
    "- __EfficientNets__:\n",
    "    - current SOTA\n",
    "    - may not be as general as ResNet (though research is ever growing)\n",
    "    - may not be as as optimized (ever changing, __potentially faster than ResNets, sometimes much faster__)\n",
    "    - __more efficient parameter-wise__ (smaller model than ResNet, on the order of `10`)\n",
    "    - __test when you want to push your accuracy__\n",
    "    - __test when you want to deploy to mobile and other constrained devices__ (and you need better results)\n",
    "- __MobileNets__:\n",
    "    - really fast (especially on CPU)\n",
    "    - __battle tested for edge deployment & constrained environments__ (AWS Lambda, Mobile)\n",
    "    - can be really really small (below `1KK` parameters) yet good enough\n",
    "    - __use for mobile, may handle a lot of tasks good enough!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number in ResNet tells us how many layers it has\n",
    "resnet = torchvision.models.mobilenet_v3_small(pretrained=True) # Loading weights trained on ImageNet\n",
    "\n",
    "# Interesting model between MobileNets and good accuracy\n",
    "mnasnet = torchvision.models.mnasnet1_0(num_classes=100) # Choosing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tasks\n",
    "\n",
    "- [Semantic Segmentation](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
    "- [Object Detection & Image Segmentation](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)\n",
    "\n",
    "![](images/segmentation_vs_detection.png)\n",
    "\n",
    "[Image Source](https://towardsdatascience.com/a-hitchhikers-guide-to-object-detection-and-instance-segmentation-ac0146fe8e11)\n",
    "\n",
    "__We will not go into details about those models during this lesson__, but important things to keep in mind:\n",
    "- Those models use `classification` models seen above as __backbone__ (feature creator for specific task), __recurring theme in vision!__\n",
    "- Usually trained on large [`COCO` dataset](https://cocodataset.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Hub\n",
    "\n",
    "> PyTorch provides hub from which one can simply download models ([page](https://pytorch.org/hub/) | [module](https://pytorch.org/docs/stable/hub.html))\n",
    "\n",
    "It works in a similar fashion to `torchvision` and is currently being developed as __official source of PyTorch models__.\n",
    "\n",
    "- Anyone can make their models work with PyTorch Hub\n",
    "- `torchvision` models are available through it\n",
    "- Other, non vision models are also provided (including NLP, Audio, Generative)\n",
    "\n",
    "One can easily see available models in repository usuing `torch.hub.list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.hub.list(github=\"intel-isl/MiDaS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find repositories?\n",
    "\n",
    "- Official repositories are linked on [PyTorch Hub](https://pytorch.org/hub/) webpage\n",
    "- Non-official and hosted by users can be found in some repositories (still not such a common practice), __look for `hubconf.py` at the root of github project__ (and see next sections)\n",
    "\n",
    "## More PyTorch Hub commands\n",
    "\n",
    "> Watch out, some models are really large!\n",
    "\n",
    "There are more commands useful for exploration, let's see the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# This directory will be removed after we leave context manager\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    # Where model will be downloaded\n",
    "    torch.hub.set_dir(directory)\n",
    "\n",
    "    print(torch.hub.list(\"pytorch/vision\"))\n",
    "\n",
    "    print(torch.hub.help(\"pytorch/vision\", model=\"mobilenet_v3_large\"))\n",
    "\n",
    "    model = torch.hub.load(\n",
    "        \"pytorch/vision\", model=\"mobilenet_v3_large\", pretrained=True, progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out more aoubt downloaded methods\n",
    "\n",
    "methods = dir(model) # available methods\n",
    "\n",
    "# Info about specific model's method\n",
    "help(model.xpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other sources\n",
    "\n",
    "What if we can't find a desirable model? There are a few available alternatives:\n",
    "- [paperswithcode](https://paperswithcode.com/) - outline current SOTA results including only papers with available source code (__quality of implementation not measured!__)\n",
    "- [arxiv](https://arxiv.org/) - except research models, links to GitHub repositories are __sometimes__ provided, __usually in the abstract__\n",
    "- GitHub accounts of respected research labs (also includes interesting technical solutions):\n",
    "    - [Facebook Research](https://github.com/facebookresearch) - General | Vision\n",
    "    - [DeepMind](https://github.com/deepmind) - General | Reinforcement Learning\n",
    "    - [Google Research](https://github.com/google-research) - General | Health, Business use\n",
    "    - [OpenAI](https://github.com/openai/) - General | NLP, large networks\n",
    "    - [Microsoft Research](https://github.com/MicrosoftResearch) (more technical and less DL based, General)\n",
    "    - [NVIDIA Research](https://github.com/NVlabs) - General | GANs, large scale networks\n",
    "- [Distill.pub](https://distill.pub/) - research reviews & other publications, sometimes with code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conversion\n",
    "\n",
    "> Some models are implemented in different frameworks (usually Tensorflow). We can use `ONNX` to make a conversion\n",
    "\n",
    "# ONNX\n",
    "\n",
    "> [ONNX](https://github.com/onnx/onnx) provides an open source format for AI models, both deep learning and traditional ML\n",
    "\n",
    "- Transform models into open exchange framework `.onnx`\n",
    "- Supported by major frameworks/tools\n",
    "\n",
    "## Downsides\n",
    "\n",
    "- Not all operations between frameworks are interchange'able\n",
    "- For SOTA models conversion might be hard\n",
    "- Puts constrains on some of the frameworks (e.g. PyTorch)\n",
    "\n",
    "> We will see another way to export models for usage in different than Python environments later during `torchscript` lesson\n",
    "\n",
    "> __`ONNX` should be used with care and only for inter-framework conversions.__\n",
    "\n",
    "> We don't want you to know `ONNX` in and out, just keep this tool in mind when the right time comes!\n",
    "\n",
    "## Why would I leave my framework?\n",
    "\n",
    "PyTorch is great, but there are a few cases you might encounter were you need to switch, including:\n",
    "- Part of team (or another team) uses different technology\n",
    "- PyTorch does not support some form of deployment (which Tensorflow might)\n",
    "- Hardware specific optimization is required and not possible in PyTorch\n",
    "- Other parts of the pipeline are implemented in different framework\n",
    "\n",
    "> Above (and many more) reasons also apply to other deep learning/machine learnig frameworks\n",
    "\n",
    "## PyTorch front end\n",
    "\n",
    "Let's see how we can export our PyTorch models to `ONNX` format using `torch.onnx` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "model.eval() # Set in evaluation mode\n",
    "\n",
    "# Batch size of `2` because of BatchNorm\n",
    "example_input = torch.randn(2, 3, 224, 224)\n",
    "with tempfile.TemporaryDirectory() as data_dir:\n",
    "    torch.onnx.export(model, (example_input, ), pathlib.Path(data_dir) / \"mobilenet_v2.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "> Transfer learning is a process of reusing model(s) taught on another task and adjusting to our needs\n",
    "\n",
    "## Per-domain models\n",
    "\n",
    "There are some rough guidelines for different tasks:\n",
    "- Vision:\n",
    "    - ImageNet models (classification)\n",
    "    - COCO pretrained models (with pretrained backbones from ImageNet classification)\n",
    "- NLP:\n",
    "    - Pretrained word embeddings\n",
    "    - Large Transformer based architectures (usually BERT and it's variations)\n",
    "    - __Still emerging approach__\n",
    "    \n",
    "For other tasks (e.g. reinforcement learning, one shot learning, GANs) transfer learning is not yet so widespread.\n",
    "\n",
    "> Probably more pretrained models for different domains will emerge, as we have seen with vision and NLP tasks after that\n",
    "\n",
    "Aforementioned domain-specific models use pretrained networks from vision (most often) as part of their model though.\n",
    "\n",
    "## How to finetune?\n",
    "\n",
    "We will focus on vision and classification tasks, though similar approach is used for NLP.\n",
    "\n",
    "## Weight freezing\n",
    "\n",
    "> Weight freezing means freezing __backbone__ (layers creating features) so __those will not learn anything__ and __only enabling last layer to learn on provided data__\n",
    "\n",
    "### Pros\n",
    "\n",
    "- __The more you freeze, the faster your neural network will run and less memory it will take!__\n",
    "- Easier to finetune and \"get right\"\n",
    "- We surely will not \"destroy\" weights learned on other task (which may sometimes occur at the beginning of training due to random initialization of layer)\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Representational power is limited (as we cannot change frozen weights)\n",
    "- We usually will not get best possible result (though we will get it faster)\n",
    "\n",
    "### Tips\n",
    "\n",
    "- There is no strict rule, you may unfreeze more parts of the network (though less common)\n",
    "- You may start with weight freezing, unfreeze afterwards and finish with small learning rate (or disciminative learning rate), though __this will make the optimization procedure significantly harder__ to implement and reason about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative learning rates\n",
    "    \n",
    "> Discriminative learning mean setting different learning rates for different part of the neural network\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Larger representation space\n",
    "- Probably better accuracy score\n",
    "- We won't destroy pretrained weights (as their learning rate is smaller\n",
    "\n",
    "### Cons\n",
    "\n",
    "- __Way longer__ time to train as the whole network is used\n",
    "- __Harder to finetune__ and \"get right\"\n",
    "\n",
    "### Tips\n",
    "\n",
    "- Divide your neural networks into few regions:\n",
    "    - head should have standard learning rate\n",
    "    - middle of the network should have it the same, but divided by `10`\n",
    "    - first layers (finding general features) should have it the same, but divided by `10`\n",
    "- `10` is not a strict rule but seems to work well in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "> __Data augmentation means changing data sample in some way__\n",
    "\n",
    "We usually do that in order to:\n",
    "- __improve generalization__:\n",
    "    - model sees more samples (sample after augmentation may be totally different from the original one)\n",
    "    - model sees different variants of samples\n",
    "- __enhance size of the dataset__\n",
    "\n",
    "Things to keep in mind:\n",
    "- Augmentations are performed __ON THE FLY__, which means:\n",
    "    - load image from disk (via `torch.utils.data.Dataset` instance)\n",
    "    - perform a transformation\n",
    "    - __Never perform transformations before training (e.g. by saving images)__ as it inflates disk usage without necessity\n",
    "    - __If you want some speedups you can perform caching of some transformations__ (or cache part of the images in memory)\n",
    "- __We have to PRESERVE LABELS__ - for object detection __we cannot__ transform target mask differently than our input image\n",
    "\n",
    "## torchvision augmentations\n",
    "\n",
    "> `torchvision` provides a few well-known augmentations __and this is a good starting point for your models__\n",
    "\n",
    "All of them are provided in [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) submodule.\n",
    "\n",
    "Keep in mind:\n",
    "- `transforms` are done (usually) on a per-element basis\n",
    "- `transforms` are (usually) passed to `torch.utils.data.Dataset` instances\n",
    "\n",
    "Let's see how to create two augmentations via `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transformation = transforms.Compose([\n",
    "     transforms.RandomCrop((224, 224)), # Randomly crop to this image size\n",
    "     transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Pass data_transformation to your torch.utils.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example augmentations\n",
    "\n",
    "Below augmentations are:\n",
    "- battle-tested\n",
    "- work well for a lot of tasks (__given you preserve labels correctly!__)\n",
    "\n",
    "### [RandomCrop](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomCrop)\n",
    "\n",
    "> __Take a random crop of the image with specified size__\n",
    "\n",
    "This augmentation allows us to:\n",
    "- Prevent model from overfitting as it sees different parts of the picture\n",
    "- Increase number of samples\n",
    "- __Hard to go wrong, BUT you shouldn't make the images much smaller__ (as it might lose too much information)\n",
    "\n",
    "![](images/random_crop.jpg)\n",
    "\n",
    "### [Random{Horizontal, Vertical}Flip](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomHorizontalFlip)\n",
    "\n",
    "> __Make a mirror-like flip of the image__\n",
    "\n",
    "Things to note:\n",
    "- __Very hard to go wrong__ (unless you are doing object detection)\n",
    "- __Used more often than VERTICAL__ (because objects in nature are more closely related to horizontal flipping)\n",
    "- __Don't go too crazy with VERTICAL flip__ - this one is probably a better choice\n",
    "\n",
    "![](images/random_horizontal_flip.png)\n",
    "\n",
    "### [RandomRotation](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomRotation)\n",
    "\n",
    "> __Randomly rotate image within `(-degree, degree)` range__\n",
    "\n",
    "Things to note:\n",
    "- __Used quite often__\n",
    "- __May not work really well__ due to necessary zeros and/or zooming in in order to fill left-out space\n",
    "- __Might not be the best first choice__\n",
    "\n",
    "### [CutOut](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomErasing)\n",
    "\n",
    "> __Simple technique where you zero-out part of the image__\n",
    "\n",
    "Relatively new augmentation for neural networks, __but seems to work really well__ due to:\n",
    "- improving generalization (as neural network does not see random parts of the image)\n",
    "- similarity to dropout, yet __not destroying `BatchNorm` layers__\n",
    "- pretty simple and quick\n",
    "\n",
    "![](images/cutout.png)\n",
    "\n",
    "### MixUp\n",
    "\n",
    "> __Mix two images together and mix their respective labels__\n",
    "\n",
    "Deemed a blasphemy, __yet very effective for CNNs__, because:\n",
    "- Smoother decision boundaries\n",
    "- Learns what \"cat-dog\" like creature could look\n",
    "- Conflicting feedback\n",
    "- __Soft targets__\n",
    "\n",
    "![](images/mixup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other augmentations libraries\n",
    "\n",
    "- __[Augly](https://github.com/facebookresearch/AugLy)__:\n",
    "    - Facebook backed (first class PyTorch support)\n",
    "    - __Non-image augmentations__ (audio, text etc.)\n",
    "    - __Wide range of available augmentations__\n",
    "- __[Albumentations](https://github.com/albumentations-team/albumentations)__:\n",
    "    - __ONLY IMAGES__\n",
    "    - __Library-agnostic__ (based on `np.ndarray` transformations)\n",
    "    - __Needs some additional efforts to make it work with PyTorch__\n",
    "    - __Wide range of available augmentations__ (more experimental than `torchvision`)\n",
    "\n",
    "## Tips\n",
    "\n",
    "- __Try not to do too many of them__ - you may lose original information\n",
    "- Use the most standard ones (the ones mentioned in this notebook work reasonably well)\n",
    "- __Start simple__ - rotations, flips, maybe cutout, move on to more sophisticated methods if needed\n",
    "- __Tailor to your data__, e.g. class label is not dependent on the color, try `channel-shuffle` augmentation\n",
    "- __Be creative__ - correct data augmentation can give you a few additional accuracy points!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "> __Get best score in `5` minutes of training from scratch using pretrained model!__\n",
    "\n",
    "- Implement `freeze` function taking in neural network  and setting `requires_grad_(False)` on each parameter\n",
    "- Do the same for `unfreeze` but set parameter's gradient to `True`\n",
    "- Load any model you want from `torchvision` (or maybe some other resource?):\n",
    "    - The larger the better, but may not fit on the GPU\n",
    "    - Use knowledge from the beginning when choosing it\n",
    "- Print the model to get a little info about it's structure (backbone, bottleneck etc.)\n",
    "\n",
    "How can I make my model perform better (or run faster)?\n",
    "\n",
    "- Use one of two freezing modes (__or mix them by freezing only a few layer!__)\n",
    "- Use data augmentations\n",
    "- Use schedulers and optimizers\n",
    "- Use our training system in order to train your models\n",
    "\n",
    "__Additional__:\n",
    "\n",
    "- Create a `MixUp` transform which:\n",
    "    - Gets a batch of data in form `(X, y)` (features, labels)\n",
    "    - Makes a random permutation of features and mixes these together with `lambda` hyperparameter (default equal to `0.5`)\n",
    "    - Makes the same permutation on labels and mixes them together with `lambda` hyperparameter (default equal to `0.5`)\n",
    "    \n",
    "Use it for the task and see whether there's any improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module: torch.nn.Module):\n",
    "    module.eval()\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    \n",
    "def unfreeze(module: torch.nn.Module):\n",
    "    module.train()\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Assessment\n",
    "\n",
    "- What is \"knowledge distillation\"? Where is it used and what are the reasons?\n",
    "- What is \"quantization\"? Why is it useful? When should we use it? Read about it in [PyTorch documentation](https://pytorch.org/docs/stable/quantization.html)\n",
    "- What is \"auto-augmentation\"? Check `torchvision`'s version [here](https://pytorch.org/vision/stable/transforms.html#autoaugment-transforms)\n",
    "\n",
    "### Non-assessment\n",
    "\n",
    "- What is [CutMix](https://arxiv.org/abs/1905.04899) regularization strategy?\n",
    "- Read about necessary steps to publish your models to PyTorch Hub [here](https://pytorch.org/hub/)\n",
    "- What are [Adapters](https://arxiv.org/pdf/1902.00751.pdf)? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiCourse",
   "language": "python",
   "name": "aicourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
