{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders & Recommender Systems\n",
    "\n",
    "In this lesson we will learn about:\n",
    "- new type of neural network architecture (autoencoders)\n",
    "- how one could use them to perform recommendation\n",
    "\n",
    "# Autoencoders\n",
    "\n",
    "> __Autoencoders are self-supervised systems which, given input `x` try to reconstruct it at the output__\n",
    "\n",
    "Let's see how one could write it using mathematical notation:\n",
    "\n",
    "$$\n",
    "L(x, d(e(x)))\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "e(x) \\rightarrow \\text{latent representation space created by encoder}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d(\\hat{x}) \\rightarrow \\text{decoder of latent representation space}\n",
    "$$\n",
    "\n",
    "> __Goal of the autoencoder is to reconstruct the input while, at the same time, finding useful latent representation of the input__\n",
    "\n",
    "There are multiple variants of autoencoders, let's see basic possibilities:\n",
    "\n",
    "## Undercomplete\n",
    "\n",
    "> __Same formulation as above, BUT dimensionality of latent space has to be smaller than that of `x` input__\n",
    "\n",
    "$$\n",
    "x^N, e(x)^M\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "M << N\n",
    "$$\n",
    "\n",
    "__Traits:__\n",
    "- If `M` too large autoencoder might try to simply copy the data\n",
    "- If `M` sufficiently smaller it has to __compress `x` representation__\n",
    "- __Shallow linear autoencoder__ - learns approximately PCA with specified dimensionality\n",
    "- __Deeper non-linear autoencoder__ - learns non-linear generalization of PCA (or T-SNE)\n",
    "\n",
    "\n",
    "## Sparse\n",
    "\n",
    "> __Sparse autoencoder forces the latent space to be sparse via L1 regularization__\n",
    "\n",
    "$$\n",
    "L(x, d(e(x))) + L1(h)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "h = e(x) \\rightarrow \\text{latent representation space}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Omega(h) \\rightarrow \\text{L1 regularization}\n",
    "$$\n",
    "\n",
    "__Traits:__\n",
    "- __Can be wider than undercomplete due to regularization__\n",
    "- Latent variables act as an explanatory terms of `x` input\n",
    "- __One could also use `nn.ReLU` for the representation (instead of `nn.Linear`) to force ACTUALLY sparse representation__\n",
    "\n",
    "## Denoising\n",
    "\n",
    "> __Very popular variation - given `x` we add some random noise to it and force the model to reconstruct `x`__\n",
    "\n",
    "$$\n",
    "L(x, d(e(\\hat{x})))\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat{x} \\rightarrow \\text{noise disturbed input}\n",
    "$$\n",
    "\n",
    "__Traits:__\n",
    "- Noise usually from normal distribution (can be different though)\n",
    "- Structure of the data has to be learned in order to denoise it\n",
    "\n",
    "## Autoencoder features\n",
    "\n",
    "There are a few important concepts one should keep in mind:\n",
    "- __One can use multiple data sources bringing them to a single latent representation__:\n",
    "    - images via convolution\n",
    "    - text via RNNs (or `nn.Conv1d`)\n",
    "    - tabular data via `nn.Linear`)\n",
    "- Try __not to__ overregularize:\n",
    "    - representation can be smaller, __yet not too small__\n",
    "    - the more regularization we introduce, __the large our latent space should be__\n",
    "- Rarely used in non-semi-supervised \n",
    "- Keep `encoder` output linear (unless you want to constrain representation)\n",
    "- __We should force the latent representation closer to our goal__ - example of sparse autoencoder\n",
    "- __We can get a little more creative with our task__ - we will see that during an exercise\n",
    "\n",
    "### Loss function\n",
    "\n",
    "> __What kind of loss function/activations should we use?__\n",
    "\n",
    "__We should always check our `x` data and act accordingly__, for example:\n",
    "- continuous data in `[0, 1]` range - `sigmoid` activation at the end and `MSELoss`\n",
    "- continuous data in `[-1, 1]` range - `tanh` activation at the end and `MSELoss`\n",
    "- categorical data - no activation and `CrossEntropyLoss` (maybe binary)\n",
    "- continuous data within unspecified range - __try to avoid it with normalization__, other than that `MSELoss`\n",
    "\n",
    "> __Remember multidimensional data can be used directly for those loss functions!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(100, 80),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(80, 60),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(60, 40),\n",
    ")\n",
    "\n",
    "\n",
    "decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(40, 60),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(60, 80),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(80, 100),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "autoencoder = AutoEncoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation systems\n",
    "\n",
    "> __This part is only a brief, field is expanding rapidly and directions often change!__\n",
    "\n",
    "![filtering](images/colab_filter.svg)\n",
    "\n",
    "\n",
    "## Content based\n",
    "\n",
    "> __Content based recommends THE USER based on HIS interactions__\n",
    "\n",
    "For example:\n",
    "- User liked multiple poor quality horror movies, hence it is likely that the next poor quality horror movie will interest him\n",
    "- User does not like high heels, hence we shouldn't recommend them\n",
    "\n",
    "__Cons:__\n",
    "- A lot of data needed for a user (__cold start problem__)\n",
    "- __Data sparsity__ (most of the items will have no interactions)\n",
    "- We will not be able to push him out of his \"comfort zone\" (we don't know we like something unless we try it)\n",
    "\n",
    "## Collaborative filtering\n",
    "\n",
    "> __Collaborative filtering makes recommendations based on actions performed by other users/entities__\n",
    "\n",
    "For example:\n",
    "- User A liked similar movies to user B, hence we might suggest similar movies\n",
    "\n",
    "__Cons:__\n",
    "- __Cold start problem__\n",
    "- __Data sparsity__\n",
    "- Creating suitable representation of a user based on his interactions\n",
    "\n",
    "\n",
    "## Tips\n",
    "\n",
    "- __Ask users to engage with the content at the beginning__\n",
    "- __Include as many data points for each user as possible__, some example could be:\n",
    "    - geolocation\n",
    "    - list of friends (integrating system with `Facebook` or `Twitter`)\n",
    "    - non-content related preferences (politics, fashion, webpages visited)\n",
    "- __DO NOT PUSH USERS TOO HARD__ as it may have inverse effect and destroy the data\n",
    "- Gather data in a transparent way for the users (so it does not interfere with their tasks/goals)\n",
    "- Make the engagement simple (e.g. liking/disliking is better than `0`-`10` ratings) because: \n",
    "    - Users usually choose __extreme options__ (either `0`/`1` or `9`/`10`)\n",
    "    - Data obtained this way is often non-representative\n",
    "- Mix a lot of approaches and data sources together (what we are going to do in the exercise)\n",
    "- Separate recommendation sources:\n",
    "    - Based on user's content\n",
    "    - Based on other users activity\n",
    "- __Verify often what works based on user feedback__\n",
    "- __Experiment__ (some approaches use reinforcement learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "> __Let's be creative!__\n",
    "\n",
    "We will try to create autoencoder which has the following goal (__at the same time!__):\n",
    "- Collaborative filtering\n",
    "- Content Based\n",
    "\n",
    "First of all, we have __randomly generated data__ of shape `(M, N)` (users, items) with the following values:\n",
    "- `0` - user did not interact with this item\n",
    "- `1` - user __liked__ the item\n",
    "\n",
    "Below are the steps we will take in order to construct a recommendation system:\n",
    "\n",
    "# Dataset\n",
    "\n",
    "## __init__\n",
    "\n",
    "- Takes two arguments:\n",
    "    - `data` (our `(M, N)` matrix)\n",
    "    - `p` - value between `(0, 1)` which specifies __positive sample probability__ (we will later see what that means)\n",
    "    - Save as attributes for later use\n",
    "    \n",
    "Now, the first __method__ we will need:\n",
    "    \n",
    "## Intersection Over Union\n",
    "\n",
    "> __Protip:__ unsqueeze matrix to obtain `(M, M, N)` tensor and apply __summation__ across last dimension\n",
    "\n",
    "Given our `(M, N)` matrix we have to:\n",
    "- Calculate intersection (__same positive values__) for each user __based on items__, getting `(M, M)` matrix (__protip: multiplication__)\n",
    "- Calculate union (__positive values for either user__) for each user __based on items__, getting `(M, M)` matrix (__protip: addition__)\n",
    "- Divide intersection by union to obtain __IoU__ (save it as `iou` attribute in the dataset)\n",
    "- Created inverse of the `iou` (`1 - iou`) and save it as `iou_inverse` attribute\n",
    "\n",
    "> __Create the above as a standalone method and run it from within `__init__`__\n",
    "\n",
    "## __getitem__(self, index)\n",
    "\n",
    "We will do the following steps:\n",
    "- Obtain user by `index`ing into `self.data`\n",
    "- Based on `self.p` probability (see [here](https://stackoverflow.com/a/5887040/10886420) for example):\n",
    "    - if `True` was sampled:\n",
    "        - Create [`WeightedRandomSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler), where:\n",
    "            - `weights` are taken from appropriate `iou` row by `index`ing into it\n",
    "            - `num_samples=1` - take a single positive example\n",
    "        - Sample from the created sampler to get `positive_index`\n",
    "        - Get value from `iou` and save it as a `similarity` variable (indexed by `index` and `positive_index`)\n",
    "        - Get `positive_sample` from `data` based on `positive_index`\n",
    "        - __Return `tuple` with three elements: `user`, `positive`, `similarity`__\n",
    "    - if `False` was sampled:\n",
    "        - Same as above, but:\n",
    "            - `iou_inverse` is used for sampling\n",
    "            - We return `user`, `negative`, `similarity`\n",
    "    - __Remove unnecessary code repetition!__\n",
    "            \n",
    "            \n",
    "> __Why all of that?__\n",
    "\n",
    "Based on this sample we can:\n",
    "- Force `latent` representation of autoencoder to be similar/dissimilar based on `positive`/`negative` sample\n",
    "\n",
    "# Autoencoder\n",
    "\n",
    "Create `AutoEncoder` similarly to what we did in the first code cell, but with the following additions:\n",
    "- `__init__` takes `p` argument (probability between `[0, 1]`) and creates `torch.nn.Dropout` using it (default value for `p` should be around `0.1`)\n",
    "- `forward` takes additional argument `mask: bool`\n",
    "- `forward` returns `tuple` with two elements:\n",
    "    - latent space (after `encoding`)\n",
    "    - recreated representation by `decoder`\n",
    "    \n",
    "Also create `3`/`4` layer encoder and decoder (separate neural networks) and pass them into `AutoEncoder` class:\n",
    "- end `decoder` with `sigmoid` layer\n",
    "\n",
    "# Loss function(s)\n",
    "\n",
    "## Similarity Measurement\n",
    "\n",
    "Create a new loss function called `EuclideanSimilarity` by inheriting from [PairwiseDistance](https://pytorch.org/docs/stable/generated/torch.nn.PairwiseDistance.html) and __overriding `forward`__:\n",
    "- Takes two arguments `x1` and `x2`\n",
    "- Passes them through `super().__call__(x1, x2)` and transforms distance (`d`) into similarity according to formula:\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{1 + d}\n",
    "$$\n",
    "\n",
    "## Generic loss\n",
    "\n",
    "Create a new loss function by inheriting from `nn.Module` and inside:\n",
    "- `__init__`:\n",
    "    - create `BCEWithLogitsLoss` and save it as `bce_logits`\n",
    "    - create `BCELoss` and save it as `bce`\n",
    "    - create `EuclideanSimilarity` and save it as `similarity`\n",
    "- `forward`:\n",
    "    - Takes three arguments:\n",
    "        - `user_reconstruction` (`torch.Tensor` of shape `(batch, features)`)\n",
    "        - `original` (`torch.Tensor` of shape `(batch, features)`)\n",
    "        - `user_latent` (`torch.Tensor` of shape `(batch, features)`)\n",
    "        - `other_latent` (`torch.Tensor` of shape `(batch, features)`)\n",
    "        - `positivity` (`torch.Tensor` of shape `(batch,)` with binary labels)\n",
    "    - `reconstruction = self.bce_logits(reconstructed, original)` - obtains reconstruction loss of our (possibly masked) sample\n",
    "    - `similarity = self.similarity(user_latent, other_latent)` - how close the representation is to positive/negative sample in terms of euclidean similarity\n",
    "    - `positive_negative_similarity = self.bce(similarity, positivity)` - measures whether the representation should be moved away in euclidean space or pushed closer together\n",
    "    - Return `reconstruction + positive_negative_similarity` as final loss function\n",
    "    \n",
    "# Training loop\n",
    "\n",
    "For each item in `Dataset` (`user`, `other`, `similarity`) do the following:\n",
    "- Pass `user` through `autoencoder` with `mask=True` to obtain `(user_latent, user_reconstruction)`\n",
    "- Pass `other` through `autoencoder` with `mask=False` to obtain `(other_latent, _)` (__second return value is not needed!__)\n",
    "- Pass (`user_reconstruction, user, user_latent, other_latent, similarity`) to out previously constructed loss function\n",
    "- Backpropagate and try to minimize loss\n",
    "\n",
    "# After training\n",
    "\n",
    "> __What (hopefully!) have we achieved at the end?__\n",
    "\n",
    "## Collaborative filtering\n",
    "\n",
    "- Pass whole dataset `(users, items)` through `encoder` to get encoded representation\n",
    "- For the user of choice find the most similar one via `EuclideanSimilarity` (or multiple similar users)\n",
    "- __Recommend items the user did not like from the other users__ (possibly with majority voting)\n",
    "\n",
    "## Content-Based\n",
    "\n",
    "- Pass specific user through both `encoder` and `decoder` (__with `mask=False` specified!__)\n",
    "- Get the largest value __which is `zero` in the original representation__\n",
    "- Recommend this item as the next one (possibly with some `threshold` for the representation)\n",
    "\n",
    "\n",
    "# Good Luck :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = 10000\n",
    "items = 100\n",
    "\n",
    "data = torch.randint(high=2, size=(users, items))\n",
    "\n",
    "# Remember to create new cells for each part of the exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "## Assessment\n",
    "\n",
    "- Contrastive autoencoder:\n",
    "    - How does it work?\n",
    "    - To which other type of autoencoder (shown in this lecture) is it connected?\n",
    "    - What is the rationale standing behind it?\n",
    "- How to tie weights in PyTorch? Create autoencoder with weights tied between `encoder` and `decoder`, use it in our project\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- Variational Autoencoders:\n",
    "    - Why is it a generative model?\n",
    "    - How does the __reparametrization trick__ work?\n",
    "    - Code an example VAE\n",
    "    \n",
    "- Check [DeepRec](https://arxiv.org/abs/1708.01715) model. Do you know how to code it?\n",
    "- Check [SimCLR](https://arxiv.org/pdf/2002.05709.pdf) research paper for creative approach to representation learning\n",
    "- What is [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) and how does it relate [Matrix Factorization](https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems) technique?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-AiCore] *",
   "language": "python",
   "name": "conda-env-.conda-AiCore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
