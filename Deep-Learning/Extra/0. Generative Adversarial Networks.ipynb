{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "So far, all the models we have worked with except the VAE have been discriminative models. This means that they are simply trying to predit something about our existing dataset. Sometimes, we would not like to discriminate but generate new examples of the data as in the case of video or image generation. Technically, this is the problem of modelling a probability distribution which we have samples of.\n",
    "\n",
    "One approach which implicitly models the distribution, the work of Ian Goodfellow, has be enjoying great success - often producing images indistinguishable from the examples on which it was trained. GANs take a game-theoretic approach, pitting two networks against eachother - the discriminator and the generator. The job of the generator is to produce images which are indistinguishable from the training set from latent variables while the job of the discriminator is to catch out the generator and discriminate real data from generated data. Initially they will both be terrible at their jobs but as the discriminator gets better, the generator is forced to get better to fool it and vice-versa. This loop continues until they are both excellent at their jobs and the generator can now be used to produce very realistic data points.\n",
    "\n",
    "![](images/GAN.png)\n",
    "\n",
    "An analogy often used to describe this is the detective and the forger. The generator is like a forger who is trying to produce paintings indistinguishable from other famous paintings by an artists while the discriminator is like a detective who is trying to catch the forger out. As the detective gets better at catching the generator out, the generator is forced to improve to fool the detective."
   ]
  },
  {
   "source": [
    "Here's the architecture for DCGAN (Deep Convolutional GAN), which we will implement today.\n",
    "\n",
    "![](images/dcgan_architecture.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We will be training a GAN on the fashion MNIST dataset so we will be able to produce images of items of clothing which look like they came from the original dataset.\n",
    "\n",
    "We begin by importing the appropriate libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our dataset into a pytorch dataloader which we will use later to produce random batches of samples from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "100.1%Extracting ./MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST-data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "113.5%Extracting ./MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST-data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "100.4%Extracting ./MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST-data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "180.4%Extracting ./MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST-data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    train=True,\n",
    "    download=True,\n",
    "    root='./MNIST-data',\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the NN model we will be using for our discriminator. It takes in the 28x28 image and performs convolutions followed by one fully connected layer to output the probability of the data point being real and not generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.4996]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 64, 3, 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "d = Discriminator()\n",
    "\n",
    "x, _ = train_data[0]\n",
    "x = x.unsqueeze(0)\n",
    "print(d(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the NN model for the generator. This takes in a latent vector of size 128 and performs fully connected layers followed by upconvolution to output us a 28x28 image.\n",
    "\n",
    "One layer that we'd like to use in a generator would be one that unflattens the input from a vector into a tensor of given dimensions. So the first thing we do in the cell below is define that custom pytorch layer. Look how easy it is to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class Unflatten(nn.Module):\n",
    "    def __init__(self, out_shape):\n",
    "        super().__init__()\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, *self.out_shape)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 1152),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.Linear(1024, 1152),\n",
    "            nn.LeakyReLU(),\n",
    "            Unflatten((128, 3, 3)),\n",
    "            nn.ConvTranspose2d(128, 64, 3, 3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 2, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.layers(z)\n",
    "\n",
    "g = Generator()\n",
    "latent_vec_size = 100\n",
    "ran_batch = torch.rand(batch_size, latent_vec_size)\n",
    "fake = g(ran_batch)\n",
    "print(fake.shape)\n",
    "\n",
    "#%%\n",
    "def show(x):\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "Now that we have the generator, we're almost ready to code up the training loop. However, before we do that let's make a function to randomly sample a vector of noise from the latent space and generate a sample from it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(writer=None, device='cpu'):\n",
    "    if writer is None:\n",
    "        writer = SummaryWriter(log_dir=f'runs/DCGAN/{time()}')\n",
    "    z = torch.randn(batch_size, latent_vec_size).to(device)\n",
    "    for img in G(z):\n",
    "        writer.add_image(f'test', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the training loop. For every batch of training data that we look at, we get the generator to produce an equally sized batch of generated images. We then get the discriminator to make predictions on both sets and calulate the cost for both networks before calculating the gradients and training each one in turn.\n",
    "\n",
    "You should run tensorboard to see the evolution of the generated images and visualise the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "89855957\n",
      "Epoch: 0 Batch: 128 Loss G: 0.2680076062679291 Loss D: 0.4070155918598175\n",
      "Epoch: 0 Batch: 129 Loss G: 0.26785269379615784 Loss D: 0.3963998854160309\n",
      "Epoch: 0 Batch: 130 Loss G: 0.2697426676750183 Loss D: 0.40959662199020386\n",
      "Epoch: 0 Batch: 131 Loss G: 0.2649030387401581 Loss D: 0.4023708701133728\n",
      "Epoch: 0 Batch: 132 Loss G: 0.26895198225975037 Loss D: 0.4059301018714905\n",
      "Epoch: 0 Batch: 133 Loss G: 0.2685399651527405 Loss D: 0.38539648056030273\n",
      "Epoch: 0 Batch: 134 Loss G: 0.26647239923477173 Loss D: 0.4242362976074219\n",
      "Epoch: 0 Batch: 135 Loss G: 0.2660166025161743 Loss D: 0.392152339220047\n",
      "Epoch: 0 Batch: 136 Loss G: 0.26944154500961304 Loss D: 0.3905590772628784\n",
      "Epoch: 0 Batch: 137 Loss G: 0.26714906096458435 Loss D: 0.42616626620292664\n",
      "Epoch: 0 Batch: 138 Loss G: 0.26920539140701294 Loss D: 0.4115142822265625\n",
      "Epoch: 0 Batch: 139 Loss G: 0.2729398012161255 Loss D: 0.3930966854095459\n",
      "Epoch: 0 Batch: 140 Loss G: 0.2719000577926636 Loss D: 0.4031646251678467\n",
      "Epoch: 0 Batch: 141 Loss G: 0.2795752286911011 Loss D: 0.43030303716659546\n",
      "Epoch: 0 Batch: 142 Loss G: 0.2768174111843109 Loss D: 0.4101274907588959\n",
      "Epoch: 0 Batch: 143 Loss G: 0.2830258011817932 Loss D: 0.4005538523197174\n",
      "Epoch: 0 Batch: 144 Loss G: 0.28270578384399414 Loss D: 0.39507704973220825\n",
      "Epoch: 0 Batch: 145 Loss G: 0.28498634696006775 Loss D: 0.40098389983177185\n",
      "Epoch: 0 Batch: 146 Loss G: 0.2884063124656677 Loss D: 0.39793652296066284\n",
      "Epoch: 0 Batch: 147 Loss G: 0.2863388657569885 Loss D: 0.3853721022605896\n",
      "Epoch: 0 Batch: 148 Loss G: 0.29005998373031616 Loss D: 0.40251681208610535\n",
      "Epoch: 0 Batch: 149 Loss G: 0.2914820611476898 Loss D: 0.3922470510005951\n",
      "Epoch: 0 Batch: 150 Loss G: 0.2922360897064209 Loss D: 0.3747525215148926\n",
      "Epoch: 0 Batch: 151 Loss G: 0.293622225522995 Loss D: 0.3979215919971466\n",
      "Epoch: 0 Batch: 152 Loss G: 0.29299670457839966 Loss D: 0.4040638208389282\n",
      "Epoch: 0 Batch: 153 Loss G: 0.28981420397758484 Loss D: 0.396623432636261\n",
      "Epoch: 0 Batch: 154 Loss G: 0.2862244248390198 Loss D: 0.3805738687515259\n",
      "Epoch: 0 Batch: 155 Loss G: 0.2872035801410675 Loss D: 0.3987693190574646\n",
      "Epoch: 0 Batch: 156 Loss G: 0.2879736125469208 Loss D: 0.38804441690444946\n",
      "Epoch: 0 Batch: 157 Loss G: 0.28375038504600525 Loss D: 0.3714061677455902\n",
      "Epoch: 0 Batch: 158 Loss G: 0.28112149238586426 Loss D: 0.4186316132545471\n",
      "Epoch: 0 Batch: 159 Loss G: 0.28197064995765686 Loss D: 0.3746155798435211\n",
      "Epoch: 0 Batch: 160 Loss G: 0.28059065341949463 Loss D: 0.39165326952934265\n",
      "Epoch: 0 Batch: 161 Loss G: 0.2778385281562805 Loss D: 0.3790111541748047\n",
      "Epoch: 0 Batch: 162 Loss G: 0.27636289596557617 Loss D: 0.39448270201683044\n",
      "Epoch: 0 Batch: 163 Loss G: 0.2817744314670563 Loss D: 0.3782167434692383\n",
      "Epoch: 0 Batch: 164 Loss G: 0.2793351411819458 Loss D: 0.3930065631866455\n",
      "Epoch: 0 Batch: 165 Loss G: 0.2786191999912262 Loss D: 0.3944145739078522\n",
      "Epoch: 0 Batch: 166 Loss G: 0.28219982981681824 Loss D: 0.38819965720176697\n",
      "Epoch: 0 Batch: 167 Loss G: 0.28251588344573975 Loss D: 0.3784405291080475\n",
      "Epoch: 0 Batch: 168 Loss G: 0.28394585847854614 Loss D: 0.3867405652999878\n",
      "Epoch: 0 Batch: 169 Loss G: 0.28687992691993713 Loss D: 0.38808727264404297\n",
      "Epoch: 0 Batch: 170 Loss G: 0.2894839644432068 Loss D: 0.3786683976650238\n",
      "Epoch: 0 Batch: 171 Loss G: 0.28814560174942017 Loss D: 0.3852933645248413\n",
      "Epoch: 0 Batch: 172 Loss G: 0.29317665100097656 Loss D: 0.379109650850296\n",
      "Epoch: 0 Batch: 173 Loss G: 0.2933149039745331 Loss D: 0.38854411244392395\n",
      "Epoch: 0 Batch: 174 Loss G: 0.29485586285591125 Loss D: 0.38246533274650574\n",
      "Epoch: 0 Batch: 175 Loss G: 0.29255157709121704 Loss D: 0.39100342988967896\n",
      "Epoch: 0 Batch: 176 Loss G: 0.2973189055919647 Loss D: 0.38281741738319397\n",
      "Epoch: 0 Batch: 177 Loss G: 0.298404335975647 Loss D: 0.3747665286064148\n",
      "Epoch: 0 Batch: 178 Loss G: 0.29218482971191406 Loss D: 0.382981538772583\n",
      "Epoch: 0 Batch: 179 Loss G: 0.29637718200683594 Loss D: 0.38914114236831665\n",
      "Epoch: 0 Batch: 180 Loss G: 0.29295656085014343 Loss D: 0.37865349650382996\n",
      "Epoch: 0 Batch: 181 Loss G: 0.29452237486839294 Loss D: 0.39519548416137695\n",
      "Epoch: 0 Batch: 182 Loss G: 0.2900111973285675 Loss D: 0.385162353515625\n",
      "Epoch: 0 Batch: 183 Loss G: 0.2888752818107605 Loss D: 0.3959049880504608\n",
      "Epoch: 0 Batch: 184 Loss G: 0.2911997437477112 Loss D: 0.3773522973060608\n",
      "Epoch: 0 Batch: 185 Loss G: 0.28486132621765137 Loss D: 0.3845137357711792\n",
      "Epoch: 0 Batch: 186 Loss G: 0.28339022397994995 Loss D: 0.3877388536930084\n",
      "Epoch: 0 Batch: 187 Loss G: 0.2795281708240509 Loss D: 0.38653650879859924\n",
      "Epoch: 0 Batch: 188 Loss G: 0.2787621021270752 Loss D: 0.3905000388622284\n",
      "Epoch: 0 Batch: 189 Loss G: 0.27604031562805176 Loss D: 0.39981764554977417\n",
      "Epoch: 0 Batch: 190 Loss G: 0.2723170518875122 Loss D: 0.3854183852672577\n",
      "Epoch: 0 Batch: 191 Loss G: 0.27519574761390686 Loss D: 0.38241544365882874\n",
      "Epoch: 0 Batch: 192 Loss G: 0.2719840407371521 Loss D: 0.3856433033943176\n",
      "Epoch: 0 Batch: 193 Loss G: 0.2743226885795593 Loss D: 0.4033268690109253\n",
      "Epoch: 0 Batch: 194 Loss G: 0.276528537273407 Loss D: 0.38537928462028503\n",
      "Epoch: 0 Batch: 195 Loss G: 0.2754373848438263 Loss D: 0.40166276693344116\n",
      "Epoch: 0 Batch: 196 Loss G: 0.28006523847579956 Loss D: 0.39859238266944885\n",
      "Epoch: 0 Batch: 197 Loss G: 0.2815820276737213 Loss D: 0.38952726125717163\n",
      "Epoch: 0 Batch: 198 Loss G: 0.2824302613735199 Loss D: 0.39396360516548157\n",
      "Epoch: 0 Batch: 199 Loss G: 0.2860218286514282 Loss D: 0.39863166213035583\n",
      "Epoch: 0 Batch: 200 Loss G: 0.29193490743637085 Loss D: 0.39891570806503296\n",
      "sampling\n",
      "Epoch: 0 Batch: 201 Loss G: 0.29378247261047363 Loss D: 0.38214725255966187\n",
      "Epoch: 0 Batch: 202 Loss G: 0.30072087049484253 Loss D: 0.395946204662323\n",
      "Epoch: 0 Batch: 203 Loss G: 0.3037753105163574 Loss D: 0.4008152186870575\n",
      "Epoch: 0 Batch: 204 Loss G: 0.3111748993396759 Loss D: 0.4030342996120453\n",
      "Epoch: 0 Batch: 205 Loss G: 0.3106497526168823 Loss D: 0.3867248594760895\n",
      "Epoch: 0 Batch: 206 Loss G: 0.3112987279891968 Loss D: 0.410465270280838\n",
      "Epoch: 0 Batch: 207 Loss G: 0.3174605965614319 Loss D: 0.38070815801620483\n",
      "Epoch: 0 Batch: 208 Loss G: 0.31836119294166565 Loss D: 0.39652422070503235\n",
      "Epoch: 0 Batch: 209 Loss G: 0.31698161363601685 Loss D: 0.3978031873703003\n",
      "Epoch: 0 Batch: 210 Loss G: 0.3212983012199402 Loss D: 0.3943028748035431\n",
      "Epoch: 0 Batch: 211 Loss G: 0.3159017860889435 Loss D: 0.3838590979576111\n",
      "Epoch: 0 Batch: 212 Loss G: 0.3193594217300415 Loss D: 0.40624749660491943\n",
      "Epoch: 0 Batch: 213 Loss G: 0.3202025890350342 Loss D: 0.38355183601379395\n",
      "Epoch: 0 Batch: 214 Loss G: 0.3244739770889282 Loss D: 0.3831351101398468\n",
      "Epoch: 0 Batch: 215 Loss G: 0.3223099708557129 Loss D: 0.3906075060367584\n",
      "Epoch: 0 Batch: 216 Loss G: 0.32525694370269775 Loss D: 0.4039230942726135\n",
      "Epoch: 0 Batch: 217 Loss G: 0.3309222459793091 Loss D: 0.3796681761741638\n",
      "Epoch: 0 Batch: 218 Loss G: 0.32978394627571106 Loss D: 0.3886958062648773\n",
      "Epoch: 0 Batch: 219 Loss G: 0.33179011940956116 Loss D: 0.3943488895893097\n",
      "Epoch: 0 Batch: 220 Loss G: 0.33509790897369385 Loss D: 0.3875609040260315\n",
      "Epoch: 0 Batch: 221 Loss G: 0.3354802131652832 Loss D: 0.40736594796180725\n",
      "Epoch: 0 Batch: 222 Loss G: 0.33834055066108704 Loss D: 0.3851112127304077\n",
      "Epoch: 0 Batch: 223 Loss G: 0.33681008219718933 Loss D: 0.3826761245727539\n",
      "Epoch: 0 Batch: 224 Loss G: 0.3379344642162323 Loss D: 0.38746461272239685\n",
      "Epoch: 0 Batch: 225 Loss G: 0.3345809280872345 Loss D: 0.3924947679042816\n",
      "Epoch: 0 Batch: 226 Loss G: 0.33469900488853455 Loss D: 0.38899874687194824\n",
      "Epoch: 0 Batch: 227 Loss G: 0.3326328992843628 Loss D: 0.3939040005207062\n",
      "Epoch: 0 Batch: 228 Loss G: 0.33104807138442993 Loss D: 0.4054908752441406\n",
      "Epoch: 0 Batch: 229 Loss G: 0.3274098038673401 Loss D: 0.40360769629478455\n",
      "Epoch: 0 Batch: 230 Loss G: 0.3229024410247803 Loss D: 0.4136420786380768\n",
      "Epoch: 0 Batch: 231 Loss G: 0.32315510511398315 Loss D: 0.4092879593372345\n",
      "Epoch: 0 Batch: 232 Loss G: 0.32020261883735657 Loss D: 0.3994957208633423\n",
      "Epoch: 0 Batch: 233 Loss G: 0.3218633830547333 Loss D: 0.37958958745002747\n",
      "Epoch: 0 Batch: 234 Loss G: 0.3174913823604584 Loss D: 0.41108494997024536\n",
      "Epoch: 0 Batch: 235 Loss G: 0.318617045879364 Loss D: 0.39288339018821716\n",
      "Epoch: 0 Batch: 236 Loss G: 0.3178730309009552 Loss D: 0.38952726125717163\n",
      "Epoch: 0 Batch: 237 Loss G: 0.3197163939476013 Loss D: 0.40703362226486206\n",
      "Epoch: 0 Batch: 238 Loss G: 0.3193228542804718 Loss D: 0.41185739636421204\n",
      "Epoch: 0 Batch: 239 Loss G: 0.3212478756904602 Loss D: 0.40567195415496826\n",
      "Epoch: 0 Batch: 240 Loss G: 0.3202463984489441 Loss D: 0.4113411605358124\n",
      "Epoch: 0 Batch: 241 Loss G: 0.3192742168903351 Loss D: 0.4070388674736023\n",
      "Epoch: 0 Batch: 242 Loss G: 0.31807202100753784 Loss D: 0.3961948752403259\n",
      "Epoch: 0 Batch: 243 Loss G: 0.3201436996459961 Loss D: 0.41886892914772034\n",
      "Epoch: 0 Batch: 244 Loss G: 0.3176335394382477 Loss D: 0.39175552129745483\n",
      "Epoch: 0 Batch: 245 Loss G: 0.3129052221775055 Loss D: 0.39881452918052673\n",
      "Epoch: 0 Batch: 246 Loss G: 0.31298089027404785 Loss D: 0.3918004333972931\n",
      "Epoch: 0 Batch: 247 Loss G: 0.3124544620513916 Loss D: 0.38731467723846436\n",
      "Epoch: 0 Batch: 248 Loss G: 0.31636521220207214 Loss D: 0.4136160910129547\n",
      "Epoch: 0 Batch: 249 Loss G: 0.31512901186943054 Loss D: 0.4045354723930359\n",
      "Epoch: 0 Batch: 250 Loss G: 0.31065064668655396 Loss D: 0.3927356004714966\n",
      "Epoch: 0 Batch: 251 Loss G: 0.3121030926704407 Loss D: 0.3837739825248718\n",
      "Epoch: 0 Batch: 252 Loss G: 0.3104250431060791 Loss D: 0.41548028588294983\n",
      "Epoch: 0 Batch: 253 Loss G: 0.3114868402481079 Loss D: 0.40692606568336487\n",
      "Epoch: 0 Batch: 254 Loss G: 0.31338801980018616 Loss D: 0.40997132658958435\n",
      "Epoch: 0 Batch: 255 Loss G: 0.3136264681816101 Loss D: 0.40479379892349243\n",
      "Epoch: 0 Batch: 256 Loss G: 0.3180956244468689 Loss D: 0.4004283845424652\n",
      "Epoch: 0 Batch: 257 Loss G: 0.32127442955970764 Loss D: 0.41018733382225037\n",
      "Epoch: 0 Batch: 258 Loss G: 0.32779660820961 Loss D: 0.3956685960292816\n",
      "Epoch: 0 Batch: 259 Loss G: 0.32880163192749023 Loss D: 0.41532158851623535\n",
      "Epoch: 0 Batch: 260 Loss G: 0.3348049223423004 Loss D: 0.4086116552352905\n",
      "Epoch: 0 Batch: 261 Loss G: 0.33244264125823975 Loss D: 0.40529462695121765\n",
      "Epoch: 0 Batch: 262 Loss G: 0.3386799991130829 Loss D: 0.4088403582572937\n",
      "Epoch: 0 Batch: 263 Loss G: 0.3408011794090271 Loss D: 0.3945659101009369\n",
      "Epoch: 0 Batch: 264 Loss G: 0.33826133608818054 Loss D: 0.4227693974971771\n",
      "Epoch: 0 Batch: 265 Loss G: 0.34200674295425415 Loss D: 0.40600481629371643\n",
      "Epoch: 0 Batch: 266 Loss G: 0.3459528088569641 Loss D: 0.40404433012008667\n",
      "Epoch: 0 Batch: 267 Loss G: 0.3455466032028198 Loss D: 0.3791702389717102\n",
      "Epoch: 0 Batch: 268 Loss G: 0.34878218173980713 Loss D: 0.37995681166648865\n",
      "Epoch: 0 Batch: 269 Loss G: 0.34659716486930847 Loss D: 0.42189204692840576\n",
      "Epoch: 0 Batch: 270 Loss G: 0.3455505967140198 Loss D: 0.3956754207611084\n",
      "Epoch: 0 Batch: 271 Loss G: 0.3494633734226227 Loss D: 0.3998679220676422\n",
      "Epoch: 0 Batch: 272 Loss G: 0.3431329131126404 Loss D: 0.39702701568603516\n",
      "Epoch: 0 Batch: 273 Loss G: 0.3510846793651581 Loss D: 0.42505547404289246\n",
      "Epoch: 0 Batch: 274 Loss G: 0.351558655500412 Loss D: 0.3912121653556824\n",
      "Epoch: 0 Batch: 275 Loss G: 0.35038936138153076 Loss D: 0.3962058424949646\n",
      "Epoch: 0 Batch: 276 Loss G: 0.3523862957954407 Loss D: 0.4029000997543335\n",
      "Epoch: 0 Batch: 277 Loss G: 0.35358473658561707 Loss D: 0.3919399380683899\n",
      "Epoch: 0 Batch: 278 Loss G: 0.3531055748462677 Loss D: 0.4060162305831909\n",
      "Epoch: 0 Batch: 279 Loss G: 0.35618630051612854 Loss D: 0.3917209506034851\n",
      "Epoch: 0 Batch: 280 Loss G: 0.35698533058166504 Loss D: 0.40789398550987244\n",
      "Epoch: 0 Batch: 281 Loss G: 0.3604137599468231 Loss D: 0.39535975456237793\n",
      "Epoch: 0 Batch: 282 Loss G: 0.35966309905052185 Loss D: 0.40156736969947815\n",
      "Epoch: 0 Batch: 283 Loss G: 0.360393762588501 Loss D: 0.397644579410553\n",
      "Epoch: 0 Batch: 284 Loss G: 0.36447060108184814 Loss D: 0.3977714478969574\n",
      "Epoch: 0 Batch: 285 Loss G: 0.3677324652671814 Loss D: 0.3970003128051758\n",
      "Epoch: 0 Batch: 286 Loss G: 0.3658279478549957 Loss D: 0.39437612891197205\n",
      "Epoch: 0 Batch: 287 Loss G: 0.3714315593242645 Loss D: 0.3883433938026428\n",
      "Epoch: 0 Batch: 288 Loss G: 0.3658783435821533 Loss D: 0.39435577392578125\n",
      "Epoch: 0 Batch: 289 Loss G: 0.36480027437210083 Loss D: 0.41012898087501526\n",
      "Epoch: 0 Batch: 290 Loss G: 0.36458057165145874 Loss D: 0.38939085602760315\n",
      "Epoch: 0 Batch: 291 Loss G: 0.3645550608634949 Loss D: 0.37529152631759644\n",
      "Epoch: 0 Batch: 292 Loss G: 0.36012008786201477 Loss D: 0.3968063294887543\n",
      "Epoch: 0 Batch: 293 Loss G: 0.35854899883270264 Loss D: 0.384548544883728\n",
      "Epoch: 0 Batch: 294 Loss G: 0.35738179087638855 Loss D: 0.3781036138534546\n",
      "Epoch: 0 Batch: 295 Loss G: 0.3533474802970886 Loss D: 0.3817713260650635\n",
      "Epoch: 0 Batch: 296 Loss G: 0.3497457206249237 Loss D: 0.39502424001693726\n",
      "Epoch: 0 Batch: 297 Loss G: 0.34748396277427673 Loss D: 0.3884678781032562\n",
      "Epoch: 0 Batch: 298 Loss G: 0.3430946469306946 Loss D: 0.3825233578681946\n",
      "Epoch: 0 Batch: 299 Loss G: 0.34139126539230347 Loss D: 0.39740967750549316\n",
      "Epoch: 0 Batch: 300 Loss G: 0.3421267569065094 Loss D: 0.3984287679195404\n",
      "sampling\n",
      "Epoch: 0 Batch: 301 Loss G: 0.34222978353500366 Loss D: 0.38101926445961\n",
      "Epoch: 0 Batch: 302 Loss G: 0.3363502323627472 Loss D: 0.39990776777267456\n",
      "Epoch: 0 Batch: 303 Loss G: 0.3377112150192261 Loss D: 0.38584810495376587\n",
      "Epoch: 0 Batch: 304 Loss G: 0.33345261216163635 Loss D: 0.3810705542564392\n",
      "Epoch: 0 Batch: 305 Loss G: 0.3324577808380127 Loss D: 0.38943904638290405\n",
      "Epoch: 0 Batch: 306 Loss G: 0.3335265517234802 Loss D: 0.3963325321674347\n",
      "Epoch: 0 Batch: 307 Loss G: 0.33475568890571594 Loss D: 0.40634334087371826\n",
      "Epoch: 0 Batch: 308 Loss G: 0.33412468433380127 Loss D: 0.40396907925605774\n",
      "Epoch: 0 Batch: 309 Loss G: 0.33140474557876587 Loss D: 0.392549067735672\n",
      "Epoch: 0 Batch: 310 Loss G: 0.3309152126312256 Loss D: 0.3877772092819214\n",
      "Epoch: 0 Batch: 311 Loss G: 0.33183690905570984 Loss D: 0.3737693428993225\n",
      "Epoch: 0 Batch: 312 Loss G: 0.33051395416259766 Loss D: 0.3775656521320343\n",
      "Epoch: 0 Batch: 313 Loss G: 0.32785120606422424 Loss D: 0.3944666087627411\n",
      "Epoch: 0 Batch: 314 Loss G: 0.3331432342529297 Loss D: 0.38491401076316833\n",
      "Epoch: 0 Batch: 315 Loss G: 0.3328624963760376 Loss D: 0.39210137724876404\n",
      "Epoch: 0 Batch: 316 Loss G: 0.33736661076545715 Loss D: 0.3735780119895935\n",
      "Epoch: 0 Batch: 317 Loss G: 0.3313473165035248 Loss D: 0.3892379105091095\n",
      "Epoch: 0 Batch: 318 Loss G: 0.33808544278144836 Loss D: 0.40194642543792725\n",
      "Epoch: 0 Batch: 319 Loss G: 0.3336738348007202 Loss D: 0.3847612142562866\n",
      "Epoch: 0 Batch: 320 Loss G: 0.3368821144104004 Loss D: 0.38586825132369995\n",
      "Epoch: 0 Batch: 321 Loss G: 0.3369506001472473 Loss D: 0.38599342107772827\n",
      "Epoch: 0 Batch: 322 Loss G: 0.34263843297958374 Loss D: 0.3777487874031067\n",
      "Epoch: 0 Batch: 323 Loss G: 0.3459616005420685 Loss D: 0.36130183935165405\n",
      "Epoch: 0 Batch: 324 Loss G: 0.347312331199646 Loss D: 0.39218413829803467\n",
      "Epoch: 0 Batch: 325 Loss G: 0.3512858748435974 Loss D: 0.38125869631767273\n",
      "Epoch: 0 Batch: 326 Loss G: 0.3524240255355835 Loss D: 0.38335442543029785\n",
      "Epoch: 0 Batch: 327 Loss G: 0.35505053400993347 Loss D: 0.40023136138916016\n",
      "Epoch: 0 Batch: 328 Loss G: 0.3603416979312897 Loss D: 0.3625822961330414\n",
      "Epoch: 0 Batch: 329 Loss G: 0.3603481650352478 Loss D: 0.3868350088596344\n",
      "Epoch: 0 Batch: 330 Loss G: 0.36530715227127075 Loss D: 0.36376792192459106\n",
      "Epoch: 0 Batch: 331 Loss G: 0.3732810914516449 Loss D: 0.3669576644897461\n",
      "Epoch: 0 Batch: 332 Loss G: 0.375798761844635 Loss D: 0.4055697023868561\n",
      "Epoch: 0 Batch: 333 Loss G: 0.37969309091567993 Loss D: 0.3742804527282715\n",
      "Epoch: 0 Batch: 334 Loss G: 0.38245099782943726 Loss D: 0.3740582764148712\n",
      "Epoch: 0 Batch: 335 Loss G: 0.3860630691051483 Loss D: 0.38264140486717224\n",
      "Epoch: 0 Batch: 336 Loss G: 0.38199755549430847 Loss D: 0.3938269019126892\n",
      "Epoch: 0 Batch: 337 Loss G: 0.38709938526153564 Loss D: 0.38984349370002747\n",
      "Epoch: 0 Batch: 338 Loss G: 0.3832728862762451 Loss D: 0.3878372013568878\n",
      "Epoch: 0 Batch: 339 Loss G: 0.3859574794769287 Loss D: 0.3746415972709656\n",
      "Epoch: 0 Batch: 340 Loss G: 0.38368383049964905 Loss D: 0.37323105335235596\n",
      "Epoch: 0 Batch: 341 Loss G: 0.3846430480480194 Loss D: 0.40764158964157104\n",
      "Epoch: 0 Batch: 342 Loss G: 0.38557663559913635 Loss D: 0.3777179718017578\n",
      "Epoch: 0 Batch: 343 Loss G: 0.376275897026062 Loss D: 0.3895816206932068\n",
      "Epoch: 0 Batch: 344 Loss G: 0.37025919556617737 Loss D: 0.3820210099220276\n",
      "Epoch: 0 Batch: 345 Loss G: 0.3666013777256012 Loss D: 0.39691928029060364\n",
      "Epoch: 0 Batch: 346 Loss G: 0.3643176555633545 Loss D: 0.3771511912345886\n",
      "Epoch: 0 Batch: 347 Loss G: 0.3581160604953766 Loss D: 0.3924586772918701\n",
      "Epoch: 0 Batch: 348 Loss G: 0.3542395830154419 Loss D: 0.409747838973999\n",
      "Epoch: 0 Batch: 349 Loss G: 0.3518495261669159 Loss D: 0.4025152921676636\n",
      "Epoch: 0 Batch: 350 Loss G: 0.352266401052475 Loss D: 0.371328204870224\n",
      "Epoch: 0 Batch: 351 Loss G: 0.34546995162963867 Loss D: 0.3725215792655945\n",
      "Epoch: 0 Batch: 352 Loss G: 0.3444979786872864 Loss D: 0.41407597064971924\n",
      "Epoch: 0 Batch: 353 Loss G: 0.34452831745147705 Loss D: 0.38767874240875244\n",
      "Epoch: 0 Batch: 354 Loss G: 0.33581167459487915 Loss D: 0.38481152057647705\n",
      "Epoch: 0 Batch: 355 Loss G: 0.3343106508255005 Loss D: 0.36995455622673035\n",
      "Epoch: 0 Batch: 356 Loss G: 0.3298168480396271 Loss D: 0.382485032081604\n",
      "Epoch: 0 Batch: 357 Loss G: 0.33061039447784424 Loss D: 0.39169588685035706\n",
      "Epoch: 0 Batch: 358 Loss G: 0.32464730739593506 Loss D: 0.39953628182411194\n",
      "Epoch: 0 Batch: 359 Loss G: 0.32549214363098145 Loss D: 0.36449915170669556\n",
      "Epoch: 0 Batch: 360 Loss G: 0.32331493496894836 Loss D: 0.39945176243782043\n",
      "Epoch: 0 Batch: 361 Loss G: 0.3172166645526886 Loss D: 0.3895077705383301\n",
      "Epoch: 0 Batch: 362 Loss G: 0.31851068139076233 Loss D: 0.3992338478565216\n",
      "Epoch: 0 Batch: 363 Loss G: 0.3219916522502899 Loss D: 0.39785248041152954\n",
      "Epoch: 0 Batch: 364 Loss G: 0.3194904923439026 Loss D: 0.40052494406700134\n",
      "Epoch: 0 Batch: 365 Loss G: 0.32611021399497986 Loss D: 0.39002785086631775\n",
      "Epoch: 0 Batch: 366 Loss G: 0.32376790046691895 Loss D: 0.4036080241203308\n",
      "Epoch: 0 Batch: 367 Loss G: 0.32781559228897095 Loss D: 0.38376832008361816\n",
      "Epoch: 0 Batch: 368 Loss G: 0.33635151386260986 Loss D: 0.38313573598861694\n",
      "Epoch: 0 Batch: 369 Loss G: 0.34014904499053955 Loss D: 0.3958355784416199\n",
      "Epoch: 0 Batch: 370 Loss G: 0.34765490889549255 Loss D: 0.4001408517360687\n",
      "Epoch: 0 Batch: 371 Loss G: 0.35030263662338257 Loss D: 0.3789091110229492\n",
      "Epoch: 0 Batch: 372 Loss G: 0.35466283559799194 Loss D: 0.39262136816978455\n",
      "Epoch: 0 Batch: 373 Loss G: 0.36256763339042664 Loss D: 0.3981368839740753\n",
      "Epoch: 0 Batch: 374 Loss G: 0.36168527603149414 Loss D: 0.368770956993103\n",
      "Epoch: 0 Batch: 375 Loss G: 0.36954599618911743 Loss D: 0.38902831077575684\n",
      "Epoch: 0 Batch: 376 Loss G: 0.3746071755886078 Loss D: 0.3903774917125702\n",
      "Epoch: 0 Batch: 377 Loss G: 0.3760233521461487 Loss D: 0.37294086813926697\n",
      "Epoch: 0 Batch: 378 Loss G: 0.38184213638305664 Loss D: 0.3687603771686554\n",
      "Epoch: 0 Batch: 379 Loss G: 0.38463056087493896 Loss D: 0.3786000609397888\n",
      "Epoch: 0 Batch: 380 Loss G: 0.38752663135528564 Loss D: 0.3936503231525421\n",
      "Epoch: 0 Batch: 381 Loss G: 0.39098381996154785 Loss D: 0.40455538034439087\n",
      "Epoch: 0 Batch: 382 Loss G: 0.3891698122024536 Loss D: 0.40317144989967346\n",
      "Epoch: 0 Batch: 383 Loss G: 0.39791181683540344 Loss D: 0.39058810472488403\n",
      "Epoch: 0 Batch: 384 Loss G: 0.40123045444488525 Loss D: 0.37215059995651245\n",
      "Epoch: 0 Batch: 385 Loss G: 0.40930312871932983 Loss D: 0.36888614296913147\n",
      "Epoch: 0 Batch: 386 Loss G: 0.4071035385131836 Loss D: 0.39826902747154236\n",
      "Epoch: 0 Batch: 387 Loss G: 0.41040751338005066 Loss D: 0.3806782066822052\n",
      "Epoch: 0 Batch: 388 Loss G: 0.4112585783004761 Loss D: 0.3914618194103241\n",
      "Epoch: 0 Batch: 389 Loss G: 0.4091002643108368 Loss D: 0.37132471799850464\n",
      "Epoch: 0 Batch: 390 Loss G: 0.4074905216693878 Loss D: 0.3914806544780731\n",
      "Epoch: 0 Batch: 391 Loss G: 0.4068942070007324 Loss D: 0.3740442991256714\n",
      "Epoch: 0 Batch: 392 Loss G: 0.41352784633636475 Loss D: 0.38488125801086426\n",
      "Epoch: 0 Batch: 393 Loss G: 0.41117537021636963 Loss D: 0.38405856490135193\n",
      "Epoch: 0 Batch: 394 Loss G: 0.40926021337509155 Loss D: 0.3887316584587097\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2cbc8cfdbab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-2cbc8cfdbab9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(G, D, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_vec_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from time import time\n",
    "\n",
    "toPILImg = transforms.ToPILImage()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def train(G, D, epochs=1):\n",
    "    optimiser_d = torch.optim.Adam(D.parameters(), lr=0.00001)\n",
    "    optimiser_g = torch.optim.Adam(G.parameters(), lr=0.0001)\n",
    "    writer = SummaryWriter(log_dir=f'runs/DCGAN/{time()}')\n",
    "    G = G.to(device)\n",
    "    D = D.to(device)\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for idx, (x, _) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            z = torch.randn(batch_size, latent_vec_size)\n",
    "            z = z.to(device)\n",
    "\n",
    "            # GENERATOR UPDATE\n",
    "            optimiser_g.zero_grad()\n",
    "            labels = torch.ones(batch_size).to(device)\n",
    "            G_loss = criterion(D(G(z)), labels)\n",
    "            # G_loss = - torch.log(1 - D(G(z)))\n",
    "            # G_loss = torch.mean(G_loss)\n",
    "            G_loss.backward(retain_graph=True)\n",
    "            optimiser_g.step()\n",
    "\n",
    "            # DISCRIMINATOR UPDATE\n",
    "            optimiser_d.zero_grad()\n",
    "            labels = torch.zeros(batch_size).to(device)\n",
    "            D_loss = criterion(D(G(z)), labels) # loss on fake examples\n",
    "            # D_loss = - (torch.log(D(x)) + torch.log(1 - D(G(z))))\n",
    "            # D_loss = torch.mean(D_loss)\n",
    "            D_loss.backward()\n",
    "            labels = torch.ones(x.shape[0]).to(device)\n",
    "            D_loss = criterion(D(x), labels)\n",
    "            D_loss.backward()\n",
    "            optimiser_d.step()\n",
    "            \n",
    "            writer.add_scalar('Loss/G', G_loss.item(), batch_idx)\n",
    "            writer.add_scalar('Loss/D', D_loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "            print(\n",
    "                'Epoch:', epoch ,\n",
    "                'Batch:', idx,\n",
    "                'Loss G:', G_loss.item(),\n",
    "                'Loss D:', D_loss.item()\n",
    "            )\n",
    "            if idx % 100 == 0:\n",
    "                print('sampling')\n",
    "                sample(writer, device)\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "train(G, D, epochs=10)"
   ]
  },
  {
   "source": [
    "## GANs are hard to train!\n",
    "\n",
    "Well that's a first version done\n",
    "\n",
    "We can see that it doesn't produce perfect results, even after many epochs.\n",
    "\n",
    "Let's try and implement these [\"GAN hacks\"](https://github.com/soumith/ganhacks) which have been shown to stabilise the training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('main': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}