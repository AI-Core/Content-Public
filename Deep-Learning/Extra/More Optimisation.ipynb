{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Optimisation for Deep Learning\n",
    "\n",
    "Learning outcomes\n",
    "- understand mathematically and intuitively the most common optimisation algorithms used for optimising deep models\n",
    "- implement your own optimiser in PyTorch\n",
    "\n",
    "## Reminder of gradient based optimisation\n",
    "\n",
    "So far we've looked at some pretty simple gradient based optimisers including gradient descent and stochastic gradient descent.\n",
    "\n",
    "In this notebook, we'll look at some more complex optimisers, which can overcome some of the shortcomings of the methods we've looked at previously, and will allow us to train deep models more quickly.\n",
    "\n",
    "Here's a visualisation of how different optimisers might iteratively update the model weights.\n",
    "\n",
    "![](images/optim_vis.gif)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Challenges with optimising deep models\n",
    "- Local structure may not be representative of global structure\n",
    "- \n",
    "\n",
    "## Again, don't be scared by local optima\n",
    "\n",
    "1. Local minima become exponentially rare with the number of parameters in the model\n",
    "For every weight, we will compute how the loss changes with respect to it.\n",
    "It becomes exponentially unlikely that the rate of change along every weight axis will be positive. \n",
    "\n",
    "2. Empirically, local minima perform well enough!\n",
    "Even local minima can achieve "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient Descent\n",
    "\n",
    "![](images/gradient_descent.jpg)\n",
    "\n",
    "## SGD\n",
    "\n",
    "![](images/SGD.jpg)\n",
    "\n",
    "## SGD with momentum\n",
    "\n",
    "![](images/momentum.jpg)\n",
    "\n",
    "## SGD with Nesterov momentum\n",
    "\n",
    "![](images/nesterov.jpg)\n",
    "\n",
    "## AdaGrad\n",
    "\n",
    "Is there a more systematic way to reduce the learning rate over time?\n",
    "\n",
    "AdaGrad assumes so, and reduces the learning rate\n",
    "\n",
    "![](images/adagrad.jpg)\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "The problem with AdaGrad is that the learning rate can never recover and increase to speed up optimisation once it has slowed down, it can only decrease further. So if a steep part of the loss surface is encountered before a flatter part, the learning rate for this parameter will be divided by the large loss surface gradient in the steep region and be too small to make meaningful progress in the flatter region.\n",
    "\n",
    "RMSProp is similar to AdaGrad except for how it accumulates the gradient to decay the learning rate for each parameter. Instead of continuuing to sum up the square of all of the gradients encountered in each given direction, it takes an *exponential moving average*. This gives the chance for the learning rate to increase if a steep gradient were not encountered recently, as the historical gradients encountered have an exponentially smaller influence on the learning rate with each optimisation step.\n",
    "\n",
    "![](images/rmsprop.jpg)\n",
    "\n",
    "## Adam\n",
    "\n",
    "![](images/adam.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## So which algorithm do I use?\n",
    "\n",
    "Well... as usual, it depends on your problem and your dataset.\n",
    "\n",
    "It's still a highly active field of research. But in general, **SGD with momentum or Adam** are the go to choices for optimising deep models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using these optimisation algorithms\n",
    "\n",
    "Let's set up the same neural network as in the previous module, and then switch out the optimiser for Adam and others and show how you can adapt it to use momentum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import NN, get_dataloaders\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# HOW TO USE DIFFERENT OPTIMISERS PROVIDED BY PYTORCH\n",
    "optimiser = torch.optim.SGD(my_nn.parameters(), lr=learning_rate, momentum=0.1)\n",
    "# optimiser = torch.optim.Adagrad(NN.parameters(), lr=learning_rate)\n",
    "# optimiser = torch.optim.RMSprop(NN.parameters(), lr=learning_rate)\n",
    "optimiser = torch.optim.Adam(my_nn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "source": [
    "The stuff below is exactly the same as before!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GET DATALOADERS\n",
    "test_loader, val_loader, train_loader = get_dataloaders()\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TRAINING LOOP\n",
    "def train(model, optimiser, tag, graph_name, epochs=1):\n",
    "    writer = SummaryWriter(log_dir=f'../../runs/{tag}') # make a different writer for each tagged optimisation run\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar(f'Optimisers/{graph_name}', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "# train(my_nn, optimiser)"
   ]
  },
  {
   "source": [
    "Let's compare the training curves generated using some of the optimisers that we explained above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "optimisers = [\n",
    "    {\n",
    "        'optimiser_class': torch.optim.SGD, \n",
    "        'tag': 'SGD'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adam,\n",
    "        'tag': 'Adam'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adagrad,\n",
    "        'tag': 'Adagrad'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.RMSprop,\n",
    "        'tag': 'RMSProp'\n",
    "    }\n",
    "]\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for optimiser_obj in optimisers:   \n",
    "    for lr in learning_rates:\n",
    "        my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "        optimiser_class = optimiser_obj['optimiser_class']\n",
    "        optimiser = optimiser_class(my_nn.parameters(), lr=lr)\n",
    "        tag = optimiser_obj['tag']\n",
    "        train(my_nn, optimiser, f'Optimisers/{tag}', graph_name=f'lr={lr}', epochs=1)\n",
    "    "
   ]
  },
  {
   "source": [
    "## Implementing our own PyTorch optimiser\n",
    "\n",
    "To understand a bit further what's happening under the hood, let's implement SGD from scratch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, model_params, learning_rate):\n",
    "        self.model_params = list(model_params) # HACK turning to list prevents len model_params being zero\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.model_params:\n",
    "                param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.model_params:\n",
    "            if param.grad is None: # if not yet set (loss.backward() not yet called)\n",
    "                print('continuing')\n",
    "                continue\n",
    "            param.grad = torch.zeros_like(param.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "optimiser = SGD(my_nn.parameters(), learning_rate=0.1)\n",
    "\n",
    "train(my_nn, optimiser, 'Loss/Train/custom_sgd')"
   ]
  },
  {
   "source": [
    "## Challenges\n",
    "- flash card match images with name of optimisation algorithm\n",
    "- roughly sketch the paths that different optimisation algorithms might take"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}