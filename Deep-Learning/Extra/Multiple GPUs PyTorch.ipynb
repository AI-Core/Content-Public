{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple GPUs\n",
    "\n",
    "There are two main modes to use multiple GPUs during your neural network training/inference:\n",
    "- __Model Parallel - Splitting model across many GPUs__\n",
    "- __Data Parallel - Splitting data across many GPUs__\n",
    "\n",
    "> __Both of those modes can be mixed, though usually we only need Data Parallel!__\n",
    "\n",
    "## Model Parallel\n",
    "\n",
    "> Model Parallel requires (see MANDATORY assessments for full picture) manual casting parts of the model to specified devices\n",
    "\n",
    "We will only take a look at single machine Model Parallel (single machine, multiple GPUs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.net1(x.to('cuda:0')))\n",
    "        return self.net2(x.to('cuda:1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parallel\n",
    "\n",
    "> __PyTorch provides special [`torch.nn.parallel.DistributedDataParallel`](https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html#distributeddataparallel) class to work with data split across multiple devices__\n",
    "\n",
    "- __Works on single and multiple machines__\n",
    "- Is currently the fastest iteration of PyTorch's\n",
    "- Works with Model Parallel (see [here](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism))\n",
    "\n",
    "### DistributedDataParallel vs DataParallel\n",
    "\n",
    "__Never use [`torch.nn.DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html), because:__\n",
    "- `DataParallel` works only on a single machine\n",
    "- It is constrained by GIL (Global Interpreter Lock), hence __can run multiple threads, not processes__\n",
    "- Due to above usually slower even on a single machine\n",
    "- __Does not work with Model Parallel__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "## Assessment \n",
    "\n",
    "- Check [PyTorch Pipeline parallelism](https://pytorch.org/docs/stable/pipeline.html) for more sensible & automated approach to model sharing across devices\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- Check out [Model Parallel tutorial](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html) to see how to use pretrained and ready models with multiple GPUs\n",
    "- Check out [Getting Started with RPC](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html) for Model Parallel across multiple machines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
