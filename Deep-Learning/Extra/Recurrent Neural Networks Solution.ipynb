{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "So far, the neural network architectures we have been using take in a single fixed size input and give a single fixed size output. What if we wanted to model something like language where we want to feed in different length words or sentences? Another issue with vanilla fully connected networks is that each output is only dependent on the current input. It has no 'memory' of previous inputs so you can't model time dependent variables. Recurrent neural networks address both these issues.\n",
    "\n",
    "They do this by having an internal hidden state which can be thought of as a form of memory. At each time step, the new hidden state (h) is calculated as a function of the previous hidden state and the current input (x). This hidden state can then be used to represent your output or can be put through another transformation to compute the outputs (y).\n",
    "\n",
    "The left diagram below shows how we represent a RNN whereas the right one shows a RNN which has been \"unrolled\" over time so we see the value of each variable at successive time steps.\n",
    "\n",
    "![image](images/RNN_basic.JPG)\n",
    "\n",
    "The diagram below shows the actual matrix representation of each of the variables in the RNN. Instead of doing two separate matrix multiplications on the input and previous hidden state to calculate the next hidden state, we can concatenate those two variables into  single I vector and W matrix\n",
    "\n",
    "![image](images/RNN_matrices.JPG)\n",
    "\n",
    "There is also an alternative way of visually representing the RNN which lets us see how similar it is to a fully connected network.\n",
    "\n",
    "![image](images/RNN_other_rep.JPG)\n",
    "\n",
    "Standard neural networks can only model one to one relationships while RNNs are extremely flexible in terms of input-output structures which is one of the reasons they are so powerful. You can imagine something like one to many being used to feed in a single image from which a caption is sequentially produced or a many to one being used to feed in a sentence sequentially and give a single output describing the sentiment of the sentence.\n",
    "\n",
    "![image](images/RNN_layouts.JPG)\n",
    "Source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "### Optimization\n",
    "Surprisingly, with this increased complexity in structure, the optimization method does not become any more difficult. Despite having a different name, back-propagation through time, it is essentially the same thing. All you do is feed in your sequence sequentially to get the output, as usual. You then just calculate your error at each timestep and sum it as opposed to calculating the error at a single timestep like standard neural networks. Then you can use gradient descent to update your weights iteratively until you are satisfied with your network's performance.\n",
    "\n",
    "![image](images/RNN_BPTT.JPG)\n",
    "\n",
    "RNNS are generally slower to optimize than standard neural networks as the output at each time step is dependent on the previous output so the operations cannot be parallelized.\n",
    "\n",
    "![image](images/RNN_parallel.JPG)\n",
    "\n",
    "For a long time it was considered difficult to train RNNs due to two problems called vanishing and exploding gradients. These problems also exist in standard neural network but are greatly emphasized in RNNs. However, modern techniques such as LSTM cells have greatly reduced this difficulty.\n",
    "\n",
    "![image](images/RNN_gradient.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We are going to be implementing a one-to-one character level text prediction model. We will be sequentially feeding in a single character and asking our network to predict the next character as a time dependent function of all the characters that came before it.\n",
    "\n",
    "First we need a dataset. This is just a text file which contains the data which we want to model. In this case, I have found a file which contains ~0.5MB of Kendrick Lamar lyrics. You can use a variety of different datasets. There are plenty of which are easily accessible online - check out the links below. Otherwise, is very easy to create your own either by copying and pasting text into a file or creating a bot to automatically do this for you.\n",
    "\n",
    "[Datasets repo 1](https://github.com/cedricdeboom/character-level-rnn-datasets/tree/master/datasets)\n",
    "\n",
    "We now define our dataset class which we can use to read the dataset and use it with a pytorch dataloader for easy sampling. \n",
    "\n",
    "We first open the file and read all the data.\n",
    "\n",
    "Each text character will be represented by a unique number so we first need all the unique characters in our text. Once we have this, we create a dictionary which maps from a unique number to a letter. After defining the reverse mapping aswell, we use the dictionary to convert our original string into a list of numbers where each number represents a text character.\n",
    "\n",
    "The labels are simply the input but shifted by one as we are always predicting the next character based on the current one.\n",
    "\n",
    "Notice how we do not one-hot encode our input, instead outputting the unique id of each character in the text. The reason for this is because we use a pytorch embedding layer which is explained later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharRNNDataset():\n",
    "    def __init__(self, txt_file_path='Data/lyrics.txt', chunk_size=100, transform=None):\n",
    "        self.txt_file_path = txt_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        #open our text file and read all the data into the rawtxt variable\n",
    "        with open(txt_file_path, 'r') as file:\n",
    "            rawtxt = file.read()\n",
    "\n",
    "        #turn all of the text into lowercase as it will reduce the number of characters that our algorithm needs to learn\n",
    "        rawtxt = rawtxt.lower()\n",
    "        \n",
    "        letters = set(rawtxt) #returns the list of unique characters in our raw text\n",
    "        self.nchars = len(letters) #number of unique characters in our text file\n",
    "        self.num_to_let = dict(enumerate(letters)) #created the dictionary mapping\n",
    "        self.let_to_num = dict(zip(self.num_to_let.values(), self.num_to_let.keys())) #create the reverse mapping so we can map from a character to a unique number\n",
    "        \n",
    "        txt = list(rawtxt)#convert string to list\n",
    "        for k, letter in enumerate(txt): #iterate through our text and change the value for each character to its mapped value\n",
    "            txt[k] = self.let_to_num[letter] #set the kth item equal to the value it maps to\n",
    "\n",
    "        self.X = np.array(txt) #convert txt to numpy array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)-self.chunk_size #the number of datapoints we have based on the chunk size and X\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx:idx+self.chunk_size] #get the chunk at the particular index\n",
    "        y = self.X[idx+1:idx+self.chunk_size+1] #get the labels which is like the input but shifted one to the left\n",
    "        \n",
    "        if self.transform: #apply the transform if any\n",
    "            x, y = self.transform((x, y))\n",
    "    \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "class ToLongTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, inp):\n",
    "        return (torch.LongTensor(var) for var in inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "chunk_size = 150 #the length of the sequences which we will optimize over\n",
    "\n",
    "train_data = CharRNNDataset('Data/lyrics.txt', chunk_size=chunk_size, transform=ToLongTensor()) #instantiate dataset from class defined above\n",
    "x, y = train_data[0]\n",
    "print('First input', x)\n",
    "print('First label', y, '\\n')\n",
    "\n",
    "nchars = train_data.nchars\n",
    "num_to_let = train_data.num_to_let\n",
    "let_to_num = train_data.let_to_num\n",
    "\n",
    "print('Number of unique chatacters:', nchars)\n",
    "print('Length of dataset:', len(train_data))\n",
    "\n",
    "train_loader = DataLoader(train_data,# make the training dataloader\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model which takes in variables defining its structure as parameters. The encoder converts each unique number into an embedding which is fed into the rnn model. The RNN calculates the hidden state which is converted into an output through a fully connected layer called the decoder.\n",
    "\n",
    "We also define the init_hidden function which outputs us a tensor of zeros of the required size for the initial hidden state.\n",
    "\n",
    "The input we get from the dataloader is a vector of integers each of which corresponds to a character. To feed this in to our model we use a vector embedding. The embedding layer in pytorch takes in each integer and converts them into one-hot encoded vectors. It then performs a linear transformation from that to our embedding size which is then fed into our RNN. In the one-hot vector space, the vector for each character is orthogonal to every other vector so each letter is equally \"similar\" to every other letter. The embedding vector is continuous hence it can learn which characters have similar usage patterns and put them closer in the embedding vector space. Our embedding length can be smaller than the one-hot vector so we can compress the input as each variable can take on continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_len, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = torch.nn.Embedding(input_size, embedding_len) #embedding layer\n",
    "        self.i2h = torch.nn.Linear(embedding_len + hidden_size, hidden_size) #linear layer from I vector to the hidden\n",
    "        self.h2y = torch.nn.Linear(hidden_size, output_size) #linear layer from hidden state to output\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.encoder(x) #encode the input into a vector embedding\n",
    "        combined = torch.cat((embedding, hidden), 1) #concatenate embedding and hidden to create I vector\n",
    "        hidden = torch.tanh(self.i2h(combined)) #apply linear layer and activation function to calculate hidden state value\n",
    "        output = self.h2y(hidden) #calculate output from hidden state\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(x.shape[0], self.hidden_size) #zeros vector of hidden size for each input example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our model, define the appropriate hyper-parameters, cost function and optimizer. We will be training on ranom samples from the text of length chunk_size so it is what batch size is to normal neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "embedding_len = 400\n",
    "hidden_size = 128\n",
    "\n",
    "myRNN = CharRNN(nchars, embedding_len, hidden_size, nchars) #instantiate the model from the class defined earlier\n",
    "criterion = torch.nn.CrossEntropyLoss() #define cost function - Cross Entropy\n",
    "optimizer = torch.optim.Adam(myRNN.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop, sequentially feeding in multiple batches of random chunks of text, summing the cost for each character in the sequence (backpropagation through time) and calculating the gradients to update our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "def train(model, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0 #stores the cost for each epoch\n",
    "        generated_string = '' #stores the text generated by our model for the 0th batch over the whole epoch\n",
    "        for idx, (x, y) in enumerate(train_loader):\n",
    "            loss = 0 #cost for this batch\n",
    "            h = model.init_hidden(x) #initialize our hidden state to 0s\n",
    "            for i in range(chunk_size): #sequentially input each character in the sequence for each batch and calculate loss\n",
    "                out, h = model.forward(x[:, i], h) #calculate outputs based on input and previous hidden state\n",
    "                \n",
    "                _, outl = out.data.max(1) #based on our output, what character id does our network assign the highest probability of being next? # This is a [batch_size] sized Tensor\n",
    "                    \n",
    "                letter = num_to_let[outl[0].item()] #what chatacter is predicted for the 0th batch item?\n",
    "                generated_string+=letter #add the predicted letter to our generated sequence\n",
    "                \n",
    "                loss += criterion(out, y[:, i]) #add the cost for this input to the cost for the current batch\n",
    "            \n",
    "            writer.add_scalar('Loss/Train', loss/chunk_size, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "            \n",
    "            #based on the sum of the cost for this sequence (backpropagation through time) calculate the gradients and update our weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss+=loss.item() #add the cost of this sequence to the cost of this epoch\n",
    "        epoch_loss /= len(train_loader.dataset) #divide by the number of datapoinst in each epoch\n",
    "\n",
    "        print('Epoch ', epoch+1, ' Avg Loss: ', epoch_loss)\n",
    "        print('Generated text: ', generated_string[0:600], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(myRNN, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text above picks the most probable next character each time. This is not the best way to do it as our model will be deterministic so it will produce the same text over and over again. To get it producing different text, we should instead sample from the probability distribution of possible next letters output by the network. That is what we will do with the generate function. It takes in a prime string which can be used to prime the hidden state of the network before it start making predictions. It essentially completes the string you prime it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should take in a string and map each value in it to a value from a dictionary\n",
    "def maparray(txt, mapdict):\n",
    "    txt = list(txt)\n",
    "    for k, letter in enumerate(txt): #iterate through our text and change the value for each character to its mapped value\n",
    "        txt[k] = mapdict[letter] #set the kth item equal to the value it maps to\n",
    "    txt = np.array(txt) #convert to numpy array\n",
    "    return txt\n",
    "\n",
    "def generate(model, prime_str='a', str_len=150, temperature=0.75):\n",
    "    generated_string = prime_str #the sequence generated so far is equal to the prime string\n",
    "    \n",
    "    prime_str = maparray(prime_str, let_to_num) #use the maparray function to map the string to its character ids\n",
    "    x = torch.LongTensor(prime_str).unsqueeze(0)  #convert to LongTensor and add dimension to make batch size 1\n",
    "    \n",
    "    h = model.init_hidden(x) #initialize hidden state\n",
    "    \n",
    "    for i in range(x.shape[1]-1): #for each input character except the last\n",
    "        out, h = model.forward(x[:, i], h) #feed that character into the network (prime hidden state)\n",
    "    \n",
    "    x = x[:, -1] #get the last letter\n",
    "    for i in range(str_len): #for each character we want to generate\n",
    "        out, h = model.forward(x, h) #feed in the last character \n",
    "        \n",
    "        out_dist = out.view(-1).div(temperature).exp() #get the output and exponentiate\n",
    "        sample = torch.multinomial(out_dist, 1).item() #turn into torch multinomial distribution and sample\n",
    "        pred_char = num_to_let[sample] #convert the sampled number into the corresponding character\n",
    "        \n",
    "        generated_string += pred_char #add the character to the generated string\n",
    "        \n",
    "        x = torch.LongTensor([sample]) #set the last letter equal to the newly generated character\n",
    "    \n",
    "    return generated_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generate(myRNN, 'starting text ', 2000, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch's built in RNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_len, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        #store input parameters in the object so we can use them later on\n",
    "        self.input_size = input_size\n",
    "        self.embedding_len = embedding_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #required functions for model\n",
    "        self.encoder = torch.nn.Embedding(input_size, embedding_len) #apply embedding layer\n",
    "        self.rnn = torch.nn.RNN(embedding_len, hidden_size, n_layers, batch_first=True) #create recurrent layer\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size) #linear mapping from hidden to output\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.encoder(x.view(-1)) #encode our input into a vector embedding\n",
    "        output, hidden = self.rnn(embedding.view(-1, 1, self.embedding_len), hidden) #calculate the output from our rnn based on our input and previous hidden state\n",
    "        output = self.decoder(output.view(-1, self.hidden_size)) #calculate output based on output of rnn\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(self.n_layers, x.shape[0], self.hidden_size) #initialize hidden state to a matrix of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "embedding_len = 400\n",
    "hidden_size = 128\n",
    "\n",
    "myRNN = CharRNN(nchars, embedding_len, hidden_size, nchars) #instantiate our model from the class defined earlier\n",
    "criterion = torch.nn.CrossEntropyLoss() #define cost function - Cross Entropy\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(myRNN, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generate(myRNN, 'starting text', 2000, 0.75)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we overcome the shortcomings of RNNs (unparallelisable training and vanishing gradient)?\n",
    "\n",
    "![image](images/RNN_LSTM.JPG)\n",
    "\n",
    "![image](images/RNN_LSTM_gradient.JPG)\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Only two things change from the above example to use an LSTM instead. Firstly, use torch.nn.LSTM instead of torch.nn.RNN when defining our model. Secondly, we change the init_hidden function so it returns an extra matrix of 0s as the LSTM not only has a hidden state but also a cell state which needs to be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_len, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        #store input parameters in the object so we can use them later on\n",
    "        self.input_size = input_size\n",
    "        self.embedding_len = embedding_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #required functions for model\n",
    "        self.encoder = torch.nn.Embedding(input_size, embedding_len)\n",
    "        self.rnn = torch.nn.LSTM(embedding_len, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.encoder(x.view(-1)) #encode our input into a vector embedding\n",
    "        output, hidden = self.rnn(embedding.view(-1, 1, self.embedding_len), hidden) #calculate the output from our rnn based on our input and previous hidden state\n",
    "        output = self.decoder(output.view(-1, self.hidden_size)) #calculate our output based on output of rnn\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        return (torch.zeros(self.n_layers, x.shape[0], self.hidden_size),\n",
    "                torch.zeros(self.n_layers, x.shape[0], self.hidden_size)) #initialize our hidden and cell state to a matrix of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-params\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "embedding_len = 400\n",
    "hidden_size = 128\n",
    "\n",
    "myLSTM = CharLSTM(nchars, embedding_len, hidden_size, nchars) #instantiate our model from the class defined earlier\n",
    "criterion = torch.nn.CrossEntropyLoss() #define cost function - Cross Entropy\n",
    "optimizer = torch.optim.Adam(myrnn.parameters(), lr=lr) #choose optimizer\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(myLSTM, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generate(myLSTM, 'starting text', 2000, 0.75)\n",
    "print(gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
