{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Datasets\n",
    "\n",
    "__Prerequisites__\n",
    "\n",
    "- [Convolutional Neural Networks](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/Convolutional%20Neural%20Networks.ipynb)\n",
    "\n",
    "So far, we have only used the MNIST dataset, which is easily accessible through torchvision. What do we do when we have our own data which we want to use with PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look in the `S40-data` directory in this folder. Within it you'll see a folder of images and a folder of annotations. This might be the sort of thing that you have collected and want to build a dataset from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What should our dataset be able to do?\n",
    "### ` __getitem__ `\n",
    "Our dataset should be a set of many examples. We should be able to index it like `my_dataset[3]` to get the example at position 3. The `__getitem__` function defines how the dataset is indexed, it is a function which should return an example datapoint given the example index as an argument. \n",
    "\n",
    "`mydataset[2]` is equivalent to `my_dataset.__getitem__(2)`\n",
    "\n",
    "### `__len__`\n",
    "The `__len__` function must return the length of the dataset we are loading in.\n",
    "\n",
    "`len(mydataset)` is equivalent to `my_dataset.__len__()`\n",
    "\n",
    "### It should also inherit from `torch.utils.data.Dataset`\n",
    "This just makes sure that we implement everything that we need to so that our dataset will be compatible with other utilities from torch such as the `DataLoader`.\n",
    "\n",
    "Today, we will implement a class which loads in the Stanford 40 detection dataset when initialised. \n",
    "\n",
    "Detection is when we want our algorithm to draw rectangles around the locations of specific objects within the image. As opposed to classification where we simply have a binary output indicating if the object is contained within the image.\n",
    "\n",
    "The dataset consists of an \"images\" folder which contains the input images and an \"annotations\" folder which, for each image, contains an xml file with the same name as the image and contains the co-ordinates for the top-left and bottom-right corners of the rectangular bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class S40dataset(Dataset): # create dataset class\n",
    "\n",
    "    def __init__(self, img_dir='S40-data/images', annotation_dir='S40-data/annotations', transform=None):\n",
    "        self.img_dir = img_dir # what directory are the images in\n",
    "        self.annotation_dir = annotation_dir # what directory are the annotations in\n",
    "        self.transform = transform # what transforms were passed to the initialiser\n",
    "\n",
    "        self.img_names = os.listdir(img_dir) # list all files in the img folder\n",
    "        self.img_names.sort() # order the images alphabetically\n",
    "        self.img_names = [os.path.join(img_dir, img_name) for img_name in self.img_names] # join folder and file names\n",
    "\n",
    "        self.annotation_names = os.listdir(annotation_dir) # list all annotation files\n",
    "        self.annotation_names.sort() # order annotation files alphabetically\n",
    "        self.annotation_names = [os.path.join(annotation_dir, ann_name) for ann_name in self.annotation_names] # join folder and file names\n",
    "\n",
    "#         print(self.img_names)\n",
    "#         print(self.annotation_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx] # get the path of the image at that index\n",
    "        img = Image.open(img_name) # open the image using the path\n",
    "\n",
    "        annotation_name = self.annotation_names[idx] # get the path to the label file\n",
    "        annotation_tree = ET.parse(annotation_name) # use xml parser to load the file\n",
    "        bndbox_xml = annotation_tree.find(\"object\").find(\"bndbox\") # get the tag which contains our labels\n",
    "        \n",
    "        # get the x and y values for the corners of the rectangle\n",
    "        xmax = int(bndbox_xml.find('xmax').text) \n",
    "        ymax = int(bndbox_xml.find('ymax').text)\n",
    "        xmin = int(bndbox_xml.find('xmin').text)\n",
    "        ymin = int(bndbox_xml.find('ymin').text)\n",
    "        #print(xmax, ymax, xmin, ymin)\n",
    "\n",
    "        # Convert from corner co-ordinates format into center co-ordinate, width and height format\n",
    "        w = xmax - xmin #\n",
    "        h = ymax - ymin\n",
    "        x = int(xmin + w / 2)\n",
    "        y = int(ymin + h / 2)\n",
    "\n",
    "        # Normlise the labels so the values are expressed as a proportion of the whole image\n",
    "        x /= img.size[0]\n",
    "        w /= img.size[0]\n",
    "        y /= img.size[1]\n",
    "        h /= img.size[1]\n",
    "\n",
    "        bndbox = (x, y, w, h) # create tuple of bounding box dimensions\n",
    "        \n",
    "        if self.transform: # if any transforms were given to initialiser\n",
    "            img = self.transform(img) # apply any transforms\n",
    "\n",
    "        bndbox = torch.tensor(bndbox) # convert bounding box tuple to tensor\n",
    "\n",
    "        return img, bndbox\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "# Convert from  center co-ordinate, width and height format into corner co-ordinates format\n",
    "def unpack_bndbox(bndbox, img):\n",
    "#     bndbox = list(bndbox[0])\n",
    "    x, y, w, h = tuple(bndbox)\n",
    "    x *= img.size[0] \n",
    "    w *= img.size[0]\n",
    "    y *= img.size[1]\n",
    "    h *= img.size[1]\n",
    "    xmin = x - w / 2\n",
    "    xmax = x + w / 2\n",
    "    ymin = y - h / 2\n",
    "    ymax = y + h / 2\n",
    "    bndbox = [xmin, ymin, xmax, ymax]\n",
    "    return bndbox\n",
    "\n",
    "def show(batch, pred_bndbox=None):\n",
    "    img, bndbox = batch\n",
    "\n",
    "#     img = img[0]\n",
    "    print(img.shape)\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    img = transforms.Resize((512, 512))(img)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    bndbox = unpack_bndbox(bndbox, img)\n",
    "    print(bndbox)\n",
    "    draw.rectangle(bndbox)\n",
    "    if pred_bndbox is not None:\n",
    "        pred_bndbox = unpack_bndbox(pred_bndbox, img)\n",
    "        draw.rectangle(pred_bndbox, outline=1000)\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'S40dataset' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0ebaf53767c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmyS40\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS40dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initialise our dataset and transform each example with a ToTensor transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'len dataset:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyS40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use our __len__ method to show the length of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyS40\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# use our __getitem__ method to get the first example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'S40dataset' has no len()"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "myS40 = S40dataset(transform=transforms.ToTensor()) # initialise our dataset and transform each example with a ToTensor transform\n",
    "print('len dataset:', len(myS40)) # use our __len__ method to show the length of the dataset\n",
    "example = myS40[0] # use our __getitem__ method to get the first example\n",
    "show(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way that we might then use this dataset would be to create a torch `DataLoader` from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "my_dataloader = # use dataset to create dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook complete\n",
    "\n",
    "You should now understand how to create your own dataset classes by inheriting from torch's `Dataset` class and overwriting the `__getitem__` and `__len__` methods.\n",
    "\n",
    "__Next Steps__\n",
    "\n",
    "- [CNN Detection](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/CNN%20Detection.ipynb) - use this dataset to train a CNN to detect single instances in images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
