{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('main': conda)",
   "display_name": "Python 3.8.5 64-bit ('main': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Segmentation\n",
    "\n",
    "## What is segmentation?\n",
    "\n",
    "In the case of images, as we will consider in this notebook, segmentation is the task of classifying every pixel as a member of a certain class.\n",
    "\n",
    "![](images/segmentation.jpeg)\n",
    "\n",
    "What you see above is called a segmentation map.\n",
    "\n",
    "Segmentation can also be performed on other types of data such as video or text.\n",
    "\n",
    "## Please note\n",
    "Segmentation is not a solved problem. And we don't expect to achieve state of the art results in this notebook.\n",
    "Instead in this notebook we'll just set up a CNN that trains, even if just a little, to show you how to hook things up from input to output for learning and inference.\n",
    "\n",
    "## Different types of segmentation\n",
    "\n",
    "**Instance segmentation** is the task of distinguishing different instances of the same class type, as well as between other classes.\n",
    "\n",
    "**Semantic segmentation** is the task of distinguishing between only different classes.\n",
    "\n",
    "![](images/semantic_vs_instance.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## What does a segmentation dataset look like?\n",
    "\n",
    "Let's read in a segmentation dataset called the PASCAL VOC segmentation dataset and look an example.\n",
    "\n",
    "You can download the dataset from [here](https://github.com/life-efficient/VOC)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=375x500 at 0x7F80E95A3340>\n(375, 500)\n\n<class 'torch.Tensor'>\ntensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]])\ntorch.Size([1, 500, 375])\n"
     ]
    }
   ],
   "source": [
    "from VOCDataset import VOC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pascal = VOC(root='/Users/ice/ai-core/VOC2007') # set the root to where the VOC dataset is stored on your machine\n",
    "\n",
    "img, maps = pascal[3]\n",
    "img.show()\n",
    "print(type(img))\n",
    "print(img)\n",
    "print(img.size)\n",
    "print()\n",
    "print(type(maps))\n",
    "print(maps)\n",
    "print(maps.shape)"
   ]
  },
  {
   "source": [
    "We can see that the input is the usual image that we're used to. \n",
    "But what about the label?\n",
    "\n",
    "It's a tensor, the same size as the input image, which contains integer values. \n",
    "Those integer values are the index of the class which the pixel in the corresponding location belongs.\n",
    "\n",
    "\n",
    "## What should our output prediction look like?\n",
    "\n",
    "This is a multiclass classification problem right?\n",
    "So we know that we are going to use the cross entropy loss function.\n",
    "That loss function takes in the prediction and the label. \n",
    "As usual, the label is of size $batch\\_size$, with an integer label in each position.\n",
    "\n",
    "However, our classification models never output the predicted class explicitly. \n",
    "Instead they output a probability distribution over all classes (a vector of probabilities for each example). \n",
    "But in the case of segmentation, we are making this vector predition at every pixel location!\n",
    "So our prediction should be $batch\\_size \\times height \\times width \\times num\\_classes$ ($B \\times H \\times W K$)\n",
    "This should give you an idea of how much more computationally expensive this task is compared to regular classification.\n",
    "\n",
    "![](images/segmentation-in-out.jpg)\n",
    "\n",
    "We know that convolutions reduce the size of the data they process because, along each spatial axis, for each region the filter covers, it produces a single number, and we hit the side/bottom of the image before we can operate on as many positions as we would need to to produce an output of the same size as the input.\n",
    "\n",
    "How can we produce an output of the same size as the input whilst using convolutional layers? This is the question of: what should the architecture of our hidden layers be?\n",
    "\n",
    "There are many ways to do this. One thing that we learnt previously was that we could add a specific amount of padding to an input to a conv layer and the output would stay the same size."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We won't dive into how the specifics of state of the art segmentation models work. They are changing often and can get very complicated. But if you're interested, you should look into:\n",
    "- FCN\n",
    "- Mask R-CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}