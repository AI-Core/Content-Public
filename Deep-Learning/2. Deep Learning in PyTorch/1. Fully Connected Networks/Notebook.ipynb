{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Networks\n",
    "\n",
    "## About\n",
    "\n",
    "- Powerful function approximators\n",
    "- Universal approximators (assuming infinite width and/or depth)\n",
    "- Essentially stacked linear regressions with multiple outputs\n",
    "\n",
    "## How should we use them?\n",
    "\n",
    "Previously we used polynomial features for non-linear datasets, but this comes up with downsides:\n",
    "- what degree of polynomial should we use?\n",
    "- maybe other functions would be better (they usually are)?\n",
    "- if so, what those functions are?\n",
    "\n",
    "![](./images/complex-fn.png)\n",
    "\n",
    "We might come to the conclusion that:\n",
    "\n",
    "> it would be best to learn those functions directly from data\n",
    "\n",
    "... and that's what neural networks do.\n",
    "\n",
    "## Perceptron and it's limitations\n",
    "\n",
    "> Perceptron is binary logistic regression, neural network with one output neuron and one layer\n",
    "\n",
    "It is able to learn __linearly separable data__ but it will fail for non-linear data.\n",
    "\n",
    "> __Let's see how it does in a case of XOR problem:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and targets for XOR\n",
    "\n",
    "XOR = [\n",
    "    ([0, 0], 0),\n",
    "    ([0, 1], 1),\n",
    "    ([1, 0], 1),\n",
    "    ([1, 1], 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how minimal XOR looks like for a neural network:\n",
    "\n",
    "![](images/xor_neural_net.png)\n",
    "\n",
    "> __Code below trains a neural network for the XOR problem__\n",
    "\n",
    "> __WE WILL LATER CREATE PROPER TRAINING LOOP FROM SCRATCH, TREAT IT AS A BLACK-BOX FOR NOW__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted logits:\n",
      " tensor([[ 0.0057],\n",
      "        [ 0.0015],\n",
      "        [-0.0037],\n",
      "        [-0.0079]], grad_fn=<AddmmBackward>)\n",
      "Predicted labels:\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "Targets:\n",
      " tensor([0., 1., 1., 0.])\n",
      "Weights:\n",
      "\n",
      "tensor([[-0.0094, -0.0042]])\n",
      "tensor([0.0057])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def xor_problem(model):\n",
    "\n",
    "    inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).float()\n",
    "    targets = torch.tensor([0, 1, 1, 0]).float()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for _ in range(10000):\n",
    "        outputs = model(inputs).squeeze()\n",
    "\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\"Predicted logits:\\n\", model(inputs))\n",
    "    print(\"Predicted labels:\\n\", (model(inputs) > 0).float())\n",
    "    print(\"Targets:\\n\", targets)\n",
    "    print(\"Weights:\\n\")\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        print(parameter.data)\n",
    "\n",
    "\n",
    "xor_problem(torch.nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple model is unable to learn classification for XOR data, __as it didn't have enough depth__.\n",
    "\n",
    "Let's see how shallow and deep neural networks compare:\n",
    "\n",
    "![](./images/shallow-vs-deep.png)\n",
    "\n",
    "Let's try adding another layer with size equal to `2` in order to replicate the structure seen at the beginning, but before that...:\n",
    "\n",
    "### nn.Sequential\n",
    "\n",
    "> __`torch.nn.Sequential` is a container like layer which STACKS multiple layers together__\n",
    "\n",
    "What essentially happens:\n",
    "- We have our input data\n",
    "- We pass it through first layer (for now linear)\n",
    "- Output from first layer is passed to the next one\n",
    "- The whole thing continues until we reach the end\n",
    "\n",
    "Below is a stack of two linear layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_problem(torch.nn.Sequential(torch.nn.Linear(2, 2), torch.nn.Linear(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single linear transformation (multiplication by weights of model):\n",
    "- stretches the input space by a certain factor in some direction\n",
    "- adding a constant (bias) shifts it\n",
    "\n",
    "> If we add more linear layers the whole transformation is still linear!\n",
    "\n",
    "![](./images/factor-proof.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "If we plot XOR problem we can see that __we cannot create a linear decision boundary which separates the data!__\n",
    "\n",
    "![](images/xor_decision_boundary.png)\n",
    "\n",
    "To combat this phenomena, we need to apply __non-linear transformation__ after __hidden layer__:\n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "> Composition of non-linear functions makes the whole transformation non-linear\n",
    "\n",
    "> __ACTIVATION FUNCTION ACTS ELEMENT-WISE ON OUR DATA!__\n",
    "\n",
    "Okay, let's see how we do after applying `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted logits:\n",
      " tensor([[ 0.0147],\n",
      "        [ 0.0407],\n",
      "        [-0.0475],\n",
      "        [-0.0253]], grad_fn=<AddmmBackward>)\n",
      "Predicted labels:\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "Targets:\n",
      " tensor([0., 1., 1., 0.])\n",
      "Weights:\n",
      "\n",
      "tensor([[-0.4231,  0.2809],\n",
      "        [ 0.5266, -0.2147]])\n",
      "tensor([-0.0147,  0.3777])\n",
      "tensor([[-0.0692, -0.5851]])\n",
      "tensor([0.3962])\n"
     ]
    }
   ],
   "source": [
    "xor_problem(\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(2, 2), torch.nn.Sigmoid(), torch.nn.Linear(2, 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation variations\n",
    "\n",
    "There are multiple available activation functions, including (but not limited to):\n",
    "- sigmoid\n",
    "- tanh\n",
    "- ReLU\n",
    "- Leaky ReLU\n",
    "\n",
    "![](./images/activ-fns.png)\n",
    "\n",
    "> Activation functions were introduced in order to combat issues of previous dominant activation functions\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "- Initial neural network activation function\n",
    "- Squashes input to `[0, 1]` range (neuron on or off)\n",
    "\n",
    "But this activation function has the following drawbacks:\n",
    "- Non zero centered\n",
    "- __Oversaturation__ (most severe drawback)\n",
    "\n",
    "### Non-zero centered\n",
    "\n",
    "> Neural networks expect data to be zero-centered\n",
    "\n",
    "If the data coming into neural network is always positive (as is the case with sigmoid) gradient will become either all positive for every neuron or negative.\n",
    "\n",
    "This leads to zig-zagging during training, especially for smaller batches\n",
    "\n",
    "> Larger batches mostly mitigate this issue, as the gradient will be averaged across many examples (some positive, some negative for different weights)\n",
    "\n",
    "## Tanh \n",
    "\n",
    "Hyperbolical tangens solves this issue, but:\n",
    "\n",
    "> Tanh also has oversaturation drawbacks\n",
    "\n",
    "## Oversaturation\n",
    "\n",
    "> When neuron activation saturates at the tails (large positive/negative values) local gradient becomes zero (close to zero)\n",
    "\n",
    "> This local gradient is multiplied by the previous layers local gradients and dies, phenomena known as __dying gradient__ (especially for deeper networks)\n",
    "\n",
    "## ReLU\n",
    "\n",
    "`ReLU` was designed to combat oversaturation and dying gradient problem.\n",
    "\n",
    "> `ReLU` is given by `max(0, x)` equation\n",
    "\n",
    "> In the linear part of activation, gradient will always be `1` or `0` (for negative and zero values)\n",
    "\n",
    "> Combination of piece-wise linear function can approximate any non-linearity (especially with increasing depth)\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Reportedly much faster training times (initially `6x` improvements)\n",
    "- Faster implementation (thresholding values on zero)\n",
    "- No oversaturation\n",
    "\n",
    "### Dead Neurons\n",
    "\n",
    "> __When a neuron outputs `0` FOR ALL EXAMPLES in our dataset__\n",
    "\n",
    "- __Invariant to the input__ (no matter the input features it returns `0`).\n",
    "- Due to that __it does not differentiate any part of our dataset__, hence useless\n",
    "- Too high learning rate may be a cause\n",
    "- Large updates to neural networks may \"knock off\" a neuron into negative regime from which it won't recover\n",
    "- __Even 50% of neurons might be dead in some architectures!__\n",
    "- __Might be useful for prunning (removing unnecessary parts of the neural network)__\n",
    "\n",
    "> __In practice it seems not to be too much of a problem (if the network is large it compensates with other neurons)__\n",
    "\n",
    "## Leaky ReLU\n",
    "\n",
    "> Leaky ReLU solves dead neurons problem using __a small negative slope__ for negative values\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}_s(x) = max(0, x) + s \\times min(0, x)\n",
    "$$\n",
    "\n",
    "> `s` is usually around `0.01`\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- Higher computational cost\n",
    "- Solves a problem which is not a problem in many cases\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Neurons can recover from \"dead\" state and be useful for neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network as a whole\n",
    "\n",
    "Let's take a moment to see the neural network as a whole:\n",
    "\n",
    "![](./images/nn.png)\n",
    "\n",
    "- __Depth:__ - how many layers are in a neural network\n",
    "- __Width:__ - `out_features` in PyTorch, how many neurons are in a certain layer\n",
    "\n",
    "> In general, we create a bottleneck with `nn.Linear` layers, starting with `N` features and finishing with `M` outputs\n",
    "\n",
    "> __This is a rule of thumb, do not treat is a concrete science!__\n",
    "\n",
    "## Quick look at backpropagation\n",
    "\n",
    "When `backward` is called, gradient is calculated on a per-layer basis and passed to the previous ones.\n",
    "\n",
    "![](./images/backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "## First part\n",
    "\n",
    "### Provided code\n",
    "\n",
    "First, let's talk about `print_gradient` function:\n",
    "- Calculates loss of `inputs` w.r.t. `targets`\n",
    "- __Runs backpropagation POPULATING GRADIENTS of parameters (weights and biases of linear model)__\n",
    "- __Prints gradients of all parameters contained in the model__\n",
    "\n",
    "> __Gradients of parameters are printed in the reversed order (LAST LAYER'S GRADIENTS ARE PRINTED FIRST!)__\n",
    "\n",
    "About the data:\n",
    "- `inputs` - random matrix of shape `(examples, features)`\n",
    "- `targets` - random __integer__ matrix which specifies `5` classes (from `0` to `5` __exclusive__) \n",
    "\n",
    "### Your task\n",
    "\n",
    "> __Analyze OVERSATURATION based on provided `inputs` and `targets`__\n",
    "\n",
    "After your analysis you should answer the following questions:\n",
    "- How does neural network width affects oversaturation?\n",
    "- How different choices of activation functions in the hidden layers affect the oversaturation?\n",
    "- How does depth influence the oversaturation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradient(model, inputs, targets):\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    for i, parameter in reversed(enumerate(model.parameters())):\n",
    "        print(f\"\\n\\n--------- PARAMETER {i} GRADIENT ---------\\n\\n\")\n",
    "        print(parameter.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(32, 10)\n",
    "targets = torch.randint(low=0, high=5, size=(32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part\n",
    "\n",
    "> __In this part we will try to see `dead` neurons in our data__\n",
    "\n",
    "### Provided code\n",
    "\n",
    "> __`fit` function trains our model based on provided `inputs`, `targets` and for specified number of steps (`epochs`)__\n",
    "\n",
    "> `lr` specifies how fast we will update our neural network (more about that in the next section)\n",
    "\n",
    "General procedure is the following:\n",
    "- `fit` your model of choice and specify `lr`\n",
    "- Run `dead.check(model, input)` to see how many of the neurons are dead for this data\n",
    "\n",
    "> __Do you know how `DeadNeurons` object works?__\n",
    "\n",
    "### Your task\n",
    "\n",
    "After your analysis you should answer the following questions:\n",
    "- How does `lr` affect dead neurons?\n",
    "- How size of the neural network affects number of dead neurons (depth, width)\n",
    "- Does it make a difference whether the network is deeper or shallower in terms of dead neurons?\n",
    "- How does data size affect percentage of dead neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, inputs, targets, epochs, lr):\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for _ in range(epochs):\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "class DeadNeurons:\n",
    "    def __init__(self):\n",
    "        self._counter = 0\n",
    "\n",
    "    def __call__(self, module, inputs, outputs):\n",
    "        neuron_activations = torch.sum(outputs, dim=0)\n",
    "        total_neurons = neuron_activations.numel()\n",
    "        zeros = total_neurons - torch.count_nonzero(neuron_activations)\n",
    "        percentage = (zeros / total_neurons) * 100\n",
    "        print(f\"Layer: {self._counter} | Name: {module} | Dead: {percentage} %\")\n",
    "        self._counter += 1\n",
    "\n",
    "    def check(self, module, *args, **kwargs):\n",
    "        hooks = [\n",
    "            submodule.register_forward_hook(self)\n",
    "            for submodule in module.children()\n",
    "            if not isinstance(submodule, torch.nn.Linear)\n",
    "        ]\n",
    "\n",
    "        module(*args, **kwargs)\n",
    "\n",
    "        for hook in hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = DeadNeurons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Linear layers work like multiclass logistic regression with `in_features` and `out_features`.\n",
    "- Perceptron has __no hidden layers__.\n",
    "- __Multilayer Perceptron (MLP)__ is standard neural network with multiple layers.\n",
    "- We need to use multiple layers interspreded with activations in order to achieve non-linear behaviour\n",
    "- Main activation function is currently `ReLU` or `LeakyReLU`\n",
    "- Main problem with `sigmoid` and `tanh` is saturation and dying gradient (though there are used in some neural network blocks like recurrent)\n",
    "- `ReLU` may suffer from dying neurons phenomena which may impact neural network\n",
    "- Though it is not the most probable cause of poor network performance\n",
    "- Remember to use wide layers (say `50`, `100` neurons), depending on task (if the model does not learn, it might need more parameters)\n",
    "- Sufficiently wide and/or deep neural networks can approximate any function.\n",
    "\n",
    "# Challenges\n",
    "\n",
    "## Assessment\n",
    "\n",
    "- How does [Parametric ReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html) (a.k.a. `PReLU`) works?\n",
    "- What are the pros and cons of using this activation?\n",
    "- What is neural network prunning? See [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html) about it\n",
    "\n",
    "## Non-assessment\n",
    "- Play around with [Tensorflow Neural Network playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.97988&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "- How Maxout activation function works? What are the upsides and downsides of using it? \n",
    "- How SeLU activation function works? What are the upsides and downsides of using it?\n",
    "- Can you somehow show the impact of non-zero centered activation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
