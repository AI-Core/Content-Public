{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "\n",
    "> Deep Learning provides __a lot__ of optimization techniques and __optimization is one of the most (if not the most) important aspects__\n",
    "\n",
    "![](images/optim_vis.gif)\n",
    "\n",
    "> __You can find optimisation related procedures for PyTorch in [`torch.optim` package](https://pytorch.org/docs/stable/optim.html)__\n",
    "\n",
    "## SGD\n",
    "\n",
    "Previously we have seen __Stochastic Gradient Descent__, basic optimization technique for gradient based models, which has the following update formula:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\frac{1}{M}\\sum_{i}^{M} \\nabla L(h(x^i;\\theta_t), y^i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha \\rightarrow \\text{learning rate}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L \\rightarrow \\text{cost function}\n",
    "$$\n",
    "\n",
    "$$\n",
    "M \\rightarrow \\text{batch size}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t \\rightarrow \\text{model parameters at timestep t}\n",
    "$$\n",
    "\n",
    "\n",
    "### Know-how\n",
    "\n",
    "- Basic, but __often used in SOTA (state of the art) neural network models__ (or it's two upcoming variants)\n",
    "- Harder (and we mean it) to fine tune, but often brings great results (especially when mixed with `scheduler`, more about that later)\n",
    "- Used for testing, whether our neural network actually works\n",
    "- __Might not be the best default (see Adam)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "clf = torch.nn.Linear(100, 10)\n",
    "\n",
    "sgd = torch.optim.SGD(clf.parameters(), lr=1e-03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum (SGD + Momentum)\n",
    "\n",
    "> Momentum is a small modification to SGD __taking into account moving average of previous parameter updates__\n",
    "\n",
    "There are two different formulas for SGD update, we will go with the one provided by PyTorch and based on [this paper](http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf):\n",
    "\n",
    "$$\n",
    "v_{t+1} = \\mu v_t + \\alpha \\frac{1}{M}\\sum_{i}^{M} \\nabla L(h(x^i;\\theta_t), y^i)\n",
    "$$\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \n",
    "$$\n",
    "$$\n",
    "v_0 = 0\n",
    "$$\n",
    "\n",
    "Here, we define `v` (velocity) which takes into account previous update multiplied by $\\mu$.\n",
    "\n",
    "> __WARNING:__ This alternative formula maybe different for other frameworks, so it can be one source of different results (e.g. between this and Tensorflow)!\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- If previous step was in the same direction, current optimization step will be even larger in this direction (snowball effect)\n",
    "- If previous step was in the opposite direction, next step will be smaller (more cautious)\n",
    "- Learning rates change more, additional oscillations and regularization due to that factor\n",
    "\n",
    "### Why?\n",
    "\n",
    "- Make the convergence faster if the loss surface is smooth and does not curve back\n",
    "- If the gradient changes rapidly, steps will become smaller and more cautious\n",
    "\n",
    "### Why not?\n",
    "\n",
    "- We may \"overshoot\" the minima\n",
    "- We may take too small steps\n",
    "- __Above because feedback from previous steps might be simply wrong__\n",
    "\n",
    "### Know-how\n",
    "\n",
    "- Set `momentum` parameter as high as possible and close to `1` (usually `0.99` or `0.999`)\n",
    "- Set `lr` as high as possible without divergence (might be hard and increase oscillations too much)\n",
    "- Rule of thumb: twice as high `lr` as with SGD (see [here](https://distill.pub/2017/momentum/)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = torch.optim.SGD(clf.parameters(), lr=1e-03, momentum=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient (NAG) - SGD with momentum and Nesterov update\n",
    "\n",
    "> Momentum with Nesterov update evaluates gradient calculated __after momentum was applied to parameters__\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = x + \\mu v\n",
    "$$\n",
    "\n",
    "$$\n",
    "v = \\mu * v - \\alpha \\nabla L(h(x^i;\\hat{\\theta}_t), y^i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta = \\theta + v\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} \\rightarrow \\text{Look ahead parameters of Nesterov}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta \\rightarrow \\text{Current parameters}\n",
    "$$\n",
    "\n",
    "![](images/momentum_vs_nesterov.jpeg)\n",
    "\n",
    "__Source:__ [Stanford CS231n](https://cs231n.github.io/neural-networks-3/)\n",
    "\n",
    "### Why?\n",
    "\n",
    "- We know momentum will take us to to different point, no matter what\n",
    "- Our estimates of gradient __did not take this shift into account previously!__\n",
    "\n",
    "### Results\n",
    "\n",
    "- Gives us better theoretical estimates of gradient\n",
    "- It might be different in practice, but it's worth a shot\n",
    "- Similar idea to pure `Momentum`\n",
    "\n",
    "### Practical concerns\n",
    "\n",
    "- We always keep \"ahead\" gradient instead of normal gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = torch.optim.SGD(clf.parameters(), lr=1e-03, momentum=0.99, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Adam optimizers\n",
    "\n",
    "> Many other optimizer versions have been proposed throughout the years but most didn't see widespread adoption\n",
    "\n",
    "Most popular predecessors of `Adam` (described below) were:\n",
    "- __AdaGrad__ - one of the first optimizers with adaptive learning rate on a per-parameter basis:\n",
    "    - __might be useful for sparse data__\n",
    "    - __keeps accumulated squared gradients__\n",
    "- __AdaDelta__ - extension of AdaGrad, tries to reduce dying learning rates as the optimization progresses:\n",
    "    - __moving window of accumulated past squared gradients__\n",
    "- __RMSProp__ - similar goal to AdaDelta, unpublished, developed independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "> ADAptive Moments makes update on a per-parameter basis and tries to normalize them with moving mean and variance\n",
    "\n",
    "__Adam (or AdamW) should be your default choice for optimization__\n",
    "\n",
    "$$\n",
    "g = \\nabla L(h(x^i;\\theta_t), y^i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g^2_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "$$\n",
    "m_t \\rightarrow \\text{moving average of first moment (mean) at timestep t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t \\rightarrow \\text{moving average of second moment (variance) at timestep t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{m}_t \\rightarrow \\text{moving average of mean with bias correction (power of timestep!)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{v}_t \\rightarrow \\text{moving average of variance with bias correction (power of timestep!)}\n",
    "$$\n",
    "\n",
    "### AdamW\n",
    "\n",
    "> AdamW is a slight change to Adam which decouples regularization of weights (`weight_decay` in PyTorch)\n",
    "\n",
    "> Originally Adam with L2 regularization had regularization term inside mean and variance, hence __moving mean/variance was influenced by regularization__, which skewed the results\n",
    "\n",
    "### Know-how\n",
    "\n",
    "- Always use `AdamW` when using weight decay\n",
    "- Use a `warm up` period - lower learning rate at the beginning of training; due to random initialization, there is large variance in the initial estimates of statistics which disrupts the training later on\n",
    "- Keep default `beta1` and `beta2` values, usually those should not be altered\n",
    "- Algorithm is quite resistant to different learning rates - easier optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW with L2\n",
    "adam = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Adam optimizers\n",
    "\n",
    "> There were a lot of other optimizers after Adam was introduced, but __not many received widespread adoption__ as the tools we have usually work fine\n",
    "\n",
    "> It might be worth checking out (and mixing) approaches below when you are pushing for the best score (e.g. on Kaggle)\n",
    "\n",
    "Few interesting cases:\n",
    "- `Nadam`, `AdaMax` etc. - small changes to adam (the first one applies Nesterov to `Adam`)\n",
    "- `RAdam` - research paper: https://arxiv.org/abs/1908.03265 - removes the need for warm up period in adaptive optimizers\n",
    "- LookAhead optimizer - research paper: https://arxiv.org/abs/1907.08610: \n",
    "    - Keeps two sets of weights\n",
    "    - One is \"faster\" - few steps taken by optimizer with larger learning rate\n",
    "    - Another one is \"slower\" - not updated during those steps\n",
    "    - After `k` steps (usually `5`) we take the mean direction of fast and slow weights\n",
    "    - __Demonstrated to improve over Adam and SGD__\n",
    "    \n",
    "> __Resist the urge to combine every possible extension!__ Analyze what might help during your training and choose wisely, otherwise you might end up with worse score than initially!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedulers\n",
    "\n",
    "> Schedulers allow us to change hyperparameters of optimizers\n",
    "\n",
    "__Almost always we change learning rate__ and we usually do that based on:\n",
    "- Epochs we ran optimization for\n",
    "- Some metric (usually on validation set)\n",
    "- Using predefined algorithm, which, supposedly, should improve convergence somehow\n",
    "\n",
    "> You can find PyTorch provided schedulers inside [`torch.optim.lr_scheduler` package](/https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "Let's see widespread schedulers for each type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step/MultiStep schedulers\n",
    "\n",
    "> After `N` epochs passed, multiply learning rate by some value\n",
    "\n",
    "PyTorch provides a few which allow us to do it:\n",
    "- [`torch.optim.lr_scheduler.MultiplicativeLR`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.MultiplicativeLR) - allows us to change learning rate based on integer number of current epoch\n",
    "- [`torch.optim.lr_scheduler.StepLR`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR)- decays learning rate by `gamma` every `N` epochs\n",
    "- [`torch.optim.lr_scheduler.MultiStepLR`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.MultiStepLR) - like above, but we can specify after integer number for each epoch, where we want to multiply the `learning_rate` by `gamma` (__used very often in many research papers on ImageNet as standard optimization procedure__)\n",
    "\n",
    "### Know-how\n",
    "\n",
    "- Usually we multiply `lr` by `0.1` (or, less often by `5` or `2`)\n",
    "- Most often used with non-adaptive optimizers, __but can also work with more advanced optimizers like Adam__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing LR on Plateau\n",
    "\n",
    "> If our metric stops improving for a few epochs (usually validation loss), we reduce `lr`\n",
    "\n",
    "- [`torch.optim.lr_scheduler.ReduceLROnPlateau`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau) - does exactly this\n",
    "\n",
    "### Know-how\n",
    "\n",
    "- Usually we multiply `lr` by `0.1` (or, less often by `5` or `2`) - same as `step` schedulers\n",
    "- Most often used with non-adaptive optimizers, __but can also work with more advanced optimizers like Adam__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "for epoch in range(10):\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    # Note that step should be called after validate()\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic schedulers\n",
    "\n",
    "> Active area of research, previous approaches are more battle tested!\n",
    "\n",
    "- [`torch.optim.lr_scheduler.CyclicLR`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CyclicLR) - propagated by `fastai`, which does the following:\n",
    "    - Initially increase learning rate (warmup phase, useful with `Adam`/`AdamW`)\n",
    "    - Get to large learning rate which distrupts parameters (jumping out of local minima)\n",
    "    - Get back to low learning rate to fine-tune new optima\n",
    "    - If the minima is large and flat, we will not jump out of it, no matter the larger learning rate\n",
    "    - See research paper by Leslie et al.: https://arxiv.org/abs/1506.01186\n",
    "    \n",
    "![](images/cyclic_lr.png)\n",
    "\n",
    "- [__`torch.optim.lr_scheduler.OneCycleLR`__](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.OneCycleLR) - as above, __use with `AdamW` or experimental optimizers__, but a increase/decrease schedule which was found to work well and allow for SuperConvergence: https://arxiv.org/abs/1708.07120\n",
    "\n",
    "> Super Convergence - phenomena, where using abruptly high learning rates (e.g. `2` or `5`) will result in fast convergence to flat minima\n",
    "\n",
    "__Above is often used with `OneCycle` learning policy!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General optimization procedure\n",
    "\n",
    "> __Below is a basic outline of the steps one might try in order to find a good optimization procedure__\n",
    "\n",
    "\n",
    "1. Find related research paper (and specified optimization procedure) if you are using well-known architecture (e.g. MobileNet we will learn later about)\n",
    "2. Iterate over that with grid __small__ search if results are unsatisfactory \n",
    "3. If architecture is custom try `lr_finder` (with appropriate optimizer, usually `Adam` or `RAdam`, see below) until divergence (and set your LR `10x` lower). Example repository implementing it [here](https://github.com/davidtvs/pytorch-lr-finder)\n",
    "4. Start with `Adam` as a good default (preferably with warm up period or use `RAdam`, example implementation [here](https://github.com/LiyuanLucasLiu/RAdam))\n",
    "5. Experiment based on loss and apply appropriate schedulers (__usually lowering learning rate is beneficial, BUT more sophisticated approaches could be checked__)\n",
    "6. If you have too much time (or really want this SOTA score) try to optimize with `SGD` & `momentum` over the optimization procedure you've found above (__for those with too much free time__)\n",
    "7. Try AutoML approaches if you have better hardware and iterate over that (see for example [this project](https://auto.gluon.ai/stable/index.html) (__last resort__)\n",
    "8. Hyperparam search if you have enough hardware to handle that and are totally lost after above steps (__last resort__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- PyTorch provides __most widely used__ optimizers within the package\n",
    "- For additional optimizers we should look to another libraries, __though it usually is not worth it__\n",
    "- __Optimizers__ come around improving weak (or perceived as weak) points of previous optimization techniques:\n",
    "    - __SGD__ - is the backbone and basic technique\n",
    "    - __Momentum__ - takes into account previous steps & makes the step smaller/larger appropriately\n",
    "    - __Nesterov__ - like momentum, but gradient is evaluated at step after momentum was applied\n",
    "    - __Adam (ADAptive Moments)__ - adaptive optimizer, each parameter has different learning rate based on mean and variance of gradients from previous steps\n",
    "- __Schedulers__ alter learning rate, usually based on:\n",
    "    - Training time (epochs) - `Step` schedulers, multiplying initial learning rate by some below `1` constant like `0.1`\n",
    "    - Training feedback - Reduce learning rate when validation loss stops improving\n",
    "    - Algorithm - predefined schedule increasing and/or decreasing learning rate. Can cycle between large and small learning rates in order to escape local minimas and find flat regions   \n",
    "- Basic PyTorch training loop consists of:\n",
    "    - Setting up criterion\n",
    "    - Casting model and data to device\n",
    "    - Backpropagating loss\n",
    "    - Taking optimizer step\n",
    "    - Zeroing out gradient in model using `optimizer.zero_grad()`\n",
    "\n",
    "# Challenges\n",
    "\n",
    "## Assessment\n",
    "\n",
    "- What is Learning Rate Finder and how does it fit with 1 Cycle policy? Check out [this blog post](https://sgugger.github.io/the-1cycle-policy.html) from `fastai` contributor\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- Check out [Stochastic Weight Averaging](https://pytorch.org/docs/stable/optim.html#stochastic-weight-averaging) - why might this work and how it works?\n",
    "- What is [Stochastic Gradient Descent with Warm Restarts](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts)?\n",
    "- What is [`torch.optim.lr_scheduler.LambdaLR`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.LambdaLR)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
