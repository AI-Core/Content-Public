{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and DataLoaders\n",
    "\n",
    "PyTorch provides a way to create your own data via [`torch.utils.data`](https://pytorch.org/docs/stable/data.html) module.\n",
    "\n",
    "It allows us to:\n",
    "- Create custom datasets (finite or infinite)\n",
    "- Quickly load data in batches using multiple processes\n",
    "- Pin data to GPU memory for faster transfers\n",
    "\n",
    "## Map-style datasets\n",
    "\n",
    "Those are the most common datasets and have a really simple structure:\n",
    "- Create a class by inheriting from [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)\n",
    "- Override `__init__` (__optionally, if needed__)\n",
    "- Define `__getitem__(self, index)` method (__responsible for obtaining SINGLE sample__)\n",
    "- Define `__len__(self)` method (__how many samples are there in total__)\n",
    "\n",
    "Let's see how this looks in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, l: typing.List[typing.Any]):\n",
    "        self.l = l\n",
    "\n",
    "    # Not dependent on index\n",
    "    def __getitem__(self, index):\n",
    "        return self.l[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.l)\n",
    "\n",
    "\n",
    "dataset = DummyDataset([0, 1, 2, 3, 4, 5])\n",
    "len(dataset), dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a dataset with `X` data and `y` labels by inheriting from `torch.utils.data.Dataset` (name it `Dataset`).\n",
    "- Code setup in `__init__(X, y)` (verify that `X` and `y` are of equal length (you can use [`assert`](https://stackoverflow.com/questions/5142418/what-is-the-use-of-assert-in-python) for that)\n",
    "- In `__getitem__` return `tuple` containing sample and it's respective label defined by `index`\n",
    "- In `__len__` return how many samples are contained in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        assert len(X) == len(y), \"Data and labels have to be of equal length!\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    # Not dependent on index\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders\n",
    "\n",
    "Please notice, our `torch.utils.data.Dataset` __returns single element__.\n",
    "\n",
    "> In deep learning, we use batches of data (as much as we can fit onto GPU usually)\n",
    "\n",
    "Because of that, PyTorch provides another abstraction: `torch.utils.data.DataLoader`:\n",
    "\n",
    "> DataLoader batches our data so it is easily consumed by neural networks\n",
    "\n",
    "### Automated batching\n",
    "\n",
    "Usually, we will go for automated batching which is really simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(torch.randn(300, 10), torch.randint(0, 5, size=(300,)))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64)\n",
    "\n",
    "for (X, y) in dataloader:\n",
    "    print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about DataLoader\n",
    "\n",
    "[`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) has a lot of arguments (all of them optional except providing `Dataset` instance), most important ones being:\n",
    "- `batch_size: int` - we've seen it above\n",
    "- `shuffle: bool` - whether dataset should be shuffled before each iteration\n",
    "- `num_workers: int` - how many workers are used for data loading\n",
    "- `pin_memory: bool` - whether tensors should be loaded into pinned memory regions which may improve tensor transfer to GPU (__more about that during Hardware lesson__)\n",
    "\n",
    "> `DataLoader` can only be iterated over, no random access is provided\n",
    "\n",
    "> Many of `DataLoader` arguments are mutually exclusive and there is more than one way to specify certain things. __Always go for the easiest and shortest option if possible__\n",
    "\n",
    "## num_workers\n",
    "\n",
    "> `num_workers` specifies how many processes in parallel will be used to load data\n",
    "\n",
    "### Not enough workers\n",
    "\n",
    "- Our __pipeline__ will wait for data while neural network sits idle\n",
    "- Underutilization\n",
    "- Longer training times\n",
    "\n",
    "### Too many workers\n",
    "\n",
    "- Likely to crash in extreme cases\n",
    "- May crash after a while (importance of model saving)\n",
    "- Might make the whole system laggy\n",
    "\n",
    "### How to find correct amount of workers?\n",
    "\n",
    "- Start with number of available cores divided by two (`multiprocessing.cpu_count()`), usually is a good default\n",
    "- Test a little larger, in case of crash continue division by two and repeat\n",
    "\n",
    "> Details about multiple workers are located [here](https://pytorch.org/docs/stable/data.html#multi-process-data-loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler\n",
    "\n",
    "> Sampler __generates indices__ which are used to to __get data from `Dataset` contained in `DataLoader`__\n",
    "\n",
    "This means, we can control __how__ our dataset is actually sampled, which allows us to:\n",
    "- perform over/undersampling\n",
    "- sample some examples more often, because:\n",
    "    - they are more important\n",
    "    - they are harder for neural network and should be seen more often\n",
    "    \n",
    "When we specify `shuffle=True` actually an instance of [`torch.utils.data.RandomSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler) is implicitly created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision\n",
    "\n",
    "`torchvision` is part of PyTorch project and described as:\n",
    "\n",
    "> __The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.__\n",
    "\n",
    "It has to be installed separately (though it's advised to download it with PyTorch).\n",
    "\n",
    "> We will focus on MNIST dataset so you can see whole PyTorch data loading pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    # Training dataset as torch.utils.data.Dataset instance\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=tmp_dir,  # where data is stored\n",
    "        transform=torchvision.transforms.ToTensor(),  # how each sample will be transformed\n",
    "        train=True,  # we want training data\n",
    "        download=True,  # should i download it if it's not already here?\n",
    "    )\n",
    "\n",
    "    # Test dataset as torch.utils.data.Dataset instance\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root=tmp_dir,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        train=False,\n",
    "    )\n",
    "    \n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_dataset[0][0]\n",
    "print(x.shape)\n",
    "plt.imshow(x.squeeze().numpy(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data\n",
    "\n",
    "Sometimes, we might need to split the data (usually to get validation dataset).\n",
    "\n",
    "PyTorch provides [`torch.utils.data.random_split`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) for those cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [50000, 10000]\n",
    ")  # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "Below, dataloaders are created as a dictionary; you may sometimes see this approach:\n",
    "- Pros:\n",
    "    - easier to read (instead of `training_dataloader`)\n",
    "    - easier to use\n",
    "- Cons:\n",
    "    - still a lot of repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    ),\n",
    "    \"validation\": torch.utils.data.DataLoader(\n",
    "        validation_dataset, batch_size=BATCH_SIZE, pin_memory=torch.cuda.is_available()\n",
    "    ),\n",
    "    \"test\": torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, pin_memory=torch.cuda.is_available()\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "> __Create custom `Images` `torch.utils.data.Dataset` with the goal of loading your custom images and it's respective labels.__\n",
    "\n",
    "First, in order to load an image we will use `pillow` library (__default choice for image loading in `torchvision`__):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below defines `DataGenerator` which will create specified number of random images\n",
    "with their respective random label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T10:24:53.479367Z",
     "start_time": "2021-06-22T10:24:53.451548Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataGenerator:\n",
    "    n_images: int\n",
    "    n_classes: int\n",
    "\n",
    "    def generate(self, path: pathlib.Path) -> pd.DataFrame:\n",
    "        path.mkdir(parents=True, exist_ok=True) \n",
    "        filenames = []\n",
    "        for i in range(self.n_images):\n",
    "            img_array = np.random.rand(100, 100, 3) * 255\n",
    "            filename = (path / str(i)).with_suffix(\".png\")\n",
    "            Image.fromarray(img_array.astype(\"uint8\")).convert(\"RGBA\").save(filename)\n",
    "\n",
    "            filenames.append(filename)\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"class\": np.random.randint(\n",
    "                    0, high=self.n_classes, size=(self.n_images)\n",
    "                ).tolist(),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task\n",
    "\n",
    "- __Analyze `DataGenerator` class__. Can you explain `dataclass` and `pathlib` usage?\n",
    "- Generate random data using `DataGenerator` and save returned `pd.DataFrame`.\n",
    "- Create `Images` `torch.utils.data.Dataset`. It should have the following methods:\n",
    "    - `__init__` - taking list of filepaths and `pd.DataFrame` as arguments\n",
    "    - `from_folder` - __static method__ taking `pathlib.Path` folder object and `pd.DataFrame`, iterating over files contained within it. Those `pathlib.Path` files should be passed to `__init__` in order to create an instance of `Images` object\n",
    "    - __Check `pathlib` modules documentation for more info!__\n",
    "    - `__getitem__(self, index)` - returns `tuple` `(image, klass)` based on `index`\n",
    "- __Add `transform` argument__ (like `torchvision`) and apply it to loaded image\n",
    "- Verify everything works correctly by using `from_folder` and creating an example `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Files(torch.utils.data.Dataset):\n",
    "    @classmethod\n",
    "    def from_folder(\n",
    "        cls, path: pathlib.Path, df: pd.DataFrame, transform=None, regex: str = \"*\"\n",
    "    ) -> \"Files\":\n",
    "        files = [file for file in path.glob(regex)]\n",
    "        return cls(files, df, transform)\n",
    "\n",
    "    def __init__(\n",
    "        self, files: typing.List[pathlib.Path], df: pd.DataFrame, transform=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Images(Files):\n",
    "    def __getitem__(self, index) -> typing.Tuple[Image, int]:\n",
    "        klass = self.df.iloc[index]\n",
    "        sample = Image.open(self.files[index])\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, klass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- In PyTorch we create datasets by inheriting from `torch.utils.data.Dataset`:\n",
    "    - those return a single item during each iteration\n",
    "    - really simple & lightweight abstraction\n",
    "- `torch.utils.DataLoader` takes care of:\n",
    "    - batching\n",
    "    - shuffling\n",
    "    - speed ups when loading data\n",
    "- __You should always provide your dataset instance to `DataLoader`!__\n",
    "- `torchvision` is part of PyTorch project and provides:\n",
    "    - datasets\n",
    "    - transformations\n",
    "    - vision related models\n",
    "- Default `pytorch` requires a lot of boilerplate code but is a battle-tested approach to training neural networks\n",
    "\n",
    "# Challenges\n",
    "\n",
    "## Assessment\n",
    "\n",
    "- what are [IterableDatasets](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)? Where those might be used?\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- check out third party module [torchdata](https://github.com/szymonmaszke/torchdata) for other ways to handle caching and using data transformations of PyTorch's `Dataset`\n",
    "- go over PyTorch's [data documentation](https://pytorch.org/docs/stable/data.html) and learn a little more about the details of design"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
