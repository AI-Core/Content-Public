1. Check out this website which visualises the loss surface for neural networks. Hopefully this illustrates how much more challenging NN optimisation is compared to that of more simple models.
    - http://losslandscape.com
    - Watch the landing page video
    - Read the section on the method to understand how we can visualise the very high dimensional parameter space in 2d
    - Check out the section showing the loss surface changing with each batch
2. Watch 3Blue1Brown talk through how neural networks learn using gradient descent 
    - https://www.youtube.com/watch?v=IHZwWFHWa-w&t=1125s
3. Read through the notebook
4. Read sections 8-8.2.8 of the deep learning book on optimisation for deep learning
5. Work through the challenges in groups
6. Draw up a grid to compare different optimisers (rows) and their distinctive features (columns)
    - Discuss this with a group
    - Compare:
        - SGD
        - SGD with momentum
        - SGD with nesterov momentum
        - AdaGrad
        - RMSProp
        - Adam
    - Features you might want to consider as column titles:
        - Adaptive learning rates?
        - Gradient based?
        - Other parameters than learning rate
        - Requires keeping track of internal parameters?
7. Find the best resource online for optimisers and share it
8. Read about AdaBound, which combines SGD and Adam, [here](https://github.com/Luolc/AdaBound)

## More Detail
1. Read further into the basic optimisation algorithms covered in the notebook in section 8.3-8.3.3 of the deep learning book
2. Read further into the more advanced optimisation algorithms covered in the notebook in section 8.5-8.6.1 of the deep learning book 