{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "\n",
    "## General\n",
    "\n",
    "> An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
    "\n",
    "- Mostly used to create neural networks\n",
    "- Specialized to use hardware acceleration (GPU, TPU, Tensor Cores etc.)\n",
    "- __API based on `numpy`__ (`torch.Tensor` instead of `np.array`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.float32 torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# Alias below is pretty common\n",
    "# one can also use torch directly\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X = torch.rand(300, 10)  # Random uniform\n",
    "print(type(X))\n",
    "\n",
    "W = torch.randn(10)  # Random normal\n",
    "b = torch.tensor([1])  # create again another random tensor\n",
    "\n",
    "y = X @ W + b\n",
    "\n",
    "print(y.dtype, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types\n",
    "\n",
    "Similarly to `numpy`, PyTorch provides multiple data types, for example: \n",
    "\n",
    "- `torch.float` (32-bit precision)\n",
    "- `torch.double` (64-bit precision)\n",
    "- `torch.half` (16-bit precision)\n",
    "\n",
    "and many others (see [here](https://pytorch.org/docs/stable/tensor_attributes.html)).\n",
    "\n",
    "> Usually we will use floating point values (either `float` or `half`), depending on context\n",
    "\n",
    "### Why not double?\n",
    "\n",
    "> Default `dtype` in PyTorch is `float` because __it doesn't take up so much memory and is accurate enough__\n",
    "\n",
    "Also, GPU memory is costly (and there isn't enough of it usually), hence lower precision (up to a certain point) might be a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting\n",
    "\n",
    "One can easily cast PyTorch tensors to desired data types, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 torch.float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "array = np.random.randn(10, 5)\n",
    "tensor = torch.from_numpy(array)\n",
    "print(array.dtype, tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast to half type\n",
    "new_tensor = tensor.half()\n",
    "\n",
    "new_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2207 , -0.4895 , -0.974  , -0.7163 , -0.3904 ],\n",
       "       [ 0.261  , -0.1913 ,  0.09644, -0.4658 ,  1.246  ],\n",
       "       [-0.2954 ,  0.4834 ,  0.8955 , -0.6025 , -1.111  ],\n",
       "       [-0.3735 , -0.6553 ,  1.651  ,  0.1934 , -0.1555 ],\n",
       "       [ 0.5166 , -0.6772 , -0.3328 , -1.351  ,  1.748  ],\n",
       "       [-0.1812 ,  0.512  , -0.475  , -0.4182 , -0.2444 ],\n",
       "       [ 1.421  , -0.3674 , -0.2566 , -0.2686 ,  1.169  ],\n",
       "       [ 0.4937 , -1.168  , -0.0638 , -0.03564, -0.572  ],\n",
       "       [-1.131  ,  1.469  ,  0.3977 ,  1.199  , -0.5757 ],\n",
       "       [-0.648  , -0.01033, -0.608  , -0.2213 , -0.4197 ]], dtype=float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy interoperability\n",
    "new_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upcasting\n",
    "(new_tensor + tensor).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "PyTorch can utilize multiple device types. In general:\n",
    "- we use CPU for data loading\n",
    "- we use specialized devices (usually GPU, sometimes TPU) for running the data through neural network\n",
    "\n",
    "> TPU support is currently experimental, see challenges for more info\n",
    "\n",
    "Let's start by checking if GPU is available on our devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this information we can create a special device type that we can later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In later sections (basics of training) you will see how we can use this device variable\n",
    "for device agnostic code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "In order for neural networks to learn we need to calculate gradients of `loss` w.r.t. parameters (like we did with linear regression previously).\n",
    "\n",
    "> This time differentiation graph (__sometimes also called a tape__) is provided by PyTorch\n",
    "\n",
    "![](./images/grad.jpg)\n",
    "\n",
    "To use PyTorch's [autograd](https://pytorch.org/docs/stable/autograd.html) we need a few changes in the above code.\n",
    "\n",
    "First, we have to mark tensors which require gradient using `requires_grad=True` argument during creation:\n",
    "\n",
    "> Most of PyTorch functions creating tensors like `rand`, `randn` etc. have `requires_grad` as an optional parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(10, requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Only tensor of floating data type can have gradient! __No integers or a-like__\n",
    "\n",
    "After that we can use them normally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X @ W + b\n",
    "\n",
    "loss = y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running backpropagation\n",
    "\n",
    "Like we did during \"Gradient Methods\" we can run backpropagation algorithm explicitly.\n",
    "\n",
    "> In PyTorch we run backpropagation __on tensor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor([147.7014, 155.7213, 148.9580, 143.3640, 146.8089, 154.9344, 145.1965,\n",
      "        145.6324, 139.8669, 152.7871]) tensor([300.])\n"
     ]
    }
   ],
   "source": [
    "print(W.grad, b.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Use .grad attribute \n",
    "print(W.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implicitly tensor with `1` is fed into `backward` __if tensor is a scalar!__\n",
    "\n",
    "> If tensor is not a scalar, you have to provide a tensor with initial gradient of specified shape, see [here](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad_fn, how PyTorch keeps track of operations\n",
    "\n",
    "- __PyTorch keeps functions which created the tensor (if any) inside `grad_fn`__ attribute\n",
    "- if `grad_fn` is `None` it is a tensor which:\n",
    "    - was created by user explicitly (either with `requires_grad` set to `True` or `False`)\n",
    "    \n",
    "See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000022884966EE0> False True\n",
      "None True True\n",
      "None True False\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn, y.is_leaf, y.requires_grad)\n",
    "\n",
    "print(W.grad_fn, W.is_leaf, W.requires_grad)\n",
    "\n",
    "print(X.grad_fn, X.is_leaf, X.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules\n",
    "\n",
    "> __PyTorch provides multiple modules based on what we intend to do__\n",
    "\n",
    "> __REFER TO THIS LIST WHENEVER IN DOUBT__\n",
    "\n",
    "> __MOST OF NUMPY FUNCTIONALITY IS IMPLEMENTED HENCE THERE IS NO NEED TO USE IT, ALWAYS USE PYTORCH COUNTERPARTS!__\n",
    "\n",
    "> __DO NOT USE PYTHON BUILT-IN FUNCTIONS LIKE SUM!__\n",
    "\n",
    "## [import torch](https://pytorch.org/docs/stable/torch.html) \n",
    "\n",
    "> __Basic functionality, acts like `numpy` high level namespace__\n",
    "\n",
    "- creating tensors, like:\n",
    "    - `torch.tensor`- creates tensor from `list` data (like `np.array`)\n",
    "    - `torch.randn` - random normal tensor of specified shape\n",
    "    - `torch.zeros`, `torch.zeros_like`\n",
    "- indexing, slicing, like:\n",
    "    - `torch.cat` - concatenate across __given dimension__\n",
    "    - `torch.stack` - stack across __new dimension__\n",
    "    - `torch.reshape` - reshape the tensor (use instead of `torch.view`)\n",
    "- random sampling (`torch.seed`)\n",
    "- mathematical functions like \n",
    "    - `torch.sin` \n",
    "    - `torch.sigmoid`\n",
    "    - `torch.abs`\n",
    "    - `torch.mean`\n",
    "- reduction operations, like:\n",
    "    - `torch.argmax` - __index of item with maximum value__ (possibly across dimensions)\n",
    "    - `torch.min` - minimum value (possibly across dimensions)\n",
    "    - `torch.var_mean` - variance and mean (possibly across dimensions)\n",
    "- comparison operations, like:\n",
    "    - `torch.top_k` - maximum `k` values (and their indices)\n",
    "    - `torch.sort` - sort values (possibly across dimensions)\n",
    "    - `torch.argsort` - as above, but return indices sorting the tensor\n",
    "- enabling/disabling gradient (`torch.no_grad`, see training & loop section)\n",
    "- __other operations on tensors__\n",
    "\n",
    "Let's see some of them in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:29:56.283399Z",
     "start_time": "2021-06-20T18:29:54.586133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3]) # use torch.tensor always as it does type inference\n",
    "t2 = torch.Tensor([1, 2, 3])\n",
    "\n",
    "t1. dtype, t2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:31:00.262569Z",
     "start_time": "2021-06-20T18:31:00.221408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4164,  0.3896, -0.2437,  0.1109,  0.1156,  0.7962, -0.9374,  0.5662,\n",
       "         0.1221])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random tensor of shape and slicing it\n",
    "\n",
    "t1 = torch.zeros(64, 18)\n",
    "t2 = torch.randn_like(t1)\n",
    "t2[0, 9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:32:07.588426Z",
     "start_time": "2021-06-20T18:32:07.582375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[2.5645, 2.4702, 2.9204, 2.1125, 2.4508, 3.1817, 2.4998, 2.2284, 2.1669,\n",
       "         2.1737, 2.0773, 4.0561, 1.7559, 2.7503, 2.2525, 1.6442, 3.5927, 1.9406],\n",
       "        [2.5129, 2.4029, 2.1267, 1.9612, 1.6177, 2.2472, 1.4460, 2.2186, 2.0504,\n",
       "         1.6029, 1.9764, 2.4437, 1.5435, 2.3339, 2.1958, 1.5503, 2.2064, 1.6870],\n",
       "        [2.3649, 1.9801, 1.4681, 1.4865, 1.6087, 1.9867, 1.4129, 1.9937, 1.9930,\n",
       "         1.5907, 1.9118, 2.3072, 1.4368, 2.2790, 1.8168, 1.4041, 2.0703, 1.4445]]),\n",
       "indices=tensor([[ 5, 29, 42, 23, 26, 14, 63, 30, 51,  6, 53, 34, 56, 13, 38, 60, 52, 57],\n",
       "        [31, 17,  1, 41, 36, 47,  7, 31, 30, 14, 21, 52,  7, 47, 24, 59, 33, 56],\n",
       "        [ 3, 42, 32,  2, 15, 10, 48, 23, 20,  3,  8, 11,  1, 62, 52, 32, 16, 48]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(t2, k=3, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:33:32.013183Z",
     "start_time": "2021-06-20T18:33:32.002295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "torch.stack((t1, t1, t1, t1), dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:33:34.286154Z",
     "start_time": "2021-06-20T18:33:34.273185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t1, t1 ,t1), dim=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [import torch.nn as nn](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "> __PyTorch neural network related modules__\n",
    "\n",
    "We will learn more about that in the following lessons, __but keep in mind that__:\n",
    "- __All of the operations (layers) are presented as classes (which you instantiate)__\n",
    "- __Layers can be mixed together and depend on `torch.nn.Module` (ALL OF THEM ARE INSTANCES OF IT__)\n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "layer = nn.Linear(10, 5)\n",
    "for param in layer.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers are callable functors\n",
    "\n",
    "another_layer = nn.Conv3d(12, 32, 3)\n",
    "inputs = torch.randn(64, 12, )\n",
    "outputs = another_layers(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [import torch.nn.functional as F](https://pytorch.org/docs/stable/nn.functional.html)\n",
    "\n",
    "> __PyTorch neural network components PROVIDED AS FUNCTIONS INSTEAD OF OBJECTS__\n",
    "\n",
    "Why would we use one over the other? __Code readability__ is the answer.\n",
    "\n",
    "First, let's see where `torch.nn` looks better than `torch.nn.fucntional`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:57:33.801051Z",
     "start_time": "2021-06-20T18:57:33.796280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup cell with data\n",
    "\n",
    "data = torch.randn(64, 10) # 64 examples, 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:57:55.378884Z",
     "start_time": "2021-06-20T18:57:55.374729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nn approach\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "layer = nn.Linear(10, 5)\n",
    "output = layer(data)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:57:57.563084Z",
     "start_time": "2021-06-20T18:57:57.559437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nn.functional approach\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "output = F.linear(data, weight=torch.randn(5, 10))\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros of torch.nn\n",
    "\n",
    "- __Creating stateful objects (NEURAL NETWORKS AND OTHER MODELS)__\n",
    "- Implicitly creates weight (with appropriate initialization!)\n",
    "- Easier to compose and divided steps\n",
    "\n",
    "Now let's see when `torch.nn.functional` is better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T19:00:51.871514Z",
     "start_time": "2021-06-20T19:00:51.861078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nn approach\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "probabilities = softmax(data)\n",
    "\n",
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T19:01:19.607523Z",
     "start_time": "2021-06-20T19:01:19.598416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nn.functional approach\n",
    "\n",
    "probabilities = F.softmax(data, dim=1)\n",
    "\n",
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros of torch.nn.functional\n",
    "\n",
    "- __Using non-stateful objects__ (softmax has no weights or attributes)\n",
    "- Directly applicable\n",
    "- __Used only one time__ (and reused when needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.Module\n",
    "\n",
    "> [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is a base class for every deep learning model in PyTorch (usually neural networks)\n",
    "\n",
    "Given that, we will inherit it from it each time we create a more complicated module.\n",
    "\n",
    "Let's see how we can code up __linear regression__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, n_features: int):\n",
    "        # This line is always required at the beginning\n",
    "        # Registers parameters of our model in graph\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = torch.nn.Parameter(torch.randn(n_features))\n",
    "        self.b = torch.nn.Parameter(torch.ones(1))\n",
    "        self.other_tensor = torch.randn(5)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X @ self.W + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Parameter\n",
    "\n",
    "> If we want a tensor to be a part of `nn.Module` we have to wrap it inside `nn.Parameter`\n",
    "\n",
    "Let's see what parameters our model currently has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W torch.Size([15])\n",
      "b torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(15)\n",
    "\n",
    "# named_parameters is a generator, you can also use parameters method\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(name, parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see `self.other_tensor` __is not registered as a parameter__.\n",
    "\n",
    "This means, we won't be able to easily optimize it and it is \"merely an attribute\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward method\n",
    "\n",
    "Users should implement logic of the model (how data goes through neural network) inside this method.\n",
    "\n",
    "> When running data through our model we will use `__call__` method. __This ensures any hooks registered for module will run correctly__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(torch.randn(64, 15))\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input shape\n",
    "\n",
    "> PyTorch requires `(batch_size, n_features1, ..., n_features2)` tensors as input\n",
    "\n",
    "In the case above, batch size was `64` with `15` input features to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create __multiclass LogisticRegression__ layer from scratch by inheriting from `nn.Module` and using `nn.Parameter`.\n",
    "\n",
    "- User can specify number of input features and `n_classes` in the initialization\n",
    "- Remember about the `forward` method (__returns logits__)\n",
    "- Create another method `predict_proba` by reusing code above (which PyTorch module should you use?)\n",
    "- Create another method `predict` which returns correct class __for each sample__ (use output from __correctly called__ `forward` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.in_features: int = in_features\n",
    "        self.n_classes: int = n_classes\n",
    "\n",
    "        # W: (in_features, n_classess)\n",
    "        self.W = torch.nn.Parameter(torch.randn(self.in_features, self.n_classes))\n",
    "        self.b = torch.nn.Parameter(torch.ones(self.n_classes))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (batch, in_features)\n",
    "        return X @ self.W + self.b\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return torch.nn.functional.softmax(self(X), dim=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return torch.argmax(self(X), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- PyTorch can be considered as `numpy` on GPU for neural networks\n",
    "- PyTorch provides different data types:\n",
    "    - `float` is a good default value (good balance between necessary precision, performance and memory usage)\n",
    "- PyTorch can run on different devices:\n",
    "    - GPU is used for running neural networks\n",
    "    - CPU is used for data loading and other intensive tasks\n",
    "- We should write device agnostic code (basics shown here)\n",
    "- We should inherit from `torch.nn.Module` when creating neural networks\n",
    "    - Override `__init__` and `forward`\n",
    "    - Use this model via functor `__call__`\n",
    "\n",
    "\n",
    "# Challenges\n",
    "\n",
    "## Assessment\n",
    "\n",
    "- Check available function and attributes of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). What are those used for, see some examples.\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- Check out [CUDA semantics](https://pytorch.org/docs/stable/notes/cuda.html) in PyTorch. How to choose specific GPU device if there are multiple of them?\n",
    "- What are the aforementioned hooks? Check out [this article](https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904) for more information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
