{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU & Data Parallel\n",
    "\n",
    "\n",
    "# Processes vs threads\n",
    "\n",
    "Those terms are often confused, and it is important to get them right, to know where and when we can use each of them.\n",
    "\n",
    "> __A coroutine lives in a thread, a thread lives in process, a process lives in core, a core lives in a CPU.__\n",
    "\n",
    "## Core\n",
    "\n",
    "> Core is a physical part of CPU (also other devices) and processes and run on them\n",
    "\n",
    "- The more cores, the more processes we can run at once\n",
    "- There are technologies (e.g. hyper threading) providing two virtual cores for one physical (this indicates how many threads can be run on a core at the same time)\n",
    "\n",
    "## Process\n",
    "\n",
    "> Process is any running program, single smallest management unit\n",
    "\n",
    "It is characterized by:\n",
    "- __Separate memory space (one part of computer program is used exclusively by one process)__\n",
    "- __Creation/Desctruction is costly__\n",
    "- __Communication between processes is costly__\n",
    "- One process per core\n",
    "- Independent (one crashing process does not interfere with others)\n",
    "\n",
    "> __`torch.utils.data.DataLoader` using multiple processes to load data!__\n",
    "\n",
    "## Threads\n",
    "\n",
    "> Threads are more lightweight and more than one can run in a single process\n",
    "\n",
    "- Program needs at least one thread to run a.k.a. __main thread__\n",
    "- __Shared memory space__ (overriding the same variable, in general, __not in Python__)\n",
    "- __Creation/Destruction is cheapter__\n",
    "- __Communication between threads is faster__\n",
    "- Multiple threads per core (only one can own the process at a time)\n",
    "- Dependent, can crash because of what the others are doing\n",
    "\n",
    "### Global Interpreter Lock\n",
    "\n",
    "> __Python has Global Interpreter Lock (GIL) which slows down threads!__\n",
    "\n",
    "It happens due to explicit use of mutex (variable can be owned only by one thread at a time), some traits:\n",
    "- __might be slower than single core__ (depending on the use case and run)\n",
    "- __could be worked around__ (different language or interpreter, the second one not advised in general)\n",
    "\n",
    "## Concurrent vs parallel\n",
    "\n",
    "> Concurrent programming, refers to threads (or other forms of execution) switching ownership of the processor\n",
    "\n",
    "> Parallel refers to running across different cores\n",
    "\n",
    "- __Process are always parallel__\n",
    "- __Threads can run concurrently__ (e.g. two threads on one core with HyperThreading) or __in parallel__ (across multiple cores)\n",
    "\n",
    "## General tips\n",
    "\n",
    "- __Try to schedule your tasks evenly across processes__ (so they take similar amount of time and resources)\n",
    "- __Try to divide your tasks into as independent parts as possible__\n",
    "- __Try not to share data__ (send parts of data and gather at the end from different execution units)\n",
    "- Circumvent GIL when speed is a factor (e.g. move execution to language like C/C++)\n",
    "- Usually use Python's `multiprocessing` module for parallel code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU\n",
    "\n",
    "- __Central Processing Unit__\n",
    "- __Focused on instructions__ - large set of instructions can be run on it\n",
    "- Optimized for general tasks\n",
    "- Typically smaller number of cores (around 4-18, twice with hyperthreading)\n",
    "- Can run up to a 1000 threads or so\n",
    "\n",
    "# GPU\n",
    "\n",
    "- __Graphics Processing Unit__\n",
    "- __Focused on data__ - specialized for SIMD (Single Instruction Multiple Data) tasks on floating point data\n",
    "- Unable to handle general tasks well\n",
    "- Typically large number of specialized cores (3000-10000 cores)\n",
    "- Can run tens/hundreds of thousands threads at once\n",
    "\n",
    "![](images/num_cores.jpg)\n",
    "\n",
    "# Which one to choose?\n",
    "\n",
    "> Fortunately, we can use both devices, where they excel\n",
    "\n",
    "For deep learning this looks more or less like this:\n",
    "\n",
    "## CPU\n",
    "\n",
    "> Remember CPU is also responsible for other tasks and it's performance may vary based on OS load!\n",
    "\n",
    "- General processing and instructions (e.g. data loading)\n",
    "- Usually performs \"slower\" tasks (use multiple threads for loading data)\n",
    "\n",
    "## GPU\n",
    "\n",
    "> GPU may also be responsible for other tasks, but usually to a lesser extent\n",
    "\n",
    "- __Running identical set of instructions with a lot of data__\n",
    "- This means most of layers benefit from this approach as those are usually GEMM (GEneric Matrix Multiplication) \n",
    "\n",
    "# Which hardware is supported\n",
    "\n",
    "- Right now __Intel CPUs__ are the most widely used for everything, including deep learning\n",
    "- __NVidia's GPUs__ are at the forefront of deep learning\n",
    "- __AMD's GPUs__ are currently not officially supported\n",
    "\n",
    "# I know I have a GPU! Why isn't it available?\n",
    "\n",
    "Correct **drivers** are needed for us to work with GPU\n",
    "\n",
    "> A driver is a piece of software that lets the operating system and a hardware device communicate with each other.\n",
    "\n",
    "- Correct drivers are usually provided out of the box (or are easily installable)\n",
    "- If we do something incorrectly with drivers, it might be hard to revert changes (occurs rarely if done with case and thought)\n",
    "- __Possible solution:__ Let someone else do that for us (e.g. cloud providers, system administrators)\n",
    "\n",
    "> Also appropriate software versions are required (e.g. compiled with CUDA support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got GPU? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda_available = torch.cuda.is_available() # check if cuda is available\n",
    "\n",
    "print('Got GPU?', cuda_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA\n",
    "\n",
    "> CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model designed for GPU\n",
    "\n",
    "Programming with CUDA can be done in multiple languages, __but usually with C/C++ (eventually Fortran)\n",
    "\n",
    "- You program in CUDA using low level macros and functions\n",
    "- This code is passed from CPU to GPU device where it is run\n",
    "- Requires specific compiler called __nvcc__ (run `nvcc --version` to get more info about your CUDA version)\n",
    "\n",
    "## Important things\n",
    "\n",
    "- Each time CPU passes control to GPU device it can run it's own set of operations\n",
    "- CPU has to wait for results of those operations at a point called __synchronization point__\n",
    "- __The less synchronization points the better!__ Always try to \"stick\" with one \"environment\" (fortunately PyTorch does most of this for us)\n",
    "\n",
    "> You can find more about PyTorch's cuda capabilities inside [`torch.cuda`](https://pytorch.org/docs/stable/cuda.html) package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Cores \n",
    "\n",
    "> Read more about them on [NVidia's site](https://www.nvidia.com/en-us/data-center/tensor-cores/)\n",
    "\n",
    "- Specialized set of instructions (and a new data type) which allows us to speed up our computations __up to 10 times!__ \n",
    "- Provided in newer graphic cards\n",
    "- Suitable for mixed precision training (see below) and high data throughput\n",
    "\n",
    "## Tensor Cores tips\n",
    "\n",
    "> In order to utilize Tensor Cores efficiently, there are a few guidelines one should follow while creating most of the architectures\n",
    "\n",
    "__Based on [this article](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)__\n",
    "\n",
    "- Use Mixed Precision training (example in PyTorch below)\n",
    "- Parameters and inputs should be:\n",
    "    - divisible by `8` for `float16` (a.k.a. half precision)\n",
    "    - divisible by `16` for `int8` (rare case in deep learning)\n",
    "    \n",
    "## Other performance tips\n",
    "\n",
    "- Use \"math-light\" operations and activations:\n",
    "    - ReLU is \"math-light\" as it only involves thresholding value\n",
    "    - Tanh is \"math-heavy\" as it involves sigmoid\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Below you have a few code cells (__up to PyTorch's AMP__) with schematic code.\n",
    "\n",
    "- Analyze them and write a comment next to each, whether Tensor Cores best practices were violated\n",
    "- If they were, why? If they weren't, why, which part of code violates the guidelines?\n",
    "- Any other issues with this code that you can spot?\n",
    "- Look closely at every line, __errors may occur not only in model, but also data loading__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Case 1 - 15 features, 3 class classification\n",
    "\n",
    "data = torch.randn(128, 15)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(128, 64),\n",
    "    torch.nn.Linear(64, 32),\n",
    "    torch.nn.Linear(32, 16),\n",
    "    torch.nn.Linear(16, 8),\n",
    "    torch.nn.Linear(8, 4),\n",
    "    torch.nn.Linear(4, 5)\n",
    ")\n",
    "\n",
    "model(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Case 2 - 15 features, 16 class classification\n",
    "\n",
    "data = torch.randn(123, 16)\n",
    "dataloader = torch.utils.data.DataLoader(data, batch_size=64)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(128, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, 16)\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    model(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Case 3 - Images and convolution\n",
    "\n",
    "data, mask = torch.randn(1024, 3, 26, 26), torch.randn(1024, 1, 26, 26)\n",
    "dataset = SuperDataset(data, mask) # let's assume this is a torch.utils.data.Dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, drop_last=True)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 64, kernel_size=3)\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(64, 128, kernel_size=3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(128, 64, kernel_size=3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(64, 1, kernel_size=3)\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "for img, mask in dataloader:\n",
    "    predicted = model(dataloader)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Mixed Precision (AMP)\n",
    "\n",
    "> __Automatic Mixed Precision__ automatically casts __parts of neural networks and inputs__ to lower precision datatype\n",
    "\n",
    "> In order to fully utilize current performance boosts (including Tensor Cores) we have to use __mixed precision training__\n",
    "\n",
    "PyTorch provides easy to use interface:\n",
    "\n",
    "## Autocasting\n",
    "\n",
    "> [`torch.cuda.amp.autocast`](https://pytorch.org/docs/stable/amp.html#id4) is a context manager or decorator which runs regions of the code in mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model and optimizer in default precision\n",
    "model = Net().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), ...)\n",
    "\n",
    "# inputs.dtype == float32\n",
    "for inputs, targets in data:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Enables autocasting for the forward pass (model + loss)\n",
    "    with autocast():\n",
    "        # Here inputs.dtype == float16, DONE AUTOMATICALLY\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "    # Exits the context manager before backward()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocasting know-how\n",
    "\n",
    "- __Only for CUDA!__\n",
    "- __Running `backward` inside `autocast` is not recommended!__ (see below)\n",
    "- Some layers (or parts of layers, e.g. weights/buffers) will have their precision unchanged\n",
    "- It is analyzed on a per-layer basis in order not to affect (at least drastically) models performance\n",
    "- Regions of autocast/no-autocast (`torch.cuda.amp.autocast(enabled=False)`) can be nested inside each other (rarely useful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Scaling\n",
    "\n",
    "> [`torch.cuda.amp.GradScaler`](https://pytorch.org/docs/stable/amp.html#id5) __prevents underflow during backward pass__ (small updates may not fit into `half` and those would be lost!) \n",
    "\n",
    "- Loss scaling is automatically determined during training and used throughout rest of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for input, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n",
    "        # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "> Requires GPU capable device!\n",
    "\n",
    "Create training loop (with specified number of epochs) with `autocast` and gradient scaling (no unscaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning-bolts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "\n",
    "with tempfile.TemporaryDirectory() as data_dir:\n",
    "    dm = CIFAR10DataModule(\n",
    "        data_dir=data_dir, shuffle=True, num_workers=1, normalize=True, batch_size=64\n",
    "    )\n",
    "    train_dataloader = dm.train_dataloader()\n",
    "    test_dataloader = dm.test_dataloader()\n",
    "\n",
    "\n",
    "# Use provided model, optimizer, criterion\n",
    "model = torchvision.models.resnet50(num_classes=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setup GradScaler here\n",
    "...\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Create train and validation loops over dataloaders\n",
    "    # Including autocasting and proper usage of GradScaler\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving/creating data on GPU\n",
    "\n",
    "Torch tensors and models have a ```.to``` method which moves them to a device (we have seen that previously)\n",
    "\n",
    "> PyTorch provides multiple arguments when moving/constructing our data on device which allows us to finetune performance\n",
    "\n",
    "Let's see two methods, which cover most of the possibilities:\n",
    "\n",
    "## torch.tensor\n",
    "\n",
    "Allows us to __create__ `torch.Tensor` instance from Python's `list` instances, is part of PyTorch's [creation ops](https://pytorch.org/docs/stable/torch.html)\n",
    "\n",
    "Let's see it's signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### device\n",
    "\n",
    "`device` can be:\n",
    "- instance of `torch.device` (advised)\n",
    "- string specifying name of the device (usually `\"cpu\"` or `\"cuda\"`)\n",
    "\n",
    "> PyTorch allows us to easily move data to different GPUs __(one tensor can reside only on a single GPU!)__\n",
    "\n",
    "Check out the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_device = torch.device(\"cpu\")\n",
    "first_available_gpu = torch.device(\"cuda\")\n",
    "\n",
    "# Same as above\n",
    "zeroth_gpu = torch.device(\"cuda:0\")\n",
    "\n",
    "N = 4\n",
    "n_th_gpu = torch.device(f\"cuda:{N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pin_memory\n",
    "\n",
    "> When `.to` method is invoked, special \"staging area\" has to be prepared on CPU and data from pageable memory is copied to it (think of it as `git add` before `git commit`)\n",
    "\n",
    "![](images/pin_memory.png)\n",
    "\n",
    "\n",
    "#### Why?\n",
    "\n",
    "- GPU cannot access pageable memory on CPU\n",
    "- Created pinned memory region and direct creation of `torch.Tensor`s on it allows us to mitigate this issue\n",
    "\n",
    "#### Pros\n",
    "\n",
    "- __Faster memory transfers__ (usually)\n",
    "- No need to create \"pinned memory\" region over and over again\n",
    "- No need for copy from CPU\n",
    "\n",
    "#### Cons\n",
    "\n",
    "- __Part of CPU memory is occupied by PyTorch for the duration of program run__\n",
    "- Due to above, it will not be available for system usage if needed\n",
    "\n",
    "### When to use?\n",
    "\n",
    "- If we create `torch.Tensor` instances of the same (similar) size on the device (which is often the case)\n",
    "- __We should measure performance__ as it depends on many variables\n",
    "- __At least try it if you need speedup__ and data loading is the __bottleneck__ \n",
    "\n",
    "> __Usually used with `torch.utils.data.DataLoader` as it has `pin_memory` argument!__\n",
    "\n",
    "> __Check [`torch.utils.bottleneck`](https://pytorch.org/docs/stable/bottleneck.html) for more info about profiling!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor.to(...) method\n",
    "\n",
    "> Allows us to __move__ tensor to device (usually GPU) and specify many details about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to(\n",
    "    device=None, # Previously\n",
    "    dtype=None, # Previously\n",
    "    non_blocking=False,\n",
    "    copy=False, # Make a new copy even if tensor matches the format\n",
    "    # You almost never should set copy=True\n",
    "    memory_format=torch.preserve_format, # OBLIGATORY ASSESSMENT!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non_blocking\n",
    "\n",
    "> `non_blocking=True` will allow CPU to run without waiting for the move to complete\n",
    "\n",
    "#### Pros\n",
    "\n",
    "- CPU does not have to wait for the move operation to finish\n",
    "- Improves parallelization of code\n",
    "- Possible speedups (see when to use)\n",
    "\n",
    "#### Cons\n",
    "\n",
    "- Not usable in many cases\n",
    "- May confuse users as to your intent\n",
    "- May be harder to profile accurately\n",
    "\n",
    "#### When to use\n",
    "\n",
    "- Setting it to `True` __should not__ hurt the performance anyhow\n",
    "- When it may logically improve performance (see below)\n",
    "- When __synchronization point__ is nonimmediate\n",
    "\n",
    "See a case where it is immediate and nonimmediate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At least 1 CUDA device required\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    \n",
    "    # IMMEDIATE (no point)\n",
    "\n",
    "    t = torch.randn(100, 100).to(\"gpu\", non_blocking=True)\n",
    "    # Control is passed back and you have to wait until move finishes\n",
    "    t += 10\n",
    "    \n",
    "    # NONIMMEDIATE (might improve performance)\n",
    "    t1 = torch.randn(100, 100).to(\"gpu\", non_blocking=True)\n",
    "    t2 = torch.randn(10, 10)\n",
    "    t2 = torch.cos(torch.sin(t2))\n",
    "    # Possibly some other operations on CPU\n",
    "    t2 = t2 @ t2\n",
    "    \n",
    "    # Synchronization point, data has to be moved to GPU by now\n",
    "    t1 += 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "## Assessment \n",
    "\n",
    "- Read about Python's coroutines in documentation [here](https://docs.python.org/3/library/asyncio-task.html) (general understanding of coroutines may be part of an assessment!)\n",
    "- __Read about TPUs [here](https://cloud.google.com/tpu/docs/tpus) and expect a few questions about them!__\n",
    "- What is the difference between BCHW format and BHWC (a.k.a. channels first vs channels last)? Read about it [here](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)\n",
    "- Go over [torch.cuda](https://pytorch.org/docs/stable/cuda.html) to see what can be done with CUDA enabled devices in PyTorch\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- Read more about Python's [GIL](https://wiki.python.org/moin/GlobalInterpreterLock). What are non Cython options to circumvent the limitations?\n",
    "- Read more about PyTorch and TPU integration via `torch-xla` package [here](https://pytorch.org/xla/release/1.8/index.html)\n",
    "- Read more about performance optimization for Deep Learning [here](https://docs.nvidia.com/deeplearning/performance/index.html). Some of the tips were provided, but there's way more to uncover if you are interested!\n",
    "- Read more about optimization of data transfers to CUDA enabled devices [here](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
