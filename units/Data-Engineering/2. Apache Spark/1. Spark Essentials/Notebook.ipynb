{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159173ce",
   "metadata": {},
   "source": [
    "# Spark basics\n",
    "\n",
    "One of the most popular ways to use Spark is with the Python library `pyspark`.\n",
    "\n",
    "# Installation\n",
    "\n",
    "PySpark has many dependencies, not only with other Python packages, but also with other modules that are not easily installed using the convenient `pip install` command. While you can install pyspark using `pip install pyspark` this is probably not going to be enough. Therefore, we recommend you to follow the next steps:\n",
    "\n",
    "1. Visit [PySpark download page](https://spark.apache.org/downloads.html) and:\n",
    "- Choose latest release\n",
    "- Download package locally\n",
    "\n",
    "2. Create a folder (for example `spark`)  in a directory that you know will be safe. `~/` is usually a good option. \n",
    "3. Extract the files from the downloaded file into the created folder. At the time of writing, the last version was Spark 3.1.2, so, in that case, your directory will look like this (in case you are using the same examples):\n",
    "```\n",
    "~/\n",
    "│\n",
    "├── spark/\n",
    "│   └── spark-3.1.2-bin-hadoop3.2  <--- SPARK_HOME\n",
    "│         ├── bin\n",
    "│         ├── conf\n",
    "│         ├── data\n",
    "... \n",
    "```\n",
    "4. It is important you set the directory as SPARK_HOME, otherwise, PySpark won't know where to find the corresponding commands. To do so, simply set it as a environment variable copying the following command in your `~/.bashrc` file:\n",
    "\n",
    "`export SPARK_HOME=<path to your home directory>/spark/spark-3.1.2-bin-hadoop3.2`\n",
    "\n",
    "_Note: The command above depends on where you extracted the files you downloaded and the version_\n",
    "\n",
    "> Don't skip this step. Having an incorrectly set `SPARK_HOME` environment variable is the cause of many common issues with Spark\n",
    "\n",
    "5. Save your `~/.bashrc`. You should be able to use PySpark now! If not, try restarting vscode, then try restarting your computer if that doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108de564",
   "metadata": {},
   "source": [
    "6. To check if the installation was successful, you can install findspark (`pip install findspark`) and run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0262f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332c901",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "  <summary> <font size=+1> For Windows Users </font> </summary>\n",
    "  \n",
    "  Depending on your environment, the last steps might not work. In that case, you have to set the environment variable manually. Look at the following gif to know how to it\n",
    "\n",
    "  <p align=center><img src=images/Spark_home.gif></p>\n",
    "\n",
    "  If this still doesn't work\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a268cc",
   "metadata": {},
   "source": [
    "### Installing Java\n",
    "\n",
    "Since Spark is dependant on a JVM(Java Virtual Machine) we will also need to make sure Java is installed. Note that Spark runs on Java 8/11 currently Java 17 isn't supported. \n",
    "\n",
    "First check whether you currently have Java install by running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e734f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"11.0.14\" 2022-01-18 LTS\n",
      "Java(TM) SE Runtime Environment 18.9 (build 11.0.14+8-LTS-263)\n",
      "Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.14+8-LTS-263, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "## Check the version of Java running on your machine\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c28309",
   "metadata": {},
   "source": [
    "If you have Java installed you will get an output similar to this:\n",
    "If your output says you're running Java 1.8.0 then it's ok and you should be able to run Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be71b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current version of Java installed\n",
    "openjdk 11.0.8 2020-07-14\n",
    "OpenJDK Runtime Environment (build 11.0.8+10-Ubuntu-1ubuntu0.18.04.4)\n",
    "OpenJDK 64-Bit Server VM (build 11.0.8+10-Ubuntu-1ubuntu0.18.04.4, mixed mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d3e8b",
   "metadata": {},
   "source": [
    "If you aren't receiving any output from the command then you can install Java 11 using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install openjdk-11-jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed615e5",
   "metadata": {},
   "source": [
    "Lastly we need need to set the environment variable `JAVA_HOME` in our `~/.bashrc` file and add it to our system `PATH`. This will allow Spark to find Java and run it's code in the JVM.\n",
    "\n",
    "Open you `~/.bashrc` file with the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83107ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febeedc9",
   "metadata": {},
   "source": [
    "Add the lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03112c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory where java is found\n",
    "# Edit this as required if you have Java 8 installed\n",
    "export JAVA_HOME=\"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# Add the directory to the system PATH \n",
    "export PATH=$PATH:$JAVA_HOME/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b0799",
   "metadata": {},
   "source": [
    "Exit and save the your bashrc file. Running `java -version` again to check you're getting the desired output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114af81",
   "metadata": {},
   "source": [
    "## findspark\n",
    "\n",
    "The Spark functionalities might not be discoverable within a script or a notebook, so you can use `findspark` which will set the script or notebook to keep using Spark interactively. Remember that:\n",
    "\n",
    "1. Inside the script you are going to define the instructions\n",
    "2. Those instructions will be orchestrated amongst the executors using Spark\n",
    "3. PySpark will be the API that helps you write in Python the instructions. Then, those instructions will be translated, so Spark actually understands it\n",
    "\n",
    "Thus, you will create the script using PySpark, and then, you will send that script to Spark, usually using spark-submit, which we will see later in this notebook.\n",
    "\n",
    "`findspark` will be useful when you are developing your application, to check if spark will respond the way you expect while you are writing your code.\n",
    "\n",
    "- Run `findspark.init()` (which will set up necessary environment variables so `pyspark` can connect to JVM)\n",
    "- You can also tun `findspark.find()` to see the directory where `SPARK_HOME` has been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e915bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ca5fa",
   "metadata": {},
   "source": [
    "## Spark config\n",
    "\n",
    "Given all of the steps above, we can set up Spark's distributed processing engine using:\n",
    "- A programmatic interface (`pyspark` in our case) - usable for application specific tasks and varying configuration\n",
    "- Command line - usable for `spark-submit` and __overriding default values__\n",
    "- Config file - usable as a base config and __when we submit job to the cluster__\n",
    "- Global config file\n",
    "\n",
    "> Above is also a priority list and the config for each overides the config from the ones below it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2254d4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1g\n",
      "spark.master=local[12]\n",
      "spark.app.name=TestApp\n",
      "spark.eventLog.enabled=False\n",
      "spark.executorEnv.VAR3=value3\n",
      "spark.executorEnv.VAR4=value4\n",
      "spark.executor.memory=1g\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import pyspark\n",
    "\n",
    "cfg = (\n",
    "    pyspark.SparkConf()\n",
    "    # Setting the master to run locally and with the maximum amount of cpu coresfor multiprocessing.\n",
    "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
    "    # Setting application name\n",
    "    .setAppName(\"TestApp\")\n",
    "    # Setting config value via string\n",
    "    .set(\"spark.eventLog.enabled\", False)\n",
    "    # Setting environment variables for executors to use\n",
    "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
    "    # Setting memory if this setting was not set previously\n",
    "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
    ")\n",
    "\n",
    "# Getting a single variable\n",
    "print(cfg.get(\"spark.executor.memory\"))\n",
    "# Listing all of them in string readable format\n",
    "print(cfg.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f8165",
   "metadata": {},
   "source": [
    "# Sessions\n",
    "\n",
    "> PySpark's session object provides a unified connection to our Spark cluster.\n",
    "\n",
    "There are a few ways to set up the Spark session:\n",
    "- directly through named/unnamed arguments\n",
    "- using `SparkConf` object (which we created and will use)\n",
    "- Providing `SparkContext` with settings (this is deprecated, avoid doing this)\n",
    "\n",
    "The Spark session is used to:\n",
    "- create `DataFrame`s (the main object containing data within cluster)\n",
    "- broadcast variables to machines within the cluster\n",
    "- Run operations across HDFS enabled systems\n",
    "\n",
    "Spark and `pyspark` provide a few objects that can be used to interact with the Spark engine:\n",
    "- `pyspark.SparkContext`\n",
    "- `org.apache.spark.sql.SQLContext` (Only for Scala)\n",
    "- `org.apache.spark.sql.hive.HiveContext` (Only Scala)\n",
    "- `pyspark.sql.SparkContext`\n",
    "\n",
    "What are these different options and why do they exist?\n",
    "\n",
    "### SparkContext\n",
    "\n",
    "> `SparkContext` is the object used by any driver to communicate with the cluster manager, execute and coordinate jobs\n",
    "\n",
    "This object is always used under the hood, if not directly, to interact with the cluster. Direct use of the Spark context is deprecated and should be avoided.\n",
    "\n",
    "### SQLContext\n",
    "\n",
    "Previously, you had to provide `SparkContext` to this object in order to interact with SQL-like capabilities (e.g. creating a `DataFrame`) using the `SparkSQL` library\n",
    "\n",
    "### HiveContext\n",
    "\n",
    "> __Extension of SQLContext providing gateway to Hive__\n",
    "\n",
    "Hive is similar in structure to SQL but provides capabilities for data warehousing and is better suited for analyzing large scale data\n",
    "\n",
    "## SparkSession\n",
    "\n",
    "In Spark `v2.0` one object to rule them all was introduced. That was `spark.SparkSession`. It wraps functionalities of all of the contexts introduced above (SparkContext, SQLContext, HiveContext) into one API.\n",
    "\n",
    "In `pyspark` one can use it via `spark.sql.SparkSession`. \n",
    "\n",
    "The the `builder` attribute has methods to obtain the appropriate `SparkSession`.\n",
    "\n",
    "It's config method can be used to firstly set the config.\n",
    "\n",
    "The `getOrCreate` method does the following:\n",
    "- If no global `Session` exists create a new one with specified config\n",
    "- If global `Session` exists:\n",
    "    - Get an instance of it\n",
    "    - Apply the new configuration to it\n",
    "\n",
    "This approach is safe as using multiple context is a bad practice (although possible)\n",
    "\n",
    "\n",
    "This SparkSession can be used just like the other context objects were historically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d90bd58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:34:35.447266Z",
     "start_time": "2021-07-10T14:34:35.246044Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/09 00:12:02 WARN Utils: Your hostname, Harrys-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.12 instead (on interface en0)\n",
      "22/03/09 00:12:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/09 00:12:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x1796cf6c) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x1796cf6c\n\tat org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\n\tat org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala)\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_10189/1423094776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tools/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/spark-3.2.1-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/spark-3.2.1-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[1;32m    147\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/spark-3.2.1-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/spark-3.2.1-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1586\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x1796cf6c) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x1796cf6c\n\tat org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\n\tat org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala)\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa53f8b",
   "metadata": {},
   "source": [
    "# Data Structures\n",
    "\n",
    "Before diving in we need to talk about `3` available data structures in `spark`:\n",
    "- `RDD` - Resilient Distributed Dataset - fault-tolerant collection of elements that can be operated on in parallel\n",
    "- `DataFrame` -  dataset organised into named columns. Conceptually equivalent to a table in a relational database or a dataframe in R/Python, but with richer optimisations under the hood.\n",
    "- `Dataset` - distributed collection of data. Provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine\n",
    "\n",
    "![](./images/rdd_df_dataset_history.png)\n",
    "\n",
    "# RDD & Core Spark API\n",
    "\n",
    "> __Core and basic of Spark applications with \"low-level\" operations__\n",
    "\n",
    "> __Fault-tolerant collection of elements that can be operated on in parallel.__\n",
    "\n",
    "This structure provides strong typing (via `JVM` objects) and can be constructued in two ways:\n",
    "- __parallelizing existing collection__ (e.g. Python's `list`)\n",
    "- __referencing dataset in external storage__ (anything compatible with Hadoop's InputFormat like HDFS, HBase, Amazon S3, text files etc.)\n",
    "\n",
    "Let's see these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bdde61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "['Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quam lacus suspendisse faucibus interdum posuere. Dui accumsan sit amet nulla facilisi morbi tempus. Lobortis scelerisque fermentum dui faucibus in ornare quam viverra. In ornare quam viverra orci sagittis eu. Urna duis convallis convallis tellus id interdum. A erat nam at lectus. Eget lorem dolor sed viverra. Scelerisque eu ultrices vitae auctor eu augue ut. Orci porta non pulvinar neque laoreet suspendisse interdum consectetur libero. Egestas tellus rutrum tellus pellentesque eu tincidunt tortor aliquam. Vivamus at augue eget arcu. Id leo in vitae turpis. Rutrum tellus pellentesque eu tincidunt tortor aliquam nulla.', '', 'Mauris a diam maecenas sed enim ut. Eget duis at tellus at urna. Dui ut ornare lectus sit amet est placerat in egestas. Non curabitur gravida arcu ac tortor dignissim convallis. Proin libero nunc consequat interdum varius sit. Sit amet commodo nulla facilisi nullam vehicula ipsum a. Eleifend quam adipiscing vitae proin sagittis nisl rhoncus. Elementum pulvinar etiam non quam lacus. Cursus eget nunc scelerisque viverra mauris in aliquam. Senectus et netus et malesuada fames ac turpis egestas. Elementum pulvinar etiam non quam lacus suspendisse. Massa eget egestas purus viverra accumsan in. Non sodales neque sodales ut etiam sit amet. Massa vitae tortor condimentum lacinia quis vel eros donec.', '', 'Magna sit amet purus gravida quis blandit. Mattis ullamcorper velit sed ullamcorper morbi tincidunt ornare massa eget. Morbi tristique senectus et netus et malesuada fames ac turpis. Laoreet suspendisse interdum consectetur libero id faucibus nisl tincidunt. Cursus risus at ultrices mi tempus. Fames ac turpis egestas maecenas pharetra convallis posuere. Ultrices eros in cursus turpis massa tincidunt dui ut. Eget egestas purus viverra accumsan in. Id aliquet lectus proin nibh nisl condimentum. Ipsum suspendisse ultrices gravida dictum fusce. Tempus quam pellentesque nec nam. Parturient montes nascetur ridiculus mus mauris vitae ultricies. Egestas dui id ornare arcu odio ut. Amet nulla facilisi morbi tempus iaculis urna id volutpat.', '', 'Eu augue ut lectus arcu bibendum at varius vel pharetra. Vitae proin sagittis nisl rhoncus mattis rhoncus urna. Lorem ipsum dolor sit amet consectetur adipiscing elit pellentesque. In hendrerit gravida rutrum quisque non tellus orci ac auctor. Vel orci porta non pulvinar. Dolor sed viverra ipsum nunc aliquet. Tortor consequat id porta nibh venenatis cras sed felis. Nisl pretium fusce id velit ut tortor pretium viverra. Nisi porta lorem mollis aliquam. Ornare massa eget egestas purus viverra accumsan in nisl. Suspendisse sed nisi lacus sed viverra tellus in hac. Ultrices eros in cursus turpis massa tincidunt dui. Morbi enim nunc faucibus a pellentesque sit. Imperdiet dui accumsan sit amet nulla facilisi morbi tempus iaculis. Arcu cursus euismod quis viverra nibh cras pulvinar mattis nunc. Viverra aliquet eget sit amet tellus cras.', '', 'Pellentesque diam volutpat commodo sed egestas. Vel elit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique. Arcu risus quis varius quam quisque id diam vel quam. Sed risus ultricies tristique nulla aliquet enim tortor at auctor. Ultrices sagittis orci a scelerisque. Sit amet nisl suscipit adipiscing bibendum est ultricies integer. Magna fringilla urna porttitor rhoncus dolor purus non enim. Enim diam vulputate ut pharetra sit. Sapien et ligula ullamcorper malesuada proin. Ullamcorper malesuada proin libero nunc consequat interdum varius. Lobortis scelerisque fermentum dui faucibus in ornare quam. Enim nec dui nunc mattis enim ut. Condimentum mattis pellentesque id nibh tortor id aliquet. Morbi tristique senectus et netus et malesuada fames ac. Sit amet est placerat in egestas. Diam maecenas sed enim ut sem viverra aliquet eget.', '', 'Ac turpis egestas maecenas pharetra convallis posuere morbi. Viverra orci sagittis eu volutpat odio facilisis mauris. Sed risus ultricies tristique nulla aliquet enim tortor. Ac tincidunt vitae semper quis lectus nulla at volutpat diam. Dui vivamus arcu felis bibendum ut. Dignissim sodales ut eu sem integer vitae. Sit amet volutpat consequat mauris nunc. Integer quis auctor elit sed vulputate mi sit. Lobortis feugiat vivamus at augue eget. Tellus at urna condimentum mattis pellentesque id. Pellentesque eu tincidunt tortor aliquam nulla facilisi cras. Nibh sed pulvinar proin gravida hendrerit lectus a. Viverra ipsum nunc aliquet bibendum enim facilisis gravida neque convallis. Elementum eu facilisis sed odio. Adipiscing elit duis tristique sollicitudin nibh. Cras sed felis eget velit aliquet sagittis id consectetur. Aliquet porttitor lacus luctus accumsan tortor. Diam sit amet nisl suscipit adipiscing bibendum est ultricies integer.', '', 'Amet consectetur adipiscing elit ut aliquam. Amet nisl suscipit adipiscing bibendum est ultricies integer. Consequat semper viverra nam libero justo laoreet. Aliquet bibendum enim facilisis gravida neque convallis. Posuere morbi leo urna molestie. Tempor id eu nisl nunc mi. Velit scelerisque in dictum non consectetur a. Aliquet lectus proin nibh nisl condimentum id venenatis a. Sem viverra aliquet eget sit amet tellus cras. Congue mauris rhoncus aenean vel elit scelerisque mauris pellentesque. Massa enim nec dui nunc. Enim ut tellus elementum sagittis. Quam lacus suspendisse faucibus interdum posuere lorem ipsum. Tempor nec feugiat nisl pretium. Sit amet cursus sit amet. Tellus cras adipiscing enim eu. Sollicitudin tempor id eu nisl. Enim nulla aliquet porttitor lacus luctus.', '', 'Est ullamcorper eget nulla facilisi. Ut enim blandit volutpat maecenas volutpat blandit aliquam etiam erat. Donec pretium vulputate sapien nec sagittis aliquam malesuada. Sit amet nisl suscipit adipiscing bibendum. Ut sem nulla pharetra diam sit amet nisl suscipit adipiscing. Massa eget egestas purus viverra accumsan in. Vitae purus faucibus ornare suspendisse sed. Elementum tempus egestas sed sed risus pretium quam vulputate dignissim. Leo urna molestie at elementum. Risus nullam eget felis eget nunc lobortis mattis aliquam faucibus. Diam phasellus vestibulum lorem sed risus ultricies tristique nulla aliquet.', '', 'Etiam dignissim diam quis enim lobortis scelerisque. Sit amet consectetur adipiscing elit ut aliquam. Vestibulum mattis ullamcorper velit sed ullamcorper. Sagittis nisl rhoncus mattis rhoncus urna neque viverra. Maecenas pharetra convallis posuere morbi. Mauris pharetra et ultrices neque ornare aenean euismod elementum nisi. Sapien eget mi proin sed. Convallis a cras semper auctor neque. Proin fermentum leo vel orci porta. Sagittis aliquam malesuada bibendum arcu vitae elementum curabitur vitae nunc.', '', 'Aenean euismod elementum nisi quis eleifend quam adipiscing vitae. Porttitor massa id neque aliquam vestibulum morbi blandit cursus risus. Integer vitae justo eget magna fermentum iaculis. Elit at imperdiet dui accumsan sit amet nulla facilisi morbi. Leo urna molestie at elementum. Est ante in nibh mauris cursus mattis. Id cursus metus aliquam eleifend mi in nulla posuere. Bibendum ut tristique et egestas quis ipsum. At ultrices mi tempus imperdiet nulla malesuada pellentesque elit eget. Fusce ut placerat orci nulla pellentesque dignissim enim sit amet. Eu non diam phasellus vestibulum lorem sed. Proin nibh nisl condimentum id venenatis a condimentum. Tristique et egestas quis ipsum suspendisse. Lacus vestibulum sed arcu non odio euismod lacinia. Pretium vulputate sapien nec sagittis aliquam malesuada bibendum arcu. Sit amet massa vitae tortor. Tellus id interdum velit laoreet id donec ultrices tincidunt arcu. Feugiat in fermentum posuere urna nec tincidunt praesent semper feugiat. Integer feugiat scelerisque varius morbi.', '', 'Ut morbi tincidunt augue interdum velit euismod in pellentesque. Nibh mauris cursus mattis molestie a iaculis. Dictum sit amet justo donec enim diam vulputate ut. Et malesuada fames ac turpis egestas integer eget aliquet. Donec massa sapien faucibus et molestie ac feugiat. Diam vel quam elementum pulvinar etiam non quam. Malesuada nunc vel risus commodo viverra maecenas accumsan. Integer quis auctor elit sed vulputate mi sit amet. Tristique senectus et netus et malesuada fames ac turpis. Faucibus vitae aliquet nec ullamcorper sit amet risus nullam eget. Arcu non odio euismod lacinia at quis. Est ultricies integer quis auctor elit sed vulputate mi sit. Eget mauris pharetra et ultrices neque ornare aenean. Consequat semper viverra nam libero justo laoreet sit amet cursus. Pharetra sit amet aliquam id diam maecenas ultricies mi eget.', '', 'Vulputate ut pharetra sit amet aliquam id diam maecenas ultricies. Vel turpis nunc eget lorem dolor sed viverra ipsum. Tellus orci ac auctor augue mauris augue neque. Sed enim ut sem viverra aliquet eget. Augue eget arcu dictum varius duis at consectetur lorem donec. Nec ullamcorper sit amet risus nullam eget felis eget nunc. In mollis nunc sed id semper risus in hendrerit. Diam quam nulla porttitor massa id. Sed libero enim sed faucibus turpis in eu. Libero justo laoreet sit amet cursus sit amet dictum. Rutrum quisque non tellus orci. Diam sollicitudin tempor id eu nisl nunc mi ipsum. Sagittis id consectetur purus ut faucibus pulvinar elementum integer enim. Ac orci phasellus egestas tellus rutrum. Viverra tellus in hac habitasse platea dictumst vestibulum.', '', 'Eget nullam non nisi est sit. Ante metus dictum at tempor commodo ullamcorper a lacus. Fermentum dui faucibus in ornare. Pellentesque massa placerat duis ultricies lacus sed. Mi bibendum neque egestas congue quisque egestas. At imperdiet dui accumsan sit amet nulla facilisi morbi. Et odio pellentesque diam volutpat. Id velit ut tortor pretium viverra suspendisse potenti. Ultrices dui sapien eget mi proin sed. Pretium nibh ipsum consequat nisl vel pretium. Nunc lobortis mattis aliquam faucibus purus. Morbi leo urna molestie at elementum eu facilisis. Molestie nunc non blandit massa enim nec dui nunc. Tincidunt praesent semper feugiat nibh. Adipiscing elit ut aliquam purus sit amet luctus venenatis. Volutpat consequat mauris nunc congue nisi vitae suscipit tellus. Ac turpis egestas sed tempus urna. Ornare arcu odio ut sem nulla pharetra diam sit amet.', '', 'Turpis tincidunt id aliquet risus feugiat in ante metus dictum. Cursus mattis molestie a iaculis at erat. Penatibus et magnis dis parturient montes nascetur ridiculus. Tincidunt praesent semper feugiat nibh sed pulvinar proin. Nisi porta lorem mollis aliquam. Commodo ullamcorper a lacus vestibulum sed arcu non. Purus ut faucibus pulvinar elementum integer enim neque volutpat ac. Ac auctor augue mauris augue neque gravida in fermentum. Donec et odio pellentesque diam. Id consectetur purus ut faucibus pulvinar. Venenatis urna cursus eget nunc scelerisque viverra mauris in.', '', 'Lectus magna fringilla urna porttitor. Vivamus arcu felis bibendum ut. Egestas fringilla phasellus faucibus scelerisque eleifend donec pretium vulputate. Vitae justo eget magna fermentum iaculis eu non. Enim ut sem viverra aliquet eget sit. Risus nec feugiat in fermentum posuere urna nec tincidunt. Nec sagittis aliquam malesuada bibendum arcu vitae elementum curabitur vitae. Facilisis volutpat est velit egestas dui id ornare. Mi tempus imperdiet nulla malesuada pellentesque elit. Morbi tristique senectus et netus et. Auctor urna nunc id cursus metus aliquam. Nunc scelerisque viverra mauris in aliquam sem fringilla ut morbi. Egestas sed sed risus pretium quam.', '', 'Sit amet est placerat in. Tellus elementum sagittis vitae et. Vitae turpis massa sed elementum tempus egestas sed sed risus. Egestas maecenas pharetra convallis posuere morbi leo urna molestie. Euismod elementum nisi quis eleifend quam. At erat pellentesque adipiscing commodo elit at. Viverra orci sagittis eu volutpat odio facilisis mauris. Mattis molestie a iaculis at erat pellentesque. Tellus at urna condimentum mattis pellentesque id nibh tortor. Nibh praesent tristique magna sit amet purus gravida quis. Dui faucibus in ornare quam viverra orci. Mattis vulputate enim nulla aliquet porttitor lacus. Pretium lectus quam id leo in vitae turpis massa sed. Aenean et tortor at risus viverra adipiscing at in. Sed augue lacus viverra vitae congue eu. In cursus turpis massa tincidunt dui. Dictum fusce ut placerat orci nulla.', '', 'Justo eget magna fermentum iaculis eu non diam phasellus. Curabitur gravida arcu ac tortor dignissim convallis aenean et. Ac felis donec et odio pellentesque diam volutpat commodo sed. Mauris pellentesque pulvinar pellentesque habitant morbi tristique senectus. Magna etiam tempor orci eu. Viverra suspendisse potenti nullam ac tortor vitae purus. Enim diam vulputate ut pharetra sit amet aliquam id. Neque egestas congue quisque egestas diam in arcu. In arcu cursus euismod quis viverra nibh cras pulvinar mattis. Eu turpis egestas pretium aenean pharetra magna ac placerat. Id leo in vitae turpis massa sed. A condimentum vitae sapien pellentesque. Porttitor leo a diam sollicitudin tempor id eu nisl. Mi proin sed libero enim sed faucibus turpis.', '', 'Malesuada pellentesque elit eget gravida cum sociis natoque. Integer vitae justo eget magna fermentum iaculis eu. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget. Id porta nibh venenatis cras sed. A pellentesque sit amet porttitor. Vulputate mi sit amet mauris commodo quis imperdiet massa tincidunt. Laoreet sit amet cursus sit amet dictum sit. Ut faucibus pulvinar elementum integer enim. Ultrices gravida dictum fusce ut. Nibh tellus molestie nunc non blandit massa enim nec dui. Odio morbi quis commodo odio aenean sed adipiscing. Vestibulum mattis ullamcorper velit sed ullamcorper. Vestibulum mattis ullamcorper velit sed. Nunc aliquet bibendum enim facilisis gravida neque convallis a. Sem integer vitae justo eget magna fermentum.', '', 'At tellus at urna condimentum mattis. Eget gravida cum sociis natoque. Tincidunt lobortis feugiat vivamus at augue eget arcu. Amet nulla facilisi morbi tempus iaculis urna id. Ullamcorper velit sed ullamcorper morbi tincidunt. Dictumst vestibulum rhoncus est pellentesque. Risus pretium quam vulputate dignissim. Dictumst vestibulum rhoncus est pellentesque elit ullamcorper dignissim cras. In metus vulputate eu scelerisque felis imperdiet proin fermentum. Eu lobortis elementum nibh tellus molestie.', '', 'Sapien pellentesque habitant morbi tristique senectus. Fusce id velit ut tortor pretium viverra suspendisse. Duis convallis convallis tellus id interdum. Blandit massa enim nec dui nunc mattis enim ut. Sit amet nulla facilisi morbi tempus iaculis. Egestas sed tempus urna et. Eget velit aliquet sagittis id. Lacus suspendisse faucibus interdum posuere lorem. Dui vivamus arcu felis bibendum ut tristique et. Scelerisque in dictum non consectetur a erat. Pretium fusce id velit ut tortor pretium viverra suspendisse. Massa eget egestas purus viverra accumsan. In egestas erat imperdiet sed euismod nisi porta.', '', 'Mattis pellentesque id nibh tortor id aliquet lectus. Sit amet mauris commodo quis imperdiet. Augue mauris augue neque gravida in fermentum. Quam elementum pulvinar etiam non quam lacus suspendisse faucibus. Aliquam ultrices sagittis orci a scelerisque purus. Orci sagittis eu volutpat odio. Cras adipiscing enim eu turpis egestas pretium aenean. Ridiculus mus mauris vitae ultricies leo integer malesuada nunc vel. A pellentesque sit amet porttitor. Porta non pulvinar neque laoreet suspendisse interdum. Id interdum velit laoreet id donec ultrices tincidunt. Lacus sed viverra tellus in. Laoreet sit amet cursus sit amet dictum. At augue eget arcu dictum varius duis. Aliquet lectus proin nibh nisl condimentum id venenatis a condimentum. Porta lorem mollis aliquam ut porttitor. Risus pretium quam vulputate dignissim suspendisse in. Eros in cursus turpis massa tincidunt dui.', '', 'Viverra accumsan in nisl nisi scelerisque eu ultrices vitae auctor. Quam elementum pulvinar etiam non quam lacus suspendisse faucibus interdum. Tellus in metus vulputate eu scelerisque felis imperdiet. Feugiat vivamus at augue eget arcu dictum varius. Sed egestas egestas fringilla phasellus. Dictum at tempor commodo ullamcorper a lacus vestibulum. Ipsum dolor sit amet consectetur. Amet commodo nulla facilisi nullam vehicula ipsum. Nunc sed id semper risus in hendrerit gravida. Enim ut tellus elementum sagittis vitae et leo duis ut. Aliquet enim tortor at auctor urna nunc id cursus. Blandit massa enim nec dui nunc mattis enim ut tellus. Eget magna fermentum iaculis eu non diam. At risus viverra adipiscing at. Morbi tincidunt augue interdum velit euismod. Tempor commodo ullamcorper a lacus vestibulum sed arcu non. Suscipit tellus mauris a diam maecenas sed. Est pellentesque elit ullamcorper dignissim.', '', 'Accumsan in nisl nisi scelerisque. Scelerisque felis imperdiet proin fermentum leo vel orci porta. At elementum eu facilisis sed odio morbi quis. Lacus sed turpis tincidunt id aliquet risus feugiat in. Cursus mattis molestie a iaculis at erat pellentesque. Mattis nunc sed blandit libero volutpat. Blandit aliquam etiam erat velit scelerisque in dictum non. Turpis massa tincidunt dui ut ornare lectus. Pulvinar elementum integer enim neque volutpat ac tincidunt vitae semper. Eros donec ac odio tempor orci dapibus ultrices. Diam volutpat commodo sed egestas egestas fringilla. Pretium quam vulputate dignissim suspendisse in est ante. Eros donec ac odio tempor orci dapibus ultrices in iaculis. Ipsum consequat nisl vel pretium lectus quam id. Luctus venenatis lectus magna fringilla urna porttitor. Volutpat lacus laoreet non curabitur gravida. Ipsum faucibus vitae aliquet nec ullamcorper.', '', 'Ut enim blandit volutpat maecenas volutpat blandit aliquam etiam erat. Pellentesque massa placerat duis ultricies lacus sed turpis. Nisl rhoncus mattis rhoncus urna. Quis eleifend quam adipiscing vitae proin sagittis nisl rhoncus. Bibendum enim facilisis gravida neque convallis. Ultricies integer quis auctor elit sed vulputate mi sit amet. Lorem ipsum dolor sit amet consectetur adipiscing elit. Placerat vestibulum lectus mauris ultrices. Eu augue ut lectus arcu. Adipiscing elit pellentesque habitant morbi tristique senectus et. Mattis molestie a iaculis at erat. Cursus turpis massa tincidunt dui ut ornare lectus. In cursus turpis massa tincidunt dui ut ornare. Turpis massa sed elementum tempus egestas sed sed risus.', '', 'Potenti nullam ac tortor vitae purus faucibus ornare. Aliquet lectus proin nibh nisl condimentum id venenatis. At risus viverra adipiscing at in tellus integer feugiat. Pharetra sit amet aliquam id diam maecenas. Cursus metus aliquam eleifend mi. Nunc vel risus commodo viverra maecenas. Sagittis orci a scelerisque purus semper eget duis at. Ultrices gravida dictum fusce ut placerat. Vitae purus faucibus ornare suspendisse sed nisi lacus sed viverra. Auctor elit sed vulputate mi sit amet mauris. Enim diam vulputate ut pharetra. Eget egestas purus viverra accumsan. Diam quis enim lobortis scelerisque fermentum dui faucibus. Amet nisl suscipit adipiscing bibendum est ultricies integer. Neque gravida in fermentum et sollicitudin ac orci.', '', 'Lobortis scelerisque fermentum dui faucibus. Scelerisque purus semper eget duis at. Vitae ultricies leo integer malesuada nunc vel risus. Diam phasellus vestibulum lorem sed. Et sollicitudin ac orci phasellus egestas tellus rutrum tellus pellentesque. Ultrices in iaculis nunc sed augue lacus viverra vitae congue. Dapibus ultrices in iaculis nunc sed augue lacus viverra. Aenean pharetra magna ac placerat vestibulum lectus mauris. Risus at ultrices mi tempus imperdiet nulla malesuada. Elit pellentesque habitant morbi tristique senectus. Feugiat sed lectus vestibulum mattis. Commodo sed egestas egestas fringilla. Eu ultrices vitae auctor eu augue ut lectus arcu bibendum. Pulvinar neque laoreet suspendisse interdum. Elit ullamcorper dignissim cras tincidunt. Nisi est sit amet facilisis. Metus dictum at tempor commodo ullamcorper a lacus vestibulum. Tincidunt lobortis feugiat vivamus at augue.', '', 'Adipiscing bibendum est ultricies integer quis auctor elit. Etiam non quam lacus suspendisse. Amet aliquam id diam maecenas ultricies mi eget mauris pharetra. Aliquam eleifend mi in nulla posuere sollicitudin aliquam. Bibendum ut tristique et egestas. Vulputate eu scelerisque felis imperdiet proin fermentum leo vel orci. Amet tellus cras adipiscing enim eu turpis egestas pretium. In fermentum posuere urna nec tincidunt. Velit laoreet id donec ultrices tincidunt arcu non sodales neque. Non enim praesent elementum facilisis leo vel fringilla est. Sagittis purus sit amet volutpat. Commodo odio aenean sed adipiscing diam donec adipiscing. Aliquet enim tortor at auctor urna. Nulla pellentesque dignissim enim sit amet venenatis urna cursus. Aenean vel elit scelerisque mauris. Sed arcu non odio euismod lacinia at. Sit amet venenatis urna cursus eget. Nisl vel pretium lectus quam id leo in vitae. Mi quis hendrerit dolor magna eget.', '', 'Pretium fusce id velit ut tortor pretium viverra suspendisse. Ut tristique et egestas quis ipsum suspendisse ultrices gravida. Tempus egestas sed sed risus pretium quam. Pellentesque habitant morbi tristique senectus et netus et. Mattis enim ut tellus elementum sagittis vitae et. Ac orci phasellus egestas tellus rutrum. In mollis nunc sed id semper risus. Condimentum lacinia quis vel eros donec ac odio tempor orci. Purus in mollis nunc sed id. Diam quam nulla porttitor massa. Nam libero justo laoreet sit amet. Ut tortor pretium viverra suspendisse. Habitant morbi tristique senectus et netus et malesuada. Enim blandit volutpat maecenas volutpat. Venenatis a condimentum vitae sapien pellentesque habitant morbi. Elit pellentesque habitant morbi tristique senectus et netus. Quam viverra orci sagittis eu volutpat odio facilisis mauris sit.', '', 'Viverra adipiscing at in tellus integer feugiat scelerisque varius morbi. Tristique senectus et netus et malesuada fames. Est ultricies integer quis auctor elit sed. Egestas fringilla phasellus faucibus scelerisque eleifend donec pretium. Faucibus pulvinar elementum integer enim neque. Odio ut enim blandit volutpat maecenas volutpat blandit. Ut ornare lectus sit amet est. Elit sed vulputate mi sit amet mauris. Tristique senectus et netus et. Adipiscing enim eu turpis egestas pretium aenean pharetra. Convallis a cras semper auctor neque vitae. Congue quisque egestas diam in arcu cursus euismod quis. Scelerisque eu ultrices vitae auctor eu augue ut lectus arcu. Viverra nibh cras pulvinar mattis.', '', 'Vestibulum mattis ullamcorper velit sed ullamcorper morbi tincidunt ornare. Adipiscing elit pellentesque habitant morbi tristique senectus et netus et. Orci nulla pellentesque dignissim enim sit amet. Lectus urna duis convallis convallis tellus. At ultrices mi tempus imperdiet nulla malesuada pellentesque. At augue eget arcu dictum varius duis. Sem et tortor consequat id porta nibh. Pretium vulputate sapien nec sagittis aliquam malesuada bibendum arcu. A lacus vestibulum sed arcu non odio euismod lacinia at. Enim praesent elementum facilisis leo vel. Elementum nibh tellus molestie nunc. Sed vulputate odio ut enim blandit volutpat. Imperdiet massa tincidunt nunc pulvinar. Faucibus in ornare quam viverra orci sagittis. At volutpat diam ut venenatis.', '', 'Aliquet nibh praesent tristique magna sit amet purus gravida quis. Mollis aliquam ut porttitor leo. Pharetra convallis posuere morbi leo. Ultricies mi quis hendrerit dolor magna eget est lorem. Pellentesque dignissim enim sit amet venenatis urna cursus. Vel facilisis volutpat est velit egestas dui. Faucibus interdum posuere lorem ipsum dolor sit. Id ornare arcu odio ut sem nulla pharetra diam. Nulla facilisi morbi tempus iaculis. Consectetur a erat nam at. Ut consequat semper viverra nam libero justo laoreet. Semper quis lectus nulla at volutpat diam ut venenatis. Tortor consequat id porta nibh. Diam quis enim lobortis scelerisque fermentum dui faucibus in ornare. Morbi tristique senectus et netus. At risus viverra adipiscing at. Eu feugiat pretium nibh ipsum consequat nisl vel pretium lectus. Pharetra vel turpis nunc eget lorem.']\n"
     ]
    }
   ],
   "source": [
    "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
    "# Use collect() to collect all elements from an RDD\n",
    "print(rddDistributedData.collect())\n",
    "print(rddDistributedFile.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c7744",
   "metadata": {},
   "source": [
    "__Things to note for files__:\n",
    "- __Each file has to be in the same path on each worker node!__ (in our case we are running locally hence this is fine)\n",
    "- All file-based methods operate on:\n",
    "    - directories - `textFile(\"/my/directory\")`\n",
    "    - wildcards - `textFile(\"/my/directory/*.txt\")`\n",
    "    - compressed files - `textFile(\"/my/directory/*.gz\")`\n",
    "- We can change number of partitions created for this file\n",
    "- See API [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.textFile.html#pyspark.SparkContext.textFile)\n",
    "\n",
    "> __Other ways to create RDD from file can be seen in [Spark Context API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#spark-context-apis), e.g. a way to create it from `pickle`__\n",
    "\n",
    "## Lazy Evaulation\n",
    "\n",
    "> Created RDDs __ARE NOT FILES__, they are merely a description of operation that __has to be run at some point__\n",
    "\n",
    "The operations we performed above were:\n",
    "- A Parallelize `list` operation\n",
    "- Read from text file `lorem.txt` operation.\n",
    "\n",
    "> All of the operations will be run when we __request an ACTION__\n",
    "\n",
    "Actions may include:\n",
    "- return number of lines in file (whole map-reduce went through)\n",
    "- sum the list and return the result\n",
    "\n",
    "## Persist\n",
    "\n",
    "> Persisting is used in order to speed-up computations (saving intermediate results in memory)\n",
    "\n",
    "If we run the line below it means:\n",
    "\n",
    "> Read data file and cache read contents in the memory (if possible)\n",
    "\n",
    "> __If we run \"action\" on the file it will use the cached data (faster) rather than loading data from disk once again!__\n",
    "\n",
    "Rule of thumb: \n",
    "\n",
    "> Use cache when the lineage (operations to run on certain RDD) of your RDD branches out or when an RDD is used multiple times like in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aff9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the operations return self\n",
    "# This allows us to chain operations (we will see it in the next cell)\n",
    "\n",
    "rddDistributedFile = rddDistributedFile.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614526c4",
   "metadata": {},
   "source": [
    "> __`.cache()` is the same as `.persist()` with `StorageLevel.MEMORY_ONLY`__\n",
    "\n",
    "There are few other options to store the data:\n",
    "- `MEMORY_ONLY` - keep everything we can in memory otherwise do not cache and compute results\n",
    "- `MEMORY_AND_DISK` - keep everything we can in memory otherwise serialize to disk (__encouraged for long running computations we would like to cache__)\n",
    "- `DISK_ONLY` - cache everything on disk, nothing in memory (__discouraged__)\n",
    "- `MEMORY_ONLY_2` - same as `MEMORY_ONLY` but replicates cache on two cluster nodes for improved fault tolerance (`DISK_ONLY_2` is also available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666b62a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, False, False, False, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.StorageLevel.DISK_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035bfe3",
   "metadata": {},
   "source": [
    "## MapReduce operations\n",
    "\n",
    "> Given parallelized data structure we can run map-reduce operations on it\n",
    "\n",
    "All of them can be seen [in the documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis), a few interesting ones:\n",
    "- [`rdd.checkpoint()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.checkpoint.html#pyspark.RDD.checkpoint) - will be saved in checkpoint directory and all the operations creating it __are discarded__ (action)\n",
    "- [`rdd.collect()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html#pyspark.RDD.collect) - __return the structure__ (collect it after operations) (action)\n",
    "- [`rdd.count()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html#pyspark.RDD.count) - count elements in the structure (action)\n",
    "- [`rdd.countByKey()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByKey.html#pyspark.RDD.countByKey) - count number of elements for each `key` in `(key, value)` pairs (similar to what the graphic before did)\n",
    "- [`rdd.countByValue()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByValue.html#pyspark.RDD.countByValue) - count __how many unique values__ are in this structure (returned as `(value, count)` dictionary)\n",
    "\n",
    "__And the essential ones we will use are:__\n",
    "- [`rdd.map(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html#pyspark.RDD.map) - apply function __to each element in the collection__\n",
    "- [`rdd.filter(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html#pyspark.RDD.filter) - __choose values which fulfill `f` function__\n",
    "- [`rdd.flatMap(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html#pyspark.RDD.flatMap) - __apply function to each element and `flatten` the list if necessary__\n",
    "- [`rdd.fold(neutralValue, f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.fold.html#pyspark.RDD.fold) - __given associative function (like `add`) takes every 2 elements together and returns the result__\n",
    "- [`rdd.sortBy(keyfunction)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html#pyspark.RDD.sortBy) - sort by specific function which returns some value from the `(key, value)` pair\n",
    "\n",
    "> __PLEASE REFER TO DOCUMENTATION WHEN LOOKING FOR AN OPERATOR! MANY OF THEM ARE ALREADY IMPLEMENTED!__\n",
    "\n",
    "> __TAKE TIME TO COME UP WITH THE OPERATORS NEEDED! EACH OPERATION SAVED MIGHT IMPROVE RUNTIME TREMENDOUSLY!__\n",
    "\n",
    "Let's see an example chaining on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3051cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc is standard name for sparkContext\n",
    "# it will be easier to use from now on\n",
    "\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63be368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "data = list(range(10,-11,-1))\n",
    "print(data)\n",
    "\n",
    "result = (\n",
    "    sc.parallelize(data)\n",
    "    .filter(lambda val: val % 3 == 0)\n",
    "    .map(operator.abs)\n",
    "    .fold(0, operator.add)\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60fbd933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([\"b\", \"a\", \"c\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18fd240d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Lorem': 3,\n",
       "             'ipsum': 11,\n",
       "             'dolor': 10,\n",
       "             'sit': 50,\n",
       "             'amet,': 1,\n",
       "             'consectetur': 12,\n",
       "             'adipiscing': 23,\n",
       "             'elit,': 1,\n",
       "             'sed': 66,\n",
       "             'do': 1,\n",
       "             'eiusmod': 1,\n",
       "             'tempor': 11,\n",
       "             'incididunt': 1,\n",
       "             'ut': 48,\n",
       "             'labore': 1,\n",
       "             'et': 34,\n",
       "             'dolore': 1,\n",
       "             'magna': 15,\n",
       "             'aliqua.': 1,\n",
       "             'Quam': 5,\n",
       "             'lacus': 21,\n",
       "             'suspendisse': 17,\n",
       "             'faucibus': 25,\n",
       "             'interdum': 12,\n",
       "             'posuere.': 3,\n",
       "             'Dui': 5,\n",
       "             'accumsan': 10,\n",
       "             'amet': 53,\n",
       "             'nulla': 31,\n",
       "             'facilisi': 11,\n",
       "             'morbi': 25,\n",
       "             'tempus.': 2,\n",
       "             'Lobortis': 4,\n",
       "             'scelerisque': 23,\n",
       "             'fermentum': 17,\n",
       "             'dui': 21,\n",
       "             'in': 48,\n",
       "             'ornare': 16,\n",
       "             'quam': 24,\n",
       "             'viverra.': 6,\n",
       "             'In': 10,\n",
       "             'viverra': 39,\n",
       "             'orci': 21,\n",
       "             'sagittis': 19,\n",
       "             'eu.': 6,\n",
       "             'Urna': 1,\n",
       "             'duis': 10,\n",
       "             'convallis': 13,\n",
       "             'tellus': 28,\n",
       "             'id': 47,\n",
       "             'interdum.': 5,\n",
       "             'A': 5,\n",
       "             'erat': 7,\n",
       "             'nam': 5,\n",
       "             'at': 36,\n",
       "             'lectus.': 5,\n",
       "             'Eget': 9,\n",
       "             'lorem': 11,\n",
       "             'Scelerisque': 5,\n",
       "             'eu': 30,\n",
       "             'ultrices': 18,\n",
       "             'vitae': 33,\n",
       "             'auctor': 15,\n",
       "             'augue': 21,\n",
       "             'ut.': 11,\n",
       "             'Orci': 3,\n",
       "             'porta': 8,\n",
       "             'non': 26,\n",
       "             'pulvinar': 18,\n",
       "             'neque': 18,\n",
       "             'laoreet': 10,\n",
       "             'libero.': 1,\n",
       "             'Egestas': 7,\n",
       "             'rutrum': 3,\n",
       "             'pellentesque': 33,\n",
       "             'tincidunt': 22,\n",
       "             'tortor': 21,\n",
       "             'aliquam.': 8,\n",
       "             'Vivamus': 2,\n",
       "             'eget': 36,\n",
       "             'arcu.': 8,\n",
       "             'Id': 9,\n",
       "             'leo': 16,\n",
       "             'turpis.': 5,\n",
       "             'Rutrum': 2,\n",
       "             'aliquam': 24,\n",
       "             'nulla.': 2,\n",
       "             'Mauris': 3,\n",
       "             'a': 24,\n",
       "             'diam': 25,\n",
       "             'maecenas': 14,\n",
       "             'enim': 45,\n",
       "             'urna.': 5,\n",
       "             'lectus': 20,\n",
       "             'est': 16,\n",
       "             'placerat': 8,\n",
       "             'egestas.': 6,\n",
       "             'Non': 3,\n",
       "             'curabitur': 4,\n",
       "             'gravida': 19,\n",
       "             'arcu': 24,\n",
       "             'ac': 20,\n",
       "             'dignissim': 12,\n",
       "             'convallis.': 4,\n",
       "             'Proin': 3,\n",
       "             'libero': 10,\n",
       "             'nunc': 30,\n",
       "             'consequat': 11,\n",
       "             'varius': 8,\n",
       "             'sit.': 10,\n",
       "             'Sit': 12,\n",
       "             'commodo': 15,\n",
       "             'nullam': 8,\n",
       "             'vehicula': 2,\n",
       "             'a.': 5,\n",
       "             'Eleifend': 1,\n",
       "             'proin': 15,\n",
       "             'nisl': 24,\n",
       "             'rhoncus.': 2,\n",
       "             'Elementum': 5,\n",
       "             'etiam': 10,\n",
       "             'lacus.': 3,\n",
       "             'Cursus': 6,\n",
       "             'mauris': 22,\n",
       "             'Senectus': 1,\n",
       "             'netus': 10,\n",
       "             'malesuada': 17,\n",
       "             'fames': 5,\n",
       "             'turpis': 22,\n",
       "             'suspendisse.': 7,\n",
       "             'Massa': 5,\n",
       "             'egestas': 37,\n",
       "             'purus': 19,\n",
       "             'in.': 9,\n",
       "             'sodales': 4,\n",
       "             'amet.': 8,\n",
       "             'condimentum': 10,\n",
       "             'lacinia': 5,\n",
       "             'quis': 27,\n",
       "             'vel': 19,\n",
       "             'eros': 4,\n",
       "             'donec.': 2,\n",
       "             'Magna': 3,\n",
       "             'blandit.': 2,\n",
       "             'Mattis': 7,\n",
       "             'ullamcorper': 20,\n",
       "             'velit': 20,\n",
       "             'massa': 25,\n",
       "             'eget.': 10,\n",
       "             'Morbi': 7,\n",
       "             'tristique': 22,\n",
       "             'senectus': 12,\n",
       "             'Laoreet': 3,\n",
       "             'tincidunt.': 7,\n",
       "             'risus': 20,\n",
       "             'mi': 19,\n",
       "             'Fames': 1,\n",
       "             'pharetra': 14,\n",
       "             'Ultrices': 7,\n",
       "             'cursus': 20,\n",
       "             'aliquet': 19,\n",
       "             'nibh': 20,\n",
       "             'condimentum.': 3,\n",
       "             'Ipsum': 4,\n",
       "             'dictum': 13,\n",
       "             'fusce.': 1,\n",
       "             'Tempus': 2,\n",
       "             'nec': 17,\n",
       "             'nam.': 1,\n",
       "             'Parturient': 1,\n",
       "             'montes': 2,\n",
       "             'nascetur': 2,\n",
       "             'ridiculus': 1,\n",
       "             'mus': 2,\n",
       "             'ultricies.': 2,\n",
       "             'odio': 20,\n",
       "             'Amet': 8,\n",
       "             'tempus': 14,\n",
       "             'iaculis': 12,\n",
       "             'urna': 26,\n",
       "             'volutpat.': 6,\n",
       "             'Eu': 7,\n",
       "             'bibendum': 17,\n",
       "             'pharetra.': 4,\n",
       "             'Vitae': 6,\n",
       "             'rhoncus': 10,\n",
       "             'mattis': 19,\n",
       "             'elit': 21,\n",
       "             'pellentesque.': 9,\n",
       "             'hendrerit': 5,\n",
       "             'quisque': 6,\n",
       "             'auctor.': 3,\n",
       "             'Vel': 4,\n",
       "             'pulvinar.': 3,\n",
       "             'Dolor': 1,\n",
       "             'aliquet.': 4,\n",
       "             'Tortor': 2,\n",
       "             'venenatis': 10,\n",
       "             'cras': 10,\n",
       "             'felis.': 1,\n",
       "             'Nisl': 3,\n",
       "             'pretium': 21,\n",
       "             'fusce': 6,\n",
       "             'Nisi': 3,\n",
       "             'mollis': 6,\n",
       "             'Ornare': 2,\n",
       "             'nisl.': 3,\n",
       "             'Suspendisse': 1,\n",
       "             'nisi': 9,\n",
       "             'hac.': 1,\n",
       "             'dui.': 5,\n",
       "             'Imperdiet': 2,\n",
       "             'iaculis.': 6,\n",
       "             'Arcu': 3,\n",
       "             'euismod': 11,\n",
       "             'nunc.': 7,\n",
       "             'Viverra': 9,\n",
       "             'cras.': 4,\n",
       "             'Pellentesque': 6,\n",
       "             'volutpat': 22,\n",
       "             'habitant': 9,\n",
       "             'tristique.': 1,\n",
       "             'quam.': 6,\n",
       "             'Sed': 8,\n",
       "             'ultricies': 16,\n",
       "             'scelerisque.': 3,\n",
       "             'suscipit': 7,\n",
       "             'integer.': 4,\n",
       "             'fringilla': 8,\n",
       "             'porttitor': 7,\n",
       "             'enim.': 3,\n",
       "             'Enim': 10,\n",
       "             'vulputate': 21,\n",
       "             'Sapien': 3,\n",
       "             'ligula': 1,\n",
       "             'proin.': 2,\n",
       "             'Ullamcorper': 2,\n",
       "             'varius.': 2,\n",
       "             'Condimentum': 2,\n",
       "             'ac.': 2,\n",
       "             'Diam': 11,\n",
       "             'sem': 8,\n",
       "             'Ac': 7,\n",
       "             'posuere': 11,\n",
       "             'morbi.': 8,\n",
       "             'facilisis': 12,\n",
       "             'mauris.': 6,\n",
       "             'tortor.': 4,\n",
       "             'semper': 14,\n",
       "             'diam.': 4,\n",
       "             'vivamus': 6,\n",
       "             'felis': 11,\n",
       "             'Dignissim': 1,\n",
       "             'integer': 16,\n",
       "             'vitae.': 5,\n",
       "             'Integer': 5,\n",
       "             'feugiat': 12,\n",
       "             'Tellus': 7,\n",
       "             'id.': 7,\n",
       "             'Nibh': 4,\n",
       "             'odio.': 2,\n",
       "             'Adipiscing': 6,\n",
       "             'sollicitudin': 6,\n",
       "             'nibh.': 4,\n",
       "             'Cras': 2,\n",
       "             'consectetur.': 2,\n",
       "             'Aliquet': 8,\n",
       "             'luctus': 2,\n",
       "             'Consequat': 2,\n",
       "             'justo': 10,\n",
       "             'laoreet.': 2,\n",
       "             'Posuere': 1,\n",
       "             'molestie.': 3,\n",
       "             'Tempor': 3,\n",
       "             'mi.': 2,\n",
       "             'Velit': 2,\n",
       "             'Sem': 3,\n",
       "             'Congue': 2,\n",
       "             'aenean': 7,\n",
       "             'elementum': 24,\n",
       "             'sagittis.': 2,\n",
       "             'ipsum.': 5,\n",
       "             'pretium.': 4,\n",
       "             'Sollicitudin': 1,\n",
       "             'luctus.': 1,\n",
       "             'Est': 5,\n",
       "             'facilisi.': 1,\n",
       "             'Ut': 9,\n",
       "             'blandit': 11,\n",
       "             'erat.': 5,\n",
       "             'Donec': 3,\n",
       "             'sapien': 7,\n",
       "             'malesuada.': 3,\n",
       "             'bibendum.': 2,\n",
       "             'adipiscing.': 3,\n",
       "             'sed.': 13,\n",
       "             'dignissim.': 3,\n",
       "             'Leo': 2,\n",
       "             'molestie': 11,\n",
       "             'elementum.': 2,\n",
       "             'Risus': 5,\n",
       "             'lobortis': 8,\n",
       "             'faucibus.': 4,\n",
       "             'phasellus': 8,\n",
       "             'vestibulum': 13,\n",
       "             'Etiam': 2,\n",
       "             'Vestibulum': 4,\n",
       "             'ullamcorper.': 3,\n",
       "             'Sagittis': 5,\n",
       "             'Maecenas': 1,\n",
       "             'nisi.': 1,\n",
       "             'Convallis': 2,\n",
       "             'neque.': 4,\n",
       "             'porta.': 3,\n",
       "             'Aenean': 4,\n",
       "             'eleifend': 8,\n",
       "             'Porttitor': 2,\n",
       "             'risus.': 5,\n",
       "             'Elit': 5,\n",
       "             'imperdiet': 11,\n",
       "             'ante': 2,\n",
       "             'mattis.': 5,\n",
       "             'metus': 7,\n",
       "             'Bibendum': 3,\n",
       "             'At': 12,\n",
       "             'Fusce': 2,\n",
       "             'Tristique': 4,\n",
       "             'Lacus': 4,\n",
       "             'lacinia.': 1,\n",
       "             'Pretium': 7,\n",
       "             'donec': 11,\n",
       "             'Feugiat': 3,\n",
       "             'praesent': 7,\n",
       "             'feugiat.': 3,\n",
       "             'Dictum': 3,\n",
       "             'Et': 3,\n",
       "             'Malesuada': 2,\n",
       "             'accumsan.': 3,\n",
       "             'Faucibus': 4,\n",
       "             'quis.': 5,\n",
       "             'aenean.': 2,\n",
       "             'cursus.': 4,\n",
       "             'Pharetra': 4,\n",
       "             'Vulputate': 3,\n",
       "             'Augue': 2,\n",
       "             'Nec': 2,\n",
       "             'hendrerit.': 1,\n",
       "             'Libero': 1,\n",
       "             'dictum.': 3,\n",
       "             'orci.': 5,\n",
       "             'rutrum.': 2,\n",
       "             'hac': 1,\n",
       "             'habitasse': 1,\n",
       "             'platea': 1,\n",
       "             'dictumst': 1,\n",
       "             'vestibulum.': 3,\n",
       "             'Ante': 1,\n",
       "             'Fermentum': 1,\n",
       "             'ornare.': 6,\n",
       "             'Mi': 4,\n",
       "             'congue': 4,\n",
       "             'potenti.': 1,\n",
       "             'Nunc': 5,\n",
       "             'purus.': 3,\n",
       "             'facilisis.': 2,\n",
       "             'Molestie': 1,\n",
       "             'Tincidunt': 4,\n",
       "             'venenatis.': 4,\n",
       "             'Volutpat': 2,\n",
       "             'tellus.': 3,\n",
       "             'Turpis': 3,\n",
       "             'Penatibus': 1,\n",
       "             'magnis': 1,\n",
       "             'dis': 1,\n",
       "             'parturient': 1,\n",
       "             'ridiculus.': 1,\n",
       "             'Commodo': 3,\n",
       "             'non.': 4,\n",
       "             'Purus': 2,\n",
       "             'fermentum.': 4,\n",
       "             'Venenatis': 2,\n",
       "             'Lectus': 2,\n",
       "             'porttitor.': 5,\n",
       "             'vulputate.': 1,\n",
       "             'Facilisis': 1,\n",
       "             'elit.': 3,\n",
       "             'et.': 10,\n",
       "             'Auctor': 2,\n",
       "             'Euismod': 1,\n",
       "             'at.': 8,\n",
       "             'Justo': 1,\n",
       "             'phasellus.': 2,\n",
       "             'Curabitur': 1,\n",
       "             'senectus.': 3,\n",
       "             'potenti': 1,\n",
       "             'Neque': 2,\n",
       "             'placerat.': 2,\n",
       "             'cum': 2,\n",
       "             'sociis': 2,\n",
       "             'natoque.': 2,\n",
       "             'Nulla': 3,\n",
       "             'Odio': 2,\n",
       "             'Dictumst': 2,\n",
       "             'Duis': 1,\n",
       "             'Blandit': 3,\n",
       "             'lorem.': 3,\n",
       "             'imperdiet.': 2,\n",
       "             'Aliquam': 2,\n",
       "             'Ridiculus': 1,\n",
       "             'vel.': 2,\n",
       "             'Porta': 2,\n",
       "             'duis.': 2,\n",
       "             'Eros': 3,\n",
       "             'gravida.': 3,\n",
       "             'euismod.': 1,\n",
       "             'Suscipit': 1,\n",
       "             'Accumsan': 1,\n",
       "             'Pulvinar': 2,\n",
       "             'semper.': 1,\n",
       "             'dapibus': 2,\n",
       "             'ultrices.': 2,\n",
       "             'fringilla.': 2,\n",
       "             'ante.': 1,\n",
       "             'Luctus': 1,\n",
       "             'Quis': 1,\n",
       "             'Ultricies': 2,\n",
       "             'Placerat': 1,\n",
       "             'Potenti': 1,\n",
       "             'maecenas.': 2,\n",
       "             'congue.': 1,\n",
       "             'Dapibus': 1,\n",
       "             'Metus': 1,\n",
       "             'augue.': 1,\n",
       "             'est.': 2,\n",
       "             'massa.': 1,\n",
       "             'Nam': 1,\n",
       "             'Habitant': 1,\n",
       "             'netus.': 2,\n",
       "             'fames.': 1,\n",
       "             'Mollis': 1,\n",
       "             'leo.': 2,\n",
       "             'Consectetur': 1,\n",
       "             'Semper': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddDistributedFile.flatMap(lambda text: text.split()).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069261a",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "## Dataset and DataFrame\n",
    "\n",
    "Dataset is a distributed collection of data which provides:\n",
    "- strong typing and powerful lambda functions from `RDD`\n",
    "- __allows for Spark SQL optimized execution engine__\n",
    "\n",
    "It can be created from JVM objects __and manipulated in the same functional manner__.\n",
    "\n",
    "> __`pyspark` has no Dataset API but many benefits of `Dataset` are available for `DataFrame`s DUE TO IT'S DYNAMIC NATURE__\n",
    "\n",
    "DataFrame shortcomings included:\n",
    "- No compile-time safety, hence __you cannot manipulate data of which structure is not specified__\n",
    "\n",
    "> DataFrame is a a  Dataset organised into named columns (__same as for `pd.DataFrame`__)\n",
    "\n",
    "From now on we will use `DataFrame`s (__not `Dataset`, also due to Python's community similarity with `pd.DataFrame`__) to keep our records.\n",
    "\n",
    "See [this discussion](https://stackoverflow.com/questions/31508083/difference-between-dataframe-dataset-and-rdd-in-spark) for an extended description.\n",
    "\n",
    "## Creating DataFrames\n",
    "\n",
    "> __For all of the operations we can use `SparkSession` directly to interact with the cluster!__\n",
    "\n",
    "There are a few options usable for us to read data residing on clusters (__for each node it has to be at the same location if reading from file!__):\n",
    "- [`session.createDataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html#pyspark.sql.SparkSession.createDataFrame) - create `pyspark.sql.DataFrame` from:\n",
    "    - `RDD`\n",
    "    - `list`\n",
    "    - `pandas.DataFrame`\n",
    "    - __Optionally: with `schema`__ which specifies datatypes and format for data contained within it. See documentation for more info.\n",
    "    - By default `schema` is inferred if possible\n",
    "- [`session.range`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.range.html#pyspark.sql.SparkSession.range) - works like Python's range but distributed and as a `spark.DataFrame`\n",
    "- [`session.sql(query)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql) - __return DataFrame which represents result of `sql` query__\n",
    "- [`session.read.{how_to_read}()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) - __returns `DataFrameReader` object__ which allows us to read `df` from:\n",
    "    - `json`\n",
    "    - `parquet`\n",
    "    - `csv`\n",
    "    - and many more\n",
    "- [`session.readStream`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.readStream.html#pyspark.sql.SparkSession.readStream) - __used for streaming, we will see it a little later__\n",
    "\n",
    "Let's see some code with `pyspark.sql.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d903104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "| 80| 82| 89| 45|\n",
      "| 34| 50| 87|  5|\n",
      "| 60| 79| 29| 74|\n",
      "| 66| 22| 14| 82|\n",
      "| 79| 66| 61| 44|\n",
      "| 63| 29| 28| 48|\n",
      "| 85| 36| 85| 30|\n",
      "| 49| 79| 34| 85|\n",
      "| 70| 93| 36| 20|\n",
      "| 72| 38| 96| 15|\n",
      "| 52| 59| 82| 79|\n",
      "| 83| 83| 35| 92|\n",
      "| 76|  0| 37| 59|\n",
      "|  4| 71| 94| 30|\n",
      "|  0| 57| 41| 90|\n",
      "| 12| 22| 64| 17|\n",
      "|  6| 68| 39| 26|\n",
      "| 12| 99| 71| 27|\n",
      "| 53| 74| 32| 15|\n",
      "| 92| 66| 71| 80|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = session.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        np.random.randint(0, 100, size=(100, 4)),\n",
    "        columns=list(\"ABCD\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e38c8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: long (nullable = true)\n",
      " |-- B: long (nullable = true)\n",
      " |-- C: long (nullable = true)\n",
      " |-- D: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a1468c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  A|\n",
      "+---+\n",
      "| 80|\n",
      "| 34|\n",
      "| 60|\n",
      "| 66|\n",
      "| 79|\n",
      "| 63|\n",
      "| 85|\n",
      "| 49|\n",
      "| 70|\n",
      "| 72|\n",
      "| 52|\n",
      "| 83|\n",
      "| 76|\n",
      "|  4|\n",
      "|  0|\n",
      "| 12|\n",
      "|  6|\n",
      "| 12|\n",
      "| 53|\n",
      "| 92|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show is an action, nothing would be returned without it\n",
    "# Just an operation representing what will happen\n",
    "df.select(\"A\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e40cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[A: bigint, (B + 1): bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df[\"A\"], df[\"B\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "785ba5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|  A|(B + 1)|\n",
      "+---+-------+\n",
      "| 80|     83|\n",
      "| 34|     51|\n",
      "| 60|     80|\n",
      "| 66|     23|\n",
      "| 79|     67|\n",
      "| 63|     30|\n",
      "| 85|     37|\n",
      "| 49|     80|\n",
      "| 70|     94|\n",
      "| 72|     39|\n",
      "| 52|     60|\n",
      "| 83|     84|\n",
      "| 76|      1|\n",
      "|  4|     72|\n",
      "|  0|     58|\n",
      "| 12|     23|\n",
      "|  6|     69|\n",
      "| 12|    100|\n",
      "| 53|     75|\n",
      "| 92|     67|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Increase column value by one\n",
    "# This operation is shown in the output\n",
    "\n",
    "df.select(df[\"A\"], df[\"B\"] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33de5702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  B|count|\n",
      "+---+-----+\n",
      "| 54|    2|\n",
      "| 22|    2|\n",
      "| 77|    2|\n",
      "| 34|    2|\n",
      "| 57|    2|\n",
      "| 43|    2|\n",
      "| 32|    2|\n",
      "| 95|    2|\n",
      "| 71|    2|\n",
      "| 87|    2|\n",
      "| 79|    3|\n",
      "|  1|    2|\n",
      "| 10|    2|\n",
      "| 67|    2|\n",
      "| 48|    2|\n",
      "| 37|    2|\n",
      "| 83|    2|\n",
      "| 55|    2|\n",
      "| 74|    2|\n",
      "| 49|    5|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counted = df.groupby(\"B\").count().persist()\n",
    "counted.filter(counted[\"count\"] > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81011b59",
   "metadata": {},
   "source": [
    "## Operations on DataFrame\n",
    "\n",
    "> __`pyspark.sql.DataFrame` supports most of the `pd.DataFrame` operations + the RDD ones__\n",
    "\n",
    "You can see the whole list [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)\n",
    "\n",
    "> __In general one can work with it similarly to how one works with `pd.DataFrame` objects__\n",
    "\n",
    "there are a few exceptions though...\n",
    "\n",
    "## Running SQL queries\n",
    "\n",
    "> In order to run SQL queries against the DataFrame __we have to register them as `TemporaryViews`__\n",
    "\n",
    "Properties of `TemporaryViews`:\n",
    "- __Session scoped__ - if session runs out of scope so will the views registered for it\n",
    "- One can set up `DataFrame` globally for any `SparkSession` by using `df.createGlobalTempView(\"name_of_database\")`\n",
    "\n",
    "After that, we can run SQL queries against __distributed data across nodes__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "420709b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "| 80| 82| 89| 45|\n",
      "| 34| 50| 87|  5|\n",
      "| 60| 79| 29| 74|\n",
      "| 66| 22| 14| 82|\n",
      "| 79| 66| 61| 44|\n",
      "| 63| 29| 28| 48|\n",
      "| 85| 36| 85| 30|\n",
      "| 49| 79| 34| 85|\n",
      "| 70| 93| 36| 20|\n",
      "| 72| 38| 96| 15|\n",
      "| 52| 59| 82| 79|\n",
      "| 83| 83| 35| 92|\n",
      "| 76|  0| 37| 59|\n",
      "|  4| 71| 94| 30|\n",
      "|  0| 57| 41| 90|\n",
      "| 12| 22| 64| 17|\n",
      "|  6| 68| 39| 26|\n",
      "| 12| 99| 71| 27|\n",
      "| 53| 74| 32| 15|\n",
      "| 92| 66| 71| 80|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"any_name\")\n",
    "\n",
    "# WE USE SESSION TO RUN QUERIES!\n",
    "sqlDf = session.sql(\"SELECT * FROM any_name\")\n",
    "sqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd8677",
   "metadata": {},
   "source": [
    "# Spark-Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673c112",
   "metadata": {},
   "source": [
    "The work you see in this notebook sent applications to a cluster interactively, meaning that you were running all cells sequentially. \n",
    "\n",
    "In a production environment, you are more likely to launch the applications from a script, where that script contains all the operations using PySpark. \n",
    "\n",
    "To do so, you can use spark-submit, which can be ran from the terminal to _submit_ your Spark applications. The syntax is as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c275d",
   "metadata": {},
   "source": [
    "```\n",
    "spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac83e21",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "- __class__ is the entrypoint for your application\n",
    "- __master__ the URL of your cluster. You can set it to `local` to run it locally\n",
    "- __deploy-mode__ Whether to deploy on the worker or locally as a client\n",
    "- __conf__ Configuration of the Spark application in a `key=value` way\n",
    "- __application-jar__: Path to a your application\n",
    "\n",
    "Within other options, you can specify number of workers or the number of cores:\n",
    "\n",
    "- __--num-executors__\n",
    "- __--num-cores__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73cc55",
   "metadata": {},
   "source": [
    "In this case, we are going to submit the same example we were working with. This application will print put the words in lorem, and the number of occurences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "180b5569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Lorem': 3, 'ipsum': 11, 'dolor': 10, 'sit': 50, 'amet,': 1, 'consectetur': 12, 'adipiscing': 23, 'elit,': 1, 'sed': 66, 'do': 1, 'eiusmod': 1, 'tempor': 11, 'incididunt': 1, 'ut': 48, 'labore': 1, 'et': 34, 'dolore': 1, 'magna': 15, 'aliqua.': 1, 'Quam': 5, 'lacus': 21, 'suspendisse': 17, 'faucibus': 25, 'interdum': 12, 'posuere.': 3, 'Dui': 5, 'accumsan': 10, 'amet': 53, 'nulla': 31, 'facilisi': 11, 'morbi': 25, 'tempus.': 2, 'Lobortis': 4, 'scelerisque': 23, 'fermentum': 17, 'dui': 21, 'in': 48, 'ornare': 16, 'quam': 24, 'viverra.': 6, 'In': 10, 'viverra': 39, 'orci': 21, 'sagittis': 19, 'eu.': 6, 'Urna': 1, 'duis': 10, 'convallis': 13, 'tellus': 28, 'id': 47, 'interdum.': 5, 'A': 5, 'erat': 7, 'nam': 5, 'at': 36, 'lectus.': 5, 'Eget': 9, 'lorem': 11, 'Scelerisque': 5, 'eu': 30, 'ultrices': 18, 'vitae': 33, 'auctor': 15, 'augue': 21, 'ut.': 11, 'Orci': 3, 'porta': 8, 'non': 26, 'pulvinar': 18, 'neque': 18, 'laoreet': 10, 'libero.': 1, 'Egestas': 7, 'rutrum': 3, 'pellentesque': 33, 'tincidunt': 22, 'tortor': 21, 'aliquam.': 8, 'Vivamus': 2, 'eget': 36, 'arcu.': 8, 'Id': 9, 'leo': 16, 'turpis.': 5, 'Rutrum': 2, 'aliquam': 24, 'nulla.': 2, 'Mauris': 3, 'a': 24, 'diam': 25, 'maecenas': 14, 'enim': 45, 'urna.': 5, 'lectus': 20, 'est': 16, 'placerat': 8, 'egestas.': 6, 'Non': 3, 'curabitur': 4, 'gravida': 19, 'arcu': 24, 'ac': 20, 'dignissim': 12, 'convallis.': 4, 'Proin': 3, 'libero': 10, 'nunc': 30, 'consequat': 11, 'varius': 8, 'sit.': 10, 'Sit': 12, 'commodo': 15, 'nullam': 8, 'vehicula': 2, 'a.': 5, 'Eleifend': 1, 'proin': 15, 'nisl': 24, 'rhoncus.': 2, 'Elementum': 5, 'etiam': 10, 'lacus.': 3, 'Cursus': 6, 'mauris': 22, 'Senectus': 1, 'netus': 10, 'malesuada': 17, 'fames': 5, 'turpis': 22, 'suspendisse.': 7, 'Massa': 5, 'egestas': 37, 'purus': 19, 'in.': 9, 'sodales': 4, 'amet.': 8, 'condimentum': 10, 'lacinia': 5, 'quis': 27, 'vel': 19, 'eros': 4, 'donec.': 2, 'Magna': 3, 'blandit.': 2, 'Mattis': 7, 'ullamcorper': 20, 'velit': 20, 'massa': 25, 'eget.': 10, 'Morbi': 7, 'tristique': 22, 'senectus': 12, 'Laoreet': 3, 'tincidunt.': 7, 'risus': 20, 'mi': 19, 'Fames': 1, 'pharetra': 14, 'Ultrices': 7, 'cursus': 20, 'aliquet': 19, 'nibh': 20, 'condimentum.': 3, 'Ipsum': 4, 'dictum': 13, 'fusce.': 1, 'Tempus': 2, 'nec': 17, 'nam.': 1, 'Parturient': 1, 'montes': 2, 'nascetur': 2, 'ridiculus': 1, 'mus': 2, 'ultricies.': 2, 'odio': 20, 'Amet': 8, 'tempus': 14, 'iaculis': 12, 'urna': 26, 'volutpat.': 6, 'Eu': 7, 'bibendum': 17, 'pharetra.': 4, 'Vitae': 6, 'rhoncus': 10, 'mattis': 19, 'elit': 21, 'pellentesque.': 9, 'hendrerit': 5, 'quisque': 6, 'auctor.': 3, 'Vel': 4, 'pulvinar.': 3, 'Dolor': 1, 'aliquet.': 4, 'Tortor': 2, 'venenatis': 10, 'cras': 10, 'felis.': 1, 'Nisl': 3, 'pretium': 21, 'fusce': 6, 'Nisi': 3, 'mollis': 6, 'Ornare': 2, 'nisl.': 3, 'Suspendisse': 1, 'nisi': 9, 'hac.': 1, 'dui.': 5, 'Imperdiet': 2, 'iaculis.': 6, 'Arcu': 3, 'euismod': 11, 'nunc.': 7, 'Viverra': 9, 'cras.': 4, 'Pellentesque': 6, 'volutpat': 22, 'habitant': 9, 'tristique.': 1, 'quam.': 6, 'Sed': 8, 'ultricies': 16, 'scelerisque.': 3, 'suscipit': 7, 'integer.': 4, 'fringilla': 8, 'porttitor': 7, 'enim.': 3, 'Enim': 10, 'vulputate': 21, 'Sapien': 3, 'ligula': 1, 'proin.': 2, 'Ullamcorper': 2, 'varius.': 2, 'Condimentum': 2, 'ac.': 2, 'Diam': 11, 'sem': 8, 'Ac': 7, 'posuere': 11, 'morbi.': 8, 'facilisis': 12, 'mauris.': 6, 'tortor.': 4, 'semper': 14, 'diam.': 4, 'vivamus': 6, 'felis': 11, 'Dignissim': 1, 'integer': 16, 'vitae.': 5, 'Integer': 5, 'feugiat': 12, 'Tellus': 7, 'id.': 7, 'Nibh': 4, 'odio.': 2, 'Adipiscing': 6, 'sollicitudin': 6, 'nibh.': 4, 'Cras': 2, 'consectetur.': 2, 'Aliquet': 8, 'luctus': 2, 'Consequat': 2, 'justo': 10, 'laoreet.': 2, 'Posuere': 1, 'molestie.': 3, 'Tempor': 3, 'mi.': 2, 'Velit': 2, 'Sem': 3, 'Congue': 2, 'aenean': 7, 'elementum': 24, 'sagittis.': 2, 'ipsum.': 5, 'pretium.': 4, 'Sollicitudin': 1, 'luctus.': 1, 'Est': 5, 'facilisi.': 1, 'Ut': 9, 'blandit': 11, 'erat.': 5, 'Donec': 3, 'sapien': 7, 'malesuada.': 3, 'bibendum.': 2, 'adipiscing.': 3, 'sed.': 13, 'dignissim.': 3, 'Leo': 2, 'molestie': 11, 'elementum.': 2, 'Risus': 5, 'lobortis': 8, 'faucibus.': 4, 'phasellus': 8, 'vestibulum': 13, 'Etiam': 2, 'Vestibulum': 4, 'ullamcorper.': 3, 'Sagittis': 5, 'Maecenas': 1, 'nisi.': 1, 'Convallis': 2, 'neque.': 4, 'porta.': 3, 'Aenean': 4, 'eleifend': 8, 'Porttitor': 2, 'risus.': 5, 'Elit': 5, 'imperdiet': 11, 'ante': 2, 'mattis.': 5, 'metus': 7, 'Bibendum': 3, 'At': 12, 'Fusce': 2, 'Tristique': 4, 'Lacus': 4, 'lacinia.': 1, 'Pretium': 7, 'donec': 11, 'Feugiat': 3, 'praesent': 7, 'feugiat.': 3, 'Dictum': 3, 'Et': 3, 'Malesuada': 2, 'accumsan.': 3, 'Faucibus': 4, 'quis.': 5, 'aenean.': 2, 'cursus.': 4, 'Pharetra': 4, 'Vulputate': 3, 'Augue': 2, 'Nec': 2, 'hendrerit.': 1, 'Libero': 1, 'dictum.': 3, 'orci.': 5, 'rutrum.': 2, 'hac': 1, 'habitasse': 1, 'platea': 1, 'dictumst': 1, 'vestibulum.': 3, 'Ante': 1, 'Fermentum': 1, 'ornare.': 6, 'Mi': 4, 'congue': 4, 'potenti.': 1, 'Nunc': 5, 'purus.': 3, 'facilisis.': 2, 'Molestie': 1, 'Tincidunt': 4, 'venenatis.': 4, 'Volutpat': 2, 'tellus.': 3, 'Turpis': 3, 'Penatibus': 1, 'magnis': 1, 'dis': 1, 'parturient': 1, 'ridiculus.': 1, 'Commodo': 3, 'non.': 4, 'Purus': 2, 'fermentum.': 4, 'Venenatis': 2, 'Lectus': 2, 'porttitor.': 5, 'vulputate.': 1, 'Facilisis': 1, 'elit.': 3, 'et.': 10, 'Auctor': 2, 'Euismod': 1, 'at.': 8, 'Justo': 1, 'phasellus.': 2, 'Curabitur': 1, 'senectus.': 3, 'potenti': 1, 'Neque': 2, 'placerat.': 2, 'cum': 2, 'sociis': 2, 'natoque.': 2, 'Nulla': 3, 'Odio': 2, 'Dictumst': 2, 'Duis': 1, 'Blandit': 3, 'lorem.': 3, 'imperdiet.': 2, 'Aliquam': 2, 'Ridiculus': 1, 'vel.': 2, 'Porta': 2, 'duis.': 2, 'Eros': 3, 'gravida.': 3, 'euismod.': 1, 'Suscipit': 1, 'Accumsan': 1, 'Pulvinar': 2, 'semper.': 1, 'dapibus': 2, 'ultrices.': 2, 'fringilla.': 2, 'ante.': 1, 'Luctus': 1, 'Quis': 1, 'Ultricies': 2, 'Placerat': 1, 'Potenti': 1, 'maecenas.': 2, 'congue.': 1, 'Dapibus': 1, 'Metus': 1, 'augue.': 1, 'est.': 2, 'massa.': 1, 'Nam': 1, 'Habitant': 1, 'netus.': 2, 'fames.': 1, 'Mollis': 1, 'leo.': 2, 'Consectetur': 1, 'Semper': 1})\n"
     ]
    }
   ],
   "source": [
    "# example.py\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    # create Spark context with Spark configuration\n",
    "    conf = SparkConf().setAppName(\"Word Count - Python\").setMaster('local[*]')\n",
    "    session = pyspark.sql.SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "    # read in text file and split each document into words\n",
    "    rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
    "    rddDistributedFile = rddDistributedFile.cache()\n",
    "    # count the occurrence of each word\n",
    "    print(rddDistributedFile.flatMap(lambda text: text.split()).countByValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d8610",
   "metadata": {},
   "source": [
    "In this repo, you will find a `example.py` files that you can try for submitting your application. You can run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5af65",
   "metadata": {},
   "source": [
    "`<SPARK_HOME>/bin/spark-submit.cmd example.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3decf223",
   "metadata": {},
   "source": [
    "If you encounter an error, you might need to paste a file `winutils.exe` for running the command above. You can download the corresponding version [here](https://github.com/steveloughran/winutils).\n",
    "\n",
    "Your directory should look like this:\n",
    "\n",
    "```\n",
    "~/\n",
    "│\n",
    "├── spark/\n",
    "│   └── spark-3.1.2-bin-hadoop3.2  <--- SPARK_HOME\n",
    "│         ├── bin\n",
    "│         ├── conf\n",
    "│         ├── data\n",
    "│         ├── examples\n",
    "│         ├── hadoop               <--- Add this new folder = HADOOP_HOME\n",
    "│         │    └── bin\n",
    "│         │         └── winutils.exe\n",
    "... \n",
    "```\n",
    "\n",
    "Then, you'll have to set the a new environment variable `HADOOP_HOME` with the directory of the folder `hadoop`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ca66b",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Check out [`rdd.aggregate`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.aggregate.html#pyspark.RDD.aggregate) method for RDDs.\n",
    "- What is the difference between `forEach` and `map`? Check [this StackOverflow answer](https://stackoverflow.com/questions/354909/is-there-a-difference-between-foreach-and-map) if in doubt\n",
    "- What is the difference between `reduce` and `fold`? check [this StackOverflow answer](https://stackoverflow.com/a/36060141/10886420). Which one is \"safer\" to use?\n",
    "- Which operations on RDDs induce `shuffle` and why is it a problem? See [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations) for more info\n",
    "- Check how to use [Hive](https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html) with PySpark. What is Hive and how does it differ from SQL?\n",
    "- Check out how to specify schema programmaticaly (presented [in this tutorial](https://spark.apache.org/docs/latest/sql-getting-started.html#programmatically-specifying-the-schema)). What are the upsides/downsides of using it?\n",
    "\n",
    "- Read more about multiple `SparkContext`s and `SparkSession`s and why would we need it in some... contexts. Check it [over here](https://www.waitingforcode.com/apache-spark-sql/multiple-sparksession-one-sparkcontext/read)\n",
    "- What is [`rdd.meanApprox`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.meanApprox.html#pyspark.RDD.meanApprox) and why might we need it?\n",
    "- Generally discouraged, but what are the options to share data between tasks and nodes in the cluster? Check out [this part of RDD tutorial](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shared-variables)\n",
    "- Check [performance tuning options for `spark.sql`](https://spark.apache.org/docs/latest/sql-performance-tuning.html). One can use them when creating `pyspark.SparkConf()` object"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6971ff02672853a145ab8a619e17e4c2b989e1ba4684228133b86b474ce57f92"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
