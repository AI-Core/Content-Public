{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Hands-On\n",
    "\n",
    "> Kafka is a Publish-Subscribe (pub-sub) messaging system used for streaming data in real-time (in addition to it's ability to process batch data).\n",
    "\n",
    "In order for Kafka to be able to handle massive volumes of data at scale coming in at rapid velocity, several components are required to setup a robust Kafka system.\n",
    "\n",
    "As a quick reminder, below is the overall Kafka topology at a high-level:\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src= \"images/Kafka_Architecture2.png\">\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "## Kafka Components\n",
    "\n",
    "Before getting into the details of what each component is responsible for, let's first define some of the main Kafka terms we'll be using:\n",
    "\n",
    "### Events\n",
    "> An _event_ in Kafka is used to store a fact that has occurred. When Kafka reads or writes any data, it does so in the form of events. Events can be thought of as data points.\n",
    "\n",
    "An event typically contains the following properties:\n",
    "- Key\n",
    "- Value\n",
    "- Timestamp\n",
    "- Metadata headers (optional). \n",
    "    \n",
    "Below is an example Event:\n",
    "\n",
    "- Event Key: \"AiCore\"\n",
    "\n",
    "- Event Value: \"User made a payment of $300\"\n",
    "\n",
    "- Event Timestamp: \"March. 22, 2021 at 3:07 p.m.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producers\n",
    "\n",
    "> A _producer_ is the component that creates data for Kafka. It's an API that enables another tool or an application to publishes/writes data to one or more Kafka topics\n",
    "\n",
    "In essence, a producer provides data that Kafka will then ingest. The producer API comes built-in with Kafka and can be configured to connect to a wide variety of data sources.\n",
    "\n",
    "#### Consumers\n",
    "\n",
    "> A _consumer_ is the component that reads data from Kafka. It's also an API that's responsible for reading data records from one or more Kafka topics\n",
    "\n",
    "One of key strengths in Kafka is that In Kafka, producers and consumers are decoupled from each other and don't depend on one another to complete their tasks. This is one design feature which provides Kafka with high scalability. We can have any number of producers and consumers in a Kafka cluster and the number of producers and consumers don't have to be equal.\n",
    "\n",
    "#### Topics\n",
    "\n",
    "> Events are organised and stored in a Kafka _topic_. A topic is analogous to a folder in a filesystem, and the events are analogous the files stored within that folder. \n",
    "\n",
    "Events (data points) stored within a specific Kafka topic can be consumed once, or as often as required. This is because data stored in Kafka is not deleted automatically after it is consumed. Instead, users can define the lifetime of the data by configuring the corresponding property manually. Regardless of how much data is stored, Kafka will remain a stable system.\n",
    "\n",
    "#### Partitions\n",
    "\n",
    "> Every Kafka topic is composed of multiple _partitions_, or sub-divisions. When a new data point is written, it is added to one of the topic's partitions. \n",
    "\n",
    "Events have keys that identify them. Each event that's consumed with the _same key_, such those having the same `Customer ID`, are written to the _same partition_. Kafka's design ensures that data consumed will always be in exactly the same order as they were written.\n",
    "\n",
    "Below is a diagram showing how a Kafka topic and partition look like:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src= \"images/Kafka_Topics.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Components\n",
    "\n",
    "To recap what we've covered so far in Lessons 1 and 2, there are __five main components__ in a Kafka System:\n",
    "\n",
    "__1.\tBroker:__ \n",
    "- A broker is a Kafka node or server which is part of the Kafka system\n",
    "- A Kafka __cluster__ is usually composed of multiple Brokers\n",
    "- Each broker has a unique ID\n",
    "- Brokers store the topic log partitions\n",
    "- Brokers handle all requests from clients (produce, consume, and metadata) and keeps data replicated within the cluster. \n",
    "- There can be one or more brokers in a cluster.\n",
    "\n",
    "For a video explanation on brokers, please watch the following:\n",
    "\n",
    "- [__Brokers Introduction Video__](https://www.youtube.com/embed/jHnyBSUVcOU)\n",
    "\n",
    "A broker can be configured by updating it's properties in the `server.properties` file.\n",
    "\n",
    "The important configuration options for a Broker are:\n",
    "1. `broker.id`\n",
    "    - This is an integer that must be set as a unique value for each broker\n",
    "2. `listeners`\n",
    "    - This is the address that the socket server listens on (hostname and the port)\n",
    "3. `advertised.listeners`\n",
    "    - This is the hostname and port the broker will advertise to the producers and consumers\n",
    "4. `log.dirs`\n",
    "    - This is a comma seperated list of directories under which to store the log files\n",
    "5. `num.partitions`\n",
    "    -  The number of partitions per topic\n",
    "6. `log.retention.hours`\n",
    "    - The minimum age of a log file before deletion\n",
    "7. `zookeeper.connect`\n",
    "    - A comma seperated host:port pairs each corresponding to a Zookeeper server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__2.\tZookeeper:__\n",
    "- Kafka uses Zookeeper to manage service discovery for brokers (e.g. if a new broker joins, or a broker dies etc.)\n",
    "- Zookeeper is part of the Hadoop technology stack, but is external to the Kafka software itself despite being required to run Kafka.\n",
    "- Zookeeper maintains the state of the cluster (brokers, topics, users).\n",
    "\n",
    "The configuration for Zookeeper can be found in the __zookeeper.proerties__ file. The important configuration options which can be found there are:\n",
    "\n",
    "1. `dataDir`\n",
    "    -   The directory where the snapshots will be stored\n",
    "2. `clientPort`\n",
    "    -   The port which clients will use to connect\n",
    "3. `maxClientCnxns`\n",
    "    -   Enables/disables the per-IP limit on the number of connections\n",
    "4. `admin.enableServer`\n",
    "    -   Enable/Disables the admin server to avoid port conflicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setting up Kafka and configuring Zookeeper\n",
    "\n",
    "Before we get into things, make sure that Kafka is downloaded and untarred on your machine.\n",
    "\n",
    "You'll need to create a new \"data\" folder which will store the data records. Create this in the same folder that contains Kafka, so that when you list the contents, it is the same as shown below.\n",
    "\n",
    "Within the \"data\" folder, we need to create a `kafka` and `zookeeper` subfolders using the commands below.\n",
    "These are required in order to update the configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir data\n",
    "mkdir data/kafka\n",
    "mkdir data/zookeeper\n",
    "ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your output should be similar to:\n",
    "\n",
    "![](images/kafka-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need to update the __Zookeeper Properties__ and the __Apache Kafka Log__ files to point to the new data directory we just created.\n",
    "\n",
    "Within the config folder, edit the `zookeeper.properties` file so that the `dataDir` key has the value `data` (the relative path to your data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir=data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, again within the `config` folder, edit the `server.properties` file so that the `listeners` key has the value `PLAINTEXT://127.0.0.1:9092`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listeners=PLAINTEXT://127.0.0.1:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to start the actual Kafka Cluster.\n",
    "\n",
    "To achieve this, first we need to run Zookeeper by running the Zookeeper executable and specifying the config file using the command shown below.\n",
    "\n",
    "_Note that at this point you may need to install Java from [here](https://www.oracle.com/java/technologies/downloads/#jdk17-mac) to prevent an error_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin/zookeeper-server-start.sh config/zookeeper.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read more about the __`zookeeper-server-start.sh`__ command and its various properties, please check the [official documentation](https://zookeeper.apache.org/doc/r3.6.3/zookeeperTools.html):\n",
    "\n",
    "_Note: `zookeeper-server-start.sh` and `zkServer.sh` are essentially the same.  The naming differnce is due to the fact that the former was named by the Kafka foundation while the latter is a Zookeeper terminology._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "There should be many items being downloaded which will take a few minutes.\n",
    "\n",
    "Now, if everything works correctly you should see an output similar to the following:\n",
    "\n",
    "![](images/kafka-zookeeper.png)\n",
    "\n",
    "\n",
    "After displaying several updates, the terminal will remain open and the cursor will be blinking.  This is normal and to be expected.  We'll leave this terminal open and continue.\n",
    "\n",
    "The next step is to run the Kafka Broker.\n",
    "\n",
    "Open a second Ubuntu terminal session (and make sure to keep the Zookeeper one open) and type the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd kafka_2.13-3.0.0 \n",
    "bin/kafka-server-start.sh config/server.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __`kafka-server-start.sh`__ command starts the Kafka server.\n",
    "\n",
    "It takes the following arguments:\n",
    "- Server Properties file path\n",
    "- Override Property Value (optional)\n",
    " \n",
    "\n",
    "\n",
    "If all runs correctly, you should see a long output that looks similar to the following:\n",
    "\n",
    "![](images/kafka-server.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__3.\tTopic:__\n",
    "- A topic is a category/feed name to which data records are stored and published\n",
    "- All Kafka data records are organized into topics\n",
    "- Producers write data to the topics and consumers read data from the topics\n",
    "- Data records plubished in the cluster remain persisted until a specified retention period has passed by\n",
    "- Each topic is divided into _partitions_, which contain records in an unchangeable sequence. Each record within a partition is assigned and identified by a unique offset (ID)\n",
    "\n",
    "Below is a visual representation of how these different components interact with one another:\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src= \"images/Kafka Zookeeper Brokers.png\" width=600>\n",
    "</p>\n",
    "\n",
    "Check out this [video](https://www.youtube.com/embed/kj9JH3ZdsBQ) for more info on topics.\n",
    "\n",
    "Now we have both the Zookeeper and Kafka server running.  This prepares us to start producing and consuming messages!\n",
    "\n",
    "The next step is to open other Ubuntu terminals and to create a Kafka topic which we'll use to share messages between the local producer and consumer.\n",
    "\n",
    "There are some required parameters such as the __partition number__ and the __replication factor__ along with the __topic name__ and the __server details__.\n",
    "\n",
    "For the time being, we'll keep things simple and limit both the number of partitions and the replication factor to 1.\n",
    "\n",
    "We can create the topic by running the command shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin/kafka-topics.sh --create --topic MyFirstKafkaTopic --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kafka-topics.sh` command is used to create and configure Kafka Topics.\n",
    "\n",
    "You can read more about `kafka-topics.sh` [here](https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-admin-TopicCommand.html)\n",
    "\n",
    "Here are some important arguments to be aware of:\n",
    "- `--create`:\n",
    "    -   Creates a new topic.  This is required ony the first time we are dealing with a new topic.\n",
    "- `--topic`:\n",
    "    -   The topic name to create, alter, describe or delete. Followed by the topic name.\n",
    "- `--partitions`\n",
    "    -   The number of partitions for the topic being created or altered.\n",
    "- `--replication-factor`:\n",
    "    -   The replication factor for messages in your topic, replication-factor of three would mean there are three copies of your message in a Kafka cluster. This is mandatory if there is no default setup in the cluster itself.\n",
    "- `--bootstrap-server`:\n",
    "    -   The Kafka server to connect to.  localhost:9092 is to be used in case of a local stand-alone environment.\n",
    "\n",
    "__Important Optional arguments:__\n",
    "- `alter`:\n",
    "    -   Alters the number of partitions, replica assignment and/or configuration for the Topic\n",
    "- `config`:\n",
    "    -   A Topic configuration override for the topic being created or altered. Allows configurations for:\n",
    "        -   Cleanup policy\n",
    "        -   Compression type\n",
    "        -   Delete retention time\n",
    "        -   Flushing messages\n",
    "- `delete`:\n",
    "    -   Deletes a topic\n",
    "- `describe`:\n",
    "    -   Lists the details for a particualr topic\n",
    "- `list`:\n",
    "    -   Lists all the available topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Your output for this command should look something like:\n",
    "\n",
    "![](images/kafka-topic.png)\n",
    "\n",
    "__4.  Producer:__ \n",
    "- Connect to a Kafka cluster either through Zookeeper or directly to a Kafka broker\n",
    "- Sends records (data) to a broker.\n",
    "\n",
    "Check out this [video](https://www.youtube.com/embed/I7zm3on_cQQ) on producers\n",
    "\n",
    "A producer can be configured and the properties can be updated in the `producer.properties` file. Here are the important configuration options you should know about:\n",
    "1. `bootstrap.servers`\n",
    "    - This is a list of Brokers used for bootstrapping knowledge about the rest of the cluster.\n",
    "2. `compression.type`\n",
    "    - Allows specifying the compression codec for all data generated (none, gzip, lz4, zstd)\n",
    "3. `partitioner.class`\n",
    "    - Name of the Partitioner class to be used for partitioning events (default is random spreading)\n",
    "4. `request.timeout.ms`\n",
    "    - The maximum amount of time the client will wait for the response of a request\n",
    "5. `buffer.memory`\n",
    "    -  The total bytes of memory the Producer can use to beffer records waiting to be sent to the server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we need to open two additional terminals (so the total opened Linux terminals will be five).  One terminal will be for the producer, and the second is for the consumer.\n",
    "\n",
    "We will create a producer which sends messages to the topic, which are then consumed by the consumer and print into the standard output channel of the terminal.\n",
    "\n",
    "To create a producer, open a new terminal and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka-console-producer.sh --topic MyFirstKafkaTopic --bootstrap-server localhost:9092 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further information about this command, check the following [documentation](https://docs.cloudera.com/runtime/7.2.0/kafka-managing/topics/kafka-manage-cli-producer.html?)\n",
    "\n",
    "__Required Arguments:__\n",
    "- `topic`:\n",
    "    -   Topic name to which the Producer will send the data to\n",
    "- `bootstrap-server`:\n",
    "    -   The Kafka server to connect to\n",
    "\n",
    "\n",
    "__Important Optional Arguments:__\n",
    "- `batch-size`:\n",
    "    -   Number of messages to send in a single batch if they're not being sent synchronously (default is set at 200)\n",
    "- `compression-code`:\n",
    "    -   The data compression codec used.  Can be one of the following (default is gzip):\n",
    "        -   None\n",
    "        - Gzip\n",
    "        - Snappy\n",
    "        - Lz4\n",
    "        - Zstd\n",
    "- `max-memory-bytes`:\n",
    "    -   The total memory used by the Producer to buffer records waiting to be sent to the server\n",
    "- `max-partitions-memory-bytes`:\n",
    "    -   The buffer memory size allocated for a Partition.  When data records are received which are smaller than this size, the Producer will attempt to group them together until the specified size is reached\n",
    "- `property`: A mechanism to pass user-defined properties in the form of Key = Value to the message reader.  This allows custom configurations for a user-defined message reader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.\tConsumer:__ \n",
    "- Consumes batches of records (data) from the broker.\n",
    "- Consumers can specify both the Topic and Partition from which they will consume data\n",
    "- There are two types of Consumers:\n",
    " \n",
    "        - Low-level \n",
    "        - High-level\n",
    "\n",
    "Check out this video on [consumers](https://www.youtube.com/embed/Z9g4jMQwog0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Consumer can be configured and the properties can be updated in the __consumer.properties__ file.\n",
    "\n",
    "__Core configurations for a Consumer consist of:__\n",
    "1. `bootstrap.servers`\n",
    "    - This is a list of Brokers used for bootstrapping knowledge about the rest of the cluster.\n",
    "2. `group.id`\n",
    "    - The Consumer group ID\n",
    "3. `auto.offset.reset`\n",
    "    - Tells the Consumer what to do when there is no initial offset in Kafka or if the current offset does not exist anymore on the server.  Options include latest, earliest, none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To continue with our hands-on example, the final remaining step is to call the Kafka Consumer.  \n",
    "\n",
    "To achieve this, in parallel, open a new Ubuntu terminal for the __Consumer__ by using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka-console-consumer.sh --topic MyFirstKafkaTopic --from-beginning --bootstrap-server localhost:9092 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __`kafka-console-consumer.sh`__ command is used to initiate the Consumer, which will then read(consume) the data records from the specified Topic.\n",
    "\n",
    "__Required Arguments:__\n",
    "- `topic`:\n",
    "    -   Topic name from which to consume the data records.\n",
    "- `bootstrap-server`:\n",
    "    -   The Kafka server to connect to.\n",
    "\n",
    "__Important Optional Arguments:__\n",
    "- `consumer-property`:\n",
    "    -   A mechansim to pass user-defined properties in the form of Kev=Value to the Consumer.\n",
    "- `consumer.config`:\n",
    "    -   The Consumer configuration properties file.  Note that the `consumer-property` settings take precendence over this file.\n",
    "- `from-beginning`\n",
    "    -   Tells the Consumer that if it doesn't already have an established Offset to consume from, start with the earliest message present in the log.\n",
    "- `max-messages`:\n",
    "    -   The maximum number of messages to consumer before exiting.  If it's not set, consumption is continual.\n",
    "- `offset`:\n",
    "    -   A non-negative number representing the Offset to consume data records from.  Can also use 'earlist' or 'latest'.  The default is 'latest'.\n",
    "- `partition`:\n",
    "    -   The Partition to consume data records from. The default is for consumption to start from the end of the Partition.\n",
    "\n",
    "As always, make sure to check [the documentation](https://docs.cloudera.com/runtime/7.2.10/kafka-managing/topics/kafka-manage-cli-consumer.html) for more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with both window terminals side by side, click on the __Producer__ terminal and enter the following JSON data:\n",
    "\n",
    "`{\n",
    "  \"vehicleId\":\"0bf45cac-d1b8-4364-a906-980e1c2bdbcb\",\n",
    "  \"vehicleType\":\"Taxi\",\n",
    "  \"routeId\":\"Route-37\",\n",
    "  \"longitude\":\"-95.255615\",\n",
    "  \"latitude\":\"33.49808\",\n",
    "  \"timestamp\":\"2017-10-16 12:31:03\",\n",
    "  \"speed\":49.0,\n",
    "  \"fuelLevel\":38.0\n",
    "}`\n",
    "\n",
    "`{\n",
    "  \"vehicleId\":\"d5fd4b42-3742-11ec-8d3d-0242ac130003\",\n",
    "  \"vehicleType\":\"Bus\",\n",
    "  \"routeId\":\"Route-32\",\n",
    "  \"longitude\":\"-81.615234\",\n",
    "  \"latitude\":\"13.56599\",\n",
    "  \"timestamp\":\"2017-10-17 14:22:03\",\n",
    "  \"speed\":37.0,\n",
    "  \"fuelLevel\":19.0\n",
    "}`\n",
    "\n",
    "`{\n",
    "  \"vehicleId\":\"04be0177-8326-4b59-a15d-19f015c0be63\",\n",
    "  \"vehicleType\":\"Passenger\",\n",
    "  \"routeId\":\"Route-19\",\n",
    "  \"longitude\":\"-15.611331\",\n",
    "  \"latitude\":\"44.59816\",\n",
    "  \"timestamp\":\"2017-10-18 09:07:08\",\n",
    "  \"speed\":75.0,\n",
    "  \"fuelLevel\":48.0\n",
    "}`\n",
    "\n",
    "You should see the messages automatically show up on the Consumer terminal similr to the below output:\n",
    "\n",
    "![](images/kafka-producer-consumer.png)\n",
    "\n",
    "Now you should be getting the hang of Kafka"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
