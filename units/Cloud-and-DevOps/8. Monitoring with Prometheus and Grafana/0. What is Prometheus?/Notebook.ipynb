{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c26c1de",
   "metadata": {},
   "source": [
    "# Prometheus\n",
    "\n",
    "## Introduction\n",
    "> Prometheus is an open-source monitoring and alerting toolkit for gathering and processing data locally. \n",
    "\n",
    "It was originally developed at SoundCloud and is currently a fully open-source, community-driven project. Prometheus is designed to collect and store metrics as time-series data. It stores metric information with a timestamp (when it was recorded), alongside key-value pairs called labels.\n",
    "\n",
    "To obtain a relatively broad overview of Prometheus and its uses, visit the main page [here](https://prometheus.io/docs/introduction/overview/).\n",
    "\n",
    "*Facts to note about Prometheus*\n",
    "- It is written in golang (Go), a programming language developed at Google.\n",
    "- It provides APIs for different languages, including Python.\n",
    "- Prometheus' query language, PromQL, can be employed to run queries.\n",
    "- It hosts a dashboard in web browsers on `localhost:9090` by default.\n",
    "\n",
    "## Main Components\n",
    "\n",
    "* **Prometheus server:** for scraping and storing time-series data.\n",
    "* **Client libraries:** for instrumenting application code.\n",
    "* **Push gateway:** for scraping metrics of short-lived jobs.\n",
    "* **Alertmanager:** for handling alerts.\n",
    "* **Exporters:** support scraping metrics for third-party services, such as HAProxy, StatsD and Graphite.\n",
    "\n",
    "![](images/prometheus_architecture.png)\n",
    "\n",
    "## Process-Flow Overview\n",
    "\n",
    "As shown above,\n",
    "- Prometheus scrapes data (e.g. metrics)\n",
    "    - from short-lived jobs via `push gateway`.\n",
    "    - from long-running jobs directly.\n",
    "- __All samples (values with timestamps) are stored locally,__ together with the necessary metadata.\n",
    "- Prometheus runs predefined rules on collected data:\n",
    "    - It gathers and aggregates new records.\n",
    "    - It processes them and sends alerts.\n",
    "- Consumers use Prometheus API to visualise data.\n",
    "\n",
    "## Applications of Prometheus\n",
    "\n",
    "- __Recording numerical time series__, including\n",
    "    - various training metrics gathered across epochs/batches.\n",
    "    - hardware-related data.\n",
    "    - network traffic and other statistics.\n",
    "- __Debugging infrastructure during network outages for example__ (if interested, you can read about Slack's outage [here](https://slack.engineering/slacks-outage-on-january-4th-2021/)).\n",
    "\n",
    "> __Data is stored locally, independent of the network storage.__\n",
    "\n",
    "Consequently, if a failure occurs, you always have access to the data, as each Prometheus server is self-contained and independent.\n",
    "\n",
    "__Do not use Prometheus if__ you will be working with highly detailed data coming in fast__ (per request metrics, with many requests from users, and exact billing when every millisecond counts).\n",
    "\n",
    "This is because\n",
    "- Prometheus is designed to scrape data every few seconds.\n",
    "- the data are stored in the local storage, which may be insufficient (in this case, consider employing large, remote storage and data rotation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ac634",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "> __All Prometheus data are stored as timestamped time series, differentiated by the metric and label (optional).__\n",
    "\n",
    "- Metric names and labels should be alphanumerical.\n",
    "- __Samples are `float64` (`double`) numerical types.__\n",
    "- __Timestamps are in milliseconds.__\n",
    "\n",
    "For instance, `process_cpu_seconds_total`, which calculates the total CPU usage of our Prometheus instance, will be displayed as\n",
    "\n",
    "`process_cpu_seconds_total{instance=\"localhost:9090\", job=\"prometheus\"}`.\n",
    "\n",
    "*Breakdown*\n",
    "- `process_cpu_seconds_total` represents the metric name.\n",
    "- `instance=\"localhost:9090\"` is the label that informs us of the instance for which the CPU time is being checked.\n",
    "- `job=prometheus` is another label that informs us of the job for which the CPU time is being checked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e50bd8",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "> Prometheus provides `4` metrics by default.\n",
    "\n",
    "_Note: The Prometheus server does not differentiate between metrics; it only keeps the data. Metrics are used by client libraries (`4` client libraries provided for `golang`, `java`, __`python`__, `ruby`)._\n",
    "\n",
    "### Counter\n",
    "\n",
    "> Provides a monotonically increasing value. It can also be restarted.\n",
    "\n",
    "It is useful for\n",
    "- determining the number of requests,\n",
    "- determining the number of tasks completed,\n",
    "- anything that can only increase or start anew.\n",
    "\n",
    "### Gauge\n",
    "\n",
    "> Provides a single value that can increase or decrease.\n",
    "\n",
    "It is useful for\n",
    "- memory-usage monitoring,\n",
    "- temperature monitoring,\n",
    "- anything whose value can change arbitrarily.\n",
    "\n",
    "### Histogram\n",
    "\n",
    "> Samples observations and groups them in configurable buckets.\n",
    "\n",
    "For demonstration, we name our histogram metric `our_super_histogram`. In this case, the following operations on the histogram are available (notice that we added suffixes to the metric name):\n",
    "- `our_super_histogram_bucket{le=\"<upper inclusive bound>\"}`: cumulative counters for the observation buckets.\n",
    "- `our_super_histogram_sum`: sum of all values.\n",
    "- `out_super_histogram_count`: count of observations.\n",
    "\n",
    "### Summary\n",
    "\n",
    "> Samples observations and provides them as a sliding time window.\n",
    "\n",
    "- Provides `sum` and `count` similar to the case with histogram.\n",
    "- `out_super_summary_quantiles{quantile=\"value\"}`: quantile of observations (this can achieved for the histogram metric using functions).\n",
    "\n",
    "### Functions\n",
    "\n",
    "Functions can be run in queries for things such as `day_of_month()`. We will cover functions in more detail later.\n",
    "\n",
    "## Jobs and Instances\n",
    "\n",
    "- An instance is an endpoint, from which data can be scraped, e.g. an EC2 instance or a Docker container.\n",
    "- A job is a collection of the same instances, e.g. multiple EC2 instances or Docker containers.\n",
    "\n",
    "For more information regarding jobs and instances, consult the Prometheus documentation [here](https://prometheus.io/docs/concepts/jobs_instances/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c719c9",
   "metadata": {},
   "source": [
    "## First Contact\n",
    "\n",
    "### Installation\n",
    "\n",
    "> Prometheus was written in `golang`; hence, __it is contained in a single compiled executable.__\n",
    "\n",
    "Due to the above, its installation and deployment are simple and can be performed efficiently via many different approaches.\n",
    "\n",
    "- Go to [the download page](https://prometheus.io/download/), and download the specific version of Prometheus for your OS. (For Mac users, download the file for the OS labelled, Darwin).\n",
    "- Alternatively, run Prometheus inside a Docker container.\n",
    "\n",
    "We will use the last option. Since we are running Prometheus from a docker container, we need to bind the Prometheus config file to the container. This will allow Prometheus to update the config file using Docker commands. First, pull the Prometheus Docker image from Docker Hub using `docker pull prom/prometheus`. Thereafter, create a `prometheus.yml` file using the following simple code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "global:\n",
    "  scrape_interval: 15s # By default, scrape targets every 15 seconds.\n",
    "  # Attach these labels to any time series or alert when communicating with\n",
    "  # external systems (federation, remote storage, Alertmanager).\n",
    "  external_labels:\n",
    "    monitor: 'codelab-monitor'\n",
    "\n",
    "# A scrape configuration containing exactly one endpoint to scrape:\n",
    "# Here it is Prometheus itself.\n",
    "scrape_configs:\n",
    "  # The job name added as a label `job=<job_name>` to any timeseries scraped\n",
    "  - job_name: 'prometheus'\n",
    "    # Override the global default and scrape targets from the job every 5 seconds.\n",
    "    scrape_interval: '5s'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5a23e",
   "metadata": {},
   "source": [
    "Subsequently, we can build the image using the command below. However, you have to change the `/path/to/prometheus.yml/` path to the directory path in which your `prometheus.yml` config file is stored locally (watch out for file paths with spaces).\n",
    "\n",
    " Additionally, we need to add the `--web.enable-lifecycle` flag, as it enables the reloading of the config from the command line. \n",
    " \n",
    " Run the below command in your CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker run --rm -d \\\n",
    "    --network=host \\\n",
    "    --name prometheus\\\n",
    "    -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml \\\n",
    "    prom/prometheus \\\n",
    "    --config.file=/etc/prometheus/prometheus.yml \\\n",
    "    --web.enable-lifecycle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9066451c",
   "metadata": {},
   "source": [
    "### Docker options\n",
    "- `-d`: runs the container in detached mode. This will run your container in the background, thereby freeing up the terminal.\n",
    "- `--config.file`: the path where the `prometheus.yml` file will be found in the Docker container.\n",
    "- `-rm`: removes the container when you kill the process.\n",
    "- `--name`: specifies the name of the container.\n",
    "- `-v`: allows us to mount the prometheus config in the container to your local config in order to edit the config freely.\n",
    "- `-network=\"host`: maps the ports on the local machine to the ports inside the Docker container for when the localhost host is specified in `prometheus.yml`. \n",
    "\n",
    "At this point, Prometheus should be running; thus, __simply go to [`localhost:9090`](http://localhost:9090) in your browser,__ and you should see the following Prometheus dashboard.\n",
    "\n",
    "We recommend selecting local time so that the metrics are logged in your time zone. Additionally, select the query history so that you can track all queries made. Finally, configure dark mode (top-right corner) for the best appearance.\n",
    "\n",
    "<img src=\"images/prom_init_dash.png?modified=232132453\">\n",
    "\n",
    "Once you begin typing in the expression field, Prometheus will suggest queries. Some metrics will not be configured to be viewable currently, although Prometheus will track them. Therefore, we could start by running the expressions beginning with prometheus. \n",
    "\n",
    "For instance, run ``prometheus_build_info``, and execute the query. Notice that you can view detailed information about the query when you hover over it from the dropdown list.\n",
    "\n",
    "<img src=\"images/prom_expression_selected_dashboard.png?modified=232132453\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d031a",
   "metadata": {},
   "source": [
    "The results of the query are provided in the panel underneath. In this case, the result was \n",
    "\n",
    "`prometheus_build_info{branch=\"HEAD\", goversion=\"go1.17.1\", instance=\"localhost:9090\", job=\"prometheus\", revision=\"b30db03f35651888e34ac101a06e25d27d15b476\", version=\"2.30.2\"}`.\n",
    "\n",
    "- **metric name:** `prometheus_build_info` \n",
    "- **labels:** goversion, instance, job, etc.\n",
    "\n",
    "A test list of all metrics currently being logged by the Prometheus server can be found at `http://localhost:9090/metrics.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a3068",
   "metadata": {},
   "source": [
    "## Prometheus Configuration\n",
    "\n",
    "### Configuration file\n",
    "\n",
    "> __The Prometheus server is configured via [YAML](https://en.wikipedia.org/wiki/YAML) files.__ The configuration file is used to define the metrics and instances we intend to scrape. \n",
    "\n",
    "Previously, we ran Prometheus __with the default configuration file__ (click [`here`](http://localhost:9090/config) to view it) or by navigating to **Status > Configuration** from the dashboard.\n",
    "\n",
    "To obtain a better understanding of the config file, consider the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section with default values\n",
    "global:\n",
    "  scrape_interval: 15s # How frequently to scrape targets from jobs\n",
    "  scrape_timeout: 10s # If there is no response from the instance, do not attempt to scrape.\n",
    "  evaluation_interval: 15s # How frequently to evaluate rules (e.g. reload graphs with new data).\n",
    "# Prometheus alert manager (left for now).\n",
    "alerting:\n",
    "  alertmanagers:\n",
    "  - follow_redirects: true\n",
    "    scheme: http\n",
    "    timeout: 10s\n",
    "    api_version: v2\n",
    "    static_configs:\n",
    "    - targets: []\n",
    "# Specific configuration for jobs\n",
    "scrape_configs:\n",
    "- job_name: prometheus # Name of the job (can be anything).\n",
    "  honor_timestamps: true # Use timestamps provided by the job.\n",
    "  scrape_interval: 15s # Same as before, but for this job.\n",
    "  scrape_timeout: 10s # ^\n",
    "  metrics_path: /metrics # Where metrics are located w.r.t. the port (localhost:9090/metrics).\n",
    "  scheme: http # Configures the protocol scheme used for requests (localhost is http).\n",
    "  follow_redirects: true\n",
    "  static_configs:\n",
    "  - targets:\n",
    "    - localhost:9090"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3fc05",
   "metadata": {},
   "source": [
    "> __Prometheus provides numerous configuration options. Click [here](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) to view all of them.__\n",
    "\n",
    "*Things to note about the config file*\n",
    "- __Amazingly, Prometheus automatically reloads its configuration based on the config file. Therefore, you can make live changes__ (you can make live changes or even force it to reload using the `/-/reload` flag).\n",
    "- __If the file is not correctly formatted, it will not be updated__ (a valid **YAML** file is required).\n",
    "- All the parameters specified under `global` are available to all other scraper configs defined in the configuration file. In other words, if you create `scrape_config`, which retrieves metrics from EC2 instances, it will use the global parameters, unless explicitly changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b16314",
   "metadata": {},
   "source": [
    "By adding the `--web.enable-lifecycle` flag when creating the Docker container and mounting our local `prometheus.yml` file in the container, we should be able to make live changes to the config. To demonstrate, we change the default `scrape_interval` and `scrape_timeout` to 1 s. The start of your config should now be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e09f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section with default values\n",
    "global:\n",
    "  scrape_interval: 1s # How frequently to scrape targets from jobs\n",
    "  scrape_timeout: 1s # If there is no response from the instance, do not attempt to scrape.\n",
    "  evaluation_interval: 15s # How frequently to evaluate rules (e.g. reload graphs with new data).\n",
    "# Prometheus alert manager (left for now)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8a034",
   "metadata": {},
   "source": [
    "Now, we can run the `/-/reload` command to update the config while the Prometheus server is running. Once you have made the changes, run the following command in your terminal (use bash for this command):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e56ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:9090/-/reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef155a",
   "metadata": {},
   "source": [
    "Now, go to `localhost:9090/config`, and examine the config. It should be updated, and you should be able to view the changes made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c10485",
   "metadata": {},
   "source": [
    "### [scrape_configs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config)\n",
    "\n",
    "> This specifies the __sets of targets__ (i.e. what should be scraped) and parameters, describing how to perform the scraping action for each target. For instance, one can specify a `scrape_config` to scrape metrics from an EC2 instance or a config file to scrape metrics from Kubernetes.\n",
    "\n",
    "Targets can be defined __statically__ or __dynamically.__\n",
    "\n",
    "- `statically`: configured in our configuration YAML in `scrape_config`.\n",
    "- `dynamically`: using __service-discovery configurations__. By defining the service-discovery options, we can allow Prometheus to track new instances of a particular service.<br>\n",
    "    For example, in the industry, docker containers may be started, stopped, and created constantly based on the needs of the business.<br>\n",
    "    If the service-discovery configuration is defined, Prometheus will be able to track any newly created docker containers without having to specify them directly in the configuration file.\n",
    "\n",
    "Many service-discovery integrations are available for commonly used services:\n",
    "- [`consul_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config): retrieve scraping targets from HashiCorp's [Consul](https://www.consul.io/) used for service discovery and network setup.\n",
    "- [`dockerswarm_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config): used with [Docker's Swarm mode](https://docs.docker.com/engine/swarm/), which allows us to connect and orchestrate many containers as one application.\n",
    "- [`ec2_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config): retrieve scrape targets from EC2 instances.\n",
    "- [`kubernetes_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config): configure scrape targets from Kubernetes REST API.\n",
    "\n",
    "\n",
    "The full documentation on creating configs can be found [here](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2102dae",
   "metadata": {},
   "source": [
    "### Command line\n",
    "\n",
    "> Instance of the Prometheus server itself can be configured using command-line flags.\n",
    "\n",
    "Run the command below to see the available options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801796c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T18:54:21.063950Z",
     "start_time": "2021-04-11T18:54:20.306793Z"
    }
   },
   "outputs": [],
   "source": [
    "docker container run --rm prom/prometheus --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8f89d",
   "metadata": {},
   "source": [
    "The most notable options are\n",
    "\n",
    "- `--web.read-timeout=5m`: maximum duration before the reading of a request times out and idle connections are closed.\n",
    "- `--web.max-connections=512`: maximum number of simultaneous connections.\n",
    "- `--web.enable-lifecycle`: enable shutdown and reload via HTTP requests (i.e. requests to `/-/reload` mentioned previously).\n",
    "- `--web.page-title=\"...\"`: change the header of the webpage we ran previously.\n",
    "- `--storage.tsdb.retention.time`: the storage duration of the data (default: `15` days).\n",
    "- `--storage.remote.read-concurrent-limit=10`: the number of targets that can be read simultaneously.\n",
    "- `--log.level=info`: verbosity of the Prometheus server. One of the following can be set: `debug, info, warn or error`.\n",
    "\n",
    "__Note: These flags have their categories first, followed by more categories (optional), with option as the last one.__\n",
    "\n",
    "This is attributed to the structure of Prometheus and the language of choice, in this case, `golang`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df70797",
   "metadata": {},
   "source": [
    "## Exporters\n",
    "\n",
    "> Exporters are libraries that simplify the export of existing metrics from third-party systems and make them available to Prometheus to track. For example, an exporter for tracking GitHub metrics.\n",
    "\n",
    "There are hundreds of exporters, currently all in different states of development. The full list of the most common ones can be found [here](https://prometheus.io/docs/instrumenting/exporters/), and a complete list of all exporters on GitHub can be found [here](https://github.com/prometheus/prometheus/wiki/Default-port-allocations):\n",
    "- **official**: best practices, and verified by Prometheus (__always pick them whenever possible__).\n",
    "- **unofficial**: working, but not verified for best practices or may have overlapping functionalities.\n",
    "- **in development**: to be released as either of the two above.\n",
    "\n",
    "*Things to note*\n",
    "- __Most exporters occupy ports `9100` to `9999`__, and any new exporter should use it if available (explore the GitHub list above to determine which ports exporters run on).\n",
    "- There are a few exporters outside the standard range (see the GitHub list).\n",
    "\n",
    "To improve our understanding, we will employ one of the commonly used exporters, which will track the hardware and software metrics on your OS: __[Node exporter](https://github.com/prometheus/node_exporter)__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ed890",
   "metadata": {},
   "source": [
    "### Setting up Node exporter (Linux/MAC)\n",
    "\n",
    "> `Node` exporter is a single static binary that can be downloaded and run straight from the workstation.\n",
    "\n",
    "- The following command will download the exporter, and unpack the `.tar.gz` archive (__we assume you have a Linux-based system__).\n",
    "- You may the commands anywhere.\n",
    "- If on MacOS, run `brew install node_exporter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5d3a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T12:50:25.449113Z",
     "start_time": "2021-04-12T12:50:19.416349Z"
    }
   },
   "outputs": [],
   "source": [
    "# change the directory to where you want to download the node exporter.\n",
    "# Use wget to download the linux node exporter tarball.\n",
    "wget https://github.com/prometheus/node_exporter/releases/download/v1.1.2/node_exporter-1.1.2.linux-amd64.tar.gz  \n",
    "# Unpack the tarball image into the temp directory.\n",
    "tar xvfz node_exporter-1.1.2.linux-amd64.tar.gz\n",
    "# Remove the tarball file after extraction.\n",
    "rm node_exporter-1.1.2.linux-amd64.tar.gz\n",
    "# Run the node exporter (Linux).\n",
    "./node_exporter-1.1.2.linux-amd64/node_exporter\n",
    "# Run the node exporter (MAC).\n",
    "node_exporter --web.listen-address 127.0.0.1:9100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c6a57",
   "metadata": {},
   "source": [
    "__Note:__ Add the following alias to your `.bashrc`, which will simplify the unpacking process of the tarball archives on Linux:\n",
    "- `unpack` -> `tar xvzf`\n",
    "\n",
    "After running the `node_exporter`, the output should be similar to that shown below:\n",
    "\n",
    "<img src=\"images/node_exporter_output.png?modified=23213234453\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43aeecd",
   "metadata": {},
   "source": [
    "### Configuring Prometheus to scrape node exporter metrics\n",
    "\n",
    "Now that the exporters are running, we need to set up the `Prometheus` server to scrape data from them.\n",
    "\n",
    "We change the `prometheus.yml` config file to contain the setup below, which will allow the `node exporter` to scrape metrics from our system. \n",
    "\n",
    "#### For linux/MAC systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "global:\n",
    "  scrape_interval: '1s'  # By default, scrape targets every 15 seconds.\n",
    "  external_labels:\n",
    "    monitor: 'codelab-monitor'\n",
    "\n",
    "scrape_configs:\n",
    "  # Prometheus monitoring itself.\n",
    "  - job_name: 'prometheus'\n",
    "    scrape_interval: '10s'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "  # OS monitoring\n",
    "  - job_name: 'node'\n",
    "    scrape_interval: '5s'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9100']\n",
    "        labels:\n",
    "          group: 'production' # Notice that we have defined two nodes to be labelled in the production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd34c0",
   "metadata": {},
   "source": [
    "As can be observed, the basic layout for defining new targets to scrape is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_configs: # All targets will be defined in a config file.\n",
    "  - job_name: 'prometheus' # Name of the job.\n",
    "    scrape_interval: '10s' # Parameters defining how to scrape the target.\n",
    "    static_configs:              # Allow you to specify a list of targets.    \n",
    "      - targets: ['localhost:9090']  # Define the targets here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c4edc3",
   "metadata": {},
   "source": [
    "Now, we need to pass this config file to the `Prometheus` server (__recall that it is contained in Docker__). There are two options for achieving this. We have already implemented the first, one which allows us to edit the config file while the server is running. \n",
    "\n",
    "There are two ways to do that:\n",
    "- Mount the directory in your `localhost` to the Docker container during runtime:\n",
    "    \n",
    "    - Do this by running the `network=host` option when starting the container.\n",
    "    - Do this if the configuration changes often and you have autoreload set.<br><br>\n",
    "\n",
    "    \n",
    "- Create a new Docker image, and copy the configuration:\n",
    "    - Do this when the configuration is static and rarely changes.\n",
    "    \n",
    "Here, we will use the first approach. We will edit the configuration file locally and subsequently upload it to the server while it is running. For more information on the second approach using a `Dockerfile`, consult this page in the [documentation](https://prometheus.io/docs/prometheus/latest/installation/). \n",
    "\n",
    "At this point, we can push the locally updated `prometheus.yml` file to the Prometheus server using the same command used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:9090/-/reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa968e2",
   "metadata": {},
   "source": [
    "__Go to [`localhost:9090/targets`](http://localhost:9090/targets) to verify that everything was set up correctly.__ On the targets dashboard, you should be able to observe that both of our targets are in the state, **UP**; thus, metrics will be collected. Targets can also be in other predefined states:\n",
    "\n",
    "- **Down**: Prometheus cannot connect to the target.\n",
    "- **Unknown**: Prometheus cannot find the target, usually due to a configuration issue.\n",
    "\n",
    "<img src=\"images/prom_target_dash.png?modified=232133242234453\">\n",
    "\n",
    "From the targets window, click on the endpoint of the node exporter: `http://localhost:9100/metrics`, to obtain a text list of all the metrics that are being scraped and that are available to Prometheus. \n",
    "\n",
    "This can be helpful when unsure of the available metrics. \n",
    "\n",
    "<img src=\"images/prom_metrics_endpoint.png?modified=23213344232453\">\n",
    "\n",
    "Once done, go to [`localhost:9100`](http://localhost:9090), and check if the commands prefixed with `node` are available:__\n",
    "\n",
    "<img src=\"images/prom_win_commands.png?modified=232123422342453\">\n",
    "\n",
    "Run a few expressions to see the result of the graphs, and check `node_hwon_temp_celsius` which monitors the temperature of your system hardware.   \n",
    "\n",
    "<img src=\"images/prom_win_process.png?modified=232122323442453\">\n",
    "\n",
    "__In the next lesson, we learn how to query data efficiently__. Prior to that, however, we discuss how to monitor metrics from Docker containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd21fd5",
   "metadata": {},
   "source": [
    "### Configuring Prometheus to track docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad7054",
   "metadata": {},
   "source": [
    "To track Docker with Prometheus, we need to edit the Docker configuration so that we can specify its metric address. This will tell Prometheus where to collect the metrics. We do this by editing the `daemon.json` docker file or the Docker Desktop configuration file.\n",
    "\n",
    "- **For Linux users,** navigate to `/etc/docker/daemon.json`; if the file does not already exist, create it using `sudo touch daemon.json`.\n",
    "- **For MAC/Windows users,** go to Docker Desktop, and click the cog to go to **settings > Docker Engine**.\n",
    "\n",
    "Add this code to either the `daemon.json` file or the Docker Engine config to configure the scraping of metrics. Add the code to the section of the `.json` file where the `experimental` key value is located.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d89c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"metrics-addr\" : \"127.0.0.1:9323\",\n",
    "    \"experimental\": true,\n",
    "    \"features\": {\n",
    "    \"buildkit\": true\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ff2a5",
   "metadata": {},
   "source": [
    "For Docker Desktop, you will probably be required to restart your Docker Desktop application. \n",
    "\n",
    "On Linux, you will need to restart Docker using `sudo service docker restart`. You will also need to restart all your containers.\n",
    "\n",
    "The next step involves updating the `Prometheus.yml` file to add docker as a target so that Prometheus can begin scraping the metrics. Add docker as a target in your `Prometheus.yml` file, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c668d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global:\n",
    "  scrape_interval: '15s'  # By default, scrape targets every 15 seconds.\n",
    "  scrape_timeout: '10s'\n",
    "  external_labels:\n",
    "    monitor: 'codelab-monitor'\n",
    "\n",
    "scrape_configs:\n",
    "\n",
    "  # Prometheus monitoring itself.\n",
    "  - job_name: 'prometheus'\n",
    "    scrape_interval: '10s'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "\n",
    "  # OS monitoring\n",
    "  - job_name: 'wmiexporter'\n",
    "    scrape_interval: '30s'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9182']\n",
    "\n",
    "  # Docker monitoring\n",
    "  - job_name: 'docker'\n",
    "         # metrics_path defaults to '/metrics'\n",
    "         # scheme defaults to 'http'.\n",
    "    static_configs:\n",
    "      - targets: ['127.0.0.1:9323'] # Metrics address from our daemon.json file.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a0c85",
   "metadata": {},
   "source": [
    "Now, update the config on the Prometheus server while it is running, using the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238be85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:9090/-/reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616416f",
   "metadata": {},
   "source": [
    "Observe the Prometheus targets pane from the server, and see that docker is now available as an endpoint to collect metrics. Remember to check its endpoint/metrics to view a list of available metrics to track.\n",
    "\n",
    "Conventionally, for docker, they will start with `engine_daemon_container`.\n",
    "\n",
    "<img src=\"images/prom_targets_docker.png?modified=23212332453\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d7301",
   "metadata": {},
   "source": [
    "Now, we attempt to start some containers. Start from Prometheus docker containers (similar to the previous case) or use Docker hub to create some containers.\n",
    "\n",
    "Once they are running, you can run the expressions to view the output of the metrics. For example, here is the result after starting and stopping some containers from the `engine_daemon_container_states_containers` expression. \n",
    "\n",
    "<img src=\"images/prom_cont_states.png?modified=232132453\">\n",
    "\n",
    "Above, the red line indicates the containers stopping, while the blue line indicates the containers running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1eb45c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of how to\n",
    "- start a Prometheus server inside a Docker container.\n",
    "- access the Prometheus dashboard, and run some query expressions.\n",
    "- create a `prometheus.yml` configuration file for a server which can be updated while the server is running.\n",
    "- integrate the node exporter with the server to track hardware/software metrics from a machine.\n",
    "- set up Prometheus to track the docker service. \n",
    "- define the targets to track in the `prometheus.yml` config file. \n",
    "\n",
    "## Further Reading \n",
    "\n",
    "- Explore the alternatives to Prometheus (read about them [here](https://prometheus.io/docs/introduction/comparison/#)), and determine when they are suitable to use. \n",
    "- Read about Prometheus's [Push Gateway](https://prometheus.io/docs/practices/pushing/) and when it should be used.\n",
    "- Read briefly about alerts in Prometheus (e.g. when a failure occurs, you will be alerted via e-mail). Go through the [documentation](https://prometheus.io/docs/alerting/latest/overview/), and obtain a basic grasp (note, however, that alerting rules will be covered in the next lesson).\n",
    "- Read about how to integrate the local disk storage (which is limited) with Prometheus's remote storage [here](https://prometheus.io/docs/prometheus/latest/storage/).\n",
    "- Read about Prometheus Federation [here](https://prometheus.io/docs/prometheus/latest/federation/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash [conda env:.conda-AiCore] *",
   "language": "bash",
   "name": "conda-env-.conda-AiCore-bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
