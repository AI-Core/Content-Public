{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "## Introduction\n",
    ">Thus far, our learning has been largely centred on the extraction, transformation and loading of data. As mentioned multiple times, this is the 'core' of data engineering (ETL). However, all these operations work in tandem to create a workflow.\n",
=======
    "<a href=https://airflow.apache.org><img src=images/Airflow_logo.png width=400></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, you have learnt a lot of things about data: Extracting the data, Transforming the data, and Loading the data. As mentioned multiple times, this \"ETL\" is the \"core\" of Data Engineering. However, all these operations work in tandem to create a workflow.\n",
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
    "\n",
    "A workflow is a series of steps that are executed in a specific order. For example, to extract data from a source, transform it, and load it into a target, the following steps are required in order:\n",
    "\n",
    "1. __Step 1:__ __Extract__ data using, for example, the web-scraping skills you have acquired.\n",
    "2. __Step 2:__ __Transform__ the data using, for example, data-cleaning skills in pandas.\n",
    "3. __Step 3:__ __Load__ the data into a target, for example, a database located in your local environment or a remote environment.<br><br>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/WorkFlow1.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflows can also be very helpful when developing a ML model. For example, you might want to train a model with data, but do not exactly know which algorithm to use. In that case, these steps should be followed in order:\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "1. __Step 1:__ __Extract__ data using, for example, the web-scraping skills you have acquired.\n",
    "2. __Step 2:__ __Transform__ the data using, for example, data-cleaning skills in pandas.\n",
    "3. ___Step 3:__ __Train__ multiple models using the data, and obtain the accuracy of each model.\n",
    "4. __Step 4:__ __Choose__ the model with the highest accuracy."
=======
    "1. __Extract__ data using, for example, the webscraping skills you learnt\n",
    "2. __Transform__ the data using, for example, the data cleaning skills in pandas\n",
    "3. __Train__ multiple models using the data, and obtain the accuracy of each model\n",
    "4. __Choose__ the model with the highest accuracy"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"images/WorkFlow2.png\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "## Airflow as a Workflow Manager"
=======
    "# Airflow as a workflow manager"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow is a task-orchestration tool that allows you to define a series of tasks to be executed in a specific order. The tasks can be run in a distributed manner using Airflow's scheduler.\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Here, we will define a workflow using DAGs (Directed Acyclic Graphs). Each node in the DAG corresponds to a task, and the directional interconnections between them represent which order those tasks happen in."
=======
    "In Airflow you use Directed Acyclic Graphs (DAGs) to define a workflow. Each node in the DAG corresponds to a task, and they will be connected to one another."
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Installing airflow is as simple as running `pip install apache-airflow`; however, that might cause dependency errors. Thus, to prevent those errors, run the following commands in your __`terminal`__ (gitbash for Windows users). At the time of writing, the latest version of Airflow is 2.1.2. If you intend to use a different version, adjust the following code accordingly:\n",
=======
    "Installing airflow would be as simple as running `pip install apache-airflow`, however, that migh cause dependency errors. Thus, in order to prevent those errors, run the following commands in your __`terminal`__ . \n",
    "<details>\n",
    "  <summary><font size=\"+2\">IMPORTANT: For Windows Users</font></summary>\n",
    "  \n",
    "  If you are on Windows make sure to download Ubuntu from the Microsoft store and install it. Then, update everything: `sudo apt update && sudo apt upgrade` and install python3-pip: `sudo apt-get install python3-pip`. Then you can follow the instructions below.\n",
    "\n",
    "</details>\n",
    "\n",
    "At the time of writing, the version of Airflow is 2.1.3, if you are going to use a different version, change it in the following code:\n",
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
    "\n",
    "```\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "AIRFLOW_VERSION=2.1.3\n",
    "PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Now, the values of Airflow and the Python version are stored in two variables that will be utilised in the following command:"
=======
    "Now you are storing the values of Airflow and your Python version in two variables that are going to be used in the following command:"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "The above command provides the corresponding version of Airflow from the relevant GitHub repo."
=======
    "Now, you will get the corresponding version of Airflow from their github repo:"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow is ready for use. It is a good idea to initialise your database at this point. The database will contain metadata and host the DAGs you create:\n",
    "\n",
    "`airflow db init`\n",
    "\n",
    "Enter your credentials:\n",
    "\n",
    "```\n",
    "airflow users create \\\n",
    "    --username <your_username> \\\n",
    "    --firstname <your_firstname> \\\n",
    "    --lastname <your_lastname> \\\n",
    "    --role Admin \\\n",
    "    --email <your_email>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that everything works, run\n",
    "\n",
    "`airflow webserver --port 8080`\n",
    "\n",
    "This will start a new server at your localhost at port 8080. Notably, even if you start the server, your scheduled DAGs will not be monitored. To do this, the scheduler is required. Open a new terminal and run\n",
    "\n",
    "`airflow scheduler`\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Note that you might receive a Warning message; however, your airflow's current performance will not be affected."
=======
    "If you receive a Warning message, don't worry, it won't affect your airflow current performance. Now, we are ready to start using the UI"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to your browser and visit `localhost:8080`, your output should be similar to that in the figure:\n",
    "\n",
    "![](images/Airflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "The image above depicts the Airflow user interface (UI). Here, the created DAGs can be observed, and thus far, only some examples and tutorials created by the Airflow team are shown. The next step involves further exploration."
=======
    "The image above depicts the Airflow UI. Here, you can see the DAGs that have been created, and so far, you will only see some examples and tutorials created by the Airflow team. Let's explore it a little bit."
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the UI, you can explore the metadata of the DAGs, such as the name (or ID), owner, status of previous runs of the whole DAG or of specific tasks inside the DAG, frequency (in the Schedule column), and run history.\n",
    "\n",
    "For more details, click on the DAG. As an example, we observe the `example_bash_operator` DAG.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow2.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DAG, we can observe the structure, average run time of each task, Gantt chart of the DAG to determine if there are overlapping tasks, details of the DAG, and code that generated the DAG. Since this DAG has not been run, there is no info about previous runs. We can, however, take a look at the code. Before that, we observe the `Graph View` tab, which displays the same information as the `Tree View` tab, but rearranged:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow4.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there are several Nodes, each one representing a task. Additionally, their dependencies, e.g. `run_after_loop`, will not start until all `runme_x` haven't finished. \n",
    "\n",
    "To understand the working mechanism, we run a single task.\n",
    "\n",
    "1. In the Airflow UI, enable the `example_bash_operator` DAG. \n",
    "2. Click the DAG to view its status. You should see two runs, which is because (as we will see later) these examples were set to run 2 days ago, whereas the schedule depicts that it is meant to run once every day. Thus, two runs are appropriate.\n",
    "3. Inside the runs, there are different statuses. In this case, we see 'success' and 'skipped'. Note that they are meant to be skipped.\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow4.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Next, we examine its flow by triggering an event. First, click Auto-refresh to see updates in real-time, and subsequently, click the Play button:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_clip.gif\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Furthermore, we can see the duration of each task and when each run occurred. We encourage you to explore further; however, for now, we will examine the code. If you click on the `Code` tab, you will see this:"
=======
    "Pretty cool, isn't it? We can also see the durantion of each task, and when each run took place. But I will let you explore more on that in the UI. For now, let's take a look at the code. If you click on the `Code` tab, you will see this:\n"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example DAG demonstrating the usage of the BashOperator.\"\"\"\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "args = {\n",
    "    'owner': 'airflow',\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='example_bash_operator',\n",
    "    default_args=args,\n",
    "    schedule_interval='0 0 * * *',\n",
    "    start_date=days_ago(2),\n",
    "    dagrun_timeout=timedelta(minutes=60),\n",
    "    tags=['example', 'example2'],\n",
    "    params={\"example_key\": \"example_value\"},\n",
    ") as dag:\n",
    "\n",
    "    run_this_last = DummyOperator(\n",
    "        task_id='run_this_last',\n",
    "    )\n",
    "\n",
    "    # [START howto_operator_bash]\n",
    "    run_this = BashOperator(\n",
    "        task_id='run_after_loop',\n",
    "        bash_command='echo 1',\n",
    "    )\n",
    "    # [END howto_operator_bash]\n",
    "\n",
    "    run_this >> run_this_last\n",
    "\n",
    "    for i in range(3):\n",
    "        task = BashOperator(\n",
    "            task_id='runme_' + str(i),\n",
    "            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\n",
    "        )\n",
    "        task >> run_this\n",
    "\n",
    "    # [START howto_operator_bash_template]\n",
    "    also_run_this = BashOperator(\n",
    "        task_id='also_run_this',\n",
    "        bash_command='echo \"run_id={{ run_id }} | dag_run={{ dag_run }}\"',\n",
    "    )\n",
    "    # [END howto_operator_bash_template]\n",
    "    also_run_this >> run_this_last\n",
    "\n",
    "# [START howto_operator_bash_skip]\n",
    "this_will_skip = BashOperator(\n",
    "    task_id='this_will_skip',\n",
    "    bash_command='echo \"hello world\"; exit 99;',\n",
    "    dag=dag,\n",
    ")\n",
    "# [END howto_operator_bash_skip]\n",
    "this_will_skip >> run_this_last\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dag.cli()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tasks were to print out something to the console, we can confirm that on the Log tab of each task. For example, consider the `also_run_this` task. It is a BashOperator object that will print out `run_id={{ run_id }} | dag_run={{ dag_run }}`. Go to the `Graph View` tab, and click on the `also_run_this` task. In the next window, click `Log`. Observe the output:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_log.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "This was a simple walkthrough of the Airflow UI. \n",
    "We saw that\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "- the workflow is represented by a DAG.\n",
    "- each node in the DAG corresponds to a task.\n",
    "- each DAG has a schedule that sets the frequency of runs.\n",
    "- tasks can be triggered by previous tasks.\n",
    "- each task corresponds to an operator object.\n",
    "- bash scripts are executed by the BashOperator.\n",
    "- the DummyOperator does nothing (according to the documentation, _'this operator does nothing. It can be used to group tasks in a DAG.')._"
=======
    "- The workflow is represented by a DAG\n",
    "- Each node in the DAG corresponds to a task\n",
    "- Each DAG has a schedule that sets the frequency of runs\n",
    "- Tasks can be triggered by previous tasks\n",
    "- Each task corresponds to an operator object\n",
    "- We saw BashOperator, which execute a bash script\n",
    "- We saw DummyOperator, which according to the documentation _'Operator that does literally nothing. It can be used to group tasks in a DAG.'_"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "In the next section, we go over how to create more operators, e.g. a PythonOperator. First, however, we practise defining a DAG with the operators we have discussed thus far."
=======
    "We will see how to create more operators, for example, a PythonOperator, in the next section. First, let's get some practice defining a DAG with the operators we have seen so far."
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "## Creating a DAG"
=======
    "# Creating Your First DAG"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "First, ensure that you followed all the steps thus far. If followed correctly, you should have a folder in your home directory named airflow. _Check by running the following cell. If no error is thrown, move on to the next step._"
=======
    "First of all, make sure you followed all steps so far. If that's the case, you should have a folder in your home directory named airflow. _Check it by running running the following cell. If no error is thrown, you are good to go_\n",
    "<details>\n",
    "  <summary><font size=\"+1\">IMPORTANT: For Windows Users</font></summary>\n",
    "  \n",
    "  If you are on Windows make sure to check it on the wsl terminal. You can simply type `ls ~` and check if there is a folder\n",
    "\n",
    "</details>"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "import os\n",
    "\n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "assert os.path.isdir(airflow_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Inside that directory, add a new folder named `dags`. Airflow will search that directory for the DAGs you create through Python. The example DAGs should be located in your PATH directory, while newly created DAGs should be in `~/airflow/dags/`. _Note that the location of the new DAGs to be accessed by Airflow can be changed in the airflow.cfg file._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG"
=======
    "Inside that directory, you have to add a new folder named `dags`. Airflow will look into that directory to check if the DAGs you create through Python. Now, the example DAGs you are using are in your PATH directory, but new DAGs you create should be placed in `~/airflow/dags/`. _You can actually change the path where Airflow will look for new DAGs in the airflow.cfg file_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"+1\">IMPORTANT: For Windows Users</font></summary>\n",
    "  \n",
    "  If you are on Windows, go to the wsl console move to `cd ~/airflow`, and create the dags folder\n",
    "</details>"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "from pathlib import Path\n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "Path(f\"{airflow_dir}/dags\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python files you create must be stored in that folder. The file should contain the DAG with the desired arguments. The arguments can be passed to the context manager and a dictionary.\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "In the context manager, simply define the tasks. Do not implement any logical flow. As shown above, tasks are defined by operators."
=======
    "In the context manager, simply define the tasks, don't implement any logical flow. As saw above, tasks are defined by operators"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from airflow.operators.bash_operator import BashOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "default_args = {\n",
    "    'owner': 'Ivan',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['ivan@theaicore.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'start_date': datetime(2020, 1, 1),\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'end_date': datetime(2022, 1, 1),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'wait_for_downstream': False,\n",
    "    # 'dag': dag,\n",
    "    # 'trigger_rule': 'all_success'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(dag_id='test_dag',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='*/1 * * * *',\n",
    "         catchup=False,\n",
    "         tags=['test']\n",
    "         ) as dag:\n",
    "    # Define the tasks. Here we are going to define only one bash operator\n",
    "    test_task = BashOperator(\n",
    "        task_id='write_date_file',\n",
    "        bash_command='cd ~/Desktop && date >> ai_core.txt',\n",
    "        dag=dag)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example can be found in the `examples` folder, under the name `dag_test.py`. Copy the example to your `dags` folder in your airflow directory.\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Once the file is in the airflow directory, run the following command (if you haven't started the scheduler yet, run `airflow scheduler -D`):\n",
=======
    "<details>\n",
    "  <summary><font size=\"+1\">IMPORTANT: For Windows Users</font></summary>\n",
    "  \n",
    "  If you are on Windows, copy the file using the command line and use the `cp` command to copy the files to `cd ~/airflow/dags`. If you struggle with these commands, and you want to copy everything manually, follow these instructions to find the folder that stores the files from the Ubuntu console.\n",
    "</details>\n",
    "\n",
    "Once the file is in the airflow directory, you can run it by running the following command (if you haven't started the scheduler yet, run `airflow scheduler -D`):\n",
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
    "\n",
    "`airflow dags unpause test_dag`\n",
    "\n",
    "If you prefer that these DAGs appear in the UI, add them by running the following command:\n",
    "\n",
    "`airflow db init`\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "This will allow you to manage them in the UI."
=======
    "So you can manage them in the UI."
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a task; however, since a workflow is composed of more than one task, we add more tasks. If the tasks are specified, they will be executed in sequence, in no specific order (to change how they are executed, change the executor in the airflow.cfg file). However, you can specify their order by setting dependencies between them.\n",
    "\n",
    "Setting dependencies is quite simple. You can specify the tasks and thereafter 'connect' them using the bit-shift operator. For example, to run task `runme_1` after task `runme_0`, do the following:\n",
    "\n",
    "`task_0 >> task_1` or `task_0.set_downstream(task_1)` or `task_1 << task_0` or `task_1.set_upstream(task_0)`.\n",
    "\n",
    "Evidently, there are many ways to set the dependencies. Thus, simply pick the one that works for you.\n",
    "\n",
    "If you intend to run both `task_1` and `task_2` after `task_0`, do the following:\n",
    "\n",
    "`task_0 >> [task_1, task_2]`.\n",
    "\n",
    "If you intend to run `task_2` only after the completion of `task_0` and `task_1`, do the following:\n",
    "```\n",
    "task_0 >> task_2\n",
    "task_1 >> task_2\n",
    "```\n",
    "\n",
    "Finally, it is also possible to set sequential dependencies between tasks. For example, if you intend to run `task_2` after `task_1`, and `task_1` after `task_0`, do the following:\n",
    "\n",
    "`task_0 >> task_1 >> task_2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows a DAG with four tasks:\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "1. date_task: a BashOperator that appends the current date into a file.\n",
    "2. add_task: a BashOperator that stages the file created by date_task.\n",
    "3. commit_task: a BashOperator that commits the file staged by add_task.\n",
    "4. push_task: a BashOperator that pushes the committed file to a remote repository."
=======
    "1. date_task: A BashOperator that appends the current date into a file\n",
    "2. add_task: A BashOperator that stages the file created by date_task\n",
    "3. commit_task: A BashOperator that commits the file staged by add_task\n",
    "4. push_task: A BashOperator that pushes the committed file to a remote repository"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Ivan',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['ivan@theaicore.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'start_date': datetime(2020, 1, 1), # If you set a datetime previous to the curernt date, it will try to backfill\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'end_date': datetime(2022, 1, 1),\n",
    "}\n",
    "with DAG(dag_id='test_dag_dependencies',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='*/1 * * * *',\n",
    "         catchup=False,\n",
    "         tags=['test']\n",
    "         ) as dag:\n",
    "    # Define the tasks. Here we are going to define only one bash operator\n",
    "    date_task = BashOperator(\n",
    "        task_id='write_date',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && date >> date.txt',\n",
    "        dag=dag)\n",
    "    add_task = BashOperator(\n",
    "        task_id='add_files',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git add .',\n",
    "        dag=dag)\n",
    "    commit_task = BashOperator(\n",
    "        task_id='commit_files',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git commit -m \"Update date\"',\n",
    "        dag=dag)\n",
    "    push_task = BashOperator(\n",
    "        task_id='push_files',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git push',\n",
    "        dag=dag)\n",
    "    \n",
    "    date_task >> add_task >> commit_task\n",
    "    add_task >> push_task\n",
    "    commit_task >> push_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the DAG, you can observe the dependencies between the tasks. Definitely, you can set them all in tandem; however, in this case, we will discuss how to set the dependencies in different ways.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Dependencies.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running it, you will find that your repo updates every minute.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_GitHub.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Once you begin creating DAGs, it might be difficult to keep track of which ones are active. As a solution, airflow offers many commands to check your work in the command line. Type `airflow -h` to see all the commands."
=======
    "## Try it out\n",
    "\n",
    "1. Create a new remote repository in your GitHub account. \n",
    "2. You will eventually use if for storing weather data, so name your repository accordingly.\n",
    "3. Clone the repository to your local machine.\n",
    "4. Copy the DAG file `dag_test_dependencies.py` to the folder `dags` in your local machine.\n",
    "5. Change the file according to the name of your repository and the directory you cloned it to.\n",
    "6. Unpause the DAG by running `airflow dags unpause dag_test_dependencies` or by going to the `DAGS` tab in the UI and clicking on the `dag_test_dependencies` DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you start creating DAGs, you might forget which one are active. Good thing is that airflow has many commands to check your works in the command line. If you type `airflow -h` you can see all comands."
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: airflow [-h] GROUP_OR_COMMAND ...\n",
      "\n",
      "positional arguments:\n",
      "  GROUP_OR_COMMAND\n",
      "\n",
      "    Groups:\n",
      "      celery         Celery components\n",
      "      config         View configuration\n",
      "      connections    Manage connections\n",
      "      dags           Manage DAGs\n",
      "      db             Database operations\n",
      "      jobs           Manage jobs\n",
      "      kubernetes     Tools to help run the KubernetesExecutor\n",
      "      pools          Manage pools\n",
      "      providers      Display providers\n",
      "      roles          Manage roles\n",
      "      tasks          Manage tasks\n",
      "      users          Manage users\n",
      "      variables      Manage variables\n",
      "\n",
      "    Commands:\n",
      "      cheat-sheet    Display cheat sheet\n",
      "      info           Show information about current Airflow and environment\n",
      "      kerberos       Start a kerberos ticket renewer\n",
      "      plugins        Dump information about loaded plugins\n",
      "      rotate-fernet-key\n",
      "                     Rotate encrypted connection credentials and variables\n",
      "      scheduler      Start a scheduler instance\n",
      "      sync-perm      Update permissions for existing roles and optionally DAGs\n",
      "      version        Show the version\n",
      "      webserver      Start a Airflow webserver instance\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help         show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "airflow -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Thus, you can view the DAGs by running"
=======
    "Thus, you can look at the dags by running"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
     "name": "stdout",
=======
     "name": "stderr",
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
     "output_type": "stream",
     "text": [
      "'airflow' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "airflow dags list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "## Airflow Variables"
=======
    "# Airflow Variables"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably observed, in the dependencies, we had to constantly pass the file path to the BashOperator, which is not efficient. To improve efficiency, we define a variable that contains the path to the directory in which the file is stored.\n",
    "\n",
    "Airflow provides a channel to define variables from the UI or the command line. In this case, we will only use the UI. The variables you include in the UI will then be available in the Python code.\n",
    "\n",
    "Hence, open your UI, click on 'Admin' and subsequently on 'Variables'.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_variables.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next window, you can add your variables. You can import a file from your computer or click the `+` sign to add a variable manually.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_variables2.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next window, you can add the name of the variable in the Key and the value in the Value.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_variables3.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking `Save`, the variable will be stored in the Airflow database, and you will be able to use it in your Python code. To access it, you must import the class Variable:\n",
    "```\n",
    "from airflow.models import Variable\n",
    "\n",
    "weather_dir = Variable.get(\"weather_dir\")\n",
    "```\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "You should be able to use that variable in your Python code now. As shown in the script in `dag_test_variables.py`, the path to the file is defined using the variable, `weather_dir`."
=======
    "Now, you will be able to use that variable in your Python code. If you look at the script in `dag_test_variables.py`, you will see that we are using the variable `weather_dir` to define the path to the file."
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "## Python Operators"
=======
    "# Python Operators"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, it is possible to run bash commands in each task. However, Airflow is not limited to these commands. You can also create PythonOperators for any task, as long as they are contained in a Python function. As an example, we create a PythonOperator that will extract information about events that occurred 'On this day' in the past.\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "The first step is to create a function that uses requests and bs4 to download the relevant information from Wikipedia."
=======
    "The first thing you have to do is creating a function that uses requests and bs4 to download that information from Wikipedia. "
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from os.path import expanduser\n",
    "import requests\n",
    "\n",
    "def get_ul(url: str):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    return soup.find('ul')\n",
    "\n",
    "def get_today_events(url: str, file_dir: str):\n",
    "    ul_soup = get_ul(url)\n",
    "    for li in ul_soup.find_all('li'):\n",
    "        write_file(li.text, file_dir)\n",
    "\n",
    "def write_file(li: str, file_dir: str):\n",
    "    with open(file_dir, 'a') as f:\n",
    "        f.write(li)\n",
    "        f.write('\\n')\n",
    "\n",
    "home = expanduser(\"~\")\n",
    "desktop_dir = os.path.join(home, 'Desktop/test_2.txt')\n",
    "get_today_events('https://en.wikipedia.org/wiki/Wikipedia:On_this_day/Today', desktop_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions can be passed to the PythonOperator as arguments. Here, we will pass the function, `get_today_events`, to the PythonOperator. Note that two fields must be included in the PythonOperator:\n",
    "\n",
    "1. The task id\n",
    "2. The Python function to be executed\n",
    "3. The arguments of the function (optional)\n",
    "\n",
    "Notably, although functions are conventionally situated at the top of code, in this case, they are specified right before the PythonOperator.\n",
    "\n",
    "### Example\n",
    "\n",
    "Create a PythonOperator that will download the events that occurred 'on this day', as shown above. The file will be uploaded to a remote repository.\n",
    "\n",
    "1. Create a new remote repository on GitHub.\n",
    "2. Clone the repository to your local machine.\n",
    "3. Add a variable in the Airflow UI to set the path to the remote repository.\n",
    "4. Create the DAG, where you will call the function. Thereafter, stage the changes, commit them, and push them to the remote repository. The DAG should run daily.\n",
    "5. Move the DAG file to the `dags` folder in your local machine.\n",
    "6. Test the DAG by running `airflow dags test <Name of your DAG>`.\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "A small template has been provided in the examples folder."
=======
    "You have a small template in the examples folder"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "## Xcom: Connecting Tasks"
=======
    "# Xcom: Connecting Tasks"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a Python script, you may need to pass information from one function to another. When working with tasks in Airflow, you can achieve this using the Xcom feature.\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "Xcom stores the variables in a special database called XCom. For more information regarding XCom, read the [official documentation](https://airflow.apache.org/concepts.html#xcom). The variables can be stored while the tasks are running, and once completed, they can be retrieved and passed to the next task. Consider the example code below (contained in `dag_test_xcom.py`), which passes information between PythonOperators."
=======
    "Xcom will store the variables in a special database called XCom. You can read more about XCom in the [official documentation](https://airflow.apache.org/concepts.html#xcom). You can store those variables as the tasks are running, and when they are finished, you can retrieve them and pass them to the next tasks. Take a look at the next code (contained in `dag_test_xcom.py`),  passes information between PythonOperators.\n"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from random import uniform\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2020, 1, 1)\n",
    "}\n",
    "\n",
    "\n",
    "def training_model(ti):\n",
    "    accuracy = uniform(0.1, 10.0) # In this case, the accuracy is a random number\n",
    "                                  # but when you use it with your ML models, you\n",
    "                                  # can call your real models inside\n",
    "    print(f'model\\'s accuracy: {accuracy}')\n",
    "    ti.xcom_push(key='model_accuracy', value=accuracy)\n",
    "\n",
    "\n",
    "def choose_best_model(ti):\n",
    "    fetched_accuracy = ti.xcom_pull(\n",
    "                            key='model_accuracy',\n",
    "                            task_ids=['training_model_A'])\n",
    "    print(f'choose best model: {fetched_accuracy}')\n",
    "\n",
    "\n",
    "with DAG('test_dag_xcom',\n",
    "         schedule_interval='@daily',\n",
    "         default_args=default_args,\n",
    "         catchup=False) as dag:\n",
    "\n",
    "    downloading_data = BashOperator(\n",
    "        task_id='downloading_data',\n",
    "        bash_command='sleep 3'\n",
    "    )\n",
    "    training_model_task = [\n",
    "        PythonOperator(\n",
    "            task_id=f'training_model_{task}',\n",
    "            python_callable=training_model\n",
    "        ) for task in ['A', 'B', 'C']]\n",
    "\n",
    "    choose_model = PythonOperator(\n",
    "        task_id='choose_model',\n",
    "        python_callable=choose_best_model\n",
    "    )\n",
    "    downloading_data >> training_model_task >> choose_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an example of a use case. Eventually, you will employ it for actual data retrieval. As you can observe, in the functions you create, ti.xcom_push is employed to pass information to the next task, while ti.xcom_pull is employed to retrieve it. As shown in the following graph, the input is passed to each of the models, and their results are passed to a model chooser.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Xcom.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Once you run it, you will see these Xcoms in the UI:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Xcom2.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Finally, in the next window, you should see the results of the Xcoms.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Xcom3.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "If you observe the names of the Xcoms, you will find that there is a name for each of the models that have been run. In particular, there is one called `return_value` (In fact, there are many `return_value`s). These Xcoms correspond to the BashOperators that have been created, which, by default, will push their output so that it can be retrieved by any task in the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow and PosgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow enables the running of SQL queries in your tasks. You can connect to virtually any database. However, in this example, we will connect to a PosgreSQL database. Connecting to a database is very easy in Airflow, particularly because the UI offers a simple and intuitive method.\n",
    "\n",
    "Before proceeding, install the PosgreSQL client library by running the following command:\n",
    "\n",
    "`pip install apache-airflow-providers-postgres`\n",
    "\n",
    "Thenceforth, Airflow will know how to connect to a postgres database. If you intend to connect to a different database, install a different provider. Check out [this webpage](https://www.astronomer.io/guides/connections) for more information.\n",
    "\n",
    "As mentioned, the Airflow UI offers a nice method to connect to a database. Open your UI, click on 'Admin' and subsequently on 'Connections'.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_SQL.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next window, select the connection you wish to configure. Consider all the possibilities offered by Airflow. Click on the `PostgreSQL` connection, and you should see the following:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_SQL2.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the fields with your data, working with `schema` as the database name and `login` as your username. For demonstration, we will use the `Pagila` database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to use PostgreSQL from Airflow. In the next example, we will create a new table with animal names. The same code has been provided in `dag_test_sql.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"test_dag_postgre\",\n",
    "    start_date=datetime.datetime(2020, 2, 2),\n",
    "    schedule_interval=\"@once\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    create_pet_table = PostgresOperator(\n",
    "        task_id=\"create_pet_table\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS pet (\n",
    "            pet_id SERIAL PRIMARY KEY,\n",
    "            name VARCHAR NOT NULL,\n",
    "            pet_type VARCHAR NOT NULL,\n",
    "            birth_date DATE NOT NULL,\n",
    "            OWNER VARCHAR NOT NULL);\n",
    "          \"\"\",\n",
    "    )\n",
    "\n",
    "    populate_pet_table = PostgresOperator(\n",
    "        task_id=\"populate_pet_table\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "            INSERT INTO pet VALUES ( 1, 'Max', 'Dog', '2018-07-05', 'Jane');\n",
    "            INSERT INTO pet VALUES ( 2, 'Susie', 'Cat', '2019-05-01', 'Phil');\n",
    "            INSERT INTO pet VALUES ( 3, 'Lester', 'Hamster', '2020-06-23', 'Lily');\n",
    "            INSERT INTO pet VALUES ( 4, 'Quincy', 'Parrot', '2013-08-11', 'Anne');\n",
    "            \"\"\",\n",
    "    )\n",
    "\n",
    "    get_all_pets = PostgresOperator(\n",
    "        task_id=\"get_all_pets\", postgres_conn_id=\"postgres_default\", sql=\"SELECT * FROM pet;\"\n",
    "    )\n",
    "\n",
    "    get_birth_date = PostgresOperator(\n",
    "        task_id=\"get_birth_date\",\n",
    "        postgres_conn_id=\"postgres_default\",\n",
    "        sql=\"\"\"\n",
    "            SELECT * FROM pet\n",
    "            WHERE birth_date\n",
    "            BETWEEN {{ params.begin_date }} AND {{ params.end_date }};\n",
    "            \"\"\",\n",
    "        params={'begin_date': '2020-01-01', 'end_date': '2020-12-31'},\n",
    "    )\n",
    "\n",
    "\n",
    "    create_pet_table >> populate_pet_table >> get_all_pets >> get_birth_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for defining tasks is very similar to that for defining PythonOperators. The only difference is the inclusion of `postgres_conn_id` in the task definition. Thus, the `sql` argument will contain the query, and the params argument will contain the variables you wish to add to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you copy the code to the `dags` folder, you can test it by running `airflow dags test dag_test_sql`. Afterwards, in your pgAdmin, you should be able to see the table, `pets`:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_SQL3.png\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to connect to AWS RDS following the same steps used to connect to your localhost. Note that we are gradually integrating everything from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything we have done thus far can also be achieved easily using the command line. For example, to change the configuration of a connection, simply run the following command:\n",
    "\n",
    "`airflow connections add <connection_name> \\ --conn-uri \"conn-type://<user>:<password>@<host>:<port>/<database>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, you can create a new variable by running\n",
    "\n",
    "`airflow variables -s <var_name> <var_value>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, explore the official documentation. \n",
    "\n",
    "Nonetheless, you should be able to create Airflow DAGs in your EC2 instances without worrying about the Airflow UI. Simply leave it running, as shown in the `Cloud Basics` module, and you should have a schedule that runs without stopping."
=======
    "Observe the names of the Xcoms. There is a name for each of the models that have been run, and there is one that is called `return_value` (In fact, there are many `return_value`s). These Xcoms correspond to the BashOperators that have been created, which _by default_ will push their output, so any task in the DAG can retrieve it."
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "## Conclusion\n",
    "\n",
    "At this point, you should have a good understanding of"
=======
    "# Summary"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
    "- how to establish a workflow when dealing with tasks in parallel or in sequence.\n",
    "- how to define your tasks in Airflow in a way that is easy to understand and maintain.\n",
    "- how Airflow orchestrates tasks using DAGs.\n",
    "- how to define DAGs in a python script (Each task is defined by an Operator, which can be a PythonOperator, BashOperator, etc.).\n",
    "- how to configure the running of tasks using the Airflow UI. \n",
    "- how to view the progress of tasks in the UI.\n",
    "- how to configure Airflow via the UI to connect, to different databases or even AWS RDS."
=======
    "- When dealing with tasks working in parallel or sequence, you will need to establish a workflow.\n",
    "- Airflow allows you to define your tasks in a way that is easy to understand and maintain.\n",
    "- Airflow orchestrates these tasks using DAGs.\n",
    "- You can define your DAGs in a python script. Each task is defined by an Operator, which can be a PythonOperator, BashOperator, etc.\n",
    "- The Airflow UI allows you to configure how these tasks will run. \n",
    "- The UI can also show the progress of the tasks.\n"
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD:Cloud-and-DevOps/5. Apache Airflow/0. Apache Airflow/Notebook.ipynb
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
=======
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
>>>>>>> dev:units/Cloud-and-DevOps/9. Apache Airflow/0. Intro to Airflow/Notebook.ipynb
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
