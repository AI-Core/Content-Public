{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Pipelines\n",
    "\n",
    "## Introduction\n",
    "> Kubeflow pipelines is a platform in `kubeflow` employed to __define the end-to-end lifecycle of a `ML` project__ and facilitate the re-use of components.\n",
    "\n",
    "They can be operated through\n",
    "- `SDK` (`python` in our case): defines the pipeline.\n",
    "- `UI`: easy visualisation of the created pipelines.\n",
    "- `k8s` based engine: transpiles `kubeflow` format to `k8s` specific one (i.e. `.yaml` files).\n",
    "\n",
    "## Important Concepts\n",
    "\n",
    "Here are some important concepts associated with Kubeflow `pipelines:`\n",
    "- `pipeline`\n",
    "- `component`\n",
    "- `graph`\n",
    "- `experiment`\n",
    "- `run` and `recurring run`\n",
    "- `run trigger`\n",
    "- `step`\n",
    "- `output artifact`\n",
    "\n",
    "In this notebook, we will learn how to define each of them using Python's SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "As the first step, start `minikube` cluster using `minikube start` (__you should still use it for local development__).\n",
    "\n",
    "> `kubeflow-pipelines` can be installed as a standalone platform via `k8s` `kustomize`.\n",
    "\n",
    "This can be done directly from `GitHub` via the three commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PIPELINE_VERSION=1.6.0\n",
    "!kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\"\n",
    "!kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io\n",
    "!kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref=$PIPELINE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you might need to wait momentarily for the initialisation of the appropriate `POD`s.\n",
    "\n",
    "To view them, check the available `POD`s and their statuses in `kubeflow` namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T18:49:17.218047Z",
     "start_time": "2021-08-23T18:49:16.934836Z"
    }
   },
   "outputs": [],
   "source": [
    "!kubectl get pods --namespace kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all of the above are `Running` (__note that restarts may occur; however, they are inconsequential__), run the following command to `forward` port `80` of `kubeflow`'s UI `POD` to `localhost:8080`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, open `localhost:8080` in your web browser to view the UI.\n",
    "\n",
    "The output should be similar to that shown in the image below.\n",
    "\n",
    "![](./images/kubeflow-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete `kubeflow-pipelines`, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export PIPELINE_VERSION=1.6.0 # Optional, as we exported envvar previously\n",
    "\n",
    "# kubectl delete -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref=$PIPELINE_VERSION\"\n",
    "# kubectl delete -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SDK`\n",
    "\n",
    "> `kubeflow`'s `SDK` is provided as a `PyPI` package and named `kfp`.\n",
    "\n",
    "It communicates with `kubernetes` Python `SDK` and indirectly by transpiling graph to `.yaml` files.\n",
    "\n",
    "### Installation\n",
    "\n",
    "As is conventional, use `pip` for the installation (__you should conduct the process within `AiCore`'s `conda` environment__):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SDK` Packages\n",
    "\n",
    "Here, we present a high-level overview of the provided functionalities after installation:\n",
    "- __[`kfp.compiler`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.compiler.html) - class and methods for compiling `dsl` to `.yaml`__:\n",
    "    - [`kfp.compiler.Compiler`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.compiler.html#module-kfp.compiler) - compiles `pipeline` functions to `yaml` workflows.\n",
    "    \n",
    "Consider the example schematic code below (__we will explore all parts later__):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "  name='name',\n",
    "  description='description'\n",
    ")\n",
    "def my_pipeline(a: int = 1, b: str = \"default value\"):\n",
    "  ...\n",
    "\n",
    "Compiler().compile(my_pipeline, 'path/to/workflow.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`kfp.components`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html) - classes and methods for interacting with pipeline components.\n",
    "- [`kfp.dsl`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.dsl.html) - contains the domain-specific language for defining and interacting with pipelines and components:\n",
    "    - includes `Pipeline` definition (as shown above).\n",
    "    - __the most utilised `SDK` core package.__\n",
    "- [`kfp.client`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.client.html) - client libraries for [Kubeflow Pipelines API](https://www.kubeflow.org/docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec/); it allows us to create experiments, run pipelines and upload pipelines.\n",
    "- [`kfp.cli.diagnose_me`](https://github.com/kubeflow/pipelines/tree/master/sdk/python/kfp/cli/diagnose_me) - approaches for debugging an environment interactively; it returns various metadata that are useful for debugging the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `kfp` in the CLI\n",
    "\n",
    "After installation, `kfp` is also available as a command-line tool.\n",
    "- `kfp pipeline <COMMAND>` - for managing `pipeline`s; the commands include\n",
    "    - `get`: retrieves detailed information about a Kubeflow pipeline from the Kubeflow Pipelines cluster.\n",
    "    - `list`: lists the pipelines that have been uploaded to the Kubeflow Pipelines cluster.\n",
    "    - `upload`: uploads a pipeline to the Kubeflow Pipelines cluster.\n",
    "- `kfp run <COMMAND>` - for managing `kubeflow`'s runs.\n",
    "    - `get`: displays the details of a pipeline run.\n",
    "    - `list`: lists the recent pipeline runs.\n",
    "    - `submit`: submits a pipeline run.\n",
    "\n",
    "Consider the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T19:32:33.000418Z",
     "start_time": "2021-08-23T19:32:32.053431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: kfp pipeline [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Manage pipeline resources\r\n",
      "\r\n",
      "Options:\r\n",
      "  --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  delete          Delete an uploaded KFP pipeline\r\n",
      "  get             Get detailed information about an uploaded KFP pipeline\r\n",
      "  list            List uploaded KFP pipelines\r\n",
      "  list-versions   List versions of an uploaded KFP pipeline\r\n",
      "  upload          Upload a KFP pipeline\r\n",
      "  upload-version  Upload a version of the KFP pipeline\r\n"
     ]
    }
   ],
   "source": [
    "!kfp pipeline --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component\n",
    "\n",
    "> __This refers to a self-contained piece of code that executes one step in the `pipeline`.__\n",
    "\n",
    "![](./images/kubeflow-graph.png)\n",
    "\n",
    "In the above image, `Xgboost train` is an example of a component.\n",
    "\n",
    "> It is analogous to a __large function__ performing __one semantically valid addition operation.__\n",
    "\n",
    "Additionally, similar to functions, it has both a name and parameters/arguments, and it returns values and a body (code).\n",
    "\n",
    "> __Each component must be packaged as a `Docker` image as they are standalone execution units.__\n",
    "\n",
    "### Component code\n",
    "\n",
    "The `python` component code comprises two parts:\n",
    "- `client` - talks to endpoints to submit jobs, e.g. submitting `spark`'s `job`.\n",
    "- `runtime` - actual code, e.g. creating `pyspark.sql.DataFrame` from `SparkSession`.\n",
    "\n",
    "The accepted convention is to keep the component's code in a `package` named by it. For example,\n",
    "- `/component`: `client` modules. \n",
    "- `/component/component.py`: `client` code.\n",
    "\n",
    "### Component definition\n",
    "\n",
    "- `metadata`: name, description, etcetera.\n",
    "- `interface`: input/output specification (name, type, description, default value, etcetera).\n",
    "- `implementation`: actual code; it __also defines how to obtain `outputs` from it.__\n",
    "\n",
    "> __Since components are defined using `python`, transpiling to `k8s` readable `.yaml` definitions, as done above, is unrequired.__\n",
    "\n",
    "> __Note: In most cases, __make sure to check `Challenges.Mandatory.Components` for alternatives.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Function-Based Components\n",
    "\n",
    "> __Python function-based components allow us to define any component solely in `python`.__\n",
    "\n",
    "This approach alleviates the additional steps required for the `component` definition, namely:\n",
    "- defining the `Docker` image file.\n",
    "- defining the `.yaml` file with component definitions.\n",
    "\n",
    "### Merits\n",
    "\n",
    "- The files above will be automatically generated from the `python` code__. \n",
    "- Improved readability as `components` are defined like functions.\n",
    "- Relatively high development speed, since only `python` knowledge is required.\n",
    "\n",
    "### Demerits\n",
    "\n",
    "- Due to automation, it is difficult to customise (__However, this can be achieved after the transpilation by `kfp`__).\n",
    "- For more complicated use cases (e.g. `Docker` image with custom dependencies), a `Dockerfile` must be created as a base for the automation, which eliminates one of the merits.\n",
    "\n",
    "> __Note: Check `Challenges.Mandatory.Components` for a direct approach to creating `components`. __This knowledge is mandatory and will be verified.__\n",
    "\n",
    "Before proceeding, we create a [`kfp.Client`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.client.html) instance that will be used henceforth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T20:51:13.641199Z",
     "start_time": "2021-08-23T20:51:13.013944Z"
    }
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "# We are using default values\n",
    "# Host is automatically inferred from within `jupyter notebook`s hence not specified\n",
    "# In our case it would be localhost\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone Functions\n",
    "\n",
    "Before we proceed, here are a few things to `note` regarding functions, __which are considerably different from the conventional functions in `Python`__:\n",
    "\n",
    "- > __Code declaration outside the function is prohibited.__\n",
    "\n",
    "See the below example of a prohibited declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 12\n",
    "\n",
    "def foo():\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > __`Import` statements are used within the function.__\n",
    "\n",
    "Although there is no apparent 'best practice', we recommend `import`ing every necessary dependency at the top of your function and dividing them with code blocks. See the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T20:59:09.946746Z",
     "start_time": "2021-08-23T20:59:09.940990Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can't do that anymore.\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "def bar():\n",
    "    ###########################################################################\n",
    "    #\n",
    "    #                               IMPORTS\n",
    "    #\n",
    "    ###########################################################################\n",
    "\n",
    "    import numpy as np\n",
    "    import this  # Well, now I'm not so sure\n",
    "\n",
    "    ###########################################################################\n",
    "    #\n",
    "    #                                 SRC\n",
    "    #\n",
    "    ###########################################################################\n",
    "\n",
    "    return np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > __Helper functions are defined within the function__\n",
    "\n",
    "### Demerits\n",
    "\n",
    "As you can probably tell, the drawbacks of this approach are significant, particularly for relatively large and highly complicated functions, where transformation into separate microservices is impractical:\n",
    "\n",
    "- Long and semi-readable code (achieved by incorporating good practices).\n",
    "\n",
    "### Alternatives\n",
    "\n",
    "In the following cases, an alternative may be better:\n",
    "- For __long-term `services` carrying out laborious tasks.__\n",
    "- For optimisation while preserving code readability (disk `I/O` is discouraged because of its high cost).\n",
    "\n",
    "*Note:* \n",
    "\n",
    "- __There is an option to `cache` results; however,__ __this does not resolve the readability problem.__\n",
    "- __Conversely, one has to create a standalone program and define `.yaml` and `Dockerfile`.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Function-Based `Component`\n",
    "\n",
    "The necessary steps are outlined below.\n",
    "\n",
    "1. __Create a `standalone` function.__ \n",
    "\n",
    "*Things to note:*\n",
    "\n",
    "- Use the `typing` Python feature for `type` inference.\n",
    "- __The repercussions will be apparent when multiple values are returned.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T21:10:18.079352Z",
     "start_time": "2021-08-23T21:10:18.066646Z"
    }
   },
   "outputs": [],
   "source": [
    "def add(a: float, b: float) -> float:\n",
    "  '''Calculates sum of two arguments'''\n",
    "  return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Create [`kfp.dsl.ContainerOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.dsl.html#kfp.dsl.ContainerOp) using `kfp.components.create_component_from_func`.__\n",
    "\n",
    "*Things to note:*\n",
    "- A factory function will be created, which can be handled in different ways.\n",
    "- __This `op` should be used within `Pipeline` (described in more detail later).__\n",
    "- A `.yaml` component definition is automatically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T21:13:41.951459Z",
     "start_time": "2021-08-23T21:13:41.863270Z"
    }
   },
   "outputs": [],
   "source": [
    "add_op = kfp.components.create_component_from_func(\n",
    "    add, output_component_file=\"add_component.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Create `Pipeline` that runs our `op`(s).__\n",
    "\n",
    "Please note the `comments` in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T21:21:17.472041Z",
     "start_time": "2021-08-23T21:21:17.459209Z"
    }
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "  name='Addition pipeline',\n",
    "  description='An example pipeline that performs addition calculations.'\n",
    ")\n",
    "def add_pipeline(\n",
    "  a='1',\n",
    "  b='7',\n",
    "):\n",
    "  # Passes a pipeline parameter and a constant value to the `add_op` factory.\n",
    "  # function.\n",
    "  first_add_task = add_op(a, 4)\n",
    "  # Passes an output reference from `first_add_task` and a pipeline parameter\n",
    "  # to the `add_op` factory function. For operations with a single return\n",
    "  # value, the output reference can be accessed as `task.output` or\n",
    "  # `task.outputs['output_name']`.\n",
    "  second_add_task = add_op(first_add_task.output, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Create and `run` the pipeline from function.__\n",
    "\n",
    "> __Please run `kubeflow` port forwarding as described previously to view the `run`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T21:22:50.504639Z",
     "start_time": "2021-08-23T21:22:50.306215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/9b376543-1d00-40f9-bafe-55101b7a6061\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/85870662-5aa5-4c56-a73e-3a343237565c\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=85870662-5aa5-4c56-a73e-3a343237565c)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify argument values for your pipeline run.\n",
    "arguments = {'a': '7', 'b': '8'}\n",
    "\n",
    "# Create a pipeline run using the client you initialised in a prior step.\n",
    "client.create_run_from_pipeline_func(add_pipeline, arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your run should be visible within the `UI`. __We encourage you to explore the relevant info in the different `UI` sections.__\n",
    "\n",
    "![](./images/example-add-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Packages in Functions\n",
    "\n",
    "> To use custom `package`s, one has to install them within a `Docker` environment.\n",
    "\n",
    "Ordered from most recommended to least recommended, here are some approaches for achieving this:\n",
    "1. Contained within `Docker` image: __choose an appropriate base image for your `microservice`__ to prevent issues; __use the `base_image` argument of `kfp.components.create_component_from_func`.__\n",
    "2. Install packages within `Docker` image: __useful when `Docker` image has most of the `packages` and a few extras are required__; __use `packages_to_install` argument.__\n",
    "3. Install packages using `subprocess` (from your `function` code): this approach is highly __discouraged__; although highly discouraged, you may use it __only for local packages.__\n",
    "\n",
    "Consider the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.components.create_component_from_func(\n",
    "    # output_component_file is optional\n",
    "    my_op,\n",
    "    output_component_file=\"add_component.yaml\",\n",
    "    base_image=\"tensorflow/tensorflow:1.11.0-py3\",\n",
    "    packages_to_install=(\"torchdata==0.2.0\", \"torchlayers==0.1.1\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional info about `create_component_from_func`\n",
    "\n",
    "- Default image: `python:3.7`\n",
    "- __For relatively large dependencies, create the `Docker` base image from scratch and deploy it.__ This is because\n",
    "    - its component runs at significantly high speeds (no need to download packages).\n",
    "    - it is less error-prone (less likely to be OS-dependent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "> __`Inputs`, `outputs` and data passing in `kubeflow`.__\n",
    "\n",
    "In general,\n",
    "- `inputs` are `CLI` arguments for `Docker` containers within `PODs`.\n",
    "- `outputs` are returned as files.\n",
    "\n",
    "### Passing parameters\n",
    "Parameters are passed in different ways.\n",
    "- Basic types (e.g. `float`, `int` or short `str`) __are passed by value.__\n",
    "- Parameters passed by file include large data, such as\n",
    "    - `csv` files\n",
    "    - images\n",
    "    - datasets\n",
    "- __Considerably large `parameters` are stored in specified `PersistentVolumes`.__\n",
    "\n",
    "### Inferring `types`\n",
    "\n",
    "> `kubeflow`'s components created from functions can infer `dtype`s __via Python's typing feature.__\n",
    "\n",
    "Consider the generated `add`'s component `.yaml`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T22:52:10.717862Z",
     "start_time": "2021-08-23T22:52:10.557505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Add\r\n",
      "description: Calculates sum of two arguments\r\n",
      "inputs:\r\n",
      "- {name: a, type: Float}\r\n",
      "- {name: b, type: Float}\r\n",
      "outputs:\r\n",
      "- {name: Output, type: Float}\r\n",
      "implementation:\r\n",
      "  container:\r\n",
      "    image: python:3.7\r\n",
      "    command:\r\n",
      "    - sh\r\n",
      "    - -ec\r\n",
      "    - |\r\n",
      "      program_path=$(mktemp)\r\n",
      "      printf \"%s\" \"$0\" > \"$program_path\"\r\n",
      "      python3 -u \"$program_path\" \"$@\"\r\n",
      "    - |\r\n",
      "      def add(a, b):\r\n",
      "        '''Calculates sum of two arguments'''\r\n",
      "        return a + b\r\n",
      "\r\n",
      "      def _serialize_float(float_value: float) -> str:\r\n",
      "          if isinstance(float_value, str):\r\n",
      "              return float_value\r\n",
      "          if not isinstance(float_value, (float, int)):\r\n",
      "              raise TypeError('Value \"{}\" has type \"{}\" instead of float.'.format(str(float_value), str(type(float_value))))\r\n",
      "          return str(float_value)\r\n",
      "\r\n",
      "      import argparse\r\n",
      "      _parser = argparse.ArgumentParser(prog='Add', description='Calculates sum of two arguments')\r\n",
      "      _parser.add_argument(\"--a\", dest=\"a\", type=float, required=True, default=argparse.SUPPRESS)\r\n",
      "      _parser.add_argument(\"--b\", dest=\"b\", type=float, required=True, default=argparse.SUPPRESS)\r\n",
      "      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\r\n",
      "      _parsed_args = vars(_parser.parse_args())\r\n",
      "      _output_files = _parsed_args.pop(\"_output_paths\", [])\r\n",
      "\r\n",
      "      _outputs = add(**_parsed_args)\r\n",
      "\r\n",
      "      _outputs = [_outputs]\r\n",
      "\r\n",
      "      _output_serializers = [\r\n",
      "          _serialize_float,\r\n",
      "\r\n",
      "      ]\r\n",
      "\r\n",
      "      import os\r\n",
      "      for idx, output_file in enumerate(_output_files):\r\n",
      "          try:\r\n",
      "              os.makedirs(os.path.dirname(output_file))\r\n",
      "          except OSError:\r\n",
      "              pass\r\n",
      "          with open(output_file, 'w') as f:\r\n",
      "              f.write(_output_serializers[idx](_outputs[idx]))\r\n",
      "    args:\r\n",
      "    - --a\r\n",
      "    - {inputValue: a}\r\n",
      "    - --b\r\n",
      "    - {inputValue: b}\r\n",
      "    - '----output-paths'\r\n",
      "    - {outputPath: Output}\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./add_component.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the volume of code automatically generated by `kubeflow`, which includes\n",
    "- argument parsing via `argparse` module.\n",
    "- outputting values __and serialising them to the desired type.__\n",
    "- saving data within the `POD`'s storage (`PersistentVolume`).\n",
    "- the whole `.yaml` structure.\n",
    "\n",
    "> The above code is not meant to be readable as it is automatically generated.\n",
    "\n",
    "Therefore, in this case, the `type`s were inferred based on the __function signature.__ \n",
    "\n",
    "> __If the function signature is not provided, it is assumed that `str` types are being passed.__\n",
    "\n",
    "Notably, this function returns one value.\n",
    "\n",
    "> To return multiple values, __use `NamedTuple` from the `typing` module to decorate the function appropriately.__\n",
    "\n",
    "Consider the more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def multiple_return_values_example(a: float, b: float) -> NamedTuple(\n",
    "  'ExampleOutputs',\n",
    "  [\n",
    "    ('sum', float),\n",
    "    ('product', float),\n",
    "    ('mlpipeline_ui_metadata', 'UI_metadata'),\n",
    "    ('mlpipeline_metrics', 'Metrics')\n",
    "  ]):\n",
    "  \"\"\"Example function that demonstrates how to return multiple values.\"\"\"\n",
    "  sum_value = a + b\n",
    "  product_value = a * b\n",
    "\n",
    "  # Export a sample tensorboard\n",
    "  metadata = {\n",
    "    'outputs' : [{\n",
    "      'type': 'tensorboard',\n",
    "      'source': 'gs://ml-pipeline-dataset/tensorboard-train',\n",
    "    }]\n",
    "  }\n",
    "\n",
    "  # Export two metrics\n",
    "  metrics = {\n",
    "    'metrics': [\n",
    "      {\n",
    "        'name': 'sum',\n",
    "        'numberValue':  float(sum_value),\n",
    "      },{\n",
    "        'name': 'product',\n",
    "        'numberValue':  float(product_value),\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "\n",
    "  from collections import namedtuple\n",
    "  example_output = namedtuple(\n",
    "      'ExampleOutputs',\n",
    "      ['sum', 'product', 'mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "  return example_output(sum_value, product_value, metadata, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example returns `metadata` for `UI` and `metrics`.\n",
    "\n",
    "> Special `str` values can be used for these two values __to improve their readability by `metrics` and `UI`.__\n",
    "\n",
    "Additionally, we must return `namedtuple`, which is __the only way to forward multiple arguments.__\n",
    "\n",
    "> Please note that these values __will be saved to the `disk` anyway.__\n",
    "\n",
    "### Caching\n",
    "\n",
    "> Note that `kubeflow` provides caching out of the box.\n",
    "\n",
    "#### Working mechanism\n",
    "\n",
    "- If `component` was run previously __with the same arguments,__ this component __will not run.__\n",
    "- Instead, outputs from the `PersistentVolume` of choice will be forwarded to the next `component` within the `pipeline`.\n",
    "\n",
    "One can disable this feature or __force recalculation after some time__. For more information, see [here](https://www.kubeflow.org/docs/components/pipelines/caching/) and [here](https://www.kubeflow.org/docs/components/pipelines/caching-v2/) (`V2` SDK)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Parameters by File\n",
    "\n",
    "> __In most cases, `parameters` will be the files (e.g. dataset) on which we wish to operate.__\n",
    "\n",
    "This raises the question of what happens if the `data` are saved within the function and `None` is __returned implicitly.__\n",
    "\n",
    "In this case, there is no __appropriate approach__ for inferring what is actually returned from the function's signature return.\n",
    "Although there is a way, it is not Python compliant with static checkers, such as `mypy`.\n",
    "\n",
    "> As a solution, we can use special `kfp.components` types __to mark the output returned by the function.__\n",
    "\n",
    "See the simple example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_lines(\n",
    "    source_path: comp.InputPath(str),\n",
    "    odd_lines_path: comp.OutputPath(str),\n",
    "    even_lines_path: comp.OutputPath(str),\n",
    "):\n",
    "    \"\"\"Splits a text file into two files, with even lines going to one file\n",
    "    and odds lines to the other.\"\"\"\n",
    "\n",
    "    with open(source_path, \"r\") as reader:\n",
    "        with open(odd_lines_path, \"w\") as odd_writer:\n",
    "            with open(even_lines_path, \"w\") as even_writer:\n",
    "                while True:\n",
    "                    line = reader.readline()\n",
    "                    if line == \"\":\n",
    "                        break\n",
    "                    odd_writer.write(line)\n",
    "                    line = reader.readline()\n",
    "                    if line == \"\":\n",
    "                        break\n",
    "                    even_writer.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "- You can find other `Input` definitions (e.g. binary) [here](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html#kfp.components.InputBinaryFile).\n",
    "- You can find other `Output` definitions (e.g. binary) [here](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html#kfp.components.OutputBinaryFile)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Best Practices\n",
    "\n",
    "> Below is a compressed list of the best practices associated with using components (a full list can be found [here](https://www.kubeflow.org/docs/components/pipelines/sdk/best-practices/)).\n",
    "\n",
    "- __Local files__ should be used with components, __unless not possible__ (Cloud ML Engine and BigQuery require Cloud Storage staging paths).\n",
    "- Use the aforementioned __pure components__ (i.e. those without side-effects, and no modification should be done without 'informing `kubeflow`').\n",
    "- __Mix and match programming languages__: use other languages if `python` is not beneficial.\n",
    "    - Create a `Docker` image containing the `golang` app which requires some `input`s and outputs.\n",
    "    - Define `.yaml` directly.\n",
    "    - __Use inter-language formats for data exchange__ (e.g. `JSON`, `CSV`, `ProtoBuf`, etcetera).\n",
    "    - It is possible to perform minor pre-processing for different languages with `shell` scripts (__Minor__).\n",
    "- __One output == one file.__\n",
    "- __Do not pollute with temporary data__: temporary data are also__preserved by `PersistentVolume`.__\n",
    "- __Stay local whenever possible__: this approach is always easy and less stressful, e.g. when unit testing a specific component.\n",
    "- __Test in isolation__: use a single container; if not possible, run `minikube` or the like (easy to debug) before going with `kubeadm` and full-blown clusters.\n",
    "\n",
    "## AiCore Recommendations\n",
    "\n",
    "- __Do not use function-based components for highly complicated workflows__ as it allows you to use packages and `python` best practices more freely.\n",
    "- __Be careful of size__: keep the services within 'reasonable size'.\n",
    "    - use `microservice` to transform data instead of the following:\n",
    "        - `microservice` to load data.\n",
    "        - `microservice` to perform one operation on data.\n",
    "        - `microservice` to rotate an image.\n",
    "    - Due to the high cost of __I/O,__ its use should be minimised where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "> __This refers to the description of a ML `workflow`, including all of the `component`s in the `workflow` and how they combine in the form of a `graph`.__\n",
    "\n",
    "Generally, it consists of code packaged in `Docker` image with some `inputs` and `outputs` (as we will later see).\n",
    "\n",
    "By observing parts of the pipeline, we note the following:\n",
    "- Some of the components can easily run in parallel (in a different `POD` scheduled on a `Node`).\n",
    "- They are directly dependent on the previous steps (similar to `Airflow`).\n",
    "- Data are shared via `artifacts` __as `POD`s do not share data directly.__\n",
    "\n",
    "The above `graph` is defined via `SDK` (`python`) and the `dsl` (or rather pseudo-dsl) specifically created for this task.\n",
    "\n",
    "> Once all the concepts have been described, we will see how to create the whole structure in `python`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a `pipeline`\n",
    "\n",
    "Here, we learn how to define a `Pipeline` using `kfp`. However, prior to that, we define the following `component`s:\n",
    "- The first one downloads the `.tar.gz` file and returns CSV.\n",
    "- The second, which is __undefined,__ downloads resources from a `url`.\n",
    "- The last is the `add` component, which showcases the functionality of the `conditional` flow in `dsl`.\n",
    "\n",
    "Before proceeding, we transform `kfp.components.create_component_from_func` into a __configurable decorator.__\n",
    "\n",
    "> Note that `kfp.components.create_component_from_func` can be used as a decorator, __although only with default arguments.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Wrapper\n",
    "@functools.wraps(kfp.components.create_component_from_func)\n",
    "def our_component_from_func(*args, **kwargs)\n",
    "    def wrapper(function):\n",
    "        return kfp.components.create_component_from_func(function, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@our_component_from_func(\n",
    "    output_component_file=\"component.yaml\",  # This is optional. It saves the component spec for future use.\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=[\"pandas==1.1.4\"],\n",
    ")\n",
    "def merge_csv(file_path: comp.InputPath(\"Tarball\"), output_csv: comp.OutputPath(\"CSV\")):\n",
    "    import glob\n",
    "    import tarfile\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    tarfile.open(name=file_path, mode=\"r|gz\").extractall(\"data\")\n",
    "    df = pd.concat(\n",
    "        [pd.read_csv(csv_file, header=None) for csv_file in glob.glob(\"data/*.csv\")]\n",
    "    )\n",
    "    df.to_csv(output_csv, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To __reuse components__, simply provide the `URI` resource that contains the `.yaml` specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_downloader_op = kfp.components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/web/Download/component.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything in place, we can define the `pipeline` via the following steps:\n",
    "\n",
    "1. Define the `pipeline` function.\n",
    "2. Decorate the `pipeline` with `dsl.pipeline` and provide the necessary information therein.\n",
    "3. Pass it to `client.create_run_from_pipeline_func` following the steps shown previously __or__ compile it and run from the `UI`.\n",
    "\n",
    "Consider the below example with the first and second steps applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"Example pipeline\", description=\"Shows basics of pipelines\")\n",
    "# Define a pipeline and create a task from a component:\n",
    "# We do not have to specify the types\n",
    "def my_pipeline(url, run_add: bool):\n",
    "  web_downloader_task = web_downloader_op(url=url)\n",
    "  merge_csv_task = create_step_merge_csv(file=web_downloader_task.outputs['data'])\n",
    "  # Only `if`\n",
    "  with kfp.dsl.Condition(run_add):\n",
    "      first_add_task = add_op(a, 4)\n",
    "  # The outputs of the merge_csv_task can be referenced using the\n",
    "  # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the first option of running it non-interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_run_from_pipeline_func(\n",
    "    my_pipeline,\n",
    "    arguments={\n",
    "        \"url\": \"https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz\",\n",
    "        \"run_add\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we see the option of running it interactively via the `UI` within the `pipelines` tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path='pipeline.yaml',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64aeec5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of \n",
    "\n",
    "- the important concepts in Kubeflow pipelines and its installation process.\n",
    "- components and standalone functions.\n",
    "- how to use packages in functions.\n",
    "- how to pass parameters by file.\n",
    "- the best practices for using components.\n",
    "- pipelines and how to define them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-AiCore] *",
   "language": "python",
   "name": "conda-env-.conda-AiCore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
