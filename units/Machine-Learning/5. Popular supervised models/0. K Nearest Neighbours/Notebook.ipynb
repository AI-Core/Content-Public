{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour (KNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction: the Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">KNN is an extremely simple algorithm, which consists of the following steps:\n",
    "\n",
    "- Take the `K` nearest neighbors (nearest data points) by some metric (usually euclidean) (`K` is a hyperparameter).\n",
    "- Take the average of their respective regression values (for regression tasks) __or__ perform __majority voting__ for labels.\n",
    "- View your output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Nearest Neighbour\n",
    "\n",
    "Here, we discuss what it means to be the nearest neighbour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=center><img width=1000 src=images/knn_data_distances.jpg></p>\n",
    "\n",
    "__Note that the neighbourhood of an example in the train set includes itself.__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Special Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this case, our model is quite special in ML for the following reasons:\n",
    "- __It has no parameters to learn__; hence, it is a __non-parametric model.__\n",
    "- __No learning phase is required,__ i.e. it is a __lazy predictor.__\n",
    "- All the data must be kept __at all times__; hence, it is not the most memory-efficient model.\n",
    "- Although the predictions occur rapidly, they might be prone to overfitting because of `K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=300, centers=6, cluster_std=0.5, random_state=0\n",
    ")\n",
    "data = pd.DataFrame(\n",
    "    data=np.concatenate((X, y.reshape(-1, 1)), axis=1),\n",
    "    columns=[\"X1\", \"X2\", \"labels\"],\n",
    ")\n",
    "\n",
    "sns.lmplot(x='X1', y='X2', hue='labels', data=data, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Distance Calculation\n",
    "\n",
    "<p align=center><img width=900 src=images/knn_distance_measures.jpg></p>\n",
    "\n",
    "Conventionally, the Euclidian distance is utilised; however, we may choose to run the algorithm using different distance metrics.\n",
    "\n",
    "We will use `scipy` to increase the computation speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "distances = scipy.spatial.distance.cdist(X, X)\n",
    "print(X.shape)\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric will be passed to our `KNN` model as a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As the first step, we perform `KNN` implementation.\n",
    "\n",
    "- Create a `KNN` classs taking `k` and `distance` as hyperparameters (assign `None` to `self.X` and `self.y`).\n",
    "- Create a `fit` method taking `X` and `y`.\n",
    "- Create a `predict` method taking `X` and predicting the respective labels. To achieve that, we do the following:\n",
    "    - calculate distances between `self.X` and `X` using `self.metric`.\n",
    "    - perform a sorting-by-index operation ([`np.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)) along a specific axis. The output shape from this step should be `(self.X.shape[0], X.shape[0])`, i.e. the distance of each point of `self.X` to every other point in `X`.\n",
    "    - Choose at most `k` samples from `self.X` (__tip: simply slice with [: self.k]__). The output shape from this step should be `(K, X.shape[0])`(i.e. the number of neighbors, number of examples in X).\n",
    "    - Use `numpy`'s fancy indexing on `labels` (`self.y`) using the sorted indices. __Tip: attempt a simplest solution__. The output shape should be the same as those in the previous steps.\n",
    "    - Count how many labels `k` has for each example using `bincount2d`. The output shape should be: `(X.shape[0], classes)`, where `classes` is the number of unique classes in `y`. Can you pass the output from the previous step directly or do we have to transform it in order for the shapes to be right?\n",
    "    - Finally, return the most occurring label using [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) along a specific axis. The output shape should be `(X.shape[0],)` (vector containing labels for each example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "#### Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Determine the accuracy on the training dataset to ensure that everything works correctly (you can use the `sklearn.metrics` module).\n",
    "\n",
    "- What is the accuracy in this case and what is the reason for your answer?\n",
    "- How can it be changed __for the worse__ (by only varying hyperparameters?)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "def bincount2d(x):\n",
    "    N = x.max() + 1\n",
    "    ids = x + (N * np.arange(x.shape[0]))[:, None]\n",
    "    return np.bincount(ids.ravel(), minlength=N * x.shape[0]).reshape(-1, N)\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class KNN:\n",
    "    k: int\n",
    "    metric: typing.Callable[[np.array], np.array]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        assert hasattr(self, \"X\"), \"fit method should be called before predicting!\"\n",
    "        distances = self.metric(self.X, X)\n",
    "        labels_indices = np.argsort(distances, axis=0)[: self.k]\n",
    "        labels = y[labels_indices]\n",
    "        frequencies = bincount2d(labels.T)\n",
    "        return np.argmax(frequencies, axis=1)\n",
    "\n",
    "\n",
    "        \n",
    "clf = KNN(k=3, metric=scipy.spatial.distance.cdist)\n",
    "clf.fit(X, y)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(clf.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "[`Numba`](https://numba.pydata.org/) is a simple Python framework, which the authors describe as follows:\n",
    "\n",
    "> Numba is an open-source JIT compiler that translates a subset of Python and NumPy code into fast machine code.\n",
    "\n",
    "Its goal is to make code as fast as `numpy` (or even faster) while allowing the use of Python native functions (such as loops, if statements, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import time\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timer(function):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    print(f\"Elapsed time for {function.__name__}: {(time.time() - start)}\")\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)  # @njit is the same\n",
    "def numba_trace(a):\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):\n",
    "        trace += np.tanh(a[i, i])\n",
    "    return a + trace\n",
    "\n",
    "\n",
    "def python_trace(a):\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):\n",
    "        trace += np.tanh(a[i, i])\n",
    "    return a + trace\n",
    "\n",
    "\n",
    "def numpy_trace(a):\n",
    "    return a + np.trace(a)\n",
    "\n",
    "\n",
    "x = np.arange(1000000).reshape(1000, 1000)\n",
    "\n",
    "# Pure Python run\n",
    "with timer(python_trace):\n",
    "    python_trace(x)\n",
    "\n",
    "# Pure numpy run\n",
    "with timer(numpy_trace):\n",
    "    python_trace(x)\n",
    "\n",
    "# First run is slow because of compilation\n",
    "with timer(numba_trace):\n",
    "    numba_trace(x)\n",
    "\n",
    "# Now,it is fast\n",
    "with timer(numba_trace):\n",
    "    numba_trace(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About numba\n",
    "\n",
    "`Numba` mostly involves the use of decorators over functions (or classes in some cases); hence, it is easy to use.\n",
    "\n",
    "__Occasionally, you need to exert extra effort to understand why a code snippet does not work as intended; however, it is usually worth it.__\n",
    "\n",
    "### Compilation phase\n",
    "\n",
    "- The first time `numba` with the `njit` decorator is run, `numba` reads the Python bytecode, conducts analyses/optimisations and finally compiles it using [LLVM](https://llvm.org/). \n",
    "- The generated machine code is tailored to your specific CPU architecture (specific low-level instructions).\n",
    "\n",
    "### Tips\n",
    "\n",
    "- Use `numba` when it is difficult to vectorise the `numpy` code (__note that this should be your last resort; always attempt to realise the vectorised solution first__).\n",
    "- Use `numba` for functions that either take long to run (so the compilation time does not impact the runtime) or are run many times.\n",
    "- Be careful with arguments and their type specification (next notebook).\n",
    "- Use `njit` whenever possible.\n",
    "- Numba provides the `parallel` argument for decorators (for `njit` as well). Use it when a single loop iteration takes a long time and is independent of the next run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What we have seen above is called __majority voting__.\n",
    "\n",
    "> In majority voting, the label with the highest occurrence frequency is chosen.\n",
    "\n",
    "This explains why `K` is usually chosen to be an odd number to avoid conflicts (e.g. `2` votes for one label and `2` for another).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Weighted majority voting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Weighted majority voting occurs when we assign a weight for each example and take them into account.\n",
    "\n",
    "Weights are assigned based on many (often different) factors (based on the end goal). __For KNN, it is reasonable to use weights based on the similarity of the provided `X` examples to the ones we have trained on__.\n",
    "\n",
    "We have calculated the similarity based on the Euclidean distance; however, __please note that those are not directly used during voting.__. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Theoretical example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us assume that we have set `K=5` and consider a single `test` example:\n",
    "- We assume that one example from the training set has a euclidean distance to our `test` example equal to `0.1`.\n",
    "- We assume that this example has label `0`.\n",
    "- Now, let us imagine the distances for `4` other training samples to be, e.g. `1000` (so the samples are not similar).\n",
    "- Let us assume that these examples have label `1`.\n",
    "- __Majority voting would assign this example a label of `1`.__\n",
    "\n",
    "If we were to do 'weighted voting', the weight for a single example would probably be large enough (in comparison) to change the `test` example label to `0` (which is most probably correct for this example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we explore the steps required to implement weighted `KNN`.\n",
    "\n",
    "__We will use `numba` for convenience.__\n",
    "\n",
    "- __Tip:__ Take specific routines out of the class and implement them separately as helpers, as demonstrated below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Implement `_weighted_frequencies`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`_weighted_frequencies` get three arguments:\n",
    "- `result_array` of shape `(M, L)`, where `M` is the number of examples, and `L` is the number of unique labels. It is filled with `zeros`.\n",
    "- `labels` of shape `(M, K)`, where `K` is the number of neighbours. Each value in the `K` dimension is the respective `KNN` label.\n",
    "- `weights` of shape `(M, K)`. Each value in the `K` dimension is the weight given to the `K`-th neighbour.\n",
    "\n",
    "Now, using two nested loops, sum the weights for specific neighbours within `result_array` and return it (__tip:__ as those are zeros, you can simply add the appropriate weights at the appropriate index).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- How does the performance change when `njit` is changed to `jit` or when the decorator is removed?\n",
    "- What can be done to achieve a non-`1.0` accuracy when evaluating on the `training` dataset (note that you cannot sabotage the implementation; you can only vary the hyperparameters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def _weighted_frequencies(result_array, labels, weights):\n",
    "    for row in range(labels.shape[0]):\n",
    "        for column in range(labels.shape[1]):\n",
    "            result_array[row, labels[row, column]] += weights[row, column]\n",
    "\n",
    "    return result_array\n",
    "\n",
    "\n",
    "class WeightedKNN(KNN):\n",
    "    def predict(self, X):\n",
    "        distances = self.metric(self.X, X)\n",
    "        labels_indices = np.argsort(distances, axis=0)[: self.k]\n",
    "        labels = y[labels_indices].T\n",
    "        weights = 1 / (np.sort(distances, axis=0)[: self.k] + 1e-7).T\n",
    "        result_array = np.zeros((labels.shape[0], np.max(labels) + 1))\n",
    "        w_frequencies = _weighted_frequencies(result_array, labels, weights)\n",
    "        return np.argmax(w_frequencies, axis=1)\n",
    "\n",
    "\n",
    "clf = WeightedKNN(k=3, metric=scipy.spatial.distance.cdist)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"With compilation phase:\")\n",
    "with timer(WeightedKNN):\n",
    "    clf.predict(X)\n",
    "\n",
    "print(\"Compiled predict:\")\n",
    "with timer(WeightedKNN):\n",
    "    clf.predict(X)\n",
    "\n",
    "\n",
    "accuracy_score(clf.predict(X), y)\n",
    "\n",
    "# No njit\n",
    "\n",
    "# With compilation phase:\n",
    "# Elapsed time for WeightedKNN: 0.07559394836425781\n",
    "# Compiled predict:\n",
    "# Elapsed time for WeightedKNN: 0.054709672927856445\n",
    "\n",
    "# njit\n",
    "\n",
    "# With compilation phase:\n",
    "# Elapsed time for WeightedKNN: 0.23263049125671387\n",
    "# Compiled predict:\n",
    "# Elapsed time for WeightedKNN: 0.0071125030517578125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of KNN\n",
    "\n",
    "- We need to find the distance between each point and every other point. The time complexity of the algorithm is dominated by this process.\n",
    "- Examples that might be close in the feature space may not necessarily be close in the label space. \n",
    "    - For example, if examples have similar feature values for features that do not influence the output label, they will be close in the feature space, but not in the label space. \n",
    "    - Proximity assumption.\n",
    "- When working with high dimensional data, it is difficult to visualise the data and hand-pick a suitable `K` (however, we can still use `grid search` or a-like hyperparameter tuning methods).\n",
    "- When making predictions, we need to store the whole dataset in the model, which is inefficient in regards to the memory.\n",
    "- For the best results, we should always scale our features to prevent the prediction from being disproportionately influenced. However, with KNN, this can affect the distances between each example along each dimension of the feature space, resulting in different nearest neighbours. Conduct experiments with and without feature scaling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN for Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "KNN can also be employed for regression as well as classification, with the following differences:\n",
    "\n",
    "- Labels are not integer-class labels, but consist of continuous values.\n",
    "- Instead of majority voting, we simply take the mean of values (possibly weighted mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "At this point, you should have a good understanding of\n",
    "\n",
    "- the KNN algorithm and its limitations.\n",
    "- Numba and voting.\n",
    "- how to implement distance calculation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
