{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "> Here, we cover classification with more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary vs Multiclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As is known, in binary classification, the output must be either `True` or `False`.\n",
    "\n",
    "An example may either fall in this class or not. Nonetheless, as has been demonstrated, we can represent this by our model having a single output node whose value is forced between 0 and 1, representing the probability that the example belongs to the positive class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multiclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center><img width=1000 src=images/binary-class.jpg></p>\n",
    "\n",
    "A case that involves two nodes representing true and false is analogous to a case where two separate models are trained.\n",
    "\n",
    "The idea of treating `True` and `False` as separate classes with separate output nodes can be extended to multiclass classification. \n",
    "\n",
    ">Simply add more nodes, ensuring that their values are positive and sum to one.\n",
    "\n",
    "__Each node is a single `logit`, and all of them combined are later passed to `softmax`.__\n",
    "\n",
    "<p align=center><img width=1000 src=images/multiclass.jpg></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multiclass vs Multilabel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we will not explore the __multilabel__ case in depth; however, be aware of the following:\n",
    "\n",
    "> In a multilabel problem, each label can exist simultaneously instead of exclusively, similar to the case in multiclass.\n",
    "\n",
    "This might be a single vector, e.g. where we have `cat` and `dog` on a picture, but not a turtle:\n",
    "\n",
    "$$\n",
    "[1, 0, 1]\n",
    "$$\n",
    "\n",
    "> In multiclass, __there is always one '`1`' label__ (not less, not more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiclass, logits are outputted as well. The only difference is that __they will be a vector of values__. Each value in the output vector corresponds to a certain class.\n",
    "\n",
    "Assuming that we wish to classify the input image into one of threes classes: `{dog=0, cat=1, turtle=2}`, our model's output may be similar to this:\n",
    "\n",
    "$$\n",
    "    [-5, -3, 2]\n",
    "$$\n",
    "\n",
    "This would be a prediction of class `turtle` as its value is the highest.\n",
    "To obtain a label from this operation, we simply use [`argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html):\n",
    "\n",
    ">`argmax` returns the __index__ of the array entry with __the highest value.__\n",
    "\n",
    "Note that logits can be transformed into probabilities here as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">The **softmax function** exponentiates each value in a vector to make it positive and subsequently divides each of them by their sum to normalise them (ensure that their sum equals 1). </font>\n",
    "\n",
    "This ensures that the vector can be interpreted as a probability distribution.\n",
    "\n",
    "<p align=center><img width=1000 src=images/softmax.jpg></p>\n",
    "\n",
    "For example, we can replace each variable with values:\n",
    "\n",
    "<p align=center><img width=1000 src=images/softmax_example.jpg></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Differentiating the Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The softmax derivative differs based on the index of the element with respect to which we obtain the derivative. \n",
    "- If it is the same as the index of the element to which we applied softmax, the derivative becomes the equation at the bottom.\n",
    "- Otherwise, it is the one above it.\n",
    "\n",
    "<p align=center><img width=1000 src=images/softmax_deriv.jpg></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Softmax properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- An increase in the value of any entry corresponds to a decrease in the value of all the others; this is because the whole vector must always sum to one. \n",
    "- An increase in one input element results in an exponential increase in its corresponding output element and a simultaneous decrease in others; this indicates that __it is very easy for the largest output element to become dominant.__ \n",
    "- This results in `softmax` being overconfident, a problem to which there are several solutions, including `label smoothing`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Softmax vs argmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- As explained above, conventionally, one input is close to `1`, while all the others are close to `0`. This is similar to the `argmax` operation mentioned previously; however, it is 'soft' as it can be differentiated.\n",
    "- `argmax` changes abruptly. A small difference between two values would result in an output of either `0` or `1`. Contrarily, softmax changes gradually when the maximum changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example\n",
    "\n",
    "Let us implement a softmax function.\n",
    "\n",
    "It should accept `x` and divide by `sum` across `axis=1` (as we are normalising along the features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    exponentials = np.exp(x)\n",
    "    return exponentials / exponentials.sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Softmax\n",
    "\n",
    "As seen in the `sigmoid` case, this version also suffers from numerical instability. Consider the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-1a709be775cf>:5: RuntimeWarning: overflow encountered in exp\n",
      "  exponentials = np.exp(x)\n",
      "<ipython-input-1-1a709be775cf>:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return exponentials / exponentials.sum(axis=1).reshape(-1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[       nan, 0.        , 0.        ],\n",
       "       [0.01714783, 0.04661262, 0.93623955]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([[1000, 9, 8], [11, 12, 15]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the result is worse as we obtain `np.nan` due to overflow. The solution to the problem is to subtract the maximum value from each row.\n",
    "\n",
    "As `softmax` works along the horizontal axis (`1`) and all values sum to `1`, we are concerned with the absolute distance between the numbers in certain rows. \n",
    "\n",
    "This implies that we can divide __any value__ from them and still obtain the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.92408247e-01, 9.04959183e-04, 6.68679417e-03]]),\n",
       " array([[9.92408247e-01, 9.04959183e-04, 6.68679417e-03]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = np.array([5, -2, 0]).reshape(1, -1)\n",
    "subtracted = original - 6\n",
    "softmax(original), softmax(subtracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The value to subtract\n",
    "\n",
    "It is impossible to know the right `const` value to remove from each row (`1000` or `1_000_000`?). \n",
    "\n",
    "Fortunately, we can find the maximum in the whole batch of data and simply subtract that.\n",
    "\n",
    "### Example\n",
    "\n",
    "Implement a stable version of the `softmax` function:\n",
    "- subtract `np.max` from `logits` across `1` axis.\n",
    "- return the calculated exponential values as done previously.\n",
    "\n",
    "Here is the `stable softmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits - np.max(logits, axis=1).reshape(-1, 1))\n",
    "    return exps / np.sum(exps, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.01714783, 0.04661262, 0.93623955]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([[1000, 9, 8], [11, 12, 15]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "Our targets can be encoded in multiple ways. Conventionally, the class numbers are passed as follows (for `5` samples):\n",
    "\n",
    "$$\n",
    "[0, 3, 1, 1, 4]\n",
    "$$\n",
    "\n",
    "Alternatively, we could use one-hot encoding:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&[1, 0, 0, 0, 0]\\\\\n",
    "&[0, 0, 0, 1, 0]\\\\\n",
    "&[0, 1, 0, 0, 0]\\\\\n",
    "&[0, 1, 0, 0, 0]\\\\\n",
    "&[0, 0, 0, 0, 1]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since most of the data work with the first option, we will code the transformation of the `labels` into one-hot-encoding and vice-versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_one_hot(labels, max_labels: int = None):\n",
    "    if max_labels is None:\n",
    "        max_labels = np.max(labels) + 1\n",
    "    return np.eye(max_labels)[labels]\n",
    "\n",
    "\n",
    "def to_labels(one_hot):\n",
    "    return np.argmax(one_hot, axis=-1)\n",
    "\n",
    "\n",
    "data = np.array([0, 1, 0, 3, 5])\n",
    "to_one_hot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding can also be applied to the inputs. However, please exercise caution with using the whole output directly in the model as this can result in a problem. \n",
    "\n",
    "To ascertain the problem, examine the sklearn library called preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that one of the arguments that can be passed is `drop`. Can you decipher why we intend to drop a column from the one hot-encoder output?\n",
    "\n",
    "<details>\n",
    "  <summary>Click to view the answer.</summary>\n",
    "\n",
    "  When one-hot encoding is performed, all the columns in the output always sum up to one. This is because only one column is 1, while the rest are 0. Now, the problem with this is that these columns are correlated. \n",
    "  \n",
    "  In ML models, correlation can lead to numerically unstable solutions. For more information on the subject, consult [this page](https://en.wikipedia.org/wiki/Multicollinearity#Consequences).\n",
    "  \n",
    "  Therefore, when you perform one-hot encoding, you should drop one column from the output.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cross-Entropy Loss Function\n",
    "\n",
    "An appropriate loss function to use for multiclass classification is the __cross-entropy loss function__.\n",
    "\n",
    "- It is a __generalisation of the BCE loss;__ therefore, it will work in the binary case as well.\n",
    "- BCE is faster and more stable than the cross-entropy loss for the binary case; thus, __it should be created and used separately__.\n",
    "\n",
    "The cross-entropy loss uses the same term as the BCE loss: __the negative natural log of the output probability__ is utilised to penalise outputs exponentially as they stray from the ground truth.\n",
    "\n",
    "> There is no need to simultaneously push down the incorrect class probabilities and push up the correct class probabilities.\n",
    "\n",
    "Therefore, if we focus on increasing the correct class likelihood element, we will implicitly be decreasing the incorrect class likelihood elements.\n",
    "\n",
    "<p align=center><img width=1000 src=images/cross_entropy_loss.jpg></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Simple Linear Models\n",
    "\n",
    "We have learnt how to create and use `linear models` for regression and classification. Shortly, we will explore more powerful models. \n",
    "\n",
    "Here is a rough summary of when the simple models should be used in practice:\n",
    "\n",
    "- As a baseline: this gives us an overview and 'starting point' for improvement.\n",
    "- To realise an easily explainable model: each weight shows the impact of a factor onto our target.\n",
    "- When there are many features (even more than the data points), and we do not want to overfit the data.\n",
    "\n",
    "With time and experience, it will become increasingly apparent when to use each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing multiclass classification in sklearn is similar to implementing binary classification in sklearn. Logistic regression can be utilised for this purpose; however, in this case, instead of loading a dataset with two possible outputs, we will load the `iris` dataset, which has three possible outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    2\n",
       "146    2\n",
       "147    2\n",
       "148    2\n",
       "149    2\n",
       "Name: target, Length: 150, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "display(X)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the dataset has 150 samples, 4 features, and 3 different classes, each one corresponding to a different flower species.\n",
    "\n",
    "Now, we split the dataset into a training set and a test set, and train the logistic regression. One argument that can be passed to the LogisticRegression class is multi_class. By default, it is set to `auto`, which selects `ovr` for the binary classification and `multinomial` for the multiclass classification. \n",
    "\n",
    "- `ovr` means One-vs-Rest, which calculates the probabilities for each pair of classes, similar to the case in a binary classification problem.\n",
    "- `multinomial` calculates the probability distribution for all the classes; however, this does not mean that it is always more suitable for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset is a toy dataset; as such, it is not difficult to train a good model on it. Be suspicious if you get a score of 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "At this point, you should have a good understanding of\n",
    "\n",
    "- how classification can be implemented with more than two classes.\n",
    "- how to implement a multiclass classifier from scratch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
