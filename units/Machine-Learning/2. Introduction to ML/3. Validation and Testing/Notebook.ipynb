{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Testing\n",
    "\n",
    "## Introduction\n",
    "\n",
    "> ML models are only useful if they can __generalise__ and make good predictions on unseen data.\n",
    "\n",
    "To estimate the performance of a model on unseen data, the initial dataset is usually split into two sets: one for training and the other for testing.\n",
    "\n",
    "## The Test Set\n",
    "\n",
    "> The test set is used for evaluating if a model meets the desired requirements and for estimating its real-world performance. It is not employed for making choices about the model.\n",
    "\n",
<<<<<<< HEAD:Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
    "Sklearn provides a method, `train_test_split()`, in its `model_selection` module for splitting datasets."
=======
    "To estimate how well a model will perform on unseen data, we split our initial dataset into two different sets. One is for training on, and the other is for testing on.\n",
    "\n",
    "> The testing set is used for evaluating whether a model meets our requirements and estimating real world performance. That is all. It is not for making choices about our model.\n",
    "\n",
    "Sklearn provides a method `train_test_split()` in it's `model_selection` module to split our data."
>>>>>>> dev:units/Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "\n",
    "print(f\"Number of samples in dataset: {len(X)}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"Number of samples in:\")\n",
    "print(f\"    Training: {len(y_train)}\")\n",
    "print(f\"    Testing: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Validation Set\n",
    "\n",
    "The decision to choose between two models cannot be based on the testing set. If that were the case, the decisions would be biased, leaning towards our expectations. \n",
    "\n",
    "Making a decision based on the testing set is analogous to seeing the answers on a test.\n",
    "\n",
    "Instead, we create another set, called the __validation set__. This set is used for comparing models or different options for the same model, and the process is referred to as __cross validation.__\n",
    "\n",
<<<<<<< HEAD:Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
    "> The validation set is created by further splitting the training set."
=======
    "> We make the validation set by further splitting the training set."
>>>>>>> dev:units/Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_validation, y_test, y_validation = train_test_split(\n",
    "    X_test, y_test, test_size=0.3\n",
    ")\n",
    "\n",
    "print(\"Number of samples in:\")\n",
    "print(f\"    Training: {len(y_train)}\")\n",
    "print(f\"    Validation: {len(y_validation)}\")\n",
    "print(f\"    Testing: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation vs Test Sets\n",
    "\n",
    "People commonly misunderstand the difference between the validation set and the test set.\n",
    "\n",
    "The difference is that the validation set, not the test set, is employed to make choices about models.\n",
    "\n",
    "Such choices may include the following:\n",
    "\n",
    "- Should I deploy the linear regression model or the neural network?\n",
    "- Should I use the model which was trained on all the features of just the three I selected?\n",
    "- Which hyperparameters should I select (which you will learn about shortly)?\n",
    "\n",
<<<<<<< HEAD:Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
    "Consider the example:"
=======
    "Let's see that now"
>>>>>>> dev:units/Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# ML algorithms you will later know, don't panic\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "models = [\n",
    "    DecisionTreeRegressor(splitter=\"random\"),\n",
    "    SVR(),\n",
    "    LinearRegression()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_loss = mean_squared_error(y_train, y_train_pred)\n",
    "    validation_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    print(\n",
    "        f\"{model.__class__.__name__}: \"\n",
    "        f\"Train Loss: {train_loss} | Validation Loss: {validation_loss} | \"\n",
    "        f\"Test Loss: {test_loss}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "As you can observe, the best validation loss is for linear regression. This is the model we should choose. Unfortunately, it occurs that on 'real' (test) data, it performs worse than the Decision Tree.\n",
    "\n",
<<<<<<< HEAD:Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
    "Once again, conventionally, no information will be provided from the test loss; however, you should now be aware that this technique is imperfect (we will learn how to mitigate the effects later on)."
=======
    "Once again: usually we won't have information from test loss, but now you should know this technique is imperfect (we will see how to mitigate those effects later on)"
>>>>>>> dev:units/Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding\n",
    "\n",
    "In the code above, note the following line: `np.random.seed(2)`. This code line is important, and its role will be covered here.\n",
    "\n",
    "### Pseudo-random number generators\n",
    "\n",
    "Many ML algorithms employ random initialisation to, for example, instantiate the parameters of a linear regression model. Depending on the algorithm, it may have a more- or less-severe effect on the result.\n",
    "\n",
    "- Each time you run an algorithm randomly, the result may vary to some degree.\n",
    "- Random number generators employ a `seed`, which is a numerical value that determines what values will be generated.\n",
    "- For each run to be the same, (or to exhibit some phenomena similar to the case above), we should __always__ seed all functions using random numbers.\n",
    "\n",
    "The last one is quite easy in `numpy` and `sklearn` as it is a single line. Seeding via this approach is common in most frameworks.\n",
    "\n",
    "### Benefits of seed initialisation\n",
    "\n",
    "- To ensure the reproducibility of experiments, which is particularly important in ML.\n",
    "- To ensure an equal outcome for all runs.\n",
    "\n",
<<<<<<< HEAD:Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
    "> Always set a random seed to ensure that your results are reproducible when a part of your code involves the generation of random numbers."
=======
    "> Always set a random seed to make sure your results are repeatable when some part of the code involves random numbers being generated."
>>>>>>> dev:units/Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Leakage\n",
    "\n",
    "__Data leakage__ refers to a situation where a model has access to information about the testing sets. Definitely, in the real world, when working with new examples, the model will not know anything about the incoming data. This means that in training, the separation must be carefully preserved.\n",
    "\n",
    "> Never make any decisions about your model design using the test set.\n",
    "\n",
    "### Causes\n",
    "\n",
    "Data leakage can be caused by bad data splitting. These include the following cases:\n",
    "- Some samples are both in training and validation.\n",
    "- The model is evaluated based on its performance on the training data.\n",
    "\n",
<<<<<<< HEAD:Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
    "However, note that there are less noticeable causes of data leakage as well. One such cause is the computation of some statistics (such as the mean) or new features (such as the difference from the mean) based on data that has not been split, and the subsequent use of the obtained values in training after data splitting. Such statistics contain data from the testing set, which have since been split off.\n",
    "\n",
    "Consider the example:"
=======
    "Let's see an example in action..."
>>>>>>> dev:units/Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_loss(X_train, y_train, X_validation, y_validation):\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Without data leakage, train on train, validate on validation\n",
    "    model.fit(X_train, y_train)\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    validation_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "\n",
    "    print(f\"Validation loss: {validation_loss}\")\n",
    "    \n",
    "# Without data leakage, train on train, validate on validation\n",
    "calculate_validation_loss(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "# With data leakage, 50 samples from validation added\n",
    "fail_X_train = np.concatenate((X_train, X_validation[:50]))\n",
    "fail_y_train = np.concatenate((y_train, y_validation[:50]))\n",
    "\n",
    "calculate_validation_loss(fail_X_train, fail_y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
    "As expected, the model exhibits a __false__ good performance, since it was exposed to a part of the validation data."
=======
    "As expected, as the model saw part of validation data and it __falsely__ performs better on it."
>>>>>>> dev:units/Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of\n",
    "\n",
<<<<<<< HEAD:Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
    "- cross-validation.\n",
    "- the test set.\n",
    "- random seeding.\n",
    "- data leakage."
=======
    "- Validation set is used to find info about best algorithms, best set of arguments to algoirthms etc.\n",
    "- Test set is used to check how our algorithm performs on unseen data\n",
    "- __As we tune algorithms according to `validation` dataset we cannot use it to check performance__\n",
    "- `seed` is used to ensure reproducibility. Also multiple runs for experiments are good if our code depends on random initialization heavily (we can take mean results of experiments)\n",
    "- Data leakage is information from `validation` (or `test`) leaking into training\n",
    "- Data leakage leads to falsely good results and should be avoided\n",
    "- Rule of thumb: imagine you only have training dataset when doing preprocessing. Anything you calculate from it cannot be used in `validation` or `test`"
>>>>>>> dev:units/Machine-Learning/2. Introduction to ML/3. Validation and Testing/Notebook.ipynb
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('main': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
