{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    ">__Linear regression predicts continuous outputs__, hence the regression part of the name.\n",
    "\n",
    "Linear regression makes predictions that are simply a __`w`eighted__ combination (a linear combination) of the inputs (plus some offset called __`b`ias__). It is described by a linear function:\n",
    "\n",
    "$$\n",
    "    y = wx + b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=center><img width=700 src=images/linear_model.jpg></p>\n",
    "\n",
    "In the future, we will explore much more complex, nonlinear relationships between the features and labels that we wish to model. \n",
    "\n",
    "However, __do not underestimate linear regression__ as it is often used in statistics and to explain numerous phenomena. At the end of the lesson, we will see when it should be used in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "Once again, we will use the `California` dataset from `sklearn`. Additionally, we will split it into validation and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 8) (14448,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, model_selection\n",
    "\n",
    "# 15% for validation and test, 70% for training in total\n",
    "X, y = datasets.fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_validation, X_test, y_validation, y_test = model_selection.train_test_split(\n",
    "    X_test, y_test, test_size=0.5\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hypothesis\n",
    "\n",
    "The functions that a model represents are often referred to as the **hypothesis**.\n",
    "\n",
    "<p align=center><img width=800 src=images/linear_model_example.jpg></p>\n",
    "\n",
    "We will create a model that can make predictions for many examples simultaneously by expressing the hypothesis in a vector form, as shown below.\n",
    "\n",
    "<p align=center><img width=700 src=images/linear_model_vector.jpg></p>\n",
    "\n",
    "Here is what the computation might look like numerically.\n",
    "\n",
    "<p align=center><img width=700 src=images/linear_model_vector_example.jpg></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formula of the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula below presents a linear regression for a single example __with multiple features__:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y = w_1x_1 + w_2x_2 + ... + w_Nx_N + b = \\sum_{i=1}^{N} w_ix_i + b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Essentially, each feature in our sample is multiplied by a weight.\n",
    "\n",
    "Next, we implement our first ML model in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features\n",
    "\n",
    "Here, we will explore multiple features. Here is what our weights will look like:\n",
    "\n",
    "<p align=center><img width=700 src=images/w_vector.jpg></p>\n",
    "\n",
    "The weights variable (w) becomes a row vector; consequently, we need to transpose it when we multiply it by the X matrix (or take a `dot` product of `data` and `weights`).\n",
    "\n",
    "<p align=center><img width=800 src=images/vector_linear_regression.jpg></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate vs Multivariate\n",
    "\n",
    "Occasionally, you may find a dychotomy between univariate and multivariate linear regressions.\n",
    "\n",
    "> Univariate linear regression involves __one or multiple features,__ but __a single prediction target.__\n",
    "\n",
    "> Conversely, __multivariate linear regression,__ as you may have guessed, involves __one or multiple variables (features),__ but __multiple prediction targets__ (which are correlated with each other).\n",
    "\n",
    "In this notebook, we will explore __univariate__ only; however, we will cover __multivariate__ when we learn multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The task here is `LinearRegression` implementation.\n",
    "\n",
    "- Create a class `LinearRegression` that accepts a single `n_features` argument during initialisation.\n",
    "    - Create `W` and `b` variables during initialisation. One of shape `(n_features, 1)` and `bias` of shape `1`, initialised with a random normal distribution.\n",
    "- Create a `__call__` function (find out what it does and what a functor is) that accepts `X` (`np.array`). It should return the predictions made by our linear-regression analysis (see the formulas in the figure above; only two operations are required).\n",
    "- Create the `update_params` function which accepts `W` and `b` and assigns them to appropriate variables in `self`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, n_features: int): # initalise parameters\n",
    "        np.random.seed(10)\n",
    "        self.W = np.random.randn(n_features, 1) ## randomly initialise weight\n",
    "        self.b = np.random.randn(1) ## randomly initialise bias\n",
    "        \n",
    "    def __call__(self, X): # how do we calculate the output from an input in our model?\n",
    "        ypred = np.dot(X, self.W) + self.b\n",
    "        return ypred # return prediction\n",
    "    \n",
    "    def update_params(self, W, b):\n",
    "        self.W = W ## set this instance's weights to the new weight value passed to the function\n",
    "        self.b = b ## do the same for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [[1482.14731583]\n",
      " [3169.69576072]\n",
      " [1131.67756962]\n",
      " [ 756.45841866]\n",
      " [ 963.42420785]\n",
      " [ 696.58323643]\n",
      " [ 859.12613003]\n",
      " [ 332.18098164]\n",
      " [ 890.04023052]\n",
      " [ 496.39589958]]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(n_features=8)  # instantiate our linear model\n",
    "y_pred = model(X_test)  # make predictions with data\n",
    "print(\"Predictions:\\n\", y_pred[:10]) # print the first 10 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(y_pred, y_true):\n",
    "    samples = len(y_pred)\n",
    "    plt.figure()\n",
    "    plt.scatter(np.arange(samples), y_pred, c='r', label='predictions')\n",
    "    plt.scatter(np.arange(samples), y_true, c='b', label='true labels', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Sample numbers')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(y_pred[:10], y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "As you can observe, the predictions of our model are __way off__. This occurred because we initialised our model with random weights and biases.\n",
    "\n",
    "In the next section, we see how to improve this model to learn from data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "Loss enables the measurement of the performances of models.\n",
    "\n",
    "As mentioned in the ML system design process, we need to set a baseline and attempt to surpass it as we train more models. However, to obtain a quantitative measure of the model performance, the loss value is required.\n",
    "\n",
    "> The **loss** should measure __the performance of our model__. \n",
    "\n",
    "The larger the value, the worse the model's performance. Thus, we should strive to __minimise__ the loss value, i.e. bring it as close to zero as possible.\n",
    "\n",
    "> The loss value needs to return a **single number**, not a vector or a matrix.\n",
    "\n",
    "__NOTE:__ minimising the objective is equivalent to maximising its negative. \n",
    "\n",
    "Commonly, the loss value is also called the __cost function__, although they do not have an exact relationship. We will go over the differences.\n",
    "\n",
    "### Squared-error loss\n",
    "\n",
    "> The loss is a function that accepts predictions and true labels and returns __a positive scalar.__\n",
    "\n",
    "- The higher the loss value, the worse the model's performance.\n",
    "- The __loss is defined on a single data point.__\n",
    "\n",
    "The squared error is one of the loss functions __used for regression tasks,__ and it is simply defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    (\\hat{y} - y)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This calculates the error (i.e. the difference between our model's prediction $\\hat{y}$ and the true value $y$):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\hat{y} - y\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Subsequently, it squares it to make the value positive. Provided that the error is not zero, the loss value will be increased, regardless of whether the prediction is below (negative error) or above (positive error) the label value.\n",
    "\n",
    "### Mean-squared error (MSE) cost function\n",
    "\n",
    "> The cost function is a generalisation of the loss functions for many data samples.\n",
    "\n",
    "Therefore, the __loss__ operates on a single sample, while the __cost__ operates on multiple samples.\n",
    "For the __MSE,__ we calculate the squared error for each sample and take the mean of that value:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    L_{mse} = \\frac{1}{N}\\sum_{i}^{N}(\\hat{y_i} - y_i)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "There are many other criteria that are useful for different tasks (e.g. the binary cross entropy (BCE) loss for classification, which we will cover later).\n",
    "\n",
    "As a demonstration, we will write a function to calculate the cost using the MSE loss function. It should accept an array of predictions for different example inputs, as well as an array of the corresponding example labels. Additionally, it should return a single number (scalar) that represents the MSE loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Implement a `mean_squared_error` function that accepts `y_pred` and `y_true`. All the required formulas have been provided above (focus on the last one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_pred, y_true):  # define our criterion (loss function)\n",
    "    errors = y_pred - y_true  ## calculate errors\n",
    "    squared_errors = errors ** 2  ## square errors\n",
    "    return np.mean(squared_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cost = mean_squared_error(y_pred, y_train)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the MSE: the analytical solution \n",
    "\n",
    "Now that we have our __loss__ equation, we can calculate its derivative w.r.t. the weights. When we set it to zero, we can calculate the __weights values (`W`),__ which minimise the MSE.\n",
    "\n",
    "<p align=center><img width=900 src=images/analytical_linear_reg.jpg></p>\n",
    "\n",
    "Next, we implement this analytical solution for the least-squares regression in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Now that we have the mathematical formula, we can proceed to the implementation.\n",
    "\n",
    "- For the matrix inverse, use the [`np.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) function.\n",
    "- Remember to return the `weights` part of the `matrix` first and the `bias` afterwards (`bias` is the `0` element of the result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minimize_loss(X_train, y_train):\n",
    "    X_with_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    optimal_w = np.matmul(\n",
    "        np.linalg.inv(np.matmul(X_with_bias.T, X_with_bias)),\n",
    "        np.matmul(X_with_bias.T, y_train),\n",
    "    )\n",
    "    return optimal_w[1:], optimal_w[0]\n",
    "\n",
    "\n",
    "weights, bias = minimize_loss(X_train, y_train)\n",
    "print(weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, there is no mention of the model bias in this analytical solution. \n",
    "In fact, we incorporated the model bias into our features matrix by adding an extra column filled with `1`.\n",
    "\n",
    "<p align=center><img width=700 src=images/bias_in_weight_matrix.jpg></p>\n",
    "\n",
    "Doing this improves the clarity of the analytical solution and enables us to solve it for only one value, $W$, rather than for $b$.\n",
    "\n",
    "Note, however, that in practice (iterative optimisation), they are treated as separate variables (we will see more about that later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating parameters\n",
    "\n",
    "Having found the `optimal_w`, we should update our model and observe its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_params(weights, bias)\n",
    "y_pred = model(X_train)\n",
    "cost = mean_squared_error(y_pred, y_train)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of computing the analytical solution\n",
    "\n",
    "This solution involves inverting a matrix of size $R^{n \\times n}$. \n",
    "Here, $n$ refers to the number of features that each example has. \n",
    "\n",
    "With eight features, the difficulty level is high. Furthermore, here, we only have ~20,000 samples, whereas in real life, millions or more is the standard.\n",
    "\n",
    "However, as we will see, most problems of practical interest contain examples with many more features. \n",
    "\n",
    "> For example, 1080p images have more than 1,000,000 features each. \n",
    "\n",
    "The time complexity of inverting a matrix of size $n \\times n$ is approximately $O(n^3)$. \n",
    "This indicates that obtaining the analytical solution for these kinds of real-world problems is often computationally expensive or even impossible.\n",
    "\n",
    "Analytical solutions, however, are not the only approach that we can take (usually, we __cannot even use them__ because the close form cannot be calculated).\n",
    "\n",
    "Shortly, we will see how to update parameters iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models in `sklearn`\n",
    "\n",
    "`sklearn` comes with some common ML models that can be utilised out of the box, with an intuitive API.\n",
    "\n",
    "(See the linear-regression [documentation](https://scikit-learn.org/stable/modules/classes.html#classical-linear-regressors) for more information).\n",
    "\n",
    "To demonstrate, we retrieve some data and load them thereafter [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Additionally, we need to import the appropriate package from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.fetch_california_housing(return_X_y=True)\n",
    "\n",
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `sklearn`'s API of models\n",
    "\n",
    "`sklearn` ML algorithms are objects with the following general syntax:\n",
    "\n",
    "- `__init__(*args, **kwargs)`: setup your algorithm (as shown above).\n",
    "- `fit(X, [y])`: train the model on `X` (features) and `y` (targets). For unsupervised algorithms, there is no `y`.\n",
    "- `predict(X)`: pass data (previously unseen) to the algorithm after the `fit` is called. This gives us predictions (`y_pred`).\n",
    "\n",
    "With that, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.13164983 3.97660644 3.67657094 3.2415985  2.41358744] \n",
      " [4.526 3.585 3.521 3.413 3.422]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print(y_pred[:5], \"\\n\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our model\n",
    "\n",
    "Conveniently, `sklearn` provides performance __metrics__ for evaluating models.\n",
    "\n",
    "You can view `sklearn`'s metrics [here](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics). In this case, we will use the [MSE](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error).\n",
    "\n",
    "Below, we import `sklearn.metrics.mean_squared_error` using the `from` import syntax and display the error between the true targets and the predicted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5243209861846072"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model persistance\n",
    "\n",
    "As is known, the training (`fitting`) process is often quite expensive (in terms of time and/or computation cost). What is important is the ability to predict on unseen data.\n",
    "\n",
    "Having established that your model works as expected, the next logical step is to save it for later use without the need to `train` on the data again.\n",
    "\n",
    "> Model persistence is achieved by saving your ML algorithm currently held in the random access memory (RAM) to a storage (usually a hard drive), from which it can be re-instantiated at any point.\n",
    "\n",
    "As is conventional, we do this with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of `sklearn`\n",
    "\n",
    "As `sklearn` is very high level, it does not require much knowledge to use.\n",
    "\n",
    "However, __adequate and somewhat diverse knowledge__ is required to do ML properly.\n",
    "\n",
    "Here are some important things to note.\n",
    "\n",
    "- Why and what for: there are many more ways (and considerably better) to do ML.\n",
    "- Knowledge of ML algorithms: it is important to know which ones to choose for different kinds of problems.\n",
    "- Knowledge of possible pitfalls: ML can easily go wrong. It is important to know more about it to improve our model's performance.\n",
    "- In-depth knowledge of the ideas: often, it might be a good idea to implement major ideas on your own.\n",
    "\n",
    "Rest assured that __we will cover all of the above.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sklearn` tips\n",
    "\n",
    "- __Always attempt the easiest solution first__: create a weak baseline algorithm, and scrutinise its performance. Do not go straight to the most complicated ones. This concept is called [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor) in philosophy and ML as well.\n",
    "- Some algorithms have attributes that might interest you. They are usually suffixed by `_`; for example, `my_algorithm.interesting_attribute_`.\n",
    "- Some `__init__` functions have __many possible arguments__. Each of them influences the performance of the algorithm. Find out the ones with the highest importance and the most influence. __In `sklearn`, those arguments come in the order of most influential to least influential.__\n",
    "- Many `sklearn` algorithms provide the `n_jobs` argument, which parallelises `fit`, `predict` and other functions. You can employ `n_jobs=-1` to use as many processes as there are virtual cores (it is often a reasonable amount), which improves performance tremendously.\n",
    "- __Use idiomatic `sklearn`__: search the documentation, and use pipelines if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "At this point, you should have a good understanding of\n",
    "\n",
    "- the difference between univariate and multivariate regressions.\n",
    "- how to implement a ML algorithm from scratch in Python.\n",
    "- how to use the analytical solution.\n",
    "- how to optimise linear regression using the analytical solution."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('main': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
