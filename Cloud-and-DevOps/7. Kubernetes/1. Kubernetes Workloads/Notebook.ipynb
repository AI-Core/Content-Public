{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workloads\n",
    "\n",
    "> <font size=+1> Workload is an application running on `Kubernetes` </font>\n",
    "\n",
    "Workloads can be divided into `Pods` and `Workload Resources`.\n",
    "\n",
    "Each `workload` runs within __a set of `Pods`__, while `Pod` __represents a set of containers running together__.\n",
    "\n",
    "- `Pod`s are scheduled to run on nodes\n",
    "- In case of `node` failure all of the `Pod`s on the node are also deleted (__`Pod`s are ephemeral, should be easy to (re)create__)\n",
    "- If `Pod` run fails it will stay in inappropriate state (more details in `Pod` section)\n",
    "\n",
    "As there are a lot of failure points, the cluster admin would have to:\n",
    "- Constantly verify whether `Pod`s are running fine\n",
    "- Manually reschedule `Pod`s to different `Node`s\n",
    "- Manually restart `Pod`s in case any container failed\n",
    "\n",
    "> __Manual handling is pretty inefficient, hence `Workload Resources` were created__\n",
    "\n",
    "\n",
    "`Workload Resources` can specify (amongst other things):\n",
    "\n",
    "- When `Pod` should be recreated\n",
    "- What to do in case of failures\n",
    "- How many times should we try to reschedule before giving up \n",
    "\n",
    "> ## `Workload Resources` should be used to manage `Pod`s lifecycles, avoid deploying \"bare Pods\"!\n",
    "\n",
    "The most common `Workload Resources` are:\n",
    "\n",
    "- Deployment\n",
    "- StatefulSet\n",
    "- DaemonSet\n",
    "- Job and CronJob\n",
    "\n",
    "In this notebook, we will see Pods, Workload Resources, and how to implement them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> <font size=+1>A group of one or more containers, with shared context and a specification for how to run the containers</font>\n",
    "\n",
    "Pods add another layer of abstraction over the containers and behave similarly to `docker-compose`, e.g. __they can connect multiple related containers into one logical grouping__.\n",
    "\n",
    "Shared context:\n",
    "- Shared storage\n",
    "- Shared network resources (e.g. IP)\n",
    "- Linux namespaces\n",
    "\n",
    "__Individual applications might be further isolated within the Pod__\n",
    "\n",
    "> Pod is minimal deployment unit in Kubernetes\n",
    "\n",
    "This means we cannot schedule containers on their own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifecycle\n",
    "\n",
    "> `Pod`s remain on scheduled `Node` until termination (according to restart `policy` if failure occured) or deletion\n",
    "\n",
    "__`Pod`s are never moved across nodes, they are eventually recreated!__\n",
    "\n",
    "Some features:\n",
    "- `Pod`s cannot self-heal (e.g. restart themselves). This is done by appropriate `Workload Resources` or by cluster admin\n",
    "- __Pods can restart failed containers though__ (using `kubelet`)\n",
    "- Related resources (e.g. `volume`s) are also deleted after `Pod` termination (unless specified otherwise)\n",
    "\n",
    "`Pod`s can be in one of multiple phases:\n",
    "- `pending` - accepted by `k8s` cluster, but:\n",
    "    - one or more containers didn't start\n",
    "    - `Pod` is waiting for node scheduling\n",
    "    - container image is currently downloaded\n",
    "- `running` - at least one `container` within `Pod` is running (or being (re)started)\n",
    "- `succeeded` - all containers in the `Pod` have suceeded __and will not restart__\n",
    "- `failed` - at least one container terminated in a failure\n",
    "- `unknown` - state of `Pod` ould not be obtained, __typically communication error with `Node`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Container states\n",
    "\n",
    "> __Kubernetes also watches the state of individual containers within `Pod`s__\n",
    "\n",
    "Containers can be in one of three states:\n",
    "- `Waiting` - downloading image or pulling `secrets`, __reason is also provided for monitoring__\n",
    "- `Running`\n",
    "- `Terminated` - either terminated successfully or not; __reason and `exit code` is provided for monitoring__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Container Pod\n",
    "\n",
    "Usually you run Pods with a single container. In this case, we can think of a Pod as a container wrapper\n",
    "\n",
    "Examples could include:\n",
    "- FastAPI server receiving requests and saving to shared database\n",
    "- Docker container receiving images as requests and forwarding the classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pods with Multiple Containers\n",
    "\n",
    "> More advanced use case, multiple tightly coupled containers __making a cohesive unit of service__\n",
    "\n",
    "<p align=center><img src=images/pod.svg width=350></p>\n",
    "\n",
    "Examples could include:\n",
    "- Training multiple machine learning models, where:\n",
    "    - First container accesses shared storage of raw data and transforms it\n",
    "    - Second container trains neural network on the presented data\n",
    "    - Third container pushes out the trained model to serving container\n",
    "    - Fourth container serves the model\n",
    "- One container serving data to the public (`read only` permissions), while another, internal one, writes data to shared storage\n",
    "\n",
    "> __Pod containers are scheduled on the same \"logical host\" (for cloud, for clusters of servers: same VM or physical computer) due to tight coupling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction, you can see the Pods in a cluster with the command `kubectl get pod`. However, this will only show the pods in the Default namespace. You can observe all the pods in your cluster by adding the flag -A.\n",
    "\n",
    "<i>_If you haven't, run `minikube start`, so you have a cluster in your local machine to start working with_</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE              NAME                                         READY   STATUS    RESTARTS       AGE\n",
      "default                hello-minikube-6ddfcc9757-mndds              1/1     Running   2 (106s ago)   16h\n",
      "kube-system            coredns-78fcd69978-8j8hz                     1/1     Running   2 (106s ago)   19h\n",
      "kube-system            etcd-minikube                                1/1     Running   2 (106s ago)   19h\n",
      "kube-system            kube-apiserver-minikube                      1/1     Running   2 (106s ago)   19h\n",
      "kube-system            kube-controller-manager-minikube             1/1     Running   2 (106s ago)   19h\n",
      "kube-system            kube-proxy-jb22b                             1/1     Running   2 (106s ago)   16h\n",
      "kube-system            kube-scheduler-minikube                      1/1     Running   2 (106s ago)   19h\n",
      "kube-system            storage-provisioner                          1/1     Running   4 (32s ago)    16h\n",
      "kubernetes-dashboard   dashboard-metrics-scraper-5594458c94-rjg6d   1/1     Running   2 (106s ago)   16h\n",
      "kubernetes-dashboard   kubernetes-dashboard-654cf69797-ggqph        1/1     Running   3 (28s ago)    16h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we already have some Pods, because `minikube` comes with some of them by default. But of course, we will want to deploy our own pods, so let's see how to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Pod (Pod template)\n",
    "\n",
    "As you saw in the last lesson, Kubernetes objects can be defined imperatively (using defined steps), or declaratively. In these lessons we will focus on defining the declarative configurations, which are specified using `.yaml` files.\n",
    "\n",
    "Pod as a basic `kind` can be specified via `.yaml` config file (__although specifying bare `Pod`s is discouraged__)\n",
    "\n",
    "Specifying \"bare `Pod`s\" is pretty straightforward:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: pod1\n",
    "  labels:\n",
    "    tier: frontend\n",
    "spec:\n",
    "  containers:\n",
    "  - name: hello1\n",
    "    image: gcr.io/google-samples/hello-app:2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we saw what the API version is, and how we should look for it in the [API docs](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/). \n",
    "\n",
    "Inside `spec` we added the `container`. This will list the Docker containers belonging to the list, and inside `container`, we specified `image`, which corresponds to the Docker image name. In this case, it is a sample image from google.\n",
    "\n",
    "## Try it out\n",
    "\n",
    "- Create a .yaml file with the configuration above\n",
    "- Before you deploy your Kubernetes resource, observe the pods in your default namespace (there shouldn't be any if it's your first time)\n",
    "- Use the corresponding `kubectl` command to deploy the Kubernetes resource using the `.yaml` config file\n",
    "- Observe the pods in your default namespace\n",
    "- Notice that the pod is now running in the default namespace. This is because in the `kubectl` command you (probably) haven't specified the namespace for the new pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No resources found in default namespace.\n"
     ]
    }
   ],
   "source": [
    "# Observe the pods in the default namesapce\n",
    "!kubectl ###Your command here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/pod1 created\n"
     ]
    }
   ],
   "source": [
    "# Spin up the pod corresponding to the single-pod configuration above\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME   READY   STATUS              RESTARTS   AGE\n",
      "pod1   0/1     ContainerCreating   0          23s\n"
     ]
    }
   ],
   "source": [
    "# Observe the pods in the default namesapce again\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, remember from last notebook what happened when we deleted a pod? It didn't take long until a new pod appeared. Let's see what happens if we try the same.\n",
    "\n",
    "## Try it out\n",
    "\n",
    "1. Delete the pod using the correct `kubectl` command\n",
    "2. Observe the pods in the default namespace again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"pod1\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the pod\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No resources found in default namespace.\n"
     ]
    }
   ],
   "source": [
    "!kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is the difference here? Why it dissapears now, but it didn't last time? The reason last time it _didn't_ dissapeared was because last time it had \"instructions\" to keep the pod alive. However, in this case, we have the \"bare\" pod, without any resource to maintaining it running after failing for any reason. \n",
    "\n",
    "Keeping it \"alive\" is one of the desired states you can specify in your configuration file, and it can be achieved with `Deployment` or `Replica Set`, and within the workload resources, you can find many more options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workload Resources\n",
    "\n",
    "Before diving into specific `Workload Resources` let's get a few concepts straight:\n",
    "\n",
    "- Each `workload` presented below uses `.spec.template` field which specifies how to create a pod\n",
    "- `template` is essentially the same as pod config except `kind` and `apiVersion` (the rest is performed the same)\n",
    "- Each `workload` has a `.spec.selector` that specifies which pods are handled by the `workload resource`\n",
    "- `.spec.selector` uses matches on defined `labels` and may \"take care of\" pods outside its config file!\n",
    "\n",
    "With the above in mind, let's dive in. We will see how to implement:\n",
    "\n",
    "- Deployment (For which we need to explain ReplicaSet very briefly)\n",
    "- DaemonSet\n",
    "- Jobs\n",
    "\n",
    "In the next lessons you will learn how to implement another type of workload, `StatefulSets`. But you will need to understand Kubernetes storage before diving into `StatefulSets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplicaSet\n",
    "\n",
    ">  Maintains a stable set of replicated pods running at any given time\n",
    "\n",
    "`ReplicaSet`:\n",
    "- creates new `POD`s accordingly to `.spec.replicas` field value\n",
    "    (from `.spec.template` config)\n",
    "- deletes `POD`s if too many of them are scheduled to nodes\n",
    "\n",
    "### Acquiring pods\n",
    "\n",
    "> `ReplicaSet` is linked to the pods via `metadata.ownerReferences` and acquired via `.spec.selector` match\n",
    "\n",
    "The above works something like this:\n",
    "- Each pod has `metadata.ownerReferences` __automatically added by `k8s`__\n",
    "- Above specifies who manages the pod (e.g. another controller)\n",
    "- __If `POD`__:\n",
    "    - has no \"owner\" (e.g. bare `POD`) __or__\n",
    "    - it's owner __is not another controller and__\n",
    "    - `.spec.selector` fields match\n",
    "- Then `POD` is acquired by the `ReplicaSet`\n",
    "\n",
    "> <font size=+1>The process above works the same for other `workload resources` (or managers)</font>\n",
    "\n",
    "### When should we use `ReplicaSet`s?\n",
    "\n",
    "> __In general it is advised to use higher level `Deployment` `workload resource`__\n",
    "\n",
    "`Deployment`, a higher level concept, __manages `ReplicaSet`s__ and in addition provides __declarative updates to pods__\n",
    "\n",
    "Only reasons to go for `ReplicaSet`s would be:\n",
    "- custom update orchestration\n",
    "- we will never need to update `config`\n",
    "\n",
    "The latter is pretty unlikely (and we are not paying large price for this feature), hence:\n",
    "\n",
    "> <font size=+1>Use Deployment instead</font>\n",
    "\n",
    "(for those interested `ReplicaSet` is described in detail [here](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "> Provides declarative updates for `Pods` and `ReplicaSets`\n",
    "\n",
    "Given about information, let's see whether we can tell what each field means in the config below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: nginx-deployment\n",
    "  labels:\n",
    "    app: nginx\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: nginx\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: nginx\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: nginx\n",
    "        image: nginx:1.14.2\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we are telling the resource to have at least 3 pods running at all times. These three pods are found using the `selector.matchLabels` which finds in the template the label whose key is `app` and value `nginx`. \n",
    "\n",
    "## Try it out\n",
    "\n",
    "1. Create a .yaml file with the configuration above\n",
    "2. Run the right `kubectl` command to run the .yaml file\n",
    "3. Observe how many pods you have\n",
    "4. Delete one pod\n",
    "5. Observe how many pods you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/nginx-deployment created\n"
     ]
    }
   ],
   "source": [
    "!kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                READY   STATUS              RESTARTS   AGE\n",
      "nginx-deployment-66b6c48dd5-7qpvr   0/1     ContainerCreating   0          17s\n",
      "nginx-deployment-66b6c48dd5-gc7z7   0/1     ContainerCreating   0          17s\n",
      "nginx-deployment-66b6c48dd5-vzsx2   0/1     ContainerCreating   0          17s\n"
     ]
    }
   ],
   "source": [
    "# Observe how many pods you have\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you there are some `nginx` containers running, you can use them to run some commands manually (you will use to run other applications, but for now, let's use them interactively).\n",
    "\n",
    "In this case, I ran it in the terminal:\n",
    "\n",
    "<p align=center><img src=images/nginx.png width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that, after deleting it, the pod is still there, but with a different name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"nginx-deployment-66b6c48dd5-vzsx2\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete one of the pods\n",
    "!kubectl delete pod nginx-deployment-66b6c48dd5-vzsx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                READY   STATUS    RESTARTS        AGE\n",
      "nginx-deployment-66b6c48dd5-7qpvr   1/1     Running   1 (8m15s ago)   8h\n",
      "nginx-deployment-66b6c48dd5-fsck5   1/1     Running   0               40s\n",
      "nginx-deployment-66b6c48dd5-gc7z7   1/1     Running   1 (8m15s ago)   8h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Just to avoid any confusion, delete the deployment resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"nginx-deployment\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the deployment resource\n",
    "!kubectl delete deployment nginx-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DaemonSet\n",
    "\n",
    "> DaemonSet ensures that a pod is deployed in all Nodes as it is added to the cluster\n",
    "\n",
    "We will have one pod per node, so if we remove a node, the number of pods will downscale.\n",
    "\n",
    "DaemonSets are generally used to monitoring services. You can use a single pod to monitor the health of each node, or to capture the logs of each node. Observe that they are quite similar to deployment, but it has no replicas.\n",
    "\n",
    "### Required fields\n",
    "\n",
    "> Similar to `Deployment`, which means `.spec.template`, `.spec.selector` and __no `.spec.replicas` (as the same `daemon` is run per-node)__\n",
    "\n",
    "- Acquisition of `POD`s happens on matchin `.spec.selector` (like previously)\n",
    "- __Acquisition of `Node`s happens via `.spec.template.spec.nodeSelector` field__\n",
    "\n",
    "### Assigning Pods to Nodes\n",
    "\n",
    "> In general we don't have to interfere with `k8s` POD deployment to specific `Node`s\n",
    "\n",
    "There might be a few reasons to do that though:\n",
    "- Ensuring `POD` ends up on a `Node` which has `SSD` attached\n",
    "- Co-locate `POD`s from different services in the same zone if they communicate frequently\n",
    "\n",
    "`k8s` comes with a set of predefined `labels` for `Node`s, full list can be seen [here](https://kubernetes.io/docs/reference/labels-annotations-taints/), but to mention a few:\n",
    "- region of deployment (in case of cloud): `topology.kubernetes.io/region=us-east-1`\n",
    "- ip address of a node: `kubernetes.io/hostname=ip-172-20-114-199.ec2.internal`\n",
    "- operating system our node is running: `kubernetes.io/os=linux`\n",
    "\n",
    "> __During `kubectl` lesson we will learn how to add our own `label`s to `Node`s__\n",
    "\n",
    "Given the above, we can use the `.spec.template.spec.nodeSelector` as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: apps/v1\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: fluentd-elasticsearch\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      name: fluentd-elasticsearch\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        name: fluentd-elasticsearch\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: fluentd-elasticsearch\n",
    "        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n",
    "        resources:\n",
    "          limits:\n",
    "            memory: 200Mi\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 200Mi\n",
    "      terminationGracePeriodSeconds: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, this is a good time to talk about memory resources. As you can see in the Kubernetes object, there is a new field in the `template.spec.container` field named `resources`, which in turn has two more fields: `limits` and `requests`. \n",
    "\n",
    "- `limits`: These values represent the maximum capacities given to a pod. If the process starts using more than 200M of RAM memory, the pod will be restarted.\n",
    "- `requests`: These values represent the minimum capacities we are guarateeing to the pod. In the example above, we are giving 200M of RAM memory, and 100 milicores. A _milicore_ is a fraction of your cores, each core is equivalent to 1000 milicores.\n",
    "\n",
    "For more information about container resources, go to the following [link](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, most of the arguments in the DaemonSet are the same as in the Deployment resource. Observe that we don't use the `replicas` key, since, as mentioned, there will be a single `DaemonSet` pod per node. For the sake of this demonstration, let's add a node to the minikube existing cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!minikube node add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to spin up the DaemonSet:\n",
    "\n",
    "## Try it out\n",
    "\n",
    "1. Observe the pods in ALL your nodes\n",
    "2. Create a .yaml file with the configuration above\n",
    "3. Run the appropriate command to deploy the DaemonSet configuration\n",
    "4. Observe again the pods in ALL your nodes\n",
    "5. Delete one of your nodes\n",
    "6. Observe, once more, the pods in your nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs\n",
    "\n",
    "`Job` will create one or more pods and will continue to retry execution of the pods until a specified number of them successfully terminate\n",
    "\n",
    "Use cases:\n",
    "- create one `Job` to make sure our task runs successfully\n",
    "- run the same `Job` in parallel for `N` times\n",
    "\n",
    "An example config `Job` workload calculating `pi` value could be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: pi\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: pi\n",
    "        image: perl\n",
    "        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n",
    "      restartPolicy: Never\n",
    "  backoffLimit: 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.spec`ification\n",
    "\n",
    "Standard fields are necessary, but in addition:\n",
    "\n",
    "- `.spec.restartPolicy` can be either `Never` or `OnFailure` (default)\n",
    "- `.spec.completions` - how many `Job`s have to finish\n",
    "- `.spec.parallelism` - how many `pod`s with our job should be scheduled at the same time\n",
    "\n",
    "Using `.spec.completions` and `.spec.parallelism` we can construct different levels of parallelism:\n",
    "\n",
    "- __non-parallel__ - specify `.spec.completions=1`, only one `Job` will be created. __New one will only start after this one fails!__\n",
    "- __parallel with fixed completion count__ - specify `.spec.completions=N` to run __at most `N` parallel jobs at a given time__ (controller will reschedule `Node`s in case of failure)\n",
    "- __parallel with work queue__ - `.spec.completions=1` and `.spec.parallelism=N` - `N` pods will run, after first one succeeds __the rest will continue execution until termination__ (for improved efficiency we need to implement direct `pod` to `pod` communication)\n",
    "\n",
    "`non-parallel` is the default mode as `.spec.completions=1` and `.spec.parallelism=1` are the default values.\n",
    "\n",
    "One could also specify [Completion Mode](https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode) which allows us to modify their behaviour upon termination.\n",
    "\n",
    "- `.spec.backoffLimit` - how many times __a single `pod`__ should be restarted before considering `Job` failed\n",
    "\n",
    "In this case:\n",
    "- Depending on the settings, but if `.spec.completions=N` is hit, __job succeeded__\n",
    "- Until this moment, try to recreate `pod` with `Job` `N` times\n",
    "- Exponential back-off delay is applied, e.g.:\n",
    "    - First retry after `10s`\n",
    "    - Second after `20s`\n",
    "    - Third after `40s`\n",
    "    - __Capped at `6m` backoff!__\n",
    "    \n",
    "- `.spec.activeDeadlineSeconds` - how many seconds __for the whole job__ until termination. Once reached, __all of the `pod`s ater terminated__ (takes precedence over `.spec.backoffLimit`)\n",
    "    \n",
    "### Cleanup\n",
    "\n",
    "> A `Job` after finishing __will not be automatically removed from the cluster__\n",
    "\n",
    "Why is that bad?\n",
    "\n",
    "> `kube-apiserver` will still query `Job` and look for it's `pod`s __putting unneeded pressure on `k8s`__\n",
    "\n",
    "Why is this the default behaviour?\n",
    "- One might want to check logs of finished jobs (which are stored within `pod` or external storage)\n",
    "- Checking status of `Job`(s)\n",
    "\n",
    "What one can do is to set up a so called __`TTL` (time to live)__:\n",
    "\n",
    "> __`TTL` specifies after how long should the `job` be removed from the cluster (including all of it's `pod`s and other dependencies)__.\n",
    "\n",
    "One could do that via `.spec.ttlSecondsAfterFinished` field as one can read below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: pi-with-ttl\n",
    "spec:\n",
    "  ttlSecondsAfterFinished: 100\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: pi\n",
    "        image: perl\n",
    "        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n",
    "      restartPolicy: Never"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, `Job`s will die when they succesfully execute a number of Pods. If you want to repeat that operation, you would need to `apply` a Kubernetes object to re run the job. Luckily, there is an easier way to generate `Job`s periodically with the frequency you desired: __`Cronjob`__\n",
    "\n",
    "A `Cronjob` creates `Job`s on a repeating schedule. You can specify said schedule in the specs:\n",
    "```\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: hello\n",
    "spec:\n",
    "  schedule: \"*/1 * * * *\"\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: hello\n",
    "            image: busybox\n",
    "            imagePullPolicy: IfNotPresent\n",
    "            command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "            - date; echo Hello from the Kubernetes cluster\n",
    "          restartPolicy: OnFailure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Out\n",
    "\n",
    "1. Create a .yaml file with the configuration above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad8bebc098a042dc0df4e42fc2ecc8fff0bd7b8741641ce29007c29766dadbe0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
