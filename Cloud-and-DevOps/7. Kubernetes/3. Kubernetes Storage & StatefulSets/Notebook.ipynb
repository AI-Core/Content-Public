{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage\n",
    "\n",
    "Pods and containers are __ephemeral__, which has it's upsides and downsides:\n",
    "\n",
    "The upsides to pods and containers as we have seen:\n",
    "- Easy to create\n",
    "- Easy to destroy \n",
    "- Immutable and easier to reason about\n",
    "- Easy to parallelize\n",
    "\n",
    "But, unfortunately, there are some downsides:\n",
    "- When a pod is removed, all of the data created by it on local disk is also removed\n",
    "- Each container is separate entity, hence sharing data between containers is not possible...\n",
    "\n",
    "... not possible unless we introduce `Volume`s!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Volumes\n",
    "\n",
    "`Volume`s in `k8s` are a little different from the ones in `Docker`.\n",
    "\n",
    "Brief overview of `volume`s in `Docker`:\n",
    "- Directory on `disk` or in another `container`\n",
    "- `Volume`s are mounted to containers during runtime\n",
    "- One can share data across instances (or using `cloud` storage) via `drivers` (see [here](https://docs.docker.com/storage/volumes/#share-data-among-machines))\n",
    "\n",
    "Take into account that, even though this resolves some of our problems, the feature set is quite limited and is not enough for handling large-scale deployments.\n",
    "\n",
    "High level features:\n",
    "- __Any `volume`s can be mounted at the same time__\n",
    "- __Ephemeral `volume`s__: Their lifetime is the same as `POD`\n",
    "- __Persistent `volume`s__: They are independent of `POD` lifetime\n",
    "- __Data is available across `container`s restart__ handled by `kubelet`\n",
    "\n",
    "You can attach volumes to pods via:\n",
    "- `.spec.volumes`: specifies which `volumes` to use\n",
    "- `.spec.containers[*].volumeMounts`:  where and which volume to mount for aspecific container\n",
    "\n",
    "Let see an example using a `DaemonSet`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: fluentd-elasticsearch\n",
    "  labels:\n",
    "    k8s-app: fluentd-logging\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      name: fluentd-elasticsearch\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        name: fluentd-elasticsearch\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: fluentd-elasticsearch\n",
    "        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n",
    "        resources:\n",
    "          limits:\n",
    "            memory: 200Mi\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 200Mi\n",
    "        # Here we can mount them with `name` matching\n",
    "        # Ephemeral Volumes\n",
    "        volumeMounts:\n",
    "        - name: varlog\n",
    "          mountPath: /var/log\n",
    "        - name: varlibdockercontainers\n",
    "          mountPath: /var/lib/docker/containers\n",
    "          readOnly: true\n",
    "      terminationGracePeriodSeconds: 30\n",
    "      # Here we define our volumes\n",
    "      # Data from POD will be mounted\n",
    "      volumes:\n",
    "      - name: varlog\n",
    "        hostPath:\n",
    "          path: /var/log\n",
    "      - name: varlibdockercontainers\n",
    "        hostPath:\n",
    "          path: /var/lib/docker/container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "1. Create a .yaml file for creating the DeamonSet\n",
    "2. Create it and use the describe command from kubectl to observe the volumes\n",
    "3. Go to the minikube dashboard and look for the volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daemonset.apps/fluentd-elasticsearch created\n"
     ]
    }
   ],
   "source": [
    "# Create the DaemonSet\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:           fluentd-elasticsearch\n",
      "Selector:       name=fluentd-elasticsearch\n",
      "Node-Selector:  <none>\n",
      "Labels:         k8s-app=fluentd-logging\n",
      "Annotations:    deprecated.daemonset.template.generation: 1\n",
      "Desired Number of Nodes Scheduled: 3\n",
      "Current Number of Nodes Scheduled: 3\n",
      "Number of Nodes Scheduled with Up-to-date Pods: 3\n",
      "Number of Nodes Scheduled with Available Pods: 3\n",
      "Number of Nodes Misscheduled: 0\n",
      "Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed\n",
      "Pod Template:\n",
      "  Labels:  name=fluentd-elasticsearch\n",
      "  Containers:\n",
      "   fluentd-elasticsearch:\n",
      "    Image:      quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n",
      "    Port:       <none>\n",
      "    Host Port:  <none>\n",
      "    Limits:\n",
      "      memory:  200Mi\n",
      "    Requests:\n",
      "      cpu:        100m\n",
      "      memory:     200Mi\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /var/lib/docker/containers from varlibdockercontainers (ro)\n",
      "      /var/log from varlog (rw)\n",
      "  Volumes:\n",
      "   varlog:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log\n",
      "    HostPathType:  \n",
      "   varlibdockercontainers:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/docker/container\n",
      "    HostPathType:  \n",
      "Events:\n",
      "  Type    Reason            Age   From                  Message\n",
      "  ----    ------            ----  ----                  -------\n",
      "  Normal  SuccessfulCreate  19m   daemonset-controller  Created pod: fluentd-elasticsearch-rkwrs\n",
      "  Normal  SuccessfulCreate  19m   daemonset-controller  Created pod: fluentd-elasticsearch-4m4wt\n",
      "  Normal  SuccessfulCreate  19m   daemonset-controller  Created pod: fluentd-elasticsearch-5wjx9\n"
     ]
    }
   ],
   "source": [
    "# Use the describe command\n",
    "!kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard should look like this, with the volumes mounted to the container:\n",
    "<p><img src=images/volume.png></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume Types\n",
    "\n",
    "`kubernetes` provides quite a few integrations for standard volumes, including:\n",
    "- `AWS Elastic Block Store`\n",
    "- Microsoft's Azure `Disk` and `File`\n",
    "- Self-hosted `cephfs`\n",
    "- [Google Cloud Persistent Disk](https://kubernetes.io/docs/concepts/storage/volumes/#gcepersistentdisk)\n",
    "\n",
    "An example config with `awsElasticBlockStore` could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test-ebs\n",
    "spec:\n",
    "  containers:\n",
    "  - image: k8s.gcr.io/test-webserver\n",
    "    name: test-container\n",
    "    volumeMounts:\n",
    "    - mountPath: /test-ebs\n",
    "      name: test-volume\n",
    "  volumes:\n",
    "  - name: test-volume\n",
    "    # This AWS EBS volume must already exist.\n",
    "    awsElasticBlockStore:\n",
    "      volumeID: \"<volume id>\"\n",
    "      fsType: ext4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see volume types in more detail [check here](https://kubernetes.io/docs/concepts/storage/volumes/#volume-types).\n",
    "\n",
    "__These are all ephemeral `volume`s hence they will only live as long as its pod!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent Volumes\n",
    "\n",
    "> `Kubernetes` provides two resources to manage persistent storage: `PersistentVolume` and `PersistentVolumeClaim`\n",
    "\n",
    "This abstraction allows us to:\n",
    "- Abstract how storage is provided\n",
    "- Abstract a way storage is consumed \n",
    "\n",
    "## PersistentVolume\n",
    "\n",
    "A persistent volume is a piece of storage in a cluster that an administrator has provisioned. Persistent volumes are 'physical' volumes on the host machine\n",
    "\n",
    "Persistent volumes have the following characteristics:\n",
    "\n",
    "- resource in the cluster (just like `Node`)\n",
    "- are volume plugins just like `Volume`s described above\n",
    "- __they have lifecycle independent of any pod using it__\n",
    "- when bounded can be used just like `volume`\n",
    "\n",
    "## PersistentVolumeClaim\n",
    "\n",
    "A persistent volume claim (PVC) is a request for the platform to create a PV for you. It will look for an 'Available' PersistentVolume that meets the specified requirements.\n",
    "\n",
    "Features:\n",
    "- Conceptually similiar to `POD`s:\n",
    "    - `POD`s consume `Node` resources\n",
    "    - `PVC`s consume `PV` resources\n",
    "    - `POD`s can request specific resources (e.g. RAM memory)\n",
    "    - `PVC`s can request specific sizes and access modes (e.g. `read` and `write`)\n",
    "    \n",
    "## Provisioning\n",
    "\n",
    "There are two ways `PV` can be provisioned:\n",
    "- `statically` - cluster admin creates `PV`s for consumption\n",
    "- `dynamically` - cluster tries to dynamically provision appropriate `PersistentVolume` based on `PersistentVolumClaim`'s __`StorageClasses`__ (administrator has to provision `StorageClass`, __described later__)\n",
    "\n",
    "Features:\n",
    "- __`Dynamic Provision` will always match exactly the requirements of `PVC`__\n",
    "- __`Static Provision` has to match AT LEAST the given claim__ (e.g. claim of `50Gb` might be given `100Gb`)\n",
    "\n",
    "## Reclaim Policy\n",
    "\n",
    "When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource\n",
    "\n",
    "There are three ways to reclaim the resource:\n",
    "- [`retain`](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain) - leave the data as is (leave `PersistentVolume` and \n",
    "- [`delete`](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#delete) - delete `PersistentVolume` __and external storage__. This one is default for `Dynamic Provisioning` (although configurable)\n",
    "- `recycle` (__now deprecated, dynamic provisioning should be used instead__, see [here](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#recycle))\n",
    "\n",
    "Volumes can show 4 states:\n",
    "\n",
    "- `Available`: It means that this volume is ready to be bound to a pod\n",
    "- `Bound`: CLI will show pod to which `PersistentVolume` is bound\n",
    "- `Released`: `PesistentVolumeClaim` ended, but resource is not yet reclaimed by `cluster`\n",
    "- `Failed` - Reclamation failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying `PersistentVolume`\n",
    "\n",
    "As with pod, we create them using `.yaml` config files, example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: task-pv-volume\n",
    "  labels:\n",
    "    type: local\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  capacity:\n",
    "    storage: 10Gi\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  hostPath:\n",
    "    path: \"/mnt/data\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe some arguments you can provide:\n",
    "1. `.spec.capacity.storage` - request `10Gb` of storage. Currently only storage can be requested (see [`k8s` resource model for description of units](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/resources.md))\n",
    "2. `.spec.volumeMode` - either `FileSystem` (default) or `Block`:\n",
    "    - `FileSystem` - directory mounted in pods\n",
    "    - `Block` - uses raw block of storage (without filesystem created). It is `Block` is rarely used as application needs to know how to access `raw` data (see [here](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support) for example)\n",
    "3. `.spec.accessModes` - how data can be accessed:\n",
    "    - `ReadWriteOnce` - volume mounted as `rw` by a single `Node`\n",
    "    - `ReadOnlyMany` - can be mounted by many `nodes` but data can only be read\n",
    "    - `ReadWriteMany` - as above but both `rw` (applications have to handle possible data races!)\n",
    "    - `ReadWriteOncePod` - single pod can `rw` data\n",
    "      \n",
    "Modes above differ by type of `PersistentVolume` provider, a few of them shown below:\n",
    "\n",
    "![](./images/modes_providers.png)\n",
    "\n",
    "4. __`.spec.storageclassName`__ - specifies `StorageClass`, if left unspecified __there is no `StorageClass` specified and only `PV` without one can be matched to `POD`!__\n",
    "5. `.spec.mountOptions`- (__NOT SUPPORTED BY ALL TYPES!__); specifies how to mount `disk`, one can leave it as is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "1. Create a .yaml file with the configuration above for creating a PersistentVolume resource\n",
    "2. `apply` it using the right `kubectl`\n",
    "3. Observe the status of the volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistentvolume/task-pv-volume created\n"
     ]
    }
   ],
   "source": [
    "!kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE\n",
      "task-pv-volume   10Gi       RWO            Retain           Available           manual                  13m\n"
     ]
    }
   ],
   "source": [
    "!kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StorageClasses\n",
    "\n",
    "> A StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n",
    "\n",
    "These allow us to __dynamically provision storage__ and acts like a template for new `PersistentVolume`.\n",
    "\n",
    "As per usual, these are defined using `.yaml` files and can be referred by `PV` config files.\n",
    "\n",
    "A small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: storage.k8s.io/v1\n",
    "kind: StorageClass\n",
    "metadata:\n",
    "  name: standard\n",
    "provisioner: kubernetes.io/aws-ebs\n",
    "parameters:\n",
    "  type: gp2\n",
    "reclaimPolicy: Retain\n",
    "allowVolumeExpansion: true\n",
    "mountOptions:\n",
    "  - debug\n",
    "volumeBindingMode: Immediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### `.metadata.name` value allows users to request this `StorageClass`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mandatory Fields\n",
    "\n",
    "> `provisioner` - which volume plugin is used for provisiong `PV`\n",
    "\n",
    "Most common ones are shipped with `k8s` under `kubernetes.io` prefix, for example:\n",
    "\n",
    "- `local` - `kubernetes.io/no-provisioner` - create `PV`s dynamically from local resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: storage.k8s.io/v1\n",
    "kind: StorageClass\n",
    "metadata:\n",
    "  name: local-storage\n",
    "provisioner: kubernetes.io/no-provisioner\n",
    "volumeBindingMode: WaitForFirstConsumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `GCEPersistentDisk` - Persistent disk from Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: storage.k8s.io/v1\n",
    "kind: StorageClass\n",
    "metadata:\n",
    "  name: slow\n",
    "provisioner: kubernetes.io/gce-pd\n",
    "parameters:\n",
    "  type: pd-standard\n",
    "  fstype: ext4\n",
    "  replication-type: none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not limited to the ones provided, `provider`s can be written by anyone and hosted\n",
    "\n",
    "> `parameters` define per-provisioner specification of volume properties\n",
    "\n",
    "You can see examples for different internal provisioners [here](https://kubernetes.io/docs/concepts/storage/storage-classes/#aws-ebs)\n",
    "\n",
    "> `reclaimPolicy` - when `PV` is freed from `PVC` what should be done with created `PersistentVolume`\n",
    "\n",
    "As mentioned previously, one of `Delete` or `Retain` available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expandable Volumes\n",
    "\n",
    "> From `k8s` 1.11 one can __expand volumes dynamically__\n",
    "\n",
    "This happens, when we change storage requirements in our `PVC` and `apply` new config.\n",
    "\n",
    "> Easiest way to use this feature is to use one of internal cloud providers\n",
    "\n",
    "__All we have to do is set `.allowVolumeExpansion: true` in our `StorageClass` definition__\n",
    "\n",
    "Below is a list of `providers` supporting expandable volumes:\n",
    "\n",
    "![](./images/expandable-volumes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage summary\n",
    "\n",
    "So, given all of the above, a rough guideline one could follow?\n",
    "\n",
    "> __Decide how your data should be shared__\n",
    "\n",
    "Answer this question first:\n",
    "- Shared between containers or pods?\n",
    "\n",
    "If it is shared between containers answer this question:\n",
    "- Should data be preserved after pod termination?\n",
    "\n",
    "__If yes, use `ephemeral volumes` to simply exchange data between applications (`LAMP` example above)__\n",
    "\n",
    "__If it is shared between pods or should be preserved use `PersistentVolumes`__\n",
    "\n",
    "## Chose `PersistentVolume`, what now?\n",
    "\n",
    "Another question to help you:\n",
    "\n",
    "- Do I need dynamic provisioning (e.g. Hard to know beforehand how much storage I will need?)\n",
    "\n",
    "If not create the following:\n",
    "- `PersistentVolume` `.yml` config (defines how to create `volume`)\n",
    "- `PersistentVolumeClaim` `.yml` config (specifies how a `volume` is requested)\n",
    "- `MyApplication` `.yml` config (your `workload resource`, __avoid bare pods__)\n",
    "\n",
    "## I need `dynamic` provisioning, what now?\n",
    "\n",
    "> __Use this for large scale apps where \"by-hand\" provisioning is infeasible__\n",
    "\n",
    "This might happen due to a few reasons:\n",
    "- A lot of pods requesting a lot of storage\n",
    "- We cannot see beforehand how many pods will run\n",
    "\n",
    "> Prefer cloud storage providers in this case, __as one might run out of local storage for large deployments__\n",
    "\n",
    "In this case, to the steps outlined above one should add another `.yml` config file:\n",
    "- `StorageClass` `.yml` config (acts as a template for giving out `PersistentVolume`s to pods in need)\n",
    "\n",
    "## Other options?\n",
    "\n",
    "Yes, one can also use aforementioned `expandable volumes` if:\n",
    "- you know how many pods __at most__ will run at any given time\n",
    "- you are not sure how much storage will be needed for each pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StatefulSets\n",
    "   \n",
    "Previously shown `workload`s are used for __stateless applications__ (e.g. the ones not writing to external storage).\n",
    "\n",
    "In a nutshell, StatefulSets are a type of workload resource (so it manages pods) that adds a storage to the pod. That way, everything you run in the pod will be stored somewhere, and next time you run the same pod, the same data will still be there, hence the name 'Stateful'. This is useful for example, if you want to run a pod that depends on a database where you want to store the data in a SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "- `Storage` must be provisioned by admin (or by `StorageClass` dynamically)\n",
    "- __Deleting will not delete `Storage`__ (preserving storage > automatic purging)\n",
    "- __Headless `Service`__ is used for network identity of `POD`s __and we have to create it__\n",
    "\n",
    "## Components\n",
    "\n",
    "Let's see `.yaml` definitions necessary for `StatefulSet`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: nginx\n",
    "  labels:\n",
    "    app: nginx\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 80\n",
    "    name: web\n",
    "  clusterIP: None\n",
    "  selector:\n",
    "    app: nginx\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: StatefulSet\n",
    "metadata:\n",
    "  name: web\n",
    "spec:\n",
    "  serviceName: \"nginx\"\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: nginx\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: nginx\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: nginx\n",
    "        image: k8s.gcr.io/nginx-slim:0.8\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "          name: web\n",
    "        volumeMounts:\n",
    "        - name: www\n",
    "          mountPath: /usr/share/nginx/html\n",
    "  volumeClaimTemplates:\n",
    "  - metadata:\n",
    "      name: www\n",
    "    spec:\n",
    "      accessModes: [ \"ReadWriteOnce\" ]\n",
    "      resources:\n",
    "        requests:\n",
    "          storage: 1Gi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Headless Service, named `nginx`, is used to control the network domain.\n",
    "- The `StatefulSet`, named web, has a Spec that indicates that 3 replicas of the `nginx` container will be launched in unique Pods.\n",
    "- The `volumeClaimTemplates` will provide stable storage using `PersistentVolumes` provisioned by a `PersistentVolume` Provisioner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "\n",
    "1. Create a .yaml file with the configuration above for creating a PersistentVolume resource\n",
    "2. `apply` it using the right `kubectl`\n",
    "3. Go to the dashboard and observe the PVCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/nginx created\n",
      "statefulset.apps/web created\n"
     ]
    }
   ],
   "source": [
    "!kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dashboard should present something like this:\n",
    "<p align=center><img src=images/PVC.png></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the volumes are already Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "## Mandatory\n",
    "\n",
    "- What is and how [Mount Propagation](https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation) works for volumes?\n",
    "- What is [Storage Object in Use Protection](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#storage-object-in-use-protection)?\n",
    "- What is [Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode) for `StorageClasses`?\n",
    "- What [Allowed Topologies](https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies) mean for `StorageClasses`?\n",
    "- Check out [Update Strategies](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies) for `StatefulSet`s\n",
    "\n",
    "## Additional\n",
    "\n",
    "- What is `DefaultStorageClass` admission plugin? How it changes behavior for __unspecified `StorageClass`__ in case of `PV` or `PVC`?\n",
    "- Check out `Volume Snapshots` feature of `PersistentVolume`s [here](https://kubernetes.io/docs/concepts/storage/volume-snapshots/)\n",
    "- How to create __non-empty `PersistentVolume`s?__ One can find answer in [Volume populators](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources) section.\n",
    "- Check how to monitor health of your `Volume`s [here](https://kubernetes.io/docs/concepts/storage/volume-health-monitoring/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
