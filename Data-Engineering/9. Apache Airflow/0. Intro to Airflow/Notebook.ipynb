{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Airflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a href=https://airflow.apache.org><img src=images/Airflow_logo.png width=400></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Up to this point, you have learnt a lot of things about data: Extracting the data, Transforming the data, and Loading the data. As mentioned multiple times, this is the \"core\" of Data Engineering (ETL). However, all these operations work in tandem to create a workflow.\r\n",
    "\r\n",
    "A workflow is a series of steps that are executed in a specific order. For example, if you want to extract data from a source, transform it, and load it into a target, you will need to follow the steps in the following order:\r\n",
    "\r\n",
    "1. __Extract__ data using, for example, the webscraping skills you learnt\r\n",
    "2. __Transform__ the data using, for example, the data cleaning skills in pandas\r\n",
    "3. __Load__ the data into a target, for example, a database located in your local environment or in a remote environment.\r\n",
    "\r\n",
    "<p align=\"center\">\r\n",
    "    <img src=\"images/WorkFlow1.png\" width=\"500\"/>\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Workflows can also be very helpful when you want to develop a ML model. For example, you might want to train a model on the data, but don't exactly know which algorithm to use. In this case, you can follow the steps in the following order:\r\n",
    "\r\n",
    "1. __Extract__ data using, for example, the webscraping skills you learnt\r\n",
    "2. __Transform__ the data using, for example, the data cleaning skills in pandas\r\n",
    "3. __Train__ multiple models using the data, and obtain the accuracy of each model\r\n",
    "4. __Choose__ the model with the highest accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\">\r\n",
    "    <img src=\"images/WorkFlow2.png\" width=\"600\"/>\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Airflow as a workflow manager"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apache Airflow is a task orchestration tool that allows you to define a series of tasks that are executed in a specific order. Tasks can be run in a distributed manner using Airflow's scheduler.\r\n",
    "\r\n",
    "In Airflow you use Directed Acyclic Graphs (DAGs) to define a workflow. Each node in the DAG corresponds to a task, and they will be connected to one another."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Installing airflow would be as simple as running `pip install apache-airflow`, however, that migh cause dependency errors. Thus, in order to prevent those errors, run the following commands in your __`terminal`__ . \r\n",
    "<details>\r\n",
    "  <summary><font size=\"+2\">IMPORTANT: For Windows Users</font></summary>\r\n",
    "  \r\n",
    "  If you are on Windows make sure to download Ubuntu from the Microsoft store and install it. Then, update everything: `sudo apt update && sudo apt upgrade` and install python3-pip: `sudo apt-get install python3-pip`. Then you can follow the instructions below.\r\n",
    "\r\n",
    "</details>\r\n",
    "\r\n",
    "At the time of writing, the version of Airflow is 2.1.3, if you are going to use a different version, change it in the following code:\r\n",
    "\r\n",
    "```\r\n",
    "export AIRFLOW_HOME=~/airflow\r\n",
    "\r\n",
    "AIRFLOW_VERSION=2.1.3\r\n",
    "PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you are storing the values of Airflow and your Python version in two variables that are going to be used in the following command:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you will get the corresponding version of Airflow from their github repo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you can start using airflow. It is a good idea to initialize your database now. This database will contain metadata and it will host the DAGs you create:\n",
    "\n",
    "`airflow db init`\n",
    "\n",
    "And pass your credentials:\n",
    "\n",
    "```\n",
    "airflow users create \\\n",
    "    --username <your_username> \\\n",
    "    --firstname <your_firstname> \\\n",
    "    --lastname <your_lastname> \\\n",
    "    --role Admin \\\n",
    "    --email <your_email>\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check everything went right. In the terminal, run:\r\n",
    "\r\n",
    "`airflow webserver --port 8080`\r\n",
    "\r\n",
    "This will start a new server at your localhost at port 8080. It is important to notice that, even if you start the server, your scheduled DAGs won't be monitored. To do so, we need to kick off the scheduler, so open a new terminal and run:\r\n",
    "\r\n",
    "`airflow scheduler`\r\n",
    "\r\n",
    "If you receive a Warning message, don't worry, it won't affect your airflow current performance. Now, we are ready to start using the UI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, if you go to your browser and visit: `localhost:8080`, you should be able to see something like this:\n",
    "\n",
    "![](images/Airflow.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The image above depicts the Airflow UI. Here, you can see the DAGs that have been created, and so far, you will only see some examples and tutorials created by the Airflow team. Let's explore it a little bit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Airflow UI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inside the UI, you can explore the metadata of the DAGs, such as the name (or ID), the owner, the status of previous runs of the whole DAG or of specific tasks inside the DAG, its frequency (in the Schedule column), and when it was ran the last time.\n",
    "\n",
    "You can see more details by clicking on the DAG. Let's observe the `example_bash_operator` DAG\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow2.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the DAG you can see its structure, the average time it took for running each task, the Gantt chart of the DAG to check if there are overlapping tasks, the details of the DAG, and the code that generated the DAG. We haven't ran this DAG yet, so there is no info about previous runs. We can, however, take a look at the code. Before, that, let's observe the `Graph View` tab, which will contain the same as the `Tree View` tab, but rearranged:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow4.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observe that we have several Nodes, each one representing a task. You can also observe their dependencies, for example, `run_after_loop` won't start until all `runme_x` haven't finished. \n",
    "\n",
    "Let's run a single task to see how it works.\n",
    "\n",
    "1. In the Airflow UI enable the `example_bash_operator` DAG. \n",
    "2. Click the DAG to see its status. You should see that there are two runs, this is because (as we will see later) these examples are set to be ran 2 days ago. If you observe the schedule, it is meant to run once every day, so two runs make sense!\n",
    "3. Inside those runs, there are different status, in this case, we can see 'success' and 'skipped'. Don't worry, they are meant to be skipped.\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow4.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Let's see its flow by triggering an event. First click Auto-refresh to see updates in real time, and then, click the Play button:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_clip.gif\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pretty cool, isn't it? We can also see the durantion of each task, and when each run took place. But I will let you explore more on that in the UI. For now, let's take a look at the code. If you click on the `Code` tab, you will see this:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"Example DAG demonstrating the usage of the BashOperator.\"\"\"\r\n",
    "\r\n",
    "from datetime import timedelta\r\n",
    "\r\n",
    "from airflow import DAG\r\n",
    "from airflow.operators.bash import BashOperator\r\n",
    "from airflow.operators.dummy import DummyOperator\r\n",
    "from airflow.utils.dates import days_ago\r\n",
    "\r\n",
    "args = {\r\n",
    "    'owner': 'airflow',\r\n",
    "}\r\n",
    "\r\n",
    "with DAG(\r\n",
    "    dag_id='example_bash_operator',\r\n",
    "    default_args=args,\r\n",
    "    schedule_interval='0 0 * * *',\r\n",
    "    start_date=days_ago(2),\r\n",
    "    dagrun_timeout=timedelta(minutes=60),\r\n",
    "    tags=['example', 'example2'],\r\n",
    "    params={\"example_key\": \"example_value\"},\r\n",
    ") as dag:\r\n",
    "\r\n",
    "    run_this_last = DummyOperator(\r\n",
    "        task_id='run_this_last',\r\n",
    "    )\r\n",
    "\r\n",
    "    # [START howto_operator_bash]\r\n",
    "    run_this = BashOperator(\r\n",
    "        task_id='run_after_loop',\r\n",
    "        bash_command='echo 1',\r\n",
    "    )\r\n",
    "    # [END howto_operator_bash]\r\n",
    "\r\n",
    "    run_this >> run_this_last\r\n",
    "\r\n",
    "    for i in range(3):\r\n",
    "        task = BashOperator(\r\n",
    "            task_id='runme_' + str(i),\r\n",
    "            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\r\n",
    "        )\r\n",
    "        task >> run_this\r\n",
    "\r\n",
    "    # [START howto_operator_bash_template]\r\n",
    "    also_run_this = BashOperator(\r\n",
    "        task_id='also_run_this',\r\n",
    "        bash_command='echo \"run_id={{ run_id }} | dag_run={{ dag_run }}\"',\r\n",
    "    )\r\n",
    "    # [END howto_operator_bash_template]\r\n",
    "    also_run_this >> run_this_last\r\n",
    "\r\n",
    "# [START howto_operator_bash_skip]\r\n",
    "this_will_skip = BashOperator(\r\n",
    "    task_id='this_will_skip',\r\n",
    "    bash_command='echo \"hello world\"; exit 99;',\r\n",
    "    dag=dag,\r\n",
    ")\r\n",
    "# [END howto_operator_bash_skip]\r\n",
    "this_will_skip >> run_this_last\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    dag.cli()\r\n",
    "```"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the tasks were supposed to print out something to the console, we can check that on the Log tab of each task. For example, look at the `also_run_this` task, it is a BashOperator object that will print out `run_id={{ run_id }} | dag_run={{ dag_run }}`. Go to the `Graph View` tab and click on the `also_run_this` task, and in the next window click `Log`. Observe the output:\r\n",
    "\r\n",
    "<p align=\"center\">\r\n",
    "    <img src=\"images/AirFlow_log.png\" width=\"500\"/>\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This was a simple walkthrough to show the Airflow UI. You saw that:\n",
    "\n",
    "- The workflow is represented by a DAG\n",
    "- Each node in the DAG corresponds to a task\n",
    "- Each DAG has a schedule that sets the frequency of runs\n",
    "- Tasks can be triggered by previous tasks\n",
    "- Each task corresponds to an operator object\n",
    "- We saw BashOperator, which execute a bash script\n",
    "- We saw DummyOperator, which according to the documentation _'Operator that does literally nothing. It can be used to group tasks in a DAG.'_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will see how to create more operators, for example, a PythonOperator, in the next section. First, let's get some practice defining a DAG with the operators we have seen so far."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Your First DAG"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all, make sure you followed all steps so far. If that's the case, you should have a folder in your home directory named airflow. _Check it by running running the following cell. If no error is thrown, you are good to go_\r\n",
    "<details>\r\n",
    "  <summary><font size=\"+1\">IMPORTANT: For Windows Users</font></summary>\r\n",
    "  \r\n",
    "  If you are on Windows make sure to check it on the wsl terminal. You can simply type `ls ~` and check if there is a folder\r\n",
    "\r\n",
    "</details>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from os.path import expanduser\r\n",
    "import os\r\n",
    "\r\n",
    "home = expanduser(\"~\")\r\n",
    "airflow_dir = os.path.join(home, 'airflow')\r\n",
    "assert os.path.isdir(airflow_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inside that directory, you have to add a new folder named `dags`. Airflow will look into that directory to check if the DAGs you create through Python. Now, the example DAGs you are using are in your PATH directory, but new DAGs you create should be placed in `~/airflow/dags/`. _You can actually change the path where Airflow will look for new DAGs in the airflow.cfg file_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<details>\r\n",
    "  <summary><font size=\"+1\">IMPORTANT: For Windows Users</font></summary>\r\n",
    "  \r\n",
    "  If you are on Windows, go to the wsl console move to `cd ~/airflow`, and create the dags folder\r\n",
    "</details>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from os.path import expanduser\r\n",
    "from pathlib import Path\r\n",
    "home = expanduser(\"~\")\r\n",
    "airflow_dir = os.path.join(home, 'airflow')\r\n",
    "Path(f\"{airflow_dir}/dags\").mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Python files you create have to be stored in that folder. The file should contain the DAG with the desired arguments. The arguments can be passed in the context manager and in a dictionary.\n",
    "\n",
    "In the context manager, simply define the tasks, don't implement any logical flow. As saw above, tasks are defined by operators"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from airflow.models import DAG\r\n",
    "from datetime import datetime\r\n",
    "from datetime import timedelta\r\n",
    "from airflow.operators.bash_operator import BashOperator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "default_args = {\r\n",
    "    'owner': 'Ivan',\r\n",
    "    'depends_on_past': False,\r\n",
    "    'email': ['ivan@theaicore.com'],\r\n",
    "    'email_on_failure': False,\r\n",
    "    'email_on_retry': False,\r\n",
    "    'retries': 1,\r\n",
    "    'start_date': datetime(2020, 1, 1),\r\n",
    "    'retry_delay': timedelta(minutes=5),\r\n",
    "    'end_date': datetime(2022, 1, 1),\r\n",
    "    # 'queue': 'bash_queue',\r\n",
    "    # 'pool': 'backfill',\r\n",
    "    # 'priority_weight': 10,\r\n",
    "    # 'wait_for_downstream': False,\r\n",
    "    # 'dag': dag,\r\n",
    "    # 'trigger_rule': 'all_success'\r\n",
    "}\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with DAG(dag_id='test_dag',\r\n",
    "         default_args=default_args,\r\n",
    "         schedule_interval='*/1 * * * *',\r\n",
    "         catchup=False,\r\n",
    "         tags=['test']\r\n",
    "         ) as dag:\r\n",
    "    # Define the tasks. Here we are going to define only one bash operator\r\n",
    "    test_task = BashOperator(\r\n",
    "        task_id='write_date_file',\r\n",
    "        bash_command='cd ~/Desktop && date >> ai_core.txt',\r\n",
    "        dag=dag)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example can be found in the `examples` folder, under the name `dag_test.py`. Copy the example to your `dags` folder in your airflow directory.\r\n",
    "\r\n",
    "<details>\r\n",
    "  <summary><font size=\"+1\">IMPORTANT: For Windows Users</font></summary>\r\n",
    "  \r\n",
    "  If you are on Windows, copy the file using the command line and use the `cp` command to copy the files to `cd ~/airflow/dags`. If you struggle with these commands, and you want to copy everything manually, follow these instructions to find the folder that stores the files from the Ubuntu console.\r\n",
    "</details>\r\n",
    "\r\n",
    "Once the file is in the airflow directory, you can run it by running the following command (if you haven't started the scheduler yet, run `airflow scheduler -D`):\r\n",
    "\r\n",
    "`airflow dags unpause test_dag`\r\n",
    "\r\n",
    "If you want these DAGs to appear in the UI, you have to add them by running the following command:\r\n",
    "\r\n",
    "`airflow db init`\r\n",
    "\r\n",
    "So you can manage them in the UI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tasks Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You just created a task, but in a workflow, you will probablu need to add more than one, so let's add a few more. If you just specify the tasks, the tasks will be executed in sequence, with no specific order(you can change how they are executed by changing the executor in the airflow.cfg file). However, you can specify hoe they are ordered by setting the dependencies between them.\r\n",
    "\r\n",
    "Setting dependencies is quite simple. You can specify the tasks and then 'connect' them using the bit-shift operator. For example, if you want to run the task `runme_1` after the task `runme_0`, you can do it like this:\r\n",
    "\r\n",
    "`task_0 >> task_1` or `task_0.set_downstream(task_1)` or `task_1 << task_0` or `task_1.set_upstream(task_0)`.\r\n",
    "\r\n",
    "You can see that there are many ways to set the dependencies, so just pick the one that works for you.\r\n",
    "\r\n",
    "Let's say the you want to run both task `task_1` and `task_2` after task `task_0` has finished. You can do it like this:\r\n",
    "\r\n",
    "`task_0 >> [task_1, task_2]`\r\n",
    "\r\n",
    "Another thing you my want to do is running `task_2` only when both `task_0` and `task_1` have finished. You can do it like this:\r\n",
    "```\r\n",
    "task_0 >> task_2\r\n",
    "task_1 >> task_2\r\n",
    "```\r\n",
    "\r\n",
    "Finally, you can also set sequencial dependencies between tasks. For example, if you want to run `task_2` after `task_1`, and `task_1` after `task_0` have finished, you can do it like this:\r\n",
    "\r\n",
    "`task_0 >> task_1 >> task_2`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The example below shows a DAG with four tasks:\n",
    "\n",
    "1. date_task: A BashOperator that appends the current date into a file\n",
    "2. add_task: A BashOperator that stages the file created by date_task\n",
    "3. commit_task: A BashOperator that commits the file staged by add_task\n",
    "4. push_task: A BashOperator that pushes the committed file to a remote repository"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from airflow.models import DAG\r\n",
    "from datetime import datetime\r\n",
    "from datetime import timedelta\r\n",
    "from airflow.operators.bash_operator import BashOperator\r\n",
    "\r\n",
    "default_args = {\r\n",
    "    'owner': 'Ivan',\r\n",
    "    'depends_on_past': False,\r\n",
    "    'email': ['ivan@theaicore.com'],\r\n",
    "    'email_on_failure': False,\r\n",
    "    'email_on_retry': False,\r\n",
    "    'retries': 1,\r\n",
    "    'start_date': datetime(2020, 1, 1), # If you set a datetime previous to the curernt date, it will try to backfill\r\n",
    "    'retry_delay': timedelta(minutes=5),\r\n",
    "    'end_date': datetime(2022, 1, 1),\r\n",
    "}\r\n",
    "with DAG(dag_id='test_dag_dependencies',\r\n",
    "         default_args=default_args,\r\n",
    "         schedule_interval='*/1 * * * *',\r\n",
    "         catchup=False,\r\n",
    "         tags=['test']\r\n",
    "         ) as dag:\r\n",
    "    # Define the tasks. Here we are going to define only one bash operator\r\n",
    "    date_task = BashOperator(\r\n",
    "        task_id='write_date',\r\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && date >> date.txt',\r\n",
    "        dag=dag)\r\n",
    "    add_task = BashOperator(\r\n",
    "        task_id='add_files',\r\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git add .',\r\n",
    "        dag=dag)\r\n",
    "    commit_task = BashOperator(\r\n",
    "        task_id='commit_files',\r\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git commit -m \"Update date\"',\r\n",
    "        dag=dag)\r\n",
    "    push_task = BashOperator(\r\n",
    "        task_id='push_files',\r\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git push',\r\n",
    "        dag=dag)\r\n",
    "    \r\n",
    "    date_task >> add_task >> commit_task\r\n",
    "    add_task >> push_task\r\n",
    "    commit_task >> push_task"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observe the last part of the DAG, you can see the dependencies between the tasks. Of course, you could simply set them all in tandem, but in this case, you will see how to set the dependencies in different ways.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Dependencies.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After running it, you will see that your repo is being updated every minute (which might be confusing, but this is a demo, so we don't care)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_GitHub.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try it out\n",
    "\n",
    "1. Create a new remote repository in your GitHub account. \n",
    "2. You will eventually use if for storing weather data, so name your repository accordingly.\n",
    "3. Clone the repository to your local machine.\n",
    "4. Copy the DAG file `dag_test_dependencies.py` to the folder `dags` in your local machine.\n",
    "5. Change the file according to the name of your repository and the directory you cloned it to.\n",
    "6. Unpause the DAG by running `airflow dags unpause dag_test_dependencies` or by going to the `DAGS` tab in the UI and clicking on the `dag_test_dependencies` DAG."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you start creating DAGs, you might forget which one are active. Good thing is that airflow has many commands to check your works in the command line. If you type `airflow -h` you can see all comands."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%bash\r\n",
    "airflow -h"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "usage: airflow [-h] GROUP_OR_COMMAND ...\n",
      "\n",
      "positional arguments:\n",
      "  GROUP_OR_COMMAND\n",
      "\n",
      "    Groups:\n",
      "      celery         Celery components\n",
      "      config         View configuration\n",
      "      connections    Manage connections\n",
      "      dags           Manage DAGs\n",
      "      db             Database operations\n",
      "      jobs           Manage jobs\n",
      "      kubernetes     Tools to help run the KubernetesExecutor\n",
      "      pools          Manage pools\n",
      "      providers      Display providers\n",
      "      roles          Manage roles\n",
      "      tasks          Manage tasks\n",
      "      users          Manage users\n",
      "      variables      Manage variables\n",
      "\n",
      "    Commands:\n",
      "      cheat-sheet    Display cheat sheet\n",
      "      info           Show information about current Airflow and environment\n",
      "      kerberos       Start a kerberos ticket renewer\n",
      "      plugins        Dump information about loaded plugins\n",
      "      rotate-fernet-key\n",
      "                     Rotate encrypted connection credentials and variables\n",
      "      scheduler      Start a scheduler instance\n",
      "      sync-perm      Update permissions for existing roles and optionally DAGs\n",
      "      version        Show the version\n",
      "      webserver      Start a Airflow webserver instance\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help         show this help message and exit\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus, you can look at the dags by running"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%bash\r\n",
    "airflow dags list"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'airflow' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Airflow Variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing you might notice in the dependencies is that we had to constantly pass the file path to the BashOperator. This is not very efficient, so let's change that. One way to do it is by defining a variable that contains the path to the directory in which the file is stored.\n",
    "\n",
    "Airflow has a way to define variables from the UI or from the command line. In this case we are going to only use the UI. The variables you include in the UI will be then available in the Python code.\n",
    "\n",
    "So, open your UI, click on 'Admin' and then on 'Variables'.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_variables.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next window, you can add the variables you will need. You can import a file from you computer, or you can click the `+` sign to add a variable manually.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_variables2.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next window, you can add the name of the variable in the Key and the value in the Value.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_variables3.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After pressing `Save`, the variable will be stored in the Airflow database, and you will be able to use it in your Python code. To do so, you have to import the class Variable:\n",
    "```\n",
    "from airflow.models import Variable\n",
    "\n",
    "weather_dir = Variable.get(\"weather_dir\")\n",
    "```\n",
    "\n",
    "Now, you will be able to use that variable in your Python code. If you look at the script in `dag_test_variables.py`, you will see that we are using the variable `weather_dir` to define the path to the file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Python Operators"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have seen that you can run bash commands in each task. Airflow is not limited to these commands, you can also create PythonOperators to do anything you, as long as it is contained in a Python function. Let's say that you want to create a PythonOperator that will extract information about events that took place 'On this day' in the past.\n",
    "\n",
    "The first thing you have to do is creating a function that uses requests and bs4 to download that information from Wikipedia. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "from os.path import expanduser\r\n",
    "import requests\r\n",
    "\r\n",
    "def get_ul(url: str):\r\n",
    "    r = requests.get(url)\r\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\r\n",
    "    return soup.find('ul')\r\n",
    "\r\n",
    "def get_today_events(url: str, file_dir: str):\r\n",
    "    ul_soup = get_ul(url)\r\n",
    "    for li in ul_soup.find_all('li'):\r\n",
    "        write_file(li.text, file_dir)\r\n",
    "\r\n",
    "def write_file(li: str, file_dir: str):\r\n",
    "    with open(file_dir, 'a') as f:\r\n",
    "        f.write(li)\r\n",
    "        f.write('\\n')\r\n",
    "\r\n",
    "home = expanduser(\"~\")\r\n",
    "desktop_dir = os.path.join(home, 'Desktop/test_2.txt')\r\n",
    "get_today_events('https://en.wikipedia.org/wiki/Wikipedia:On_this_day/Today', desktop_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These functions can be passed to the PythonOperator as arguments. In this case, we will pass the function `get_today_events` to the PythonOperator. There are two things that you have to include to the PythonOperator:\n",
    "\n",
    "1. The task id\n",
    "2. The Python function that will be executed\n",
    "3. Optional: the arguments of the function (if any)\n",
    "\n",
    "One thing to note is that, even if functions are usually at the top of your code, in this case, that convention is not applied, and you usually specify the function right before the PythonOperator.\n",
    "\n",
    "## Try it out\n",
    "\n",
    "For the following example, you are going to create a PythonOperator that will download the events that took place on this day as shown above. That file is going to be uploaded to a remote repository.:\n",
    "\n",
    "1. Create a new remote repository in your GitHub account.\n",
    "2. Clone the repository to your local machine.\n",
    "3. Add a variable in the Airflow UI to set the path to the remote repository.\n",
    "4. Create the DAG, where you will call for the function, then stage the changes, commit them, and push them to the remote repository. The DAG should run daily.\n",
    "5. Move the DAG file to the folder `dags` in your local machine.\n",
    "6. Test the DAG by running `airflow dags test <Name of your DAG>`.\n",
    "\n",
    "You have a small template in the examples folder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Xcom: Connecting Tasks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you are working in a Python script, you might want to pass information from one function to another. When you work with tasks in Airflow, you can do the same by using the Xcom feature.\n",
    "\n",
    "Xcom will store the variables in a special database called XCom. You can read more about XCom in the [official documentation](https://airflow.apache.org/concepts.html#xcom). You can store those variables as the tasks are running, and when they are finished, you can retrieve them and pass them to the next tasks. Take a look at the next code (contained in `dag_test_xcom.py`),  passes information between PythonOperators.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from airflow import DAG\r\n",
    "from airflow.operators.bash import BashOperator\r\n",
    "from airflow.operators.python import PythonOperator\r\n",
    "from random import uniform\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "default_args = {\r\n",
    "    'start_date': datetime(2020, 1, 1)\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "def training_model(ti):\r\n",
    "    accuracy = uniform(0.1, 10.0) # In this case, the accuracy is a random number\r\n",
    "                                  # but when you use it with your ML models, you\r\n",
    "                                  # can call your real models inside\r\n",
    "    print(f'model\\'s accuracy: {accuracy}')\r\n",
    "    ti.xcom_push(key='model_accuracy', value=accuracy)\r\n",
    "\r\n",
    "\r\n",
    "def choose_best_model(ti):\r\n",
    "    fetched_accuracy = ti.xcom_pull(\r\n",
    "                            key='model_accuracy',\r\n",
    "                            task_ids=['training_model_A'])\r\n",
    "    print(f'choose best model: {fetched_accuracy}')\r\n",
    "\r\n",
    "\r\n",
    "with DAG('test_dag_xcom',\r\n",
    "         schedule_interval='@daily',\r\n",
    "         default_args=default_args,\r\n",
    "         catchup=False) as dag:\r\n",
    "\r\n",
    "    downloading_data = BashOperator(\r\n",
    "        task_id='downloading_data',\r\n",
    "        bash_command='sleep 3'\r\n",
    "    )\r\n",
    "    training_model_task = [\r\n",
    "        PythonOperator(\r\n",
    "            task_id=f'training_model_{task}',\r\n",
    "            python_callable=training_model\r\n",
    "        ) for task in ['A', 'B', 'C']]\r\n",
    "\r\n",
    "    choose_model = PythonOperator(\r\n",
    "        task_id='choose_model',\r\n",
    "        python_callable=choose_best_model\r\n",
    "    )\r\n",
    "    downloading_data >> training_model_task >> choose_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is just an example of how to use it. You will eventually use it for something that will actually retrieve data. Observe that, in the functions you create, you are using ti.xcom_push to pass information to the next task, and ti.xcom_pull to retrieve it. As you can see in the following graph, the input is passed to each of the models, and their results are passed to a model chooser.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Xcom.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Once you run it, you will see these Xcoms in the UI:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Xcom2.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Finally, in the next window, you can see the results of the Xcoms.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Xcom3.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Observe the names of the Xcoms. There is a name for each of the models that have been run, and there is one that is called `return_value` (In fact, there are many `return_value`s). These Xcoms correspond to the BashOperators that have been created, which _by default_ will push their output, so any task in the DAG can retrieve it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Airflow and PostgreSQL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Airflow allows you to run SQL queries in your tasks. You can connect to virtually any database, but in this example we will connect to a PosgreSQL database. Connecting to a database is very easy in Airflow, especially because the UI has a nice way to do it.\n",
    "\n",
    "Before you start, you have to install the PosgreSQL client library. To do so, you can run the following command:\n",
    "\n",
    "`pip install apache-airflow-providers-postgres`\n",
    "\n",
    "From now on, Airflow will know how to connect to a postgres database. _If you want to connect to a different database, you can do it by installing a different provider. Check out [this webpage](https://www.astronomer.io/guides/connections) to get more info_\n",
    "\n",
    "As mentioned, the Airflow UI has a nice way to connect to a database. Open your UI, click on 'Admin' and then on 'Connections'.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_SQL.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next window, you can select the connection you want to configure. Take a look at all possibilities Airflow offers! Let's click on the `PostgreSQL` connection. You will see the following:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_SQL2.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fill the fields with your data, but take into account that `schema` here is the name of your database and `login` is your username. Just for demonstration purposes, we will use the `Pagila` database."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You are ready to use PostgreSQL from Airflow. In the next example, we are creating a new table with animal names. You will find the same code in `dag_test_sql.py`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import datetime\r\n",
    "\r\n",
    "from airflow import DAG\r\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\r\n",
    "\r\n",
    "with DAG(\r\n",
    "    dag_id=\"test_dag_postgre\",\r\n",
    "    start_date=datetime.datetime(2020, 2, 2),\r\n",
    "    schedule_interval=\"@once\",\r\n",
    "    catchup=False,\r\n",
    ") as dag:\r\n",
    "\r\n",
    "    create_pet_table = PostgresOperator(\r\n",
    "        task_id=\"create_pet_table\",\r\n",
    "        postgres_conn_id=\"postgres_default\",\r\n",
    "        sql=\"\"\"\r\n",
    "            CREATE TABLE IF NOT EXISTS pet (\r\n",
    "            pet_id SERIAL PRIMARY KEY,\r\n",
    "            name VARCHAR NOT NULL,\r\n",
    "            pet_type VARCHAR NOT NULL,\r\n",
    "            birth_date DATE NOT NULL,\r\n",
    "            OWNER VARCHAR NOT NULL);\r\n",
    "          \"\"\",\r\n",
    "    )\r\n",
    "\r\n",
    "    populate_pet_table = PostgresOperator(\r\n",
    "        task_id=\"populate_pet_table\",\r\n",
    "        postgres_conn_id=\"postgres_default\",\r\n",
    "        sql=\"\"\"\r\n",
    "            INSERT INTO pet VALUES ( 1, 'Max', 'Dog', '2018-07-05', 'Jane');\r\n",
    "            INSERT INTO pet VALUES ( 2, 'Susie', 'Cat', '2019-05-01', 'Phil');\r\n",
    "            INSERT INTO pet VALUES ( 3, 'Lester', 'Hamster', '2020-06-23', 'Lily');\r\n",
    "            INSERT INTO pet VALUES ( 4, 'Quincy', 'Parrot', '2013-08-11', 'Anne');\r\n",
    "            \"\"\",\r\n",
    "    )\r\n",
    "\r\n",
    "    get_all_pets = PostgresOperator(\r\n",
    "        task_id=\"get_all_pets\", postgres_conn_id=\"postgres_default\", sql=\"SELECT * FROM pet;\"\r\n",
    "    )\r\n",
    "\r\n",
    "    get_birth_date = PostgresOperator(\r\n",
    "        task_id=\"get_birth_date\",\r\n",
    "        postgres_conn_id=\"postgres_default\",\r\n",
    "        sql=\"\"\"\r\n",
    "            SELECT * FROM pet\r\n",
    "            WHERE birth_date\r\n",
    "            BETWEEN {{ params.begin_date }} AND {{ params.end_date }};\r\n",
    "            \"\"\",\r\n",
    "        params={'begin_date': '2020-01-01', 'end_date': '2020-12-31'},\r\n",
    "    )\r\n",
    "\r\n",
    "\r\n",
    "    create_pet_table >> populate_pet_table >> get_all_pets >> get_birth_date"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining the tasks is very similar to defining PythonOperators. The only difference is that you have to include the `postgres_conn_id` in the task definition. Then, the `sql` argument will contain the query, and the params argument will contain the variables you might want to add to the query."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you copy that code to the `dags` folder, you can test it by running `airflow dags test dag_test_sql`. Then, in your pgAdmin, you can see the table `pets`:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_SQL3.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same way you connected to your localhost, you can also connect to your AWS RDS, we are slowly integrating everything from Python!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Note"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything we have done so far can be also done using the command line in an easy manner as well. For example, for changing the configuration of a connection you can run the following command:\n",
    "\n",
    "`airflow connections add <connection_name> \\ --conn-uri \"conn-type://<user>:<password>@<host>:<port>/<database>`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or another example you can do in the command line is to create a new variable:\n",
    "\n",
    "`airflow variables -s <var_name> <var_value>`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check out the official documentation for more information. \r\n",
    "\r\n",
    "With this in mind, you can now start creating your own Airflow DAGs in your EC2 instances without having to worry about the Airflow UI. Just leave it running as shown in the `Cloud Basics` module and you will have a schedule running nonstop!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- When dealing with tasks working in parallel or sequence, you will need to establish a workflow.\n",
    "- Airflow allows you to define your tasks in a way that is easy to understand and maintain.\n",
    "- Airflow orchestrates these tasks using DAGs.\n",
    "- You can define your DAGs in a python script. Each task is defined by an Operator, which can be a PythonOperator, BashOperator, etc.\n",
    "- The Airflow UI allows you to configure how these tasks will run. \n",
    "- The UI can also show the progress of the tasks.\n",
    "- You can set the configuration of Airflow in the UI, so you can connect to different databases, or even AWS RDS."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad8bebc098a042dc0df4e42fc2ecc8fff0bd7b8741641ce29007c29766dadbe0"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}