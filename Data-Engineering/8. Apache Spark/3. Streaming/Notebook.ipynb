{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "\n",
    "> Streaming is an extension to Spark's core which allows to perform __real life data processing__\n",
    "\n",
    "> __Not all functionalities are available on Python currently, in order to work with full powered framework one should use `scala` or `java`__\n",
    "\n",
    "![](./images/streaming-arch.png)\n",
    "\n",
    "Flow looks as below:\n",
    "1. We hook up to data stream source (e.g. `Apache Kafka`)\n",
    "2. Incoming data is divided into batches \n",
    "3. Batches are processed by `spark` engine\n",
    "4. Resulting data batches are returned (e.g. after `MapReduce` transforms)\n",
    "\n",
    "![](./images/streaming-flow.png)\n",
    "\n",
    "## DStreams (Discretized Streams)\n",
    "\n",
    "> High level abstraction over stream of data which allows us to easily work it\n",
    "\n",
    "These can be created either by:\n",
    "- Reading directly our data source\n",
    "- Modifying  existing `DStreams` (and creating new ones at the same time)\n",
    "\n",
    "> __`DStreams` internally are represented as a sequence of `RDD`s__\n",
    "\n",
    "### Receivers\n",
    "\n",
    "> Every `DStream` (except file) is connected to the __`Receiver`__\n",
    "\n",
    "Receiver's job is to:\n",
    "- receive data from a source\n",
    "- store in Spark's memory for processing\n",
    "\n",
    "__Points to remember:__\n",
    "- __SINGLE `Receiver` USED ON THREAD__\n",
    "- If we use a single thread (or not enough resources in terms of cluster) there will be no threads left to actually run data processing!\n",
    "- __We should give more cores than `Receivers` we would like to run__\n",
    "- We can run multiple streams to get data from different sources\n",
    "- __`FileStreams` are exception as  they periodically query the system for new files__ (and this can be done on a single thread)\n",
    "\n",
    "Let's start by creating `Session` (__you should always start like that!__)\n",
    "\n",
    "#### Receivers reliability\n",
    "\n",
    "Receivers can be split into two wide categories:\n",
    "\n",
    "- __reliable__:\n",
    "    - gets data from the reliable stream\n",
    "    - sends acknowledgmenet to stream when the data has been processed and saved to Spark memory\n",
    "    - source receives the acknowledgment and verifies everything is correct\n",
    "    - __Example: `Apache Kafka`__\n",
    "- __unreliable__\n",
    "    - either `receiver` or `source` cannot process acknowledgments \n",
    "    - we don't care about sending acknowledgment and data loss is not a problem\n",
    "\n",
    "__First category ensures no packages have been lost during transmission__\n",
    "\n",
    "## Coding\n",
    "\n",
    "Let's start by creating `Session` (__you should always start like that!__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# We should always start with session in order to obtain\n",
    "# context and session if needed\n",
    "session = pyspark.sql.SparkSession.builder.config(\n",
    "    conf=pyspark.SparkConf()\n",
    "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
    "    .setAppName(\"TestApp\")\n",
    ").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StreamingContext\n",
    "\n",
    "After that we can create `pyspark.streaming.StreamingContext` ([docs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.html)) object which:\n",
    "- takes `sparkContext` as input\n",
    "- it can be the one we're using for other things (SQL or core PySpark functionality)\n",
    "- __it does not work with `pyspark.SparkContext` object directly though!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# This context can be used with PySpark streaming\n",
    "# You might have to specify batchDuration (e.g. on which time window operation will be run)\n",
    "# By default data is collected every 0.5 seconds\n",
    "ssc = StreamingContext(session.sparkContext, batchDuration=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important things to notice:\n",
    "- __We set up the whole computation pipeline first__\n",
    "- __NOTHING__ is started until we use `ssc.start()` and finish with `ssc.end()`\n",
    "\n",
    "Methods of interest which allow us to work with streams:\n",
    "- [`ssc.awaitTermination([timeout])`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.awaitTermination.html#pyspark.streaming.StreamingContext.awaitTermination)\n",
    "    processes data indefinitely or up to a moment `timeout` is hit\n",
    "- [`ssc.checkpoint(directory)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.checkpoint.html#pyspark.streaming.StreamingContext.checkpoint) -\n",
    "    periodically checkpoint data for increased fault tolerance\n",
    "- [`scc.getActiveOrCreate(path, function)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.getActiveOrCreate.html#pyspark.streaming.StreamingContext.getActiveOrCreate) -\n",
    "    If there is an active stream (`start`ed and not `stop`ped) return it, otherwise recreate if from checkpoint\n",
    "\n",
    "Methods we will run each time:\n",
    "- [`scc.start()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.start.html#pyspark.streaming.StreamingContext.start) - starts execution of streams\n",
    "- [`scc.stop()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.stop.html#pyspark.streaming.StreamingContext.stop) - stops executions of streams\n",
    "\n",
    "Creating streams from context:\n",
    "- [`ssc.socketTextStream(hostname, port [,storageLevel])`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.socketTextStream.html#pyspark.streaming.StreamingContext.socketTextStream) - create input stream by listening on specified `hostname` and `port`\n",
    "- [`ssc.textFileStreams(directory)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.textFileStream.html#pyspark.streaming.StreamingContext.textFileStream) - watch for new files created on Hadoop compatible system (e.g. HDFS) in specified directory and read them as text files\n",
    "- [`ssc.binaryRecordsStream(directory, recordLength)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.binaryRecordsStream.html#pyspark.streaming.StreamingContext.binaryRecordsStream) - as above, but reads the files as binary\n",
    "\n",
    "Given all of that, let's create a `DStream` via `socketTextStream` which will listen on `localhost` for data incoming to the machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will send lines of data to this socketTextStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also apply same transformations which:\n",
    "- Will split input text into `words` and flatten the result\n",
    "- Count unique words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = lines.flatMap(lambda text: text.split()).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print the incoming words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notice nothing happened yet!__\n",
    "\n",
    "This is due to our `computation` not starting yet. In order to do that, we have to run `ssc.start()`.\n",
    "\n",
    "Before we do that though, let's run `netcat` to push some data to our socket.\n",
    "\n",
    "Please notice that:\n",
    "- `--listen` flag is specified __which means we have created server listening on specified port__\n",
    "- PySpark's `socket` __expects to find server which it can connect to under specified address!__\n",
    "\n",
    "Let's run this interactive command. It will allow us to send text data to `localhost:9999` (make sure you have `nc` or `netcat` available on your system!).\n",
    "\n",
    "> __Run it in the terminal in order not to block the execution of the notebook!__\n",
    "\n",
    "After you've run it you can send textual data to the server setup by `netcat`.\n",
    "Type some words in the terminal and they will be added to the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install with sudo apt install netcat\n",
    "# !nc -lk 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can run `start` which:\n",
    "- __will run indefinitely__, BUT\n",
    "- __will NOT stop Python's program execution__\n",
    "\n",
    "Due to above we will be able to run next cell (in our program it would be next line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "# After starting the stream run the command, from your spark folder.\n",
    "# ./bin/spark-submit --master \"local[2]\" examples/src/main/python/streaming/network_wordcount.py localhost 9999\n",
    "# This we begin the example wordcount_stream script.\n",
    "# You can then type words in your terminal and they should be outputted to the stream. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the cell below in order to wait for `seconds` until `pyspark` terminates the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds = 180\n",
    "ssc.awaitTermination(seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can stop `pyspark` client handling incoming data.\n",
    "\n",
    "Specifying `stopGraceFully=True` will allow it to finish only after consuming whole data posted to `9999` port:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze\n",
    "\n",
    "Let's analyze our results. This infographic will help us clear the confusion:\n",
    "\n",
    "![](./images/streaming-dstream.png)\n",
    "\n",
    "![](./images/streaming-dstream-ops.png)\n",
    "\n",
    "As one can see:\n",
    "\n",
    "- __Operations are done on the batch of gathered data, NOT ON THE WHOLE DATASET__\n",
    "- If we want to do that we should `persist` the results to disk periodically\n",
    "- Streams will be automatically cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How different transformations work with Streams\n",
    "\n",
    "## updateStateByKey\n",
    "\n",
    "> `stream.updateStateByKey(func)` allows us to __keep one state and update it continuously with values coming from a stream__\n",
    "\n",
    "Given a `func` taking two arguments `(new, accumulated)`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "could accumulate count of words (as the function is applied on a per-key value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window operations \n",
    "\n",
    "> __Windowed computations allow you to apply operations over sliding window of data__\n",
    "\n",
    "![](./images/streaming-dstream-window.png)\n",
    "\n",
    "These type of operations:\n",
    "- slide over `DStream`'s `RDD`s\n",
    "- `RDD`s being within the window are combined via our operation\n",
    "- These combined values create new `DStream`\n",
    "\n",
    "For windowed operations we need to specify:\n",
    "- size of sliding window (`3` in this case) a.k.a. __window length__\n",
    "- interval at which operation is performed (`2` in this case) a.k.a. __window interval__\n",
    "\n",
    "An example function could be [`reduceByKeyAndWindow`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.streaming.DStream.reduceByKeyAndWindow.html), others include:\n",
    "- `reduceByWindow`\n",
    "- `countByValueAndWindow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins\n",
    "\n",
    "> __Streams can be joined with other streams and `DataFrame`s we have in memory__\n",
    "\n",
    "Above allows us to:\n",
    "- accumulate data in memory\n",
    "- mix multiple streams from different data sources\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream1 = ...\n",
    "stream2 = ...\n",
    "joinedStream = stream1.join(stream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __The same types of joins are supported as in case of `pyspark.sql`__\n",
    "\n",
    "One can also join streams with `spark.sql.DataFrame` objects via `transform`, __notice one can change the `dataset` we're joining against!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ... # some DataFrame\n",
    "\n",
    "windowedStream = stream.window(20)\n",
    "joinedStream = windowedStream.transform(lambda rdd: rdd.join(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Streams\n",
    "\n",
    "One can also save data afternecessary transformations were applied.\n",
    "\n",
    "> __These operations will trigger stream running, same as `print`!__\n",
    "\n",
    "Most common two options supported by Python API:\n",
    "- [`saveAsTextFiles`](http://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.streaming.DStream.saveAsTextFiles.html) - __save each `RDD` in `DStream` as `str` type to a text file__\n",
    "- [`foreachRDD(func)`](http://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.streaming.DStream.foreachRDD.html?highlight=foreachrdd) - generic function applying transformation on each `RDD` within `DStream`, __also can be used for saving specific parts of data__\n",
    "\n",
    "An example of `foreach`-like function could be sending partitions of data to another system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendPartition(iter):\n",
    "    # ConnectionPool is a static, lazily initialized pool of connections\n",
    "    connection = ConnectionPool.getConnection()\n",
    "    for record in iter:\n",
    "        connection.send(record)\n",
    "    # return to the pool for future reuse\n",
    "    ConnectionPool.returnConnection(connection)\n",
    "\n",
    "dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see the code snippet below which shows how one can mix `pyspark.sql` with `streams`.\n",
    "Can you tell what this code does and explain all of the lines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazily instantiated global instance of SparkSession\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "...\n",
    "\n",
    "# DataFrame operations inside your streaming program\n",
    "\n",
    "words = ... # DStream of strings\n",
    "\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    try:\n",
    "        # Get the singleton instance of SparkSession\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        rowRdd = rdd.map(lambda w: Row(word=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame\n",
    "        wordsDataFrame.createOrReplaceTempView(\"words\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = spark.sql(\"select word, count(*) as total from words group by word\")\n",
    "        wordCountsDataFrame.show()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "words.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## persist and Dstreams\n",
    "\n",
    "From the previous lesson we have learned about `persistence` and what it does.\n",
    "\n",
    "> Using `persist()` method on a DStream will automatically persist every RDD of that DStream in memory\n",
    "\n",
    "Things to note:\n",
    "- We run multiple operations on the stream (so it doesn't have to be loaded from disk, __speedup over Hadoop's approach__\n",
    "- By default data from sources are replicated on two nodes for improved fault tolerance\n",
    "- For some operations (e.g. windowed ones) this is done automatically (as we will compute over multiple RDDs hence it's faster to do that in-memory if possible)\n",
    "\n",
    "# Checkpointing \n",
    "\n",
    "> As streaming services run 24/7 Spark checkpoints data in order to be resistent to faulty nodes, JVM crashes etc.\n",
    "\n",
    "Checkpointing has two types:\n",
    "- __Metadata checkpointing__ - saves information __defining the computations__ (steps we have to undertake). Examples would include:\n",
    "    - Configuration\n",
    "    - `DStream` operations\n",
    "    - Yet unprocessed batches of data\n",
    "- __Data checkpointing__ -  saving generated RDD to distributed storage. Used when:\n",
    "    - Combining data incoming from stream\n",
    "    - The longer the chain (the longer service runs) the chain of operations needed to get the result increases\n",
    "    - To keep it reasonably small periodic checkpointing of RDDs is done and the rest of the chain is discarded \n",
    "    \n",
    "> __In some cases we need to provide directory for checkpointing, ALWAYS CHECK THE DOCS OF OPERATIONS YOU ARE APPLYING ON STREAMS__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure to follow when working with Streams\n",
    "\n",
    "Here are the steps one usually employs when working with streams:\n",
    "\n",
    "1. Define `SparkStreamContext` from Session's context\n",
    "2. Define the input sources by creating input `DStreams`.\n",
    "3. Define the streaming computations by applying transformation and output operations to `DStreams`.\n",
    "4. Start receiving data and processing it using `streamingContext.start()`.\n",
    "5. Wait for the processing to be stopped (manually or due to any error) using `streamingContext.awaitTermination()`.\n",
    "6. The processing can be manually stopped using `streamingContext.stop()`.\n",
    "\n",
    "In addition we should remember (especially when working with multiple streams):\n",
    "\n",
    "1. __Once a context has been started, no new streaming computations can be set up or added to it__.\n",
    "2. Once a context has been stopped, it cannot be restarted.\n",
    "3. Only one `StreamingContext` can be active in a JVM at the same time.\n",
    "4. `stop()` on `StreamingContext` also stops the `SparkContext`. \n",
    "    To stop only the `StreamingContext`, set the optional parameter of `stop()` `stopSparkContext` to false.\n",
    "5. A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped \n",
    "    (without stopping the SparkContext) before the next StreamingContext is created.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66a4c1a82e4b740877da9482796a67c3d19e50649dddfb2985c1182127ae3fd0"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-AiCore] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
