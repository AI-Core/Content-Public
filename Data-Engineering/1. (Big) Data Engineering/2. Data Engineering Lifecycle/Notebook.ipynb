{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Engineering  Lifecycle </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Data Engineering\n",
    "\n",
    "> As is the case with general software engineering, data engineering is also a process that follows a series of well-defined steps. \n",
    "\n",
    "In the previous section, we introduced the main (big) data engineering concepts, and looked at the various tools available for organizations to use.  In this notebook, we'lll take a closer look at the process of deploying data engineering solutions to production environments, as well as discuss some of the similarities and differences between the roles of the data engineer, data scientist and the data (sometimes called business) analyst.\n",
    "\n",
    "From a bird's eye view, the end goal of data engineering is to address the challenge that many organizations have, which is getting access to large volumes of clean, accurate, complete and well-labelled data to be able to perform reliable analytics and data to properly create and train data science models.\n",
    "\n",
    "Data science teams, on the other hand, are one of the main stakeholders who rely on the output of a company's data engineering platform. Looking at it from this angle, data engeering is a pre-requisite to data science activities in large enterprises.\n",
    "\n",
    "One way to visualize this is using the below pyramid, which portrays the data science hierarchy of needs.  In order for data scientists to properly create, train and test predictive models, a significant amount of back-end effort is required to collect, integrate and prepare the data infrstructure and files:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/data-science-pyramid.png\" width=600>\n",
    "</p>\n",
    "\n",
    "The pyramid consists of 6 sequential steps in total.  Starting from the bottom, the steps are:\n",
    "\n",
    "1. Data Collection.\n",
    "2. Data Movement and Storage.\n",
    "3. Data Transformation and Exploration.\n",
    "4. Data Aggregation and Labeling.\n",
    "5. Data Training and Optimization.\n",
    "6. AI and Deep Learning.\n",
    "\n",
    "Data engineering encompasses steps 1, 2 and 3, while data science includes steps 4, 5 and 6.  The steps are normally done sequentially, and completing one step is usually a pre-requisite for the next step in this process.\n",
    "\n",
    "Of course, depending on the size of an organization and the complexity of its data architecture, sometimes the roles of the data engineer and data scientist overlap. For example, we could have a data engineer implementing some data aggregations and labeling (step # 4).  We can also have data scientists performing some data movement (step #2), transformation and cleaning (step #3).  In general however, large global companies have mature systems and teams and the roles for each team are well-defined and the roles rarely overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Data Engineering Lifecycle\n",
    "\n",
    "> A technology lifecycle refers to a process for planning, creating, testing and deploying an information system to production environments.\n",
    "\n",
    "Data engineering is a core part of the data lifecycle. Data preparation and engineering tasks represent over 80% of the time consumed in most AI and machine learning projects. But what exactly does data dngineering entail? Data engineering comprises all engineering and operational tasks required to make data available for analytics and data science purposes.\n",
    "\n",
    "Now that we've seen where data engineering fits in the bigger picture, we'll take a closer look at the actual steps involved in a data engineering lifecycle.\n",
    "\n",
    " From a high-level, a typical data engineering lifecycle in large organizations includes the following 7 steps:\n",
    "\n",
    "1. Requirements gathering and planning\n",
    "2. Solution Architecture.\n",
    "3. Deploying data stores.\n",
    "4. Data ingestion.\n",
    "5. ETL/ELT (gathering, importing, wrangling, querying, and analyzing data).\n",
    "6. Solution deployment.\n",
    "7. System monitoring and performance tuning.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/data-eng-lifecycle3.png\" width=500>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "Depending on the complexity of the data platform and the defined business requirements, there could be some variation from the above steps as it's not an exact science.  In general however, the typical data engineering project goes through these phases one way or another.  It's also important to understand that each step can go through several iterations before it's finalized.  Under the currently used Agile Scrum project management model in most data oriented organizations, short incremental iterations with frequent feedback are preferred over long and complex time durations.\n",
    "\n",
    "Next, we'll explore each one of these phases in greater detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Requirements Gathering and Planning\n",
    "\n",
    "> During this initial lifecycle phase, the enterprise Architects, product owners and relevant experts meticulously collect precise requirements from business.\n",
    "\n",
    "In any major corporate project, a business unit will be tasked to fund and oversee the project from planning to completion.  The aim of this phase is to present a solution fine-tuned to the needs of the business and fitting the identified business requirements. Any unclarities must be identified and addressed as early as possible to avoid future delays. All these details are captured in a document called the business requirements document (BRD).\n",
    "\n",
    "Business requirements are a brief set of business functionalities that the system needs to meet to be successful. This phase does not define technical details such as the type of technology implemented in the system. A sample business requirement might look like “The system must track all the employees by their respective department, region, and the designation.” This requirement shows no such detail as to how the system will implement this requirement, but rather what the system must do concerning the business.\n",
    "\n",
    "During this phase, business requirements are captured and any potential risks are identified. This step can include a feasibility study, which defines all fortes and weak points of the project to assess the overall project viability.\n",
    "\n",
    "The planning phase will determine project goals and establish a high-level plan for the intended project. Planning is, by definition, a fundamental and critical organizational phase. \n",
    "\n",
    "The three primary activities involved in the planning phase are as follows:\n",
    "\n",
    "- Identification of the system for development\n",
    "- Feasibility assessment\n",
    "- Creation of project plan\n",
    "\n",
    "The main output of this phase is the detailed project plan, which will explain the business requirements, highlight important milestones, identify milestone dates and determine the acceptance criteria that will be used to approve the system for deployment into production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Solution Architecture\n",
    "\n",
    "> In the solution architecture phase, the desired features and operations of the system are identified. This phase includes business rules, pseudo-code, screen layouts, and other necessary documentation. \n",
    "\n",
    "During theis phase, the enterprise technical architects (who are have the ultimate responsibility for delivering this step successfully) along with senior data engineers start the high-level design of the software and systems required to be able to deliver each business requirement.The technical details of the design is discussed with the stakeholders and various parameters such as risks, technologies to be used, capability of the team, project constraints, time and budget are reviewed and then the best design approach is selected for the product. \n",
    "\n",
    "The high-level design also defines all the components that needs to be developed, communications with third party services, user flows and stakeholder communications as well as front-end representations and behaviour of each components. The design is usually kept in the Design Specification Document (DSD). Among other things, the project team will mull over the core components, structure, processing, and procedures for the system to reach the stated goal.\n",
    "\n",
    "The two primary activities involved in the design phase are as follows:\n",
    "- Designing the IT infrastructure\n",
    "- Designing the system model\n",
    "\n",
    "\n",
    "The IT infrastructure should have solid foundations to avoid any crash, malfunction, or reduction in performance.  In this phase, the specialist recommends the clients and servers required on a cost and time basis and the system’s technical feasibility. The organization also creates user interaction interfaces, data models, and entity relationship diagrams (ERDs) in this phase.\n",
    "\n",
    "\n",
    "Successful completion of the solution architecture phase should comprise:\n",
    "- Transformation of all requirements into detailed specifications covering all aspects of the\n",
    "system\n",
    "- Assessment and planning for security risks\n",
    "- Approval to progress to the actual system development and deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deploying Data Stores\n",
    "\n",
    "> In this phase, actual system development and deployment of the data infrastructure begins.  In big data projects, we need to first establish a landing zone that'll be used in the next phase to ingest raw data.\n",
    "\n",
    "One of the first components that need preperation and deployment in a big data ecosystem is a central data repository to capture and store all incoming raw data.  This is often called a data lake. Contrary to other types of common software development projects, data engineering projects must start with capturing and storing the data which will be used throughout the various lifecycle phases.  \n",
    "\n",
    "Using the technical details specified in the solution architecture phase, the necessary data storage infrastructure will be prepared and deployed. Furthermore, the required security rules, approvals, API's and any other pre-requisite activity are performed in this step.\n",
    "\n",
    "Depending on the nature of incoming data, the appropriate data storage technology will be prepared.  As mentioned in the previous lesson, the most common types of industry data stores include:\n",
    "\n",
    "- Hadoop HDFS\n",
    "- Cloud storage (such as S3)\n",
    "- SQL databases and enterprise datawarehouses (EDW)\n",
    "- NoSQL data stores (such as HBase, MongoDB and Cassandra)\n",
    "\n",
    "Another important aspect of this activity is identifying and implementing all the necessary integration to prepare the environment for the data ingestion step which comes next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Ingestion\n",
    "\n",
    "> Within the context of big data, data ingestion involves getting data out of source systems and ingesting it into a data lake.\n",
    "\n",
    "Data ingestion is the transportation of raw data from assorted sources to a storage medium where it can be persisted, accessed, used, and analyzed by an organization. The destination is typically a data lake, datawarehouse, data mart, database, or a NoSQL document store.\n",
    "\n",
    "In this step, the following information is identified:\n",
    "- Each individual data source\n",
    "- The size of the generated data file/record\n",
    "- The frequency of data generation and whether its in batch or real-time\n",
    "- The data format that will be ingested\n",
    "\n",
    "Accordingly, an appropraite data pipeline needs to be created to be able to ingest data from each data source and move that data into the landing zone.  It's common to use Kafka and Flume for these types of data movement operations.\n",
    "\n",
    "It's important to note here that the ingested data is usually raw data, exactly in the same form and shape that it was produced in.  For instance, if a mobile application sends raw data in a JSON file, the exact file will be moved via the data pipeline and into a folder in the data lake.  Sometimes very minor updates are performed on the data while it's in-motion before arriving in the landing zone, but this is not common.\n",
    "\n",
    "Nonetheless, what could occur as part of this step is a very high-level data quality and sanity check _after_ the data lands in the big data lake. This can be as simple as counting the number of files arriving, checking for filename extensions etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ETL/ELT\n",
    "\n",
    "> ETL/ELT is the process of transforming and mapping data from raw data form into another format to make it more appropriate for a variety of downstream purposes such as analytics\n",
    "\n",
    "Extract, transform and load (ETL) was the traditional approach used in relational database systems for decades.  In this process, data is extracted from source systems, transformed using the appropriate schema required by the database, and then loaded into database or a datawherehouse tables where it'll be used for analytical purposes.\n",
    "\n",
    "In the modern big data world, this approach has been somewhat modified due to the evolving nature of the incomming data.  The approach currently is to:\n",
    "- Extract all required data from source systems (or other data stores if required)\n",
    "- Move and load all that raw data into a central repository such a big data lake\n",
    "- Perform the required transformation and cleaning tasks\n",
    "\n",
    "Accordingly, this new approach is now called extract, load then transform, otherwise known as ELT. Additionally, big data processing for many companies has moved to the cloud. This means that rather than housing and storing data in-house, transformed data as well as real-time streaming data can all be pushed to the cloud. This allows companies to have flexibility, agility, simplification of operation, better reliability, and security.\n",
    "\n",
    "This phase is the main activity whereby the majority of a data engineer's work occurs on a daily base.  Depending on the platforms and technologies used, code will be constantly created and tested to perform the varoius transformation and cleaning tasks necessary to prepare the data for consumption by stakeholders such as data scientists and other downstream systems. It's quite common to have several iterations of code creation and testing before moving on to the next step of the lifecycle.\n",
    "\n",
    "Some of the main tasks in this phase include:\n",
    "- Performing detailed data quality checks on the raw data to ensure it meets requirements.\n",
    "- Integrating the various types of raw data which arrived from differnt sources into a standardized format such as Parquet.\n",
    "- Enriching the data with external data sources to increase its business value.\n",
    "- Performing data cleaning tasks such as removing duplicates and handling missing or null values.\n",
    "- Applying a data model to the cleaned and transformed data (if required).\n",
    "- Performing detailed quality engineering assessments such as integration testing, unit testing, and regression testing.\n",
    "- Testing the performance of the solution in lower environments (such as the development and user acceptance testing enviornments) before promoting it to run in actual production environments\n",
    "- Ensuring stakeholders have access to the required data and that it meets their requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Solution Deployment\n",
    "\n",
    "> During this step, the developed and tested solution is promoted to the production enviornment, where it will run on real data.\n",
    "\n",
    "After thorough testing is performed, the next step of the process is to begin actual use of the newly developed solution. This involves adding all necessary files, dependencies, integrations, API's etc. in the actual production server environment so that it'll be available for stakeholders to be able to use it.\n",
    "\n",
    "There are a number of steps that are normally followed in order to deploy a new solution to the production envionrment:\n",
    "- The solution must have already been thoroughly tested.\n",
    "- Business has provided an approval that the solution meets the initial requirements as outlined in the BRD.\n",
    "- Technical infrastructure team has performed an assessment and provided an approval to deploy the system and that it won't negatively impact existing systems.\n",
    "\n",
    "Sometimes, once the stakeholders start using the system, they have feedback regarding some of the features or they request new features.  These new requests can't be immediately made on the production envionrment directly.  Rather, they need to be documented, approved and added to something called a __change request__. Change requests are requests for updates or changes that are to be applied to a system or code already running in production. \n",
    "\n",
    "It should be noted that, in the past, solution deployment was generally handled by a seperate team called the Deployment team.  Nowadays however, most organizations use continuous integration and continuous development (CI/CD) software which automates a large part of the deployment process.  CI/CD is used by the data engineers directly to promote code through the various environment levels in the organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. System Monitoring and Performance Tuning\n",
    "\n",
    "> After a new solution is deployed, the system is continuously monitored to assess its performance, impact and behavior.\n",
    "\n",
    "Once a version of the software is released to production, there is usually a maintenance team that look after any post-production issues. Oftentimes, that team will be the same team that originally developed the solution, as they are the ones familiar with the majority of the details and how the code behaves.\n",
    "\n",
    "If, for any reason, an issue is encountered in production environments, the development team is informed and depending on how severe the issue is, it might either require a hot-fix which is created and shipped in a short period of time or, if not very severe, it can wait until the next version of the software/system to be deployed.  This process is tracked by a ticketing system software (such as JIRA) where the bug will be captured, details regarding the error are provided (along with any screenshots or log files) and the impact severity determined. Any necessary enhancements, corrections, and changes are made during this phase to ensure the system continues to work and remains updated to meet business goals. It's necessary to maintain and upgrade the system from time to time to adapt to future needs. \n",
    "\n",
    "The three primary activities involved in the system monitoring phase are as follows:\n",
    "- Support the system users\n",
    "- System maintenance\n",
    "- System changes and adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineer vs. Data Scientist vs Data Analysts\n",
    "\n",
    "> Data engineers design and build pipelines, systems and frameworks that ingest, transport, transform and clean raw data into useful information.\n",
    "<p></p>\n",
    "\n",
    "> Data scientists are specialists who leverage the systems put in place by the Data Engineers and the data stored within those system to build, train and deploy predictive models\n",
    "<p></p>\n",
    "\n",
    "> Data analysts are less technical and more business-facing professionals.  They explore, interact and analyze data to gain insights and present the findings to business leaders/executives with the goal of improved corporate decision making.\n",
    "<p></p>\n",
    "\n",
    "Many of us don't have a clear picture on how the data engineer, data science and data analyst roles differ from one another.  One of the main sources of this confusion is the fact that it's there really isn't a universally accepted clear-cut definition for each role.  Oftentimes, we would read job descriptions titled data engineers and find data analyst responsibilites within it.  Similarly, we can find data science job descriptions that have data engineering tasks.  Another source of this lack of clarity is that the skillsets required for these roles overlap with one another.  Below is a diagram that gives a high-level view of how the skills can overlap:\n",
    " \n",
    "<p align=\"center\">\n",
    "  <img src=\"images/eng-sci-analyst2.png\" width=600>\n",
    "</p>\n",
    "\n",
    "There is no denying that all 3 roles are quickly gaining popularity and that they are in high demand. For instance, for data engineering roles, they are witnessing a rapid increase in job postings.  One of the main reasons for that is the exponential growth in data. According to one report by DICE, it was the fastest growing role in 2019 with a 50% YOY growth.\n",
    "\n",
    "To highlight the differences in more detail, we can view the data engineer as the \"back-end\" data professional who does the heavy-lifting of data ingestion, transformation and cleaning.  This helps prepare raw data into more useful information.\n",
    "\n",
    "The useful information will then be used by:\n",
    "- Business Intelligence and Analytics teams\n",
    "- Data Science Teams\n",
    "- Data Analysts\n",
    "\n",
    "Based on this understanding, data scientists are next in the sequence of data flow steps.  They are experts in statistics and modelling, and they leverage the data and enviornment already prepared by the data engineers.  Data analysts (sometimes also called business analysts) can either use the output of data science teams and present them to business, or leverage the data provided by the data engineering teams to perform their own analytical work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills to become successful in Data Engineering\n",
    "\n",
    "> Merging of some of the Software Development skillsets with Database skillsets led to the introduction of the Data Engineer.\n",
    "\n",
    "Now that we've discussed some of the differences between a data engineer, data scientist and data analyst, we'll take a look at the skills that companies need engineers to have in order to be able to join their teams and contribute to their data frameworks.\n",
    "\n",
    "In the past, companies required database experts as their main data engineers. Database experts have been around as a professional for a few decades. These roles starting being introduced during the late 70’s and early 80’s when database systems started becomming a more mainstream technology.  Some of the roles we might have seen are similar to:\n",
    "\n",
    "- Database Administrator\n",
    "- SQL Developer\n",
    "- Data Modeler\n",
    "\n",
    "Eventually, in the 90’s and early 2000’s, two major technology changes occurred:\n",
    "\n",
    "- Internet\n",
    "- Object Oriented Programming\n",
    "\n",
    "Starting around 2010 and afterwards, we started hearing terms like:\n",
    "- Big Data (Hadoop, Spark)\n",
    "- Batch Data Processing\n",
    "- Real Time Data Processing\n",
    "- NoSQL\n",
    "\n",
    "Hence, a Big Data Engineer can be viewed as the evolution of the previous role of a Database Developer with a more diverse skillet which ideally include the following:\n",
    "\n",
    "- Coding (Python, Java, Scala..)\n",
    "- Automation and Scripting (Linux)\n",
    "- Relational Database Systems (SQL)\n",
    "- Non-Relational Database Systems \n",
    "- ETL \n",
    "- System Architecture\n",
    "- Cloud Computing (AWS, Microsoft…)\n",
    "- Big Data Frameworks (Hadoop, Spark…)\n",
    "- Agile Project Management (Scrum)\n",
    "- DevOps, MLOps\n",
    "- Data Visualization/Presentation (Tableau)\n",
    "\n",
    "Below is a diagram showing the important skills that data engineer job descriptions are looking for:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/skills.png\" width=600>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- Data engeering is an imortant pre-requisite for data science and artificial intelligence activities in enterprise organizations as it creates the required data infrstracture needed for predictive model development, testing and deployment.\n",
    "- The role of the data engineer, although sometimes overlapping with that of the data scientist and data analyst, is actually somewhat unique.  The key differences are that a data engineer is expected to perform the bulk of the work on the back-end system data using software development and data transformation expertise, while the data scientist is a user of the systems that data engineers create and a data analyst is more business-facing.\n",
    "- The data engineering lifecycle consists of 7 steps, and it's an important process that enterprise companies follow in order to create and deploy data solutions to the production enviornment. \n",
    "- The expertise required to become successful as a data engineer is vast and diverse, and covers several subject matter areas with the top being software development, database, cloud and big data.\n",
    "- The role of the data engineer evolved as a hybrid between the role of a database professional and that of a software developer. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
