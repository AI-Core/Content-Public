{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache HBase - Installation\n",
    "\n",
    "> It's highly recommended using a Linux system as HBase is known to cause issues when used with Windows\n",
    "\n",
    "Now that we have a high-level understanding of what HBase is, let's download and install HBase to get a better feel of how it operates. \n",
    "\n",
    "At a high-level, to get HBase working on your system, the steps involve:\n",
    "\n",
    "- Downloading and installing Hadoop (Hadoop's file system is required for HBase in pseudo-distributed mode, but is not required if HBase is run in standalone mode)\n",
    "    -   _Note: Refer to the Hadoop notebook for detailed instructions on how to implement this step._\n",
    "    <p></p>\n",
    "    \n",
    "- Configuring the following files:\n",
    "    -   `bashrc`\n",
    "    -   `hadoop-env.sh`\n",
    "    -   `core-site.xml`\n",
    "    -   `hdfs-site.xml`\n",
    "    -   `mapred-site-xml`\n",
    "    -   `yarn-site.xml`\n",
    "    <p></p>\n",
    "\n",
    "- Downloading and installing HBase in pseudo-distributed mode (to leverage HDFS for data storage)\n",
    "    <p></p>\n",
    "    \n",
    "- Configuring the following HBase files:\n",
    "    -   `hbase-env.sh`\n",
    "    -   `hbase-site.xml`\n",
    "\n",
    "_Note: We'll be using Linux (Ubuntu) for this tutorial, so if you're on a different operating system the steps might differ._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin:\n",
    "\n",
    "#### 1. Open your terminal and run the following commands to update all existing applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt update\n",
    "sudo apt -y upgrade\n",
    "# sudo reboot # if you run into issues, try restarting, which is what this command will do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ensure that __Java__ is installed on your system. \n",
    "\n",
    "_Note: It is highly recommended to use `Java 8` as this is the version fully compatible with both Hadoop and HBase. Otherwise, we may face some errors._\n",
    "\n",
    "To do this, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Java version\n",
    "java -version\n",
    "\n",
    "# Check the Java copmiler version\n",
    "javac -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Java is installed correctly, you'll see an output showing the Java and compiler version. Otherwise, you will need to download and install Java and the Java compiler. \n",
    "\n",
    "For detailed steps on how to do this, please refer to the Hadoop notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Next, we need to ensure that the `JAVA_HOME` variable is correctly set up on your system.  To do that, run the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the variable is correctly set up, you should see a path show up similar to the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME\n",
    "\n",
    "# Expected output should be similar to:\n",
    "/usr/lib/jvm/java-8-openjdk-amd64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get no output, or if the output is simply repeating JAVA_HOME, then the variable is not set up correctly.\n",
    "\n",
    "To fix this, refer to the Hadoop notebook for detailed steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ensure Hadoop is already installed as our next objective is to download and setup HBase.  \n",
    "\n",
    "> Remember that we'll be using version 1.7.1 as it is compatible with both Hadoop and Java 8.\n",
    "\n",
    "Recall that HBase is composed of 3 components:\n",
    "-   HMaster (coordinating the region server and admin functions)\n",
    "-   Region Server (maps the region to the server)\n",
    "-   Zookeeper (coordinates with Hadoop)\n",
    "\n",
    "We need to see all of these 3 components correctly running to be able to use HBase.\n",
    "\n",
    "Let's start by downloading the HBase installation files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Version variable\n",
    "VERSION=\"2.4.10\"\t\n",
    "# Download the HBase version\n",
    "wget https://dlcdn.apache.org/hbase/${VERSION}/hbase-${VERSION}-bin.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Extract the downloaded archive and move it to `/usr/local/HBase` (create that folder if it's not already there):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar -xzvf hbase-${VERSION}-bin.tar.gz\n",
    "sudo mv hbase-${VERSION}-bin/ /usr/local/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. We need to set your `HBASE_HOME` variable similar to what we did with `JAVA_HOME`.  To do this, copy the path of your HBase folder and open the `bashrc` file to add the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the below lines (use your HBase folder if the path is different):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export HBASE_HOME=/usr/local/hbase-2.4.10\n",
    "export PATH=$PATH:$HBASE_HOME/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Next, we need to update the HBase environment configuration file, which contains the configurable parameters for the HBase environment.  \n",
    "\n",
    "For running HBase in pseudo-distributed mode, we need to set 3 properties within this file:\n",
    "-   `JAVA_HOME`\n",
    "-   `HBASE_MANAGES_ZK`\n",
    "-   `HBASE_REGIONSERVERS`\n",
    "\n",
    "The file is located in the `conf` folder within the location which we unpacked HBase into, which should be `/usr/local/HBase-2.4.10/`.\n",
    "\n",
    "Go ahead and open the `hbase-env.sh` file and uncomment/add the below settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano hbase-env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add/uncomment the below settings\n",
    "export HBASE_MANAGES_ZK=true\n",
    "export HBASE_REGIONSERVERS=/usr/local/hbase-2.4.10/conf/regionservers\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Before the next step, we should create a Zookeeper data folder first. Place a nested folder called `data/zookeeper` inside the HBase folder (not inside the `conf` folder).\n",
    "\n",
    "The folder structure should look like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-data-zookeeper.png\" width=400>\n",
    "  <figcaption align=\"center\"><cite>Zookeeper Folder Structure</cite></figcaption>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 9. Then, we will need to update another file inside the same `conf` folder called the `hbase-site.xml` file.  \n",
    "Add the following between the `<configuration>` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<property>\n",
    "\t<name>hbase.cluster.distributed</name>\n",
    "\t<value>true</value>\n",
    "</property>\n",
    "<property>\n",
    "\t<name>hbase.tmp.dir</name>\n",
    "\t<value>./tmp</value>\n",
    "</property>\n",
    "<property>\n",
    "\t<name>hbase.unsafe.stream.capability.enforce</name>\n",
    "\t<value>false</value>\n",
    "</property>\n",
    "<property>\n",
    "  \t<name>hbase.rootdir</name>\n",
    "  \t<value>hdfs://localhost:9000/hbase</value>\n",
    "</property>\n",
    "<property>\n",
    "  \t<name>hbase.zookeeper.property.dataDir</name>\n",
    "  \t<value>/usr/local/hbase-2.4.10/data/zookeeper</value>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hbase.zookeeper.quorum</name>\n",
    "    <value>localhost</value>\n",
    "</property>\n",
    "<property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hbase.zookeeper.property.clientPort</name>\n",
    "    <value>2181</value>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is what each of these parameters do:\n",
    "\n",
    "- `hbase.cluster.distributed`\n",
    "    -   This parameter tells HBase to run in a stand-alone local mode or on a distributed cluster via Hadoop.\n",
    "\n",
    "- `hbase.tmp.dir`\n",
    "    -   This is the HDFS temporary data storage folder\n",
    "\n",
    "- `hbase.unsafe.stream.capability.enforce`\n",
    "    -   Controls whether or not HBase will check for stream capabilities\n",
    "    -   This is used for toggling on or off advanced data flushing by HBase using something called Hflush and Hsync which help guarantee data durability. See more [here](https://hadoop-hbase.blogspot.com/2012/05/hbase-hdfs-and-durable-sync.html)\n",
    "\n",
    "- `hbase.rootdir`\n",
    "    -   Specifies the root HDFS folder location\n",
    "\n",
    "- `hbase.zookeeper.property.dataDir`\n",
    "    -   Tells Zookeeper where to store its data files\n",
    "    \n",
    "- `hbase.zookeeper.quorum`\n",
    "    -   This is the list of one or more server nodes that are available for clients requests.  \n",
    "\n",
    "- `dfs.replication`\n",
    "    -   The replication factor for HDFS data (this should match the Hadoop settings we configured earlier)\n",
    "    \n",
    "- `hbase.zookeeper.property.clientPort`\n",
    "    -   Tells Zookeeper which port it should use for communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Start all the Hadoop daemons first by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It hasn't been specified to add Hadoop home yet\n",
    "\n",
    "cd $HADOOP_HOME/sbin\n",
    "bash start-dfs.sh\n",
    "bash start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything runs smoothly, you should see output similar to the below:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/start-dfs-yarn.png\" width=600>\n",
    "  <figcaption align=\"center\"><cite>Hadoop Daemons</cite></figcaption>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks good, we'll stop the services and grant the Hadoop user access to Hbase. Run the below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all running Hadoop processes\n",
    "stop-all.sh\n",
    "\n",
    "# Change the directory to your HADOOP_HOME \n",
    "cd $HADOOP_HOME\n",
    "\n",
    "\n",
    "# Do we need to specify adding HBase and Hadoop to their own user? This may just confuse users. \n",
    "# Change the owner of the hadoop directory from root to the Hadoop account\n",
    "sudo chown -R hadoop:root hadoop \n",
    "\n",
    "# Change the access permission of the hadoop directory to allow read and execute access to all users and write access for the new account owner\n",
    "sudo chmod -R 755 hadoop\n",
    "\n",
    "\n",
    "# Change the owner of the HBase directory from root to the Hadoop account\n",
    "sudo chown -R hadoop:root Hbase\n",
    "\n",
    "# Change the access permission of the HBase directory to allow read and execute access to all users and write access for the new account owner\n",
    "sudo chmod -R 755 Hbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Test HDFS to make sure everything is working smoothly.\n",
    "\n",
    "To do this, we'll create a `test` directory using the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -mkdir /test\n",
    "hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: The Hadoop file system (HDFS) is _not_ the same as the local file system. In reality, HDFS will be hosted on multiple servers across a distributed network._\n",
    "\n",
    "The output should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -ls /\n",
    "\n",
    "# Expected output\n",
    "2021-12-22 12:27:34,309 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 1 items\n",
    "drwxr-xr-x   - hadoop supergroup      \t0 2021-12-22 12:26 /test\n",
    "hadoop@dodz-vm:/usr/local/hadoop/hadoop-3.3.1/etc/hadoop$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Next, we'll start HBase by running the `start-hbase.sh` script. This starts the 3 components we mentioned earlier: HMaster, the region server, and Zookeeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash start-hbase.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If `permission denied` error shows up, we may need to grant the Hadoop user access to the HBase folders.  To do this, run the below commands (using your corresponding HBase and Zookeeper paths)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sudo chmod -R 755 /usr/local/hbase/*\n",
    "\n",
    "sudo chown -R hadoop:hadoop /usr/local/hbase/\n",
    "\n",
    "sudo chmod -R 755 /usr/lib/data/zookeeper/*\n",
    "\n",
    "sudo chown -R hadoop:hadoop /usr/lib/data/zookeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure all the proper HBase processes are running, run the below Linux command which shows the status of all active Java processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output should include all of the below processes (3 for HBase and 5 for Hadoop plus jps itself):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps\n",
    "\n",
    "# Expected output\n",
    "9298 HMaster\n",
    "5652 SecondaryNameNode\n",
    "5286 NameNode\n",
    "9238 HQuorumPeer\n",
    "9399 HRegionServer\n",
    "5784 ResourceManager\n",
    "6684 DataNode\n",
    "5918 NodeManager\n",
    "9486 Jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the required processes run, we now need to run the HBase shell to ensure that we can start interacting with HBase.\n",
    "\n",
    "To do this, run the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be inside the HBase shell as we can see below:  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-shell.png\" width=600>\n",
    "  <figcaption align=\"center\"><cite>HBase Shell</cite></figcaption>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are inside HBase and can begin to use its commands.\n",
    "\n",
    "Try to run the `status` command to ensure HBase is working successfully.  This command shows the list of active HBase servers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase(main):001:0> status\n",
    "\n",
    "#Expected output\n",
    "1 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load\n",
    "\n",
    "hbase(main):002:0>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error that mentions HMaster is not running, double-check the `/etc/hosts` file to ensure the VM and the localhost both have the same IP (127.0.0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano /etc/hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We've successfully set up HBase on your local machine. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- HBase can be installed in different modes including stand-alone (on a local machine), pseudo-distributed (using Hadoop as the underlying data store) and fully distributed (across a corporate cluster).\n",
    "- To download and install HBase in pseudo-distributed mode, we'll need to have a compatible Java and Hadoop version installed beforehand. Using a Linux operating system is highly recommended.\n",
    "- To run HBase in pseudo-distrubted mode, we also need to download and install Hadoop as HBase uses HDFS to store the data\n",
    "- We need to ensure that JAVA_HOME, HADOOP_HOME and HBASE_HOME are properly set up in the `.bashrc` file\n",
    "- HBase has 2 configuration files that must be properly set up: `hbase-env.sh` and `hbase-site.xml`\n",
    "- All Hadoop and HBase daemons must be initiated before we can open the HBase shell which enables us to type commands\n",
    "- To start the HBase shell, type `hbase shell` from the terminal\n",
    "- Once inside the shell, the `status` command can be used to provide information about the HBase cluster and to see if it is running properly\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
