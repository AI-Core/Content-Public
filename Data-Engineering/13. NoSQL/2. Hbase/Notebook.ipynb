{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoSQL - HBase\n",
    "\n",
    "## What is HBase?\n",
    "\n",
    "> HBase is an open-source, column-oriented distributed data store that runs typically in a Hadoop environment. \n",
    "\n",
    "Although it can handle structured data, Hbase is designed mainly to store semi-structured and unstructured data types that a traditional relational databases can't.\n",
    "\n",
    "HBase can store and interact with massive amounts of data (terabytes to petabytes) which are stored in _table_ structures. The tables present in HBase can consist of billions of rows having millions of columns. HBase is built for low latency operations, which provides benefits compared to traditional relational models.\n",
    "\n",
    "For an introductory video on HBase, check out this Huwaei lecture:\n",
    "- [Introduction to HBase](https://www.youtube.com/embed/VUkPIT97J9A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## HBase Architecture\n",
    "\n",
    "> HBase can run independently in a stand-alone mode, or in a distributed environment running on top of Hadoop's HDFS (in a pseudo-distributed or fully distributed mode).\n",
    "\n",
    "Hbase's flexible architecture allows the tool to be installed in 3 different modes:\n",
    "### Stand-alone mode \n",
    "  - This is mainly used for testing and proof of concept purposes\n",
    "  - Data will be stored on the local disk storage\n",
    "\n",
    "\n",
    "### Fully-distributed mode  \n",
    "  - In this mode, the 3 HBase components (which we'll explain shortly) run on seperate computer nodes\n",
    "  - In global companies using large-scale production environments, HBase is normally integrated with Hadoop to leverage HDFS as the back-end storage repository\n",
    "  - This enables massive scaling and strong fault-tolerance\n",
    "\n",
    "### Pseudo-distributed mode\n",
    "  - In this mode, the 3 HBase components run as seperate processes but on a _single_ machine/node\n",
    "  - Hadoop's HDFS will be a seperate cluster network to be able to scale up and down as required\n",
    "  - This mode is normally used in smaller organisations with less intensive data needs\n",
    "\n",
    "Although HBase can run on top of different storage systems like Amazon S3, the reason HDFS is popular to use with HBase it due to its low cost, fault tolerance and scalability. \n",
    "\n",
    "\n",
    "The main storage entity in Hbase is a _table_, which consist of rows and columns. The intersection of a row and column is called a _cell_, which stores data. Tables are sorted by the row. Table schemas are defined using something called a _column family_, whereby each column family can have any number of columns associated with it. Each column is a collection of _key value_ pairs.\n",
    "\n",
    "Below is a visual representation of a typical HBase table:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-column-family.png\" width=600>\n",
    "</p>\n",
    "\n",
    "Notice the flexible schema structure allows rows to have varying number of columns, unlike relational databases don't allow. Moreover, columns don't always have to be in the exact same order nor contain the exact same data. For example, for row `101`, the first columns is the `email`, while on the other hand, the first column for `104` is `name`.\n",
    "\n",
    " _Regions_ are machines that store the actual data. The data stored on a region consists of all the rows between the start key and the end key which are assigned to that region. In practice, the size of regions is usually between 5GB to 20GB. \n",
    " \n",
    " _Region servers_ are the machines which store the information about the data hosted in the various regions under its supervision and coordinate reads/writes. One region server is usually resonsible for many regions. As a best practice, the number of regions per region server should be between 20 and 200 (although increasing them above 200 is possible). See [here](https://www.cloudaeon.co.uk/regions-in-hbase.html) for more details.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-architecture.png\" width=600>\n",
    "</p>\n",
    "\n",
    "Overall, in HBase:\n",
    "- Regions - tables are split into regions, with each region storing a \"range\" of rows. They usually store the data in HDFS.\n",
    "- Region server - this server communicates with the user of the system and oversees a group of regions. It coordinates all read/write data related requests to the regions under its command.\n",
    "- Table - is a collection of rows\n",
    "- Row - is all of the key-value pairs for a record which may be spread across a collection of column families\n",
    "- Column - is a collection of the same key-value pairs for different rows\n",
    "- Column family - is a collection of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## HBase Components\n",
    "\n",
    "> HBase consists of 3 main components: HMaster, the Region Server and Zookeeper\n",
    "\n",
    "_Note: It should be noted that for most daily tasks, data engineers don't have to worry about directly dealing with the various HBase components described below, as most of these are abstracted away by HBase._\n",
    "\n",
    "### 1. HMaster\n",
    "\n",
    "HMaster represents the master server in Hbase. Mainly, the master handles task assignment, network load balancing and cluster operations. To be more specific, the main responsibilities of the master include:\n",
    "\n",
    "-   Assigning regions to the region servers with help from Apache ZooKeeper\n",
    "-   Handling load balancing of the regions across region servers. It unloads the busy servers and shifts the regions to less occupied servers.\n",
    "-   Being responsible for schema changes and other metadata operations such as creation of tables and column families\n",
    "\n",
    "For information related to metadata and for performing any schema changes, the client contacts the _HMaster_\n",
    "\n",
    "### 2. Region Server\n",
    "\n",
    "HBase tables are divided horizontally into regions which contain groups of row key. Regions are simply Hbase tables split up and spread across a distributed network called region servers. This split improves performance and data reliability. Region servers run on top of Hadoop's HDFS data nodes. They are essentially the worker nodes which handle read, write, update and delete requests from the various clients.\n",
    "\n",
    "The region servers have regions under their control that:\n",
    "-   Communicate with the client and handle data-related operations\n",
    "-   Handle read and write requests\n",
    "-   Decide the size of the region by following the region size thresholds\n",
    "\n",
    "For read and write operations, the client will communicate directly with the region server, which will then coordinate the reads and writes\n",
    "\n",
    "### 3. Zookeeper\n",
    "\n",
    "Zookeeper is an open-source Apache project that provides services like maintaining configuration information, server/host naming, and providing distributed synchronization. It allows HBase to communicate with other data storage platforms such as AWS S3 or HDFS by acting as a distributed coordination service that can integrate various tools together.\n",
    "\n",
    "Some of the main tasks include:\n",
    "-   Discovering available servers\n",
    "-   Tracking server failures and repairing failed nodes\n",
    "-   Remembering what is stored in which network partition\n",
    "-   Enabling communication between clients and region servers\n",
    "\n",
    "HBase itself will take care of zookeeper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HBase data redundancy\n",
    "\n",
    "> To help ensure that the stored data is not lost if the node storing it crashes, most NoSQL tools store replicas of the same data\n",
    "\n",
    "The vast majority of modern big data tools, such as Hadoop and NoSQL data stores, replicate data on seperate, physically isolated computer nodes, perhaps in different regions to ensure resiliancy to system failures. The cost of this is that you have to pay for storing redundant data which exists elsewhere.\n",
    "\n",
    "It is standard to store 3 replicas of the same data to ensure data durability of a data system. For instance, Hadoop's HDFS is configured to have triple replication by default. The number of replicas can also be modified using the various parameters available in configuration files. In HBase, data replication occurs at the column family granularity. It is also possible to replicate entire HBase clusters, not just specific tables. For a more detailed explanation on HBase replication, [check this link](https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_bdr_hbase_replication.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HBase Features\n",
    "\n",
    "Below are the main features provided by Hbase:\n",
    "\n",
    "- HBase is built for low latency operations\n",
    "- HBase provides fast random read operations.  It does so because it uses Hash tables and indexes the data stored in HDFS.\n",
    "- HBase can store large amounts of data easily (terabytes and even petabytes) as clusters can be scaled up and down as required\n",
    "- Automatic and configurable sharding (division) of tables\n",
    "- Automatic failover supports between region servers\n",
    "- Convenient base classes available for backing Hadoop MapReduce jobs in HBase tables\n",
    "- Easy to use API for client access\n",
    "- Supports real-time querying efficiently\n",
    "\n",
    "It's also important to note what HBase is __not__:\n",
    "\n",
    "-   It's not a SQL database and doesn't store data using the relational model\n",
    "-   It's not designed for Online Transaction Processing (OLTP)\n",
    "-   It doesn't provide typical database features like ACID (atomicity, consistency, isolation and durability) or data normalization\n",
    "-   It's not designed to be used with small datasets - that would be overkill\n",
    "-   Data is referenced _only_ using the row key, like in a key-value data store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and install HBase\n",
    "\n",
    "> It's highly recommended to use a Linux system as HBase is known to cause issues when used with Windows\n",
    "\n",
    "Now that we have a high-level understanding of what HBase is, let's download and install HBase to get a better feel of how it operates. \n",
    "\n",
    "At a high-level, to get HBase working on your system, the steps involve:\n",
    "\n",
    "- Downloading and installing Hadoop (Hadoop's file system is required for HBase in pseudo-distributed mode, but is not required if HBase is run in standalone mode)\n",
    "    -   _Note: Refer to the Hadoop notebook for detailed instructions on how to implement this step._\n",
    "    <p></p>\n",
    "    \n",
    "- Configuring the following files:\n",
    "    -   `bashrc`\n",
    "    -   `hadoop-env.sh`\n",
    "    -   `core-site.xml`\n",
    "    -   `hdfs-site.xml`\n",
    "    -   `mapred-site-xml`\n",
    "    -   `yarn-site.xml`\n",
    "    <p></p>\n",
    "\n",
    "- Downloading and installing HBase in pseudo-distributed mode (to leverage HDFS for data storage)\n",
    "    <p></p>\n",
    "    \n",
    "- Configuring the following HBase files:\n",
    "    -   `hbase-env.sh`\n",
    "    -   `hbase-site.xml`\n",
    "\n",
    "_Note: We'll be using Linux (Ubuntu) for this tutorial, so if you're on a different operating system the steps might differ._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin:\n",
    "\n",
    "#### 1. Open your terminal and run the following commands to update all existing applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt update\n",
    "sudo apt -y upgrade\n",
    "# sudo reboot # if you run into issues, try restarting, which is what this command will do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ensure that __Java__ is installed on your system. \n",
    "\n",
    "_Note: It is highly recommended to use `Java 8` as this is the version fully compatible with both Hadoop and HBase. Otherwise, we may face some errors._\n",
    "\n",
    "To do this, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Java version\n",
    "java -version\n",
    "\n",
    "# Check the Java copmiler version\n",
    "javac -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Java is installed correctly, you'll see an output showing the Java and compiler version. Otherwise, you will need to download and install Java and the Java compiler. \n",
    "\n",
    "For detailed steps on how to do this, please refer to the Hadoop notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Next, we need to ensure that the `JAVA_HOME` variable is correctly set up on your system.  To do that, run the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the variable is correctly set up, you should see a path show up similar to the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME\n",
    "\n",
    "# Expected output should be similar to:\n",
    "/usr/lib/jvm/java-8-openjdk-amd64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get no output, or if the output is simply repeating JAVA_HOME, then the variable is not set up correctly.\n",
    "\n",
    "To fix this, refer to the Hadoop notebook for detailed steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Now that Hadoop has been successfully installed, our next objective is to download and setup HBase.  \n",
    "\n",
    "> Remember that we'll be using version 1.7.1 as it is compatible with both Hadoop and Java 8.\n",
    "\n",
    "Recall that HBase is composed of 3 components:\n",
    "-   HMaster (coordinating the region server and admin functions)\n",
    "-   Region Server (maps the region to the server)\n",
    "-   Zookeeper (coordinates with Hadoop)\n",
    "\n",
    "We need to see all of these 3 components correctly running to be able to use HBase.\n",
    "\n",
    "Let's start by downloading the HBase installation files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Version variable\n",
    "VERSION=\"1.7.1\"\t\n",
    "# Download the HBase version\n",
    "wget https://dlcdn.apache.org/hbase/${VERSION}/hbase-${VERSION}-bin.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Extract the downloaded archive and move it to `/usr/local/HBase` (create that folder if it's not already there):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar -xzvf hbase-${VERSION}-bin.tar.gz\n",
    "sudo mv hbase-${VERSION}/ /usr/local/HBase/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. We need to set your `HBASE_HOME` variable similar to what we did with `JAVA_HOME`.  To do this, copy the path of your HBase folder and open the `bashrc` file to add the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the below lines (use your HBase folder if the path is different):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export HBASE_HOME=/usr/local/hbase/hbase-1.7.1\n",
    "export PATH=$PATH:$HBASE_HOME/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Next, we need to update the HBase environment configuration file, which contains the configurable paramters for the HBase environment.  \n",
    "\n",
    "For running HBase in pseudo-distributed mode, we need to set 3 properties within this file:\n",
    "-   `JAVA_HOME`\n",
    "-   `HBASE_MANAGES_ZK`\n",
    "-   `HBASE_REGIONSERVERS`\n",
    "\n",
    "The file is located in the `conf` folder within the location which we unpacked HBase into, which should be `/usr/local/HBase/`.\n",
    "\n",
    "Go ahead and open the `hbase-env.sh` file and uncomment/add the below settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano hbase-env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add/uncomment the below settings\n",
    "export HBASE_MANAGES_ZK=true\n",
    "export HBASE_REGIONSERVERS=/usr/local/hbase/hbase-1.7.1/conf/regionservers\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Before the next step, we should create a Zookeeper data folder first. Place a nested folder called `data/zookeeper` inside the HBase folder (not inside the `conf` folder).\n",
    "\n",
    "The folder structure should look like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-data-zookeeper.png\" width=400>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 9. Then, we will need to update another file inside the same `conf` folder called the `hbase-site.xml` file.  \n",
    "Add the following between the `<configuration>` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<property>\n",
    "\t<name>hbase.cluster.distributed</name>\n",
    "\t<value>true</value>\n",
    "</property>\n",
    "<property>\n",
    "\t<name>hbase.tmp.dir</name>\n",
    "\t<value>./tmp</value>\n",
    "</property>\n",
    "<property>\n",
    "\t<name>hbase.unsafe.stream.capability.enforce</name>\n",
    "\t<value>false</value>\n",
    "</property>\n",
    "<property>\n",
    "  \t<name>hbase.rootdir</name>\n",
    "  \t<value>hdfs://localhost:9000/hbase</value>\n",
    "</property>\n",
    "<property>\n",
    "  \t<name>hbase.zookeeper.property.dataDir</name>\n",
    "  \t<value>/usr/local/hbase/data/zookeeper</value>\n",
    "</property>\n",
    "<property>\n",
    "    \t<name>hbase.zookeeper.quorum</name>\n",
    "    \t<value>localhost</value>\n",
    "</property>\n",
    "<property>\n",
    "    \t<name>dfs.replication</name>\n",
    "    \t<value>1</value>\n",
    "</property>\n",
    "<property>\n",
    "    \t<name>hbase.zookeeper.property.clientPort</name>\n",
    "    \t<value>2181</value>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is what each of these parameters do:\n",
    "\n",
    "- `hbase.cluster.distributed`\n",
    "    -   This parameter tells HBase to run in a stand-alone local mode or on a distributed cluster via Hadoop.\n",
    "\n",
    "- `hbase.tmp.dir`\n",
    "    -   This is the HDFS temporary data storage folder\n",
    "\n",
    "- `hbase.unsafe.stream.capability.enforce`\n",
    "    -   Controls whether or not HBase will check for stream capabilities\n",
    "    -   This is used for toggling on or off advanced data flushing by HBase using something called Hflush and Hsync which help guarantee data durability. See more [here](https://hadoop-hbase.blogspot.com/2012/05/hbase-hdfs-and-durable-sync.html)\n",
    "\n",
    "- `hbase.rootdir`\n",
    "    -   Specifies the the root HDFS folder location\n",
    "\n",
    "- `hbase.zookeeper.property.dataDir`\n",
    "    -   Tells Zookeeper where to store its data files\n",
    "    \n",
    "- `hbase.zookeeper.quorum`\n",
    "    -   This is the list of one or more server nodes that are available for clients requests.  \n",
    "\n",
    "- `dfs.replication`\n",
    "    -   The replication factor for HDFS data (this should match the Hadoop settings we configured earlier)\n",
    "    \n",
    "- `hbase.zookeeper.property.clientPort`\n",
    "    -   Tells Zookeeper which port it should use for communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Start all the Hadoop daemons first by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HADOOP_HOME/sbin\n",
    "bash start-dfs.sh\n",
    "bash start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything runs smoothly, you should see output similar to the below:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/start-dfs-yarn.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks good, we'll stop the services and grant the Hadoop user access to Hbase. Run the below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all running Hadoop processes\n",
    "stop-all.sh\n",
    "\n",
    "# Change the directory to your HADOOP_HOME \n",
    "cd $HADOOP_HOME\n",
    "\n",
    "# Change the owner of the hadoop directory from root to the Hadoop account\n",
    "sudo chown -R hadoop:root hadoop \n",
    "\n",
    "# Change the access permission of the hadoop directory to allow read and execute access to all users and write access for the new account owner\n",
    "sudo chmod -R 755 hadoop\n",
    "\n",
    "\n",
    "# Change the owner of the HBase directory from root to the Hadoop account\n",
    "sudo chown -R hadoop:root Hbase\n",
    "\n",
    "# Change the access permission of the HBase directory to allow read and execute access to all users and write access for the new account owner\n",
    "sudo chmod -R 755 Hbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Test HDFS to make sure everything is working smoothly.\n",
    "\n",
    "To do this, we'll create a `test` directory using the below comand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -mkdir /test\n",
    "hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: The Hadoop file system (HDFS) is _not_ the same as the local file system. In reality, HDFS will be hosted on multiple servers across a distributed network._\n",
    "\n",
    "The output should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -ls /\n",
    "\n",
    "# Expected output\n",
    "2021-12-22 12:27:34,309 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 1 items\n",
    "drwxr-xr-x   - hadoop supergroup      \t0 2021-12-22 12:26 /test\n",
    "hadoop@dodz-vm:/usr/local/hadoop/hadoop-3.3.1/etc/hadoop$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Next, we'll start HBase by running the `start-hbase.sh` script. This starts the 3 components we mentioned earlier: HMaster, the region server, and Zookeeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash start-hbase.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If `permission denied` error shows up, we may need to grant the Hadoop user access to the HBase folders.  To do this, run the below commands (using your correspnoding HBase and Zookeeper paths)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo chmod -R 755 /usr/local/hbase/*\n",
    "\n",
    "sudo chown -R hadoop:hadoop /usr/local/hbase/\n",
    "\n",
    "sudo chmod -R 755 /usr/lib/data/zookeeper/*\n",
    "\n",
    "sudo chown -R hadoop:hadoop /usr/lib/data/zookeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure all the proper HBase processes are running, run the below Linux command which shows the status of all active Java processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output should include all of the below processes (3 for HBase and 5 for Hadoop plus jps itself):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps\n",
    "\n",
    "# Expected output\n",
    "9298 HMaster\n",
    "5652 SecondaryNameNode\n",
    "5286 NameNode\n",
    "9238 HQuorumPeer\n",
    "9399 HRegionServer\n",
    "5784 ResourceManager\n",
    "6684 DataNode\n",
    "5918 NodeManager\n",
    "9486 Jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the required processes run, we now need to run the HBase shell to ensure that we can start interacting with HBase.\n",
    "\n",
    "To do this, run the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be inside the HBase shell as we can see below:  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-shell.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are inside HBase and can begin to use it's commands.\n",
    "\n",
    "Try to run the `status` command to ensure HBase is working successfully.  This command shows the list of active HBase servers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase(main):001:0> status\n",
    "\n",
    "#Expected output\n",
    "1 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load\n",
    "\n",
    "hbase(main):002:0>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error that mentions HMaster is not running, double check the `/etc/hosts` file to ensure the VM and the localhost both have the same IP (127.0.0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano /etc/hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We've successfully set up HBase. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data into HBase\n",
    "\n",
    "#### 1. The first step is to download the Retail data CSV file\n",
    "\n",
    "You can [download it from here](https://aicore-files.s3.amazonaws.com/Data-Eng/retail.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The next step is to import the Retail data file into HBase.\n",
    "\n",
    "To do that, we need to first create a new Hbase table and specify the Column Family. To do this, type the below command from inside the `hbase shell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create 'retail_table',{NAME => 'cf'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the table was created successfully, run the `list` command to see all available HBase tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list\n",
    "\n",
    "#Expected output:\n",
    "hbase(main):002:0> list\n",
    "retail_table                                                                         \t \n",
    "1 row(s) in 0.3180 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the table is created, we need to run the below command to copy the CSV file to HDFS so we can import it into HBase:\n",
    "\n",
    "_Note: Ensure you are using your folder path where you saved the `retail.csv` file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -put /YOURPATH/retail.csv /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to check that the file has been properly copied to HDFS, type the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -ls /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -ls /data\n",
    "\n",
    "# Expected output\n",
    "22/01/26 17:20:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 1 items\n",
    "-rw-r--r--   1 hadoop supergroup   45580638 2022-01-26 17:20 /data/retail.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to load the `retail.csv` file into HBase. To do this, run the below command from the __terminal__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=',' -Dimporttsv.columns=HBASE_ROW_KEY,cf:description,cf:quantity,cf:price,cf:customer,cf:country retail_table /data/retail.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: You could of course write code to generate the column names if you have too many to write out by hand._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If you get any errors, such as \"Bad Lines\" or \"Failed Map\", check that you didn't miss any characters from the above code and attempt to type it directly yourself instead of copy and pasting it._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works smoothly, you should see output similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2022-01-07 13:34:28,910 INFO  [main] mapreduce.Job: erations=0\n",
    "   \t HDFS: Number of bytes read=756\n",
    "   \t HDFS: Number of bytes written=0\n",
    "   \t HDFS: Number of read operations=2\n",
    "   \t HDFS: Number of large read operations=0\n",
    "   \t HDFS: Number of write operations=0\n",
    "    Job Counters\n",
    "   \t Launched map tasks=1\n",
    "   \t Data-local map tasks=1\n",
    "   \t Total time spent by all maps in occupied slots (ms)=5154\n",
    "   \t Total time spent by all reduces in occupied slots (ms)=0\n",
    "   \t Total time spent by all map tasks (ms)=5154\n",
    "   \t Total vcore-milliseconds taken by all map tasks=5154\n",
    "   \t Total megabyte-milliseconds taken by all map tasks=5277696\n",
    "    Map-Reduce Framework\n",
    "   \t Map input records=15\n",
    "   \t Map output records=15\n",
    "   \t Input split bytes=104\n",
    "   \t Spilled Records=0\n",
    "   \t Failed Shuffles=0\n",
    "   \t Merged Map outputs=0\n",
    "   \t GC time elapsed (ms)=77\n",
    "   \t CPU time spent (ms)=1600\n",
    "   \t Physical memory (bytes) snapshot=183992320\n",
    "   \t Virtual memory (bytes) snapshot=1874804736\n",
    "   \t Total committed heap usage (bytes)=137953280\n",
    "    ImportTsv\n",
    "   \t Bad Lines=0\n",
    "    File Input Format Counters\n",
    "   \t Bytes Read=652\n",
    "    File Output Format Counters\n",
    "   \t Bytes Written=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Now, we need to go into the HBase shell and check that the data is correctly loaded. \n",
    "\n",
    "To do that, we'll use the `scan` command, which is similar to a SQL `SELECT`. It will scan over the entire table and retrieve the relevant data.\n",
    "\n",
    "For example, the below code will return the _first 5 rows_ of the Retail table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan 'retail_table', {'LIMIT', 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output:\n",
    "Hbase::Table - retail_table\n",
    "hbase(main):009:0> scan 'retail_table', {'LIMIT', 5}\n",
    "ROW                \tCOLUMN+CELL                                               \t \n",
    " 1                 \tcolumn=cf:country, timestamp=1643213704999, value=United Kingdo\n",
    "                   \tm                                                         \t \n",
    " 1                 \tcolumn=cf:customer, timestamp=1643213704999, value=17850  \t \n",
    " 1                 \tcolumn=cf:description, timestamp=1643213704999, value=WHITE HAN\n",
    "                   \tGING HEART T-LIGHT HOLDER                                 \t \n",
    " 1                 \tcolumn=cf:price, timestamp=1643213704999, value=2.55      \t \n",
    " 1                 \tcolumn=cf:quantity, timestamp=1643213704999, value=6      \t \n",
    " 10                \tcolumn=cf:country, timestamp=1643213704999, value=United Kingdo\n",
    "                   \tm                                                         \t \n",
    " 10                \tcolumn=cf:customer, timestamp=1643213704999, value=13047  \t \n",
    " 10                \tcolumn=cf:description, timestamp=1643213704999, value=ASSORTED\n",
    "                   \tCOLOUR BIRD ORNAMENT                                      \t \n",
    " 10                \tcolumn=cf:price, timestamp=1643213704999, value=1.69      \t \n",
    " 10                \tcolumn=cf:quantity, timestamp=1643213704999, value=32     \t \n",
    " 100               \tcolumn=cf:country, timestamp=1643213704999, value=United Kingdo\n",
    "                   \tm                                                         \t \n",
    " 100               \tcolumn=cf:customer, timestamp=1643213704999, value=14688  \t \n",
    " 100               \tcolumn=cf:description, timestamp=1643213704999, value=60 TEATIM\n",
    "                   \tE FAIRY CAKE CASES                                        \t \n",
    " 100               \tcolumn=cf:price, timestamp=1643213704999, value=0.55      \t \n",
    " 100               \tcolumn=cf:quantity, timestamp=1643213704999, value=24     \t \n",
    " 1000              \tcolumn=cf:country, timestamp=1643213704999, value=United Kingdo\n",
    "                   \tm                                                         \t \n",
    " 1000              \tcolumn=cf:customer, timestamp=1643213704999, value=14729  \t \n",
    " 1000              \tcolumn=cf:description, timestamp=1643213704999, value=TOAST ITS\n",
    "                    \t- HAPPY BIRTHDAY                                         \t \n",
    " 1000              \tcolumn=cf:price, timestamp=1643213704999, value=1.25      \t \n",
    " 1000              \tcolumn=cf:quantity, timestamp=1643213704999, value=2      \t \n",
    " 10000             \tcolumn=cf:country, timestamp=1643213704999, value=United Kingdo\n",
    "                   \tm                                                         \t \n",
    " 10000             \tcolumn=cf:customer, timestamp=1643213704999, value=13174  \t \n",
    " 10000             \tcolumn=cf:description, timestamp=1643213704999, value=SET OF 2\n",
    "                   \tTINS VINTAGE BATHROOM                                     \t \n",
    " 10000             \tcolumn=cf:price, timestamp=1643213704999, value=4.25      \t \n",
    " 10000             \tcolumn=cf:quantity, timestamp=1643213704999, value=2      \t \n",
    "5 row(s) in 0.1360 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a detailed look at how the data is displayed in HBase as it may seem confusing at first.  Unlike a relational database which stores data in a row-based manner, HBase stores the data in a __column-based__ approach. \n",
    "\n",
    "Each line in HBase represents a column value and also includes an automatic timestamp. The __Row__ is a unique Rowkey identifier that tells HBase how each of the columns are connected to each other (i.e. if they are part of the same logical row or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying data in HBase\n",
    "\n",
    "You can of course, run more complex queries.\n",
    "\n",
    "To find out how many total rows we have in the table, we can use the `count` command as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count `retail_table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look like:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-count.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do more advanced querying using filters (similar to SQL's WHERE command), we'll first need to import 3 HBase classes:\n",
    "\n",
    "- `SingleColumnValueFilter`\n",
    "- `CompareFilter`\n",
    "- `BinaryComparator`\n",
    "    \n",
    "These 3 classes work together to provide flexible filtering criteria.\n",
    "\n",
    "To achieve this, run the below 3 commands from inside the HBase shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required 3 classes \n",
    "import org.apache.hadoop.hbase.filter.SingleColumnValueFilter \n",
    "import org.apache.hadoop.hbase.filter.CompareFilter\n",
    "import org.apache.hadoop.hbase.filter.BinaryComparator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be similar to:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-filter-import.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run queries with specific filters. First, let's query the table for all data that have the `country as United Kingdom`. The query would have the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan 'retail_table', { FILTER => SingleColumnValueFilter.new(Bytes.toBytes('cf'), Bytes.toBytes('country'), CompareFilter::CompareOp.valueOf('EQUAL'),BinaryComparator.new(Bytes.toBytes('United Kingdom')))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will look something like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-scan-filter.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's run a query to check how many products have a `price equal to 12.75`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan 'retail_table', { FILTER => SingleColumnValueFilter.new(Bytes.toBytes('cf'), Bytes.toBytes('price'), CompareFilter::CompareOp.valueOf('EQUAL'),BinaryComparator.new(Bytes.toBytes('12.75')))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `826 products` as indicated by the number of rows seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-filter-price.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the combination of above filters, we can use the below comparison operators inside the `CompareFilter::CompareOp.valueOf` on column values:\n",
    "\n",
    "- `EQUAL`\n",
    "- `GREATER`\n",
    "- `GREATER_OR_EQUAL`\n",
    "- `LESS`\n",
    "- `LESS_OR_EQUAL`\n",
    "- `NOT_EQUAL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HBase commands:\n",
    "\n",
    "Below are some of the typical commands you would be using to interact with data in HBase:\n",
    "\n",
    "- `put`\n",
    "    -   This command allows you to update the data in an already existing cell.\n",
    "\n",
    "- `get`\n",
    "    -   This command are used to read data from a table in HBase. It returns a the values associated with a row of data at a time.\n",
    "\n",
    "- `delete`\n",
    "    -   This command allows you to delete a specific cell in an HBase table.\n",
    "\n",
    "- `deleteall`\n",
    "    -   This command deletes all of the cells in a table.\n",
    "\n",
    "- `scan`\n",
    "    -   This command is used to view the data stored in an HBase table.\n",
    "\n",
    "- `count`\n",
    "    -   This command is used to count the number of rows of a table.\n",
    "\n",
    "- `disable`\n",
    "    -   This command disables (turns off) a table so that it can be deleted.\n",
    "\n",
    "- `drop`\n",
    "    -   This commands deletes a disabled table.\n",
    "\n",
    "-   `truncate`\n",
    "    -   This commands does 3 things in sequence:\n",
    "        -   Disables a table\n",
    "        -   Drops a table\n",
    "        -   Recreates the table with the same name\n",
    "\n",
    "\n",
    "For a detailed explanation of HBase commands, check the following guide:\n",
    "-    [HBase Cheat Sheet](https://sparkbyexamples.com/hbase/hbase-shell-commands-cheat-sheet/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- HBase is a modern tool for storing and analyzing big data in tables.  It does so using a column-oriented approach.  This should not be confused with the row-oriented approach that traditional relational databases use.\n",
    "- The intersection of a row and column in a table is called a _cell_.  Cells store data, which in turn is accessed using a unique ID called the _row key_.\n",
    "- Related columns in HBase are grouped together into _column families_. An HBase table can have more than one column family. \n",
    "- HBase's architecture is composed of 3 main components: _HMaster_ (which acts as the master server), _Region Servers_ (which are various nodes that store tables), and _Zookeeper_ (which coordinates the various administrative tasks).\n",
    "- HBase is designed to efficiently handle unstructured and semi-structured data using low-latency operations.  The tool is easy to scale and support batch and real-time querying of data.\n",
    "- Hbase can be installed in different modes including stand-alone (on a local machine), pseudo-distributed (using Hadoop as the underlying data store) and fully distributed (across a corporate cluster).\n",
    "- To download and install HBase in pseudo-distributed mode, we'll need to have a compatible Java and Hadoop version installed beforehand. Using a Linux operating system is highly recommended.\n",
    "- Tables in HBase can be created using the `create` command.  Table querying can be done using `scan` and `get` commands, while inserting data can be done using the `put` command.\n",
    "- In order to delete an HBase table, we first need to `disable` the table and then `drop` the disabled table.  Alternatively, the `truncate` command can be used to implement all of these actions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
