{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Pipelines project: Your first data pipeline\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What?\n",
    "The main idea is that you'll use web scraping to collect data from the internet.\n",
    "You'll put this data together to create a dataset which you can use for future projects.\n",
    "\n",
    "One of the coming units consists on __Machine Learning__. You'll learn that the data is an enabling component which big machine learning systems depend on. You'll also learn that supervised learning is just finding a mathematical function which maps from input to output, and that the dataset defines these inputs and outputs. For this project, the dataset you collect should be __supervised__, that means that you will extract the __output__ and the __features__ that lead to said result.\n",
    "\n",
    "You should also use cron to schedule your Python web scraping script so that it runs automatically at intervals. For this goal, we will also see AWS EC2 instances.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Why?\n",
    "\n",
    "Creating a dataset relevant to work that you want to do in the future will give you something uniquely interesting to companies that you might apply to. As such, the data you scrape must be something appealing for industrial purposes. \n",
    "\n",
    "By creating your own dataset, you can start working on an interesting problem that might be relevant to awesome people that you can reach out to. Later in the course, we'll help introduce you to people who can give advice on the kind of problems you're working on.\n",
    "\n",
    "In addition, the webscraper you create will showcase that you are comfortable working with Python Classes, packages, and modules. It will also show that you understand advanced Python concepts such as OOP, multithreading, and that you can work with Cloud services such as AWS. \n",
    "\n",
    "What problems do you think you might you be able to tackle using AI? What interests you? What kind of data would jobs you want to apply to value your experience in working with?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deliverables\n",
    "- A Github repo containing all of the code\n",
    "- Obviously, the dataset (probably not pushed to GitHub because it will be huge)\n",
    "    - it must contain at least one numerical feature and one numerical or categorical label (so we can apply ML in the next unit)\n",
    "    - at least 1000 examples\n",
    "- A slideshow presentation explaining\n",
    "    - The different locations on the internet that your script collects data from\n",
    "    - The layout of a example webpages you scraped and how you targeted elements within them\n",
    "    - In what format you chose to store your data and why (data lake vs data warehouse, file type, database table schema)\n",
    "    - How you cleaned the data (No imputation required now)\n",
    "    - Suggestions of which variable in your dataset you will attempt to predict using some of the remaining variables\n",
    "    - Do not include screenshots of code\n",
    "    - The presentation must be that: a presentation (Don't show us your code and call it a day)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Deadline\n",
    "The project deadline is for 3 weeks from when announced.\n",
    "\n",
    "## Marking criteria\n",
    "\n",
    "Each of the bullets under the following headings will be scored as \"Not Attempted\" (0 points), \"Attempted\" (1 point) or \"Successfully Applied\" (2 points).\n",
    "\n",
    "## Readability\n",
    "- Your repository has a README.md file summarising what the project does, the motivation behind it, how to use it and what you achieved. Consider this README file as part of your CV.\n",
    "- You added comments explaining each decision and docstrings for each function/class. Don't flood the code with comments, it should have the right ammount. \n",
    "- Your repo contains all the elements necessary to run your code and does not contain deprecated files.\n",
    "\n",
    "### Programming\n",
    "- Code is object oriented with logical chunks of code within functions, and related functions encapsulated withing classes\n",
    "- It contains at least one package for utilities\n",
    "- It contains one package for tests\n",
    "- `.py` files.\n",
    "- No unnecessary loops allowed (O(n^2) is only allowed in special cases)\n",
    "\n",
    "### Data storage\n",
    "- Tabular data stored in a SQL database\n",
    "- Raw data stored in a data lake (S3), if appropriate (Take a look at the following [video to create an S3 bucket](https://www.youtube.com/watch?v=y10gLspWCYE&list=PLQuVqKqF3P6qX9A27D3miOxuNGsnUKBSJ&index=21))\n",
    "- Images, if part of the dataset, are downloaded, stored in a data lake and named by example id, as in the tabular data.\n",
    "\n",
    "## Presentation\n",
    "- Presentation is part of the repository\n",
    "- Presentation lasted between 5 and 11 minutes (Please, rehearse, no recruiter wants to hear you mumbling or saying random things during the whole interview)\n",
    "\n",
    "## Gold stars\n",
    "- Run the scraper on a remote server using AWS EC2 (or another cloud provider) so that it doesn't run on your local machine? (Don't worry, all the cloud providers give free credits).\n",
    "- Your data contains tabular data, free text & images\n",
    "- You used 2 or more sources of data in your project\n",
    "- Used Multithreading to accelerate the scraping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}