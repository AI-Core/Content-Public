{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Integrating Kafka with Apache Spark </h1>\n",
    "\n",
    "> One of the most popular use cases in industry is to connect Kafka with Apache Spark, whereby Kafka feeds data into Spark which implements data processing.\n",
    "\n",
    "In large global organizations, Kafka is not used in isolation by itself, but rather it's usually part of a larger data system that involves multiple steps and phases.\n",
    "\n",
    "As a quick reminder, Apache Spark is a fast and efficient unified data analytics engine for Big Data and machine learning.  If you haven't already done so, check out the AiCore Spark content for more details.\n",
    "\n",
    "Below is a high-level overview of the Apache Spark ecosystem:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/spark-ecosystem.png\" width=600>\n",
    "  </p>\n",
    "\n",
    "For the purpose of this lesson, we'll be focusing on __Spark Streaming__.\n",
    "\n",
    "If you are unfamiliar with Spark, pleasure ensure to review the Spark module.\n",
    "\n",
    "\n",
    "To get more detailed information about Spark, you can also read the below link\n",
    "  - __[What is Apache Spark](https://www.ibm.com/cloud/learn/apache-spark)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka and Apache Spark\n",
    "\n",
    "It is common in global companies to use both Kafka and Spark as part of a larger data processing ecosystem.\n",
    "\n",
    "Below is an example of how such a system would typically look like:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/kafka-spark.png\" width=600>\n",
    "  </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that, in such a system, we have multiple phases or steps in the data flow.  Typically, the flow would be as follows:\n",
    "\n",
    "__1. Data Streaming:__\n",
    "- This is where the raw data is normally produced. \n",
    "-  It could be either real-time data (such as machine/IoT generated data) or batch data (such as manually entered data into a system).\n",
    "\n",
    "<p></p>\n",
    "\n",
    "__2. Data Collection:__  \n",
    "- The data would then go through a Kafka system, which will be reponsible to connect to the data producer on one hand, and the data consumer (in this case Apache Spark) on the other hand.\n",
    "-  Kafka also buffers the data until its consumed by Spark.\n",
    "<p></p>\n",
    "\n",
    "__2. Data Processing__\n",
    "- In this topology, Spark (in general) or the Spark Streaming module in particular would be the data consumer, as it'll be on the recieving end of Kafka.\n",
    "- Data arriving into Spark can be analyzed and processed on the fly in real-time (batch processing is also supported).\n",
    "- Post-processed data can then be sent to a persistent storage layer where it will be kept for downstream processing.\n",
    "<p></p>\n",
    "\n",
    "__3. Data Storage__\n",
    "- The data storage layer is where the post-processed information is stored.\n",
    "- There are several options to chose from, including Hadoop HDFS, Amazon S3, NoSQL data stores such as HBase or MongoDB or other commodity hard drives among others.  It's also possible to produce the data to a seperate Kafka topic which can be connected to other downstream systems.\n",
    "- The cleaned, formatted and processed data will then be accessed by downstream systems that perform various analytics and data science work.\n",
    "<p></p>\n",
    "\n",
    "__4. Data Analysis/Visualization__\n",
    "- In this layer of the process, the prepared data is fed to front-end systems that are used for business intelligence, analytics and data science purposes.\n",
    "- Data is connected to dashboards and visualization tools which can be leveraged to create reports for top-level business executives.\n",
    "- Data Science teams use the preapred information to train and test their models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how Shopify use Kafka, Python and Spark Streaming for real-time risk management in this [video](https://databricks.com/session/realtime-risk-management-using-kafka-python-and-spark-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to implement Kafka-Spark integration\n",
    "\n",
    "Now that we have a solid understanding of Kafka, Spark and how both are used together in the real-world, it's time to roll-up our sleeves and start coding.\n",
    "\n",
    "> In this tutorial, Kafka will act as the streaming platform to produce and consumer events to the Spark framework, while Spark will be used as the data processing engine display the value of each message batch recieved from Kafka.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high-level, the steps involved to implement this excercise include:\n",
    "\n",
    "1. Setting up the Kafka environment\n",
    "2. Creating a new Kafka topic\n",
    "3. Setting up the Spark environment\n",
    "4. Create/use Spark Streaming Python code \n",
    "5. Open a Kafka producer \n",
    "6. Print the output as the data arrives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up the Kafka environment\n",
    "\n",
    "> Ensure that you've followed the detailed instructions in the Kafka module - to properly setup your Kafka environment.\n",
    "\n",
    "As usual, we'll need to start the Kafka server and zookeeper first. To do so, run the following commands in the terminal (ensure you're in the Kafka folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Kafka Server\n",
    "bash bin/kafka-server-start.sh config/server.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Zookeeper Server\n",
    "bash bin/zookeeper-server-start.sh config/zookeeper.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming this ran successfully, you should see output similar to:\n",
    "\n",
    "![](images/kafka-zookeeper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating a new Kafka topic\n",
    "\n",
    "In order to be able to use Kafka, we'll need a topic to store the data.  To create a new topic, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Kafka topic\n",
    "# Run this from your Kafka bin folder\n",
    "bash kafka-topics.sh --create --bootstrap-server localhost:9092 --topic pythontokafka --partitions 1 --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setting up the Spark environment\n",
    "\n",
    "> Make sure to have followed the steps in the Spark lessons to setup Spark locally on your machine.  If you haven't yet done so, please go to the __Spark Basics__ and __Spark Streaming__ notebooks and follow the steps there before proceeding.\n",
    "\n",
    "We'll also need to have PySpark installed to be able to run this application.  To check if its already installed, run the following command in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If PySpark is properly setup, you should see output similar to the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/spark-download-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If PySpark is not setup, you can install it using the following `Pip` command (also, please refer to the Spark Basics and Spark Streaming notebooks for full instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Spark Streaming Python code\n",
    "\n",
    "We now need to create the code to allow streaming from our Kafka topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Download spark sql kakfa package from Maven repository and submit to PySpark at runtime. \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 pyspark-shell'\n",
    "# specify the topic we want to stream data from.\n",
    "kafka_topic_name = \"pythontokafka\"\n",
    "# Specify your Kafka server to read data from.\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"KafkaStreaming \") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Only display Error messages in the console.\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Construct a streaming DataFrame that reads from topic\n",
    "stream_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "        .option(\"subscribe\", kafka_topic_name) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "# Select the value part of the kafka message and cast it to a string.\n",
    "stream_df = stream_df.selectExpr(\"CAST(value as STRING)\")\n",
    "\n",
    "# outputting the messages to the console \n",
    "stream_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PYSPARK_SUBMIT_ARGS:__\n",
    "\n",
    "Is used to submit arguments to the PySpark shell, some of the commonly used arguments are shown below:\n",
    "\n",
    "- `--packages`:\n",
    "    -   Specifies Apache Maven coordinates of jar files to include to include when running PySpark. You can find these custom packages on the Maven central repository at https://mvnrepository.com/. They are specified in the format groupId:artifactId:version.\n",
    "- `--master`:\n",
    "    -   Specifies the manager to use for the cluster.  In this example, we use local, but Spark also supports Mesos, Kubernetes, stand-alone and Yarn.\n",
    "- `--deploy-mode`:\n",
    "    -   Specifies whether to deploy the driver program on the worker nodes (cluster mode) or on the local machine (client mode)\n",
    "- `--conf`\n",
    "    -   Specifies the configuration parameters to use.  These configurations are numerous and are used to specify application configurations, shuffle parameters and other runtime configurations.\n",
    "- `--jars`:\n",
    "    -    Path to the bundled jar file containing the application code and all required dependencies.\n",
    "- `<python script>.py`:\n",
    "    -    The Python file to run\n",
    "\n",
    "For a detailed description of all `spark-submit` arguments, run the following command in an open terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark-submit detailed options description\n",
    "bash /bin/spark-submit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all goes well, you should see the application running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Open a Kafka producer\n",
    "\n",
    "Next, we need a Kafka producer to provide data to Spark.  To create one, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka producer for the topic pythontokafka\n",
    "# Run this from your kafka folder \n",
    "bash kafka-console-producer.sh --bootstrap-server localhost:9092 --topic pythontokafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can enter data in the terminal, which will then be picked up by Spark, and the message value will be displayed in real-time.\n",
    "\n",
    "To test this, type the following data in the Kafka producer window:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Customer ID`\n",
    "- `Product ID`\n",
    "- `Timestamp`\n",
    "- `Address`\n",
    "- `Price`\n",
    "- `Phone Number`\n",
    "- `Email`\n",
    "- `Gender`\n",
    "- `First Name`\n",
    "- `Last Name`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how fast you type, in the Spark Streaming application window, you'll see output similar to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Time: 2016-01-06 14:18:00`\n",
    "\n",
    "- `Customer ID`\n",
    "- `Product ID`\n",
    "- `Timestamp`\n",
    "- `...`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the hands-on tutorial and the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "- Kafka and Spark are designed to integrate smoothly together without major configuration or coding effort.\n",
    "- Kafka can connect to flat files, Twitter feeds and other types of data.\n",
    "- Kafka can be used to produce a constant stream of data that Spark Streaming can ingest and manipulate.\n",
    "- Kafka and Spark Streaming together can handle both batch and real-time data\n",
    "- It's possible to implement different transformations on the data in real-time such as Count and Sort.\n",
    "- To run a more complex application, we should use `spark-submit` and pass it a file containing our PySpark code and run this command from the terminal.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6971ff02672853a145ab8a619e17e4c2b989e1ba4684228133b86b474ce57f92"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
