{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Overview and Architecture\n",
    "\n",
    "## What is Apache Kafka?\n",
    "\n",
    "Apache Kafka is a relatively new open-source technology for distributed data storage optimized for __ingesting__ and __processing__ streaming data in real-time. \n",
    "\n",
    "Streaming data is data that is continuously generated by potentially numerous data sources.  An example of this are Internet of Thing (IoT) devices such as sensors and smartphones. Such devices are usually numerous and typically send the data records simultaneously. Accordingly, in order to properly capture and process this constant influx of data, a streaming tool is needed.\n",
    "\n",
    "Kafka provides three main functions to its users:\n",
    "\n",
    "1.\tPublish and subscribe to streams of records\n",
    "2.\tEffectively store streams of records in the order in which records were generated\n",
    "3.\tProcess streams of records in real-time\n",
    "\n",
    "Kafka is primarily used to build real-time streaming data pipelines and applications that adapt to the data streams. It combines __messaging__, __storage__, and __stream processing__ to allow storage and analysis of both historical and real-time data.  \n",
    "\n",
    "\n",
    "## Why is it important?\n",
    "\n",
    "Apache Kafka is one of the fastest growing, open-source messaging solutions in the market today. This is mainly due to the architectural design pattern that provides a superior logging mechanism for distributed systems.\n",
    "\n",
    "Kafka's importance has skyrocketed in recent years due to the widespread proliferation of Big Data, particularly real-time streaming data.\n",
    "\n",
    "Being purpose-built for real-time log streaming, Apache Kafka is ideally suited for applications that need:\n",
    "\n",
    "- Reliable data exchange between disparate components\n",
    "- The ability to partition messaging workloads as application requirements change\n",
    "- Real-time streaming for data processing\n",
    "- Native support for data/message replay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Industry Use-Cases Kafka\n",
    "\n",
    "Due to Kafka’s powerful features, a vast number of well-known Fortune 500 companies leverage the tool in their technology stack. Kafka has become the corporation tool of choice for essentially any type of situation that involves real-time data streaming  and real-time data processing at scale.\n",
    "\n",
    "Netflix, Spotify, Uber, Lyft, Linkedin, Twitter, Goldman Sachs, New York Times, Emirates Airlines, Paypal, Intel, Tesla, Walmart, Airbnb, Adidas and Barclays are just some of the popular companies that use Kafka every day.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/Kafka_System.png\" width=900>\n",
    "</p>\n",
    "\n",
    "For a detailed list of organisations powered by Kafka, take a look at the following link:\n",
    "\n",
    "- [Global Companies Powered by Kafka](https://kafka.apache.org/powered-by)\n",
    "\n",
    "The top Industries that use Kafka include the following:\n",
    "\n",
    "1.\tFinancial Services (Banks, Fintechs…)\n",
    "2.\tTechnology and Social Media\n",
    "3.\tAutomotive\n",
    "4.\tTelecommunications\n",
    "5.\tRetail\n",
    "6.\tHealthcare\n",
    "7.\tGaming\n",
    "8.\tAirlines\n",
    "\n",
    "Some of the most popular Use Cases for Kafka include:\n",
    "\n",
    "- Fraud Detection\n",
    "- Customer Satisfaction\n",
    "- Risk Modelling\n",
    "- Predictive Analytics\n",
    "- Trading\n",
    "- Machine Failure Prediction\n",
    "- Dynamic Energy Management\n",
    "- Recommendation Engines\n",
    "- Data Streaming\n",
    "\n",
    "\n",
    "For a detailed explanation of the top Kafka Use Cases by industry, please watch the following 40 minute video (you will need to sign up and provide an email address):\n",
    "\n",
    "- [Top Kafka Use Cases in Industry](https://videos.confluent.io/watch/5AA8GugNNDgdSs8acTHQFB?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Architecture and Concepts\n",
    "\n",
    "Kafka is a distributed system consisting of __servers__ and __clients__ that communicate via a high-performance TCP network protocol. It can be deployed on bare-metal hardware, virtual machines, and containers in on-premise as well as cloud environments.\n",
    "\n",
    "Below is a high-level overview of a typical Kafka architecture:\n",
    "<p align=\"center\">\n",
    " <img src=\"images/Kafka_Overview.png\" width=500>\n",
    "</p>\n",
    "\n",
    "\n",
    "__Servers__: Kafka is operated as a cluster of one or more servers that can span multiple datacenters or cloud regions. Some of these servers form the storage layer, called the __Brokers__. Other servers run Kafka Connect to continuously import and export data as event streams to integrate Kafka with your existing systems such as Relational Databases as well as other Kafka clusters. \n",
    "\n",
    "To let you implement mission-critical use cases, a Kafka cluster is highly scalable and fault-tolerant: if any of its servers fails, the other servers will take over their work to ensure continuous operations without any data loss.\n",
    "\n",
    "__Clients__: They allow you to write distributed applications and microservices that read, write, and process streams of events in parallel, at scale, and in a fault-tolerant manner even in the case of network problems or machine failures. Kafka comes with such clients included, which are augmented by dozens of clients provided by the Kafka open-source community: clients are available for Java and Scala including the higher-level Kafka Streams library, for Go, Python, C/C++, and many other programming languages as well as REST APIs.\n",
    "\n",
    "The important components in Kafka are:\n",
    "- __Producers__ - which write (produce) the data\n",
    "- __Consumers__ - which read (consume) the data\n",
    "- __Topics__ - which is used as a storage mechanism for data records\n",
    "\n",
    "More details can be found in the below video and guide\n",
    "\n",
    "[Kafka Introduction Video](https://kafka.apache.org/intro) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Zookeeper and why is it important?\n",
    "\n",
    "Zookeeper is a top-level, 3rd party software maintained by the Apache Foundation.  It's not part of the Kafka ecosystem directly, but  rather acts as a centralised service and is used to maintain naming and configuration data and to provide flexible and robust synchronization within distributed systems. Zookeeper keeps track of status of the Kafka cluster nodes and it also keeps track of Kafka Topics, Partitions etc.\n",
    "\n",
    "Zookeeper itself allows multiple clients to perform simultaneous reads and writes and acts as a shared configuration service within the system. The Zookeeper Atomic Broadcast (ZAB) protocol is the brains of the entire system, making it possible for Zookeeper to act as an Atomic Broadcast system and issue orderly updates.\n",
    "\n",
    "Zookeeper is mainly responsible for the following tasks in a Kafka system:\n",
    "\n",
    "### Controller election\n",
    "The controller is one of the most important Broking entities in a Kafka ecosystem, and it also has the responsibility to maintain the leader-follower relationship across all the nodes. If a node for some reason is shutting down, it’s the controller’s responsibility to tell all the replicas to act as leader nodes in order to fulfill the duties of the leaders on the node that is about to fail. So, whenever a node shuts down, a new controller can be elected and it can also be made sure that at any given time, there is only one controller and all the follower nodes have agreed on that. The reason this is done is that there always has to be one node that is the leader.  The leader node is required to manage the cluster in a Master/Worker style architecture.\n",
    "\n",
    "### Configuration Of Topics\n",
    "The configuration regarding all the topics including the list of existing topics, the number of partitions for each topic, the location of all the replicas, list of configuration overrides for all topics and which node is the preferred leader, etc.\n",
    "\n",
    "### Access Control Lists\n",
    "Access Control Lists (or ACLs) for all the topics are also maintained within Zookeeper. An ACL is essentially a table which defines the access privilages that each user in a system has (for instance, who has read-only access).\n",
    "\n",
    "### Membership of the cluster\n",
    "Zookeeper also maintains a list of all the Brokers that are functioning at any given moment and are a part of the cluster. \n",
    "\n",
    "Zookeeper is also required in order to properly run Apache Kafka especially in a real-world production system.\n",
    "\n",
    "For details regarding Zookeeper, please visit the following link:\n",
    "\n",
    "- [What is Zookeeper](https://dattell.com/data-architecture-blog/what-is-zookeeper-how-does-it-support-kafka/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Apache Kafka for Linux\n",
    "\n",
    "Now that we've covered the basics on what Apache Kafka is, let's get hands-on.\n",
    "\n",
    "You'll need:\n",
    "- A Linux environment setup with a suitable distribution (such as Ubuntu).  \n",
    "- Java 8 or higher installed and properly set up\n",
    "\n",
    "There are two different ways to download Kafka:\n",
    "\n",
    "1. Install Kafka via a Package Manager\n",
    "2. Download the Kafka tarball directly and extract it to your local machine\n",
    "\n",
    "Run the following terminal command below to install Kafka from the tarball directly using Apache Kafka version 2.12:\n",
    "\n",
    "_Note: Please ensure you are downloading the Kafka __Binary__ not the Source tar_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "wget https://dlcdn.apache.org/kafka/3.0.0/kafka_2.13-3.0.0.tgz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the command completes successfully, you should see output similar to this:\n",
    "\n",
    "![](images/kafka-download2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to unpack the tarball to be able to access the Kafka files.  Use the following command to do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tar -xzf kafka_2.13-3.0.0.tgz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/kafka-untar2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go into the new folder and check the contents.  As I'm sure you know, we can run the following commands to do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd kafka_2.13-3.0.0\n",
    "\n",
    "ls -al\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look similar to:\n",
    "\n",
    "![](images/kafka-folder3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, you've successfully downloaded Kafka and unzipped the files locally into their respective folders!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll take a deeper dive into the Kafka topology and components, and we'll also start using Kafka hands-on."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6971ff02672853a145ab8a619e17e4c2b989e1ba4684228133b86b474ce57f92"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
