{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "import os\n",
    "from PIL import Image\n",
    "from utils.client import Content, Projects, Project\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "# Here since image is too large thinks it's a decompression DOS attack\n",
    "Image.MAX_IMAGE_PIXELS = 933120000\n",
    "\n",
    "\n",
    "def get_name(id: str, modules: list) -> str:\n",
    "    for module in modules:\n",
    "        if module[\"id\"] == id:\n",
    "            return module[\"name\"]\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self) -> None:\n",
    "        self.content = Content()\n",
    "\n",
    "    def generate_full_content_graph(self):\n",
    "        g = self.get_graph_from_lesson_subset(self.content.lessons)\n",
    "        self._show_and_save_and_archive(g)\n",
    "\n",
    "    def get_graph_from_lesson_subset(self, lessons, color_mode=\"default\"):\n",
    "        graph = nx.DiGraph()\n",
    "        # TODO solve issue: this needs to always build the graph including all lessons so that all recursive prerequisites show\n",
    "        for lesson in lessons:\n",
    "            # see other attrs here https://graphviz.gitlab.io/doc/info/attrs.html\n",
    "            attrs = {\"fillcolor\": self._get_node_color(\n",
    "                lesson, mode=color_mode)}\n",
    "            graph.add_node(lesson.name, **attrs)\n",
    "            if len(lesson.prerequisites) == 0:\n",
    "                print(f\"{lesson.path} has no prerequisites\")\n",
    "                # graph.add_node(lesson)\n",
    "            for prerequisite in lesson.prerequisites:\n",
    "                try:\n",
    "                    prerequisite = self.content.get_lesson_from_id(\n",
    "                        prerequisite)\n",
    "                except Exception as e:\n",
    "                    print('Error with', prerequisite, 'in', lesson.path)\n",
    "                    raise Exception(e)\n",
    "\n",
    "                graph.add_edge(prerequisite, lesson.name)\n",
    "        return graph\n",
    "\n",
    "    def _get_node_color(self, lesson, mode=\"default\"):\n",
    "\n",
    "        green = \"#71D271\"\n",
    "        purple = '#BE2ED6'\n",
    "        blue = '#3C68DB'\n",
    "        amber = '#FBBB63'\n",
    "        red = '#E05151'\n",
    "\n",
    "        if mode == \"default\":\n",
    "            if lesson.video:\n",
    "                return green\n",
    "            # elif lesson.recorded and lesson.notebook:\n",
    "            #     return '#F2672C'}  # dark orange\n",
    "            elif lesson.needs_uploading:\n",
    "                return purple\n",
    "            elif lesson.recorded:\n",
    "                return blue\n",
    "            elif lesson.notebook:\n",
    "                return amber\n",
    "            else:\n",
    "                return red\n",
    "        elif mode == \"practicals_exist\":\n",
    "            if lesson.practicals:\n",
    "                return green\n",
    "            else:\n",
    "                return red\n",
    "\n",
    "    def show_and_save(self, G, fp=\"workflow_graph/images/tree.png\"):\n",
    "        path, obj = nx.nx_agraph.view_pygraphviz(G)\n",
    "\n",
    "        save_dir = os.path.join(*fp.split('/')[:-1])\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        img = Image.open(path)\n",
    "        img.save(fp)\n",
    "        return img\n",
    "\n",
    "    def _show_and_save_and_archive(self, *args):\n",
    "        img = self.show_and_save(*args)\n",
    "        img.save(f\"workflow_graph/images/evolution/{datetime.now()}.png\")\n",
    "\n",
    "    # def save(self):\n",
    "\n",
    "    def generate_project_content_graphs(self):\n",
    "        # TODO remove old projects causing issues\n",
    "\n",
    "        projects = self.set_all_projects_graph_attr()\n",
    "        for project in projects:\n",
    "            self.show_and_save(\n",
    "                project.graph, fp=f\"workflow_graph/images/project-dependencies/{project.name}.png\")\n",
    "            print()\n",
    "        # input(\"hit enter to continue\")\n",
    "\n",
    "    def set_all_projects_graph_attr(self, graph_mode=\"default\"):\n",
    "        projects = Projects()\n",
    "        for project in projects:\n",
    "            # TODO put in project client\n",
    "            print(project.name)\n",
    "            print(project.path)\n",
    "            project_prereqs = []\n",
    "            for task in project.tasks:\n",
    "                prerequisites_for_this_task = [self.content.get_lesson_from_id(\n",
    "                    p) for p in task.prerequisites]\n",
    "                recursive_tasks = []\n",
    "                for prerequisite in prerequisites_for_this_task:\n",
    "                    message = f\"Creating prerequisites for {prerequisite.name}\"\n",
    "                    # print(f\"{message:*^100}\")\n",
    "                    recursive_tasks.extend(\n",
    "                        self.content.get_recursive_prerequisites(prerequisite))\n",
    "                    # We need to add the prerequisite itself to the list of recursive tasks\n",
    "                    recursive_tasks.append(prerequisite)\n",
    "                    # print(recursive_tasks)\n",
    "                recursive_tasks = self.__keep_only_first(recursive_tasks)\n",
    "                # task_graph = self.get_graph_from_lesson_subset(recursive_tasks)\n",
    "\n",
    "                print(\n",
    "                    f\"\\nPrerequisite for task {task.id}: {recursive_tasks}\\n\")\n",
    "                project_prereqs.extend(recursive_tasks)\n",
    "            project_prereqs = self.__keep_only_first(project_prereqs)\n",
    "            graph = self.get_graph_from_lesson_subset(\n",
    "                project_prereqs, color_mode=graph_mode)\n",
    "            project.graph = graph\n",
    "        return projects\n",
    "\n",
    "    def save_all_project_task_recursive_prerequisites(self):\n",
    "        projects = Projects()\n",
    "        for project in projects:\n",
    "            self.save_project_task_recursive_prerequisites(project)\n",
    "\n",
    "    def save_project_task_recursive_prerequisites(self, project):\n",
    "        print(project.name)\n",
    "        print(project.path)\n",
    "\n",
    "        task_to_recursive_prereqs = {}\n",
    "\n",
    "        for task in project.tasks:\n",
    "            prerequisites_for_this_task = [self.content.get_lesson_from_id(\n",
    "                p) for p in task.prerequisites]\n",
    "            recursive_tasks = []\n",
    "            for prerequisite in prerequisites_for_this_task:\n",
    "                print(f\"Creating prerequisites for {prerequisite.name}\")\n",
    "                recursive_tasks.extend(\n",
    "                    self.content.get_recursive_prerequisites(prerequisite))\n",
    "                # We need to add the prerequisite itself to the list of recursive tasks\n",
    "                recursive_tasks.append(prerequisite)\n",
    "                # print(recursive_tasks)\n",
    "            recursive_tasks = self.__keep_only_first(recursive_tasks)\n",
    "\n",
    "            if recursive_tasks:\n",
    "                task_graph = self.get_graph_from_lesson_subset(\n",
    "                    recursive_tasks)\n",
    "                task_graph_save_dir = os.path.join(\n",
    "                    project.path.replace(\"/specification.yaml\", \"\"), \"status\", \"task-prereqs\")\n",
    "                if not os.path.exists(task_graph_save_dir):\n",
    "                    os.makedirs(task_graph_save_dir, exist_ok=True)\n",
    "                task_graph_save_path = os.path.join(\n",
    "                    task_graph_save_dir,\n",
    "                    f\"M{task.milestone_idx}T{task.idx}.png\"\n",
    "                )\n",
    "                self.show_and_save(\n",
    "                    task_graph,\n",
    "                    task_graph_save_path\n",
    "                )\n",
    "\n",
    "            # print(\n",
    "            #     f\"\\nPrerequisite for task {task.id}: {recursive_tasks}\\n\")\n",
    "            recursive_prereqs_for_this_task = [\n",
    "                l.id for l in recursive_tasks]\n",
    "            task_to_recursive_prereqs[task.id] = recursive_prereqs_for_this_task\n",
    "\n",
    "        # print(task_to_recursive_prereqs)\n",
    "        prereqs_file = os.path.join(\n",
    "            *project.path.split('/')[:-1], \"task-to-recursive-prereqs.yaml\")\n",
    "        with open(prereqs_file, 'w') as f:\n",
    "            yaml.dump(task_to_recursive_prereqs, f)\n",
    "        print()\n",
    "\n",
    "    def generate_project_practicals_exist(self):\n",
    "        projects = self.set_project_graph_attr(graph_mode=\"practicals_exist\")\n",
    "        for project in projects:\n",
    "            self.show_and_save(\n",
    "                project.graph, fp=f\"Projects/scenarios/{project.name}/status/practicals.png\")\n",
    "            print()\n",
    "\n",
    "    @staticmethod\n",
    "    def __keep_only_first(prereq_list: list) -> list:\n",
    "        '''\n",
    "        Iterate through the list of prerequisites and keep only the first instance of each prerequisite.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prereq_list : list\n",
    "            List of prerequisites\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of prerequisites with only the first instance of each prerequisite\n",
    "        '''\n",
    "        already_seen = []\n",
    "        for task in prereq_list:\n",
    "            if task not in already_seen:\n",
    "                already_seen.append(task)\n",
    "        return already_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivanyingxuan/miniconda3/envs/content-projects_new/lib/python3.10/site-packages/nbformat/__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n",
      "/home/ivanyingxuan/miniconda3/envs/content-projects_new/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '308f9b23' detected. Corrected to '61c01d1b'.\n",
      "  validate(nb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content/units/Machine-Learning/2. Introduction to ML/0. What is machine learning? has no prerequisites\n",
      "Content/units/Data-Analytics/1. Tableau/1. What is Data Analytics? has no prerequisites\n",
      "Content/units/MLOps/0. Intro to MLOps/0. What is MLOps? has no prerequisites\n",
      "Content/units/Data-Handling/3. SQL/0. What is SQL? has no prerequisites\n",
      "Content/units/Cloud-and-DevOps/2. Docker/0. What is Docker? has no prerequisites\n",
      "Content/units/Cloud-and-DevOps/0. Intro to Cloud/0. What is the Cloud? has no prerequisites\n",
      "Content/units/Essentials/2. The Command Line/0. What is the command line has no prerequisites\n",
      "Content/units/Essentials/5. Common File Types for Working with Data/0. Markdown has no prerequisites\n",
      "Content/units/Essentials/4. Common Data Types /0. Tabular Data has no prerequisites\n",
      "Content/units/Essentials/4. Common Data Types /2. Text Data has no prerequisites\n",
      "Content/units/Essentials/4. Common Data Types /1. Image Data has no prerequisites\n",
      "Content/units/Essentials/4. Common Data Types /7. Audio Data has no prerequisites\n",
      "Content/units/Essentials/3. VSCode/0. A tour of VSCode has no prerequisites\n",
      "Content/units/Essentials/0. Setup/0. Installation Notebook has no prerequisites\n",
      "Content/units/Essentials/6. The Python Programming Environment/0. What is Python? has no prerequisites\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    tree = Tree()\n",
    "    graph = tree.get_graph_from_lesson_subset(tree.content.lessons)\n",
    "    # # tree.generate_project_content_graphs()\n",
    "    # project = Project(\n",
    "    #     \"Projects/scenarios/Movie-Recommendation\")\n",
    "    # tree.save_project_task_recursive_prerequisites(project)\n",
    "    # tree.save_all_project_task_recursive_prerequisites()\n",
    "    # tree.generate_project_practicals_exist()\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tree.content.lessons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView(('Integrating Kafka & Spark', Kafka Essentials, Spark Essentials, 'Data Storage', Data Pipelines, 'Data Transformation - ETL & ELT', Batch Processing and Streaming, 'Data Ingestion', 'Batch Processing and Streaming', Data Ingestion, 'Data Pipelines', The Data Engineering Lifecycle, 'Enterprise Data Warehouses', Data Storage, 'What is Data Engineering?', Summary of Common File Types, Summary of Common Data Types, pyscopg2 and SQLAlchemy, 'The Data Engineering Landscape', Data Transformation - ETL & ELT, Enterprise Data Warehouses, 'The Data Engineering Lifecycle', What is Data Engineering?, 'What is Kafka?', The Data Engineering Landscape, 'Streaming in Kafka', 'Kafka Essentials', What is Kafka?, 'Kafka-Python', 'Columnar NoSQL Storage', What is NoSQL?, 'What is NoSQL?', 'Key-Value NoSQL Storage', 'Document-Oriented NoSQL Storage', 'Graph-Oriented NoSQL Storage', 'What is Apache Hadoop?', 'Installing Apache Hadoop', What is Apache Hadoop?, 'What is Apache Spark?', 'Integrating Pyspark & Cassandra', How to Integrate Spark with other tools using Spark Connectors, 'Spark Essentials', What is Apache Spark?, 'How to Integrate Spark with other tools using Spark Connectors', 'Pyspark Essentials', 'Integrating PySpark & AWS S3', 'Spark Streaming', 'Loading Data into Apache HBase', Installing Apache HBase, 'What is Apache HBase?', 'Installing Apache HBase', What is Apache HBase?, 'What is Presto?', 'Installing Presto', What is Presto?, 'What is Apache Cassandra?', 'Installing Apache Cassandra', What is Apache Cassandra?, 'Loading Data into Apache Cassandra', Installing Apache Cassandra, 'Installing Apache Cassandra on Mac', 'Integrating Presto & Cassandra', Installing Presto, Loading Data into Apache Cassandra, 'Tokenisation', Imports, What is natural language processing?, 'Building a Vocab in TorchText', Tokenisation, 'What is natural language processing?', Text Data, 'Essential ML Concepts Review', Data for ML, What is a model?, Validation and Testing, Bias & variance, Hyperparameters, Regularisation, 'Validation and Testing', Intro to Models - Linear regression, 'Bias & variance', 'Grid Search & K-fold Cross Validation', 'Intro to Models - Linear regression', Numpy - Reshape and Broadcasting, 'Regularisation', Gradient based optimisation, 'What is machine learning?', 'What is a model?', What is machine learning?, 'Data for ML', 'Hyperparameters', 'Gradient based optimisation', 'Evaluation Metrics - Regression', 'Maximum Likelihood Estimation', Multiclass Classification in Sklearn, 'Evaluation Metrics - Classification', 'Classification Trees', 'Regression Trees', Classification Trees, 'Support Vector Machines', 'K Nearest Neighbours', 'PCA and t-SNE', 'DBSCAN', 'K-means clustering', 'Boosting and Adaboost', Random forests and Bagging, 'Random forests and Bagging', 'XGBoost', Gradient Boosting, 'Gradient Boosting', Boosting and Adaboost, 'Softmax', What is Classification?, 'What is Classification?', 'Multiclass Classification', Softmax, 'Multiclass Classification in Sklearn', 'Cross Entropy', Multiclass Classification, 'Create Tableau Dashboards', Installing Tableau, 'What is Data Analytics?', 'Installing Tableau', What is Tableau?, 'Integrating Tableau with PostgreSQL', CRUD, 'What is Tableau?', What is Data Analytics?, 'Influential points - Leverages and Outliers', Multicollinearity, 'Multicollinearity', Data Cleaning in Pandas, 'EDA and Basic Visualisation', Data visualisation, 'Data visualisation', Pandas Dataframes, 'What is Data Versioning?', 'DVC', What is Data Versioning?, 'What is MLOps?', 'Why Documentation & Processes are important for MLOps', What is MLOps?, 'Kubeflow Setup', Intro to Kubeflow, 'Kubeflow Pipelines', Kubeflow Setup, 'Intro to Kubeflow', Kubernetes Basics, 'Intro to ML Monitoring and Observability', Essential ML Concepts Review, 'The MLFlow Model Registry', Model Deployment with MLFlow, 'MLFlow Projects', Python + CSV, JSON, YAML, Images, Audio, Video, Docker Essentials, 'Model Deployment with MLFlow', Experiment tracking with MLFlow, 'Experiment tracking with MLFlow', MLFlow Projects, 'Feature Stores with Feast', Intro to Feature Stores, 'Intro to Feature Stores', S3 and boto3, 'What is Data Cleaning?', 'Subqueries', 'SQL Setup', What is SQL?, 'Joins', 'pyscopg2 and SQLAlchemy', 'What is SQL?', 'Aggregations', 'CRUD', SQL Basics, 'SQL Basics', SQL Setup, 'Numpy - Intro', 'Numpy - Reshape and Broadcasting', Numpy - Array Operations, 'Numpy - Array Operations', Numpy - Intro, 'Data Cleaning in Pandas', Pandas - Advanced Dataframe Operations, 'Missing Data', 'What is Pandas?', Tabular Data, 'Pandas Dataframes', What is Pandas?, 'Pandas - Advanced Dataframe Operations', 'Python + CSV, JSON, YAML, Images, Audio, Video', Python files and Python Notebooks, 'GitHub Actions', What is Github?, 'Kubernetes Networking', Kubernetes Workloads, 'Kubernetes Basics', 'Kubernetes Workloads', 'Kubernetes Storage & StatefulSets', Kubernetes Networking, 'What is Docker?', 'Docker Volumes', 'Docker Essentials', What is Docker?, The VSCode integrated terminal, Defining Functions, 'Docker Compose', 'EC2 Instances', What is AWS?, 'What is AWS?', What is the Cloud?, 'S3 and boto3', 'AWS RDS', 'Intro to Airflow', 'Airflow and Postgresql', Intro to Airflow, 'AWS API GateWay', AWS Lambda, 'AWS Cloudwatch', EC2 Instances, 'AWS Cloudformation', AWS API GateWay, 'AWS Lambda', 'What is Prometheus?', 'Prometheus Essentials', What is Prometheus?, 'Grafana', Prometheus Essentials, 'APIs and Requests - Overview', 'Getting Started with FastAPI', APIs and Requests - Overview, 'Sending Data to FastAPI', Why FastAPI is Great, 'Why FastAPI is Great', Getting Started with FastAPI, 'What is the Cloud?', 'AWS AI Services Overview', 'Running terminal commands', What is the command line, 'Editing files with nano', Running terminal commands, 'Running Python files from the terminal', Editing files with nano, 'A deeper understanding of the terminal', File Manipulation, 'Package managers', 'What is the command line', 'Command Line Basics Recap', The Essentials of Git, 'The in-Portal Terminal', 'File Manipulation', 'Python files and Python Notebooks', What is Python?, Markdown, 'JSON', 'YAML', JSON, 'Markdown', 'HTML', 'Summary of Common File Types', YAML, CSV Data, 'CSV Data', 'Loops', Control Flow, 'Lists', Numbers, Strings, Booleans, 'Imports', conda, 'The __main__.py File Runs When you Run a Folder', 'Comments', How (and how not) to run Python files from VSCode, 'Getting User Input', Variables, 'Essentials of Error Handling in Python', 'What is `if __name__ == \"__main__\"`?', 'Functions and function calls', Printing things in Python, 'Magic Methods', Object Oriented Programming, 'Regular Expressions', 'Debugging', Essentials of Error Handling in Python, 'Docstrings', 'Decorators', 'Generators', Lists, 'Class Decorators', Decorators, 'Tuples', 'Printing things in Python', 'Booleans', 'Lambda Functions', 'Strings', 'Assertions', 'Control Flow', 'Object Oriented Programming', 'Variables', 'Sets', 'Operators', Dictionaries, 'Context Managers', Functions and function calls, 'Defining Functions', 'Numbers', 'Variable attributes', 'Dictionaries', 'What is `self` in Python?', 'Typing', 'Structured vs Unstructured Data', Image Data, 'Tabular Data', 'Text Data', 'Image Data', 'Base 64 Encoded Images', Everything on a Computer is Stored in Bytes!, 'Summary of Common Data Types', Structured vs Unstructured Data, 'Audio Data', 'Everything on a Computer is Stored in Bytes!', 'Git Clone - Getting Code from a GitHub Repo', 'Putting a Local Git Repo on GitHub Correctly', Git Clone - Getting Code from a GitHub Repo, 'The Essentials of Git', 'Pull Requests', 'What is Github?', 'A tour of VSCode', 'The VSCode integrated terminal', A tour of VSCode, 'Installation Notebook', 'Google Colab', 'Exactly cloning a conda environment', 'How (and how not) to run Python files from VSCode', Running Python files from the terminal, 'pip', Package managers, 'What is Python?', 'conda', pip, 'Principles of OOP - Encapsulation', 'Principles of OOP - Inheritance', 'Checks for code quality', Principles of OOP - Polymorphism, Variable naming clarity, Principles of OOP - Abstraction, Principles of OOP - Inheritance, Principles of OOP - Encapsulation, 'Variable naming clarity', Code Consistency, 'Principles of OOP - Polymorphism', 'Code Consistency', 'Principles of OOP - Abstraction', 'What is Transfer Learning?', Building our first neural network in PyTorch, 'Transfer Learning in PyTorch', Exploring Pre-trained Models on PyTorch Hub, PyTorch Hotdog vs Dog Dataset, What is Transfer Learning?, 'What is deep learning?', 'Flat Training Curve & Constant Predictions Issue', Optimisation in PyTorch - Linear regression example, 'Making a Pytorch City Image Classification Dataset', PyTorch Transforms, 'Custom PyTorch Datasets', PyTorch Datasets, 'PyTorch Transforms', Custom PyTorch Datasets, 'PyTorch Data Loaders', 'PyTorch Hotdog vs Dog Dataset', 'Creating a PyTorch Dataset from the Classic Diabetes Dataset', 'PyTorch Datasets', PyTorch Tensors, 'Navigating the PyTorch docs', 'Autograd', Backpropagation, 'What is PyTorch?', What is deep learning?, 'PyTorch Tensors', What is PyTorch?, 'Loading a Pre-trained Lane Detection Model from PyTorch Hub', 'Exploring Pre-trained Models on PyTorch Hub', 'Why you need activation functions', Neural network notation, 'Common activation functions', Why you need activation functions, 'Neural network notation', 'Backpropagation', Common activation functions, 'Tensorboard', Training Loops in PyTorch - Linear regression example, 'Saving and Loading PyTorch Models', Building Convolutional Networks in PyTorch, 'AlexNet', What are Convolutional Neural Networks?, 'What are Convolutional Neural Networks?', 'Building Convolutional Networks in PyTorch', The Convolution Operation, 'The Convolution Operation', 'Building ML models in PyTorch - Linear regression example', PyTorch Data Loaders, 'Building our first neural network in PyTorch', Multiclass classification in PyTorch, 'Logistic regression in PyTorch', 'Optimisation in PyTorch - Linear regression example', Tensorboard, 'Training Loops in PyTorch - Linear regression example', Building ML models in PyTorch - Linear regression example, Autograd, 'Multiclass classification in PyTorch', Logistic regression in PyTorch, 'Project Structure', 'Principles of OOP Design', 'Scraping Google Jobs', Web Scraping with Selenium, 'Web Scraping - Using Selenium', 'Web Scraping - Advanced Selenium', Web Scraping - Using Selenium, 'Lyric Scraper with Categories', Lyric Scraper, 'Web Scraping- HTML and BeautifulSoup', 'Lyric Scraper', 'Basic Scraper Code Review', Downloading Images from URLs, 'Web Scraping with Selenium', Web Scraping- HTML and BeautifulSoup, 'Web Scraping Demo - Data Cleaning', Basic Scraper Code Review, 'Google Images Dataset Collection Demo', 'Downloading Images from URLs', 'Big O Notation', Generators, 'Unit Testing', What is testing?, 'What is testing?', Class Decorators))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-projects_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b961f8166aad6ccb4cf65d0f9c742ef9c6c23ffe83ad932438cd83ed96aebaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
